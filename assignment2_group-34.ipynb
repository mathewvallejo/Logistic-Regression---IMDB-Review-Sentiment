{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP 551 Assignment 2\n",
        "\n",
        "## Classification of Textual Data from IMDB Dataset"
      ],
      "metadata": {
        "id": "UXaR4lywVkzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "TukMmI8lzIit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1: Load the IMDB dataset into training and testing datasets and preprocess"
      ],
      "metadata": {
        "id": "IJzYqpzchMpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we setup a new function for the IMDB dataset that will allow us to download the dataset and organize the reviews by their index, the general sentiment (positive or negative), and the rating given.  Following this we organize the reviews into positive and negative groups within larger groups of training and testing data.  This reflects the way the data is organized from the download itself with 50% of the data in the train folder, and 50% of the data in the test folder."
      ],
      "metadata": {
        "id": "ZGY4_V-jZbfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to download and extract the IMDB dataset\n",
        "def download_and_extract_imdb_dataset(url: str, download_dir: str):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Extracting the tar.gz file\n",
        "        with tarfile.open(fileobj=BytesIO(response.content), mode=\"r:gz\") as tar:\n",
        "            tar.extractall(download_dir)\n",
        "        print(\"Dataset downloaded and extracted successfully.\")\n",
        "    else:\n",
        "        print(\"Failed to download the dataset.\")\n",
        "\n",
        "# URL to download the dataset\n",
        "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "# Directory to download the dataset\n",
        "download_dir = \"./\"\n",
        "\n",
        "# Download and extract the dataset\n",
        "download_and_extract_imdb_dataset(url, download_dir)\n",
        "\n",
        "# Now, you can proceed with your existing code to load the dataset\n",
        "# Your existing code to load and process the dataset goes here...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBn43sYu5uMO",
        "outputId": "38749f58-b9fe-49ec-e350-f6099a867e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded and extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Function to load IMDB reviews\n",
        "def load_imdb_reviews(aclImdb_dir: str, sentiment: str) -> list:\n",
        "    \"\"\"Load reviews from the aclImdb folder\"\"\"\n",
        "    instances = []\n",
        "    for filename in os.listdir(os.path.join(aclImdb_dir, sentiment)):\n",
        "        with open(os.path.join(aclImdb_dir, sentiment, filename), 'r') as file:\n",
        "            review = file.read()\n",
        "            instances.append((review, sentiment))\n",
        "    return instances\n",
        "\n",
        "# Function to extract features from reviews\n",
        "def extract_features(reviews, filtered_words):\n",
        "    features = []\n",
        "    for review, _ in reviews:\n",
        "        words = review.split()\n",
        "        feature_vector = [1 if word in words else 0 for word in filtered_words]\n",
        "        features.append(feature_vector)\n",
        "    return features\n",
        "\n",
        "# Load training instances\n",
        "train_pos_reviews = load_imdb_reviews('./aclImdb/train', 'pos')\n",
        "train_neg_reviews = load_imdb_reviews('./aclImdb/train', 'neg')\n",
        "\n",
        "# Load test instances\n",
        "test_pos_reviews = load_imdb_reviews('./aclImdb/test', 'pos')\n",
        "test_neg_reviews = load_imdb_reviews('./aclImdb/test', 'neg')\n",
        "\n",
        "# Count word occurrences in training data\n",
        "word_counts = Counter()\n",
        "for review, _ in train_pos_reviews + train_neg_reviews + test_pos_reviews + test_neg_reviews:\n",
        "    words = review.split()\n",
        "    word_counts.update(words)\n",
        "\n",
        "# Filter out rare and stopwords\n",
        "min_df = len(train_pos_reviews + train_neg_reviews + test_pos_reviews + test_neg_reviews) * 0.01\n",
        "max_df = len(train_pos_reviews + train_neg_reviews + test_pos_reviews + test_neg_reviews) * 0.5\n",
        "filtered_words = [word for word, count in word_counts.items() if min_df < count < max_df]\n",
        "\n",
        "# Extract features from training data\n",
        "X_train_pos = extract_features(train_pos_reviews, filtered_words)\n",
        "X_train_neg = extract_features(train_neg_reviews, filtered_words)\n",
        "X_train = np.array(X_train_pos + X_train_neg)\n",
        "y_train = np.array([1] * len(train_pos_reviews) + [0] * len(train_neg_reviews))\n",
        "\n",
        "# Extract features from test data\n",
        "X_test_pos = extract_features(test_pos_reviews, filtered_words)\n",
        "X_test_neg = extract_features(test_neg_reviews, filtered_words)\n",
        "X_test = np.array(X_test_pos + X_test_neg)\n",
        "y_test = np.array([1] * len(test_pos_reviews) + [0] * len(test_neg_reviews))\n",
        "\n"
      ],
      "metadata": {
        "id": "RHbtw7g_KPBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction with Simple Linear Regression\n",
        "\n",
        "Below we have our Linear Regression function designed to extract features with the greatest contribution to both positive and negative sentiment scores.  We fit the training data to the model, make predictions on the test set, and calculate the Mean Squared Error (MSE).  We then use the coefficient weights to determine the top 100 features for each class and use a sorting function to report the results.  The listed terms contributing to positive/negative sentiments largely make intuitive sense such as \"great\" and \"excellent\" for the positive sentiment, with terms like \"waste\" and \"worst\" weighing highly for negative sentiment.  "
      ],
      "metadata": {
        "id": "MLuqtc4oLsVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LinearRegression:\n",
        "\n",
        "    def __init__(self, learning_rate: float=1e-2, n_iters: int=2000) -> None:\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.w, self.bias =  None, None\n",
        "\n",
        "    def fit(self, X: np.array, y: np.array) -> None:\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # gradient descent optimization\n",
        "        for _ in range(self.n_iters):\n",
        "            y_hat = np.dot(X, self.w) + self.bias\n",
        "            dj_dw = (1/n_samples) * np.dot(X.T, (y_hat - y))\n",
        "            dj_db = (1/n_samples) * (y_hat - y).sum()\n",
        "\n",
        "            self.w = self.w - self.lr*dj_dw\n",
        "            self.bias = self.bias - self.lr*dj_db\n",
        "\n",
        "    def predict(self, X: np.array) -> np.array:\n",
        "        return np.dot(X, self.w) + self.bias\n",
        "\n",
        "    def weights(self) -> np.array:\n",
        "        return self.w"
      ],
      "metadata": {
        "id": "2SlP2JB2MF73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = np.mean((y_pred - y_test)**2)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Get important input features (coefficients)\n",
        "coefficients = model.weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWXKsmZfQajk",
        "outputId": "e735528e-62b9-4b41-810b-21292da30a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.13266577363117874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 10 +/- Features From Linear Regression\n",
        "Here you can find a graph representing the top 10 positive features and top 10 negative features by their coefficient from the linear regression."
      ],
      "metadata": {
        "id": "rZdjoUfXkdfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def sort_top_features_by_coefficients(feature_names, coefficients, D):\n",
        "    # Calculate absolute coefficients\n",
        "    abs_coefficients = np.abs(coefficients)\n",
        "\n",
        "    # Get indices of top D features based on absolute coefficients\n",
        "    sorted_indices = np.argsort(abs_coefficients)[::-1][:D]\n",
        "\n",
        "    # Adjust indices to account for bias if it exists\n",
        "    if len(feature_names) == len(coefficients) - 1:\n",
        "        sorted_indices = [i - 1 for i in sorted_indices if i != len(coefficients) - 1]\n",
        "\n",
        "    # Extract top D features and their corresponding coefficients\n",
        "    top_features_with_coefficients = [(feature_names[i], coefficients[i]) for i in sorted_indices]\n",
        "\n",
        "    # Separate positive and negative coefficients\n",
        "    positive_features = [(feature, coef) for feature, coef in top_features_with_coefficients if coef > 0]\n",
        "    negative_features = [(feature, coef) for feature, coef in top_features_with_coefficients if coef < 0]\n",
        "\n",
        "    # Sort the top positive and negative features by coefficient values\n",
        "    sorted_top_positive_features = sorted(positive_features, key=lambda x: np.abs(x[1]), reverse=True)\n",
        "    sorted_top_negative_features = sorted(negative_features, key=lambda x: np.abs(x[1]), reverse=True)\n",
        "\n",
        "    # Convert to DataFrame for better readability\n",
        "    df_positive_features = pd.DataFrame(sorted_top_positive_features, columns=['Feature', 'Coefficient'])\n",
        "    df_negative_features = pd.DataFrame(sorted_top_negative_features, columns=['Feature', 'Coefficient'])\n",
        "\n",
        "    return df_positive_features, df_negative_features\n",
        "\n",
        "# Choose D = 100\n",
        "D = 100\n",
        "\n",
        "# Call the function to sort the top D features by coefficients for positive and negative reviews\n",
        "df_positive_features, df_negative_features = sort_top_features_by_coefficients(filtered_words, coefficients, D)\n",
        "\n",
        "# Print the top positive features\n",
        "print(\"Top 100 features contributing to a positive review:\")\n",
        "print(df_positive_features[:10])\n",
        "\n",
        "# Print the top negative features\n",
        "print(\"\\nTop 100 features contributing to a negative review:\")\n",
        "print(df_negative_features[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onD1waGSUJ7b",
        "outputId": "23f9c0b2-801a-487e-a102-115eab40bf02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 100 features contributing to a positive review:\n",
            "     Feature  Coefficient\n",
            "0      great     0.104445\n",
            "1  excellent     0.099427\n",
            "2       best     0.093569\n",
            "3    perfect     0.072433\n",
            "4      loved     0.071704\n",
            "5  wonderful     0.071465\n",
            "6    enjoyed     0.069760\n",
            "7   favorite     0.066854\n",
            "8       well     0.064684\n",
            "9       love     0.063270\n",
            "\n",
            "Top 100 features contributing to a negative review:\n",
            "    Feature  Coefficient\n",
            "0     worst    -0.159869\n",
            "1     waste    -0.115960\n",
            "2       bad    -0.109012\n",
            "3      poor    -0.079165\n",
            "4   nothing    -0.076872\n",
            "5     awful    -0.075401\n",
            "6    boring    -0.073486\n",
            "7  supposed    -0.069068\n",
            "8    poorly    -0.066930\n",
            "9      bad.    -0.061575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot the top positive and negative features on the same plot\n",
        "def plot_combined_top_features(df_positive, df_negative, title):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot positive features\n",
        "    plt.barh(df_positive['Feature'], df_positive['Coefficient'], color='skyblue', label='Positive')\n",
        "\n",
        "    # Plot negative features\n",
        "    plt.barh(df_negative['Feature'], df_negative['Coefficient'], color='salmon', label='Negative')\n",
        "\n",
        "    plt.xlabel('Coefficient')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest coefficient on top\n",
        "    plt.show()\n",
        "\n",
        "# Plot the top positive and negative features on the same plot\n",
        "plot_combined_top_features(df_positive_features[:10], df_negative_features[:10], \"Top 10 Features Contributing to Sentiment (Linear Regression)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "UmP2skjGkAQR",
        "outputId": "a126d903-3e77-461a-b135-b2ba9128a4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Logistic Regression With Gradient Descent\n",
        "\n",
        "Here we implement our defined Logistic Regression model with a sigmoid Logistic Function, Cross Entropy as our cost function, and a gradient descent function.  The implementation we conducted was limited to a maximum of 10,000 iterations (1e4 as opposed to the to the default 1e5) to offset long processing times, and defaults to a learning rate of 0.1."
      ],
      "metadata": {
        "id": "aRcPzqMqy1rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic function\n",
        "logistic = lambda z: 1. / (1 + np.exp(-z))\n",
        "\n",
        "# Cost function\n",
        "def cost_fn(x, y, w):\n",
        "    N, D = x.shape\n",
        "    z = np.dot(x, w)\n",
        "    J = np.mean(y * np.log1p(np.exp(-z)) + (1 - y) * np.log1p(np.exp(z)))\n",
        "    return J\n",
        "\n",
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.max_iters = int(max_iters)  # Convert max_iters to integer\n",
        "        self.verbose = verbose\n",
        "        self.cost_history_train = []  # List to store the cost function value for training set\n",
        "        self.cost_history_val = []  # List to store the cost function value for validation set\n",
        "        self.w = None # Initialize weights to be set later\n",
        "    def gradient(self, x, y):\n",
        "        N, D = x.shape\n",
        "        yh = logistic(np.dot(x, self.w))  # Predictions\n",
        "        grad = np.dot(x.T, yh - y) / N    # Compute gradient\n",
        "        return grad\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val=None, y_val=None):\n",
        "        if x_train.ndim == 1:\n",
        "            x_train = x_train[:, None]\n",
        "        if self.add_bias:\n",
        "            N_train = x_train.shape[0]\n",
        "            x_train = np.column_stack([x_train, np.ones(N_train)])  # Add bias term\n",
        "        N_train, D = x_train.shape\n",
        "\n",
        "        if self.w is None:\n",
        "          self.w = np.zeros(D)  # Initialize weights\n",
        "        print('x_train shape after bias term:', x_train.shape)\n",
        "        print('self.w shape:', self.w.shape)\n",
        "\n",
        "        if x_val is not None:\n",
        "            if x_val.ndim == 1:\n",
        "                x_val = x_val[:, None]\n",
        "            if self.add_bias:\n",
        "                N_val = x_val.shape[0]\n",
        "                x_val = np.column_stack([x_val, np.ones(N_val)])  # Add bias term to validation data\n",
        "\n",
        "        self.w = np.zeros(D)  # Initialize weights\n",
        "        g = np.inf\n",
        "        t = 0\n",
        "        # Gradient descent loop\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g = self.gradient(x_train, y_train)  # Compute gradient\n",
        "            self.w = self.w - self.learning_rate * g  # Update weights\n",
        "            cost_train = cost_fn(x_train, y_train, self.w)  # Calculate cost for training set\n",
        "            self.cost_history_train.append(cost_train)  # Append cost to history for training set\n",
        "            if x_val is not None and y_val is not None:\n",
        "                cost_val = cost_fn(x_val, y_val, self.w)  # Calculate cost for validation set\n",
        "                self.cost_history_val.append(cost_val)  # Append cost to history for validation set\n",
        "            if self.verbose:\n",
        "                print(f'Iteration {t}, Norm of Gradient: {np.linalg.norm(g)}, Cost (Train): {cost_train}')\n",
        "                if x_val is not None and y_val is not None:\n",
        "                    print(f'Iteration {t}, Cost (Validation): {cost_val}')\n",
        "            t += 1\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f'Terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'The weight found: {self.w}')\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x, np.ones(Nt)])  # Add bias term\n",
        "        yh = logistic(np.dot(x, self.w))  # Predict output\n",
        "        return yh\n"
      ],
      "metadata": {
        "id": "qBvoVJwsWp-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split X_train and y_train into training and validation sets\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "print('X_train_split shape:', X_train_split.shape)\n",
        "print('X_val_split shape:', X_val_split.shape)\n",
        "\n",
        "# Initialize logistic regression model\n",
        "logistic_regression = LogisticRegression(add_bias=True, learning_rate=0.1, epsilon=1e-4, max_iters=10000, verbose=True)\n",
        "\n",
        "# Fit the model on training data with validation data\n",
        "logistic_regression.fit(X_train_split, y_train_split, X_val_split, y_val_split)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = logistic_regression.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5G1-aBRoC_W",
        "outputId": "9eff158c-d0c7-409a-ce46-b9b635008e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 7501, Cost (Validation): 0.34700271244221415\n",
            "Iteration 7502, Norm of Gradient: 0.0056776283142384995, Cost (Train): 0.28865495779051875\n",
            "Iteration 7502, Cost (Validation): 0.34700332777844634\n",
            "Iteration 7503, Norm of Gradient: 0.005677021654448901, Cost (Train): 0.2886517351052007\n",
            "Iteration 7503, Cost (Validation): 0.34700394342779706\n",
            "Iteration 7504, Norm of Gradient: 0.005676415121850418, Cost (Train): 0.28864851310845113\n",
            "Iteration 7504, Cost (Validation): 0.3470045593901189\n",
            "Iteration 7505, Norm of Gradient: 0.005675808716400019, Cost (Train): 0.28864529180005205\n",
            "Iteration 7505, Cost (Validation): 0.3470051756652646\n",
            "Iteration 7506, Norm of Gradient: 0.005675202438054689, Cost (Train): 0.2886420711797859\n",
            "Iteration 7506, Cost (Validation): 0.3470057922530868\n",
            "Iteration 7507, Norm of Gradient: 0.005674596286771427, Cost (Train): 0.2886388512474348\n",
            "Iteration 7507, Cost (Validation): 0.3470064091534383\n",
            "Iteration 7508, Norm of Gradient: 0.005673990262507261, Cost (Train): 0.28863563200278114\n",
            "Iteration 7508, Cost (Validation): 0.3470070263661719\n",
            "Iteration 7509, Norm of Gradient: 0.005673384365219237, Cost (Train): 0.2886324134456075\n",
            "Iteration 7509, Cost (Validation): 0.3470076438911407\n",
            "Iteration 7510, Norm of Gradient: 0.00567277859486442, Cost (Train): 0.28862919557569644\n",
            "Iteration 7510, Cost (Validation): 0.3470082617281976\n",
            "Iteration 7511, Norm of Gradient: 0.005672172951399897, Cost (Train): 0.2886259783928306\n",
            "Iteration 7511, Cost (Validation): 0.3470088798771957\n",
            "Iteration 7512, Norm of Gradient: 0.005671567434782773, Cost (Train): 0.2886227618967928\n",
            "Iteration 7512, Cost (Validation): 0.3470094983379882\n",
            "Iteration 7513, Norm of Gradient: 0.005670962044970176, Cost (Train): 0.288619546087366\n",
            "Iteration 7513, Cost (Validation): 0.3470101171104283\n",
            "Iteration 7514, Norm of Gradient: 0.005670356781919255, Cost (Train): 0.288616330964333\n",
            "Iteration 7514, Cost (Validation): 0.34701073619436934\n",
            "Iteration 7515, Norm of Gradient: 0.005669751645587172, Cost (Train): 0.288613116527477\n",
            "Iteration 7515, Cost (Validation): 0.3470113555896647\n",
            "Iteration 7516, Norm of Gradient: 0.005669146635931118, Cost (Train): 0.28860990277658105\n",
            "Iteration 7516, Cost (Validation): 0.34701197529616784\n",
            "Iteration 7517, Norm of Gradient: 0.005668541752908302, Cost (Train): 0.28860668971142844\n",
            "Iteration 7517, Cost (Validation): 0.3470125953137322\n",
            "Iteration 7518, Norm of Gradient: 0.005667936996475952, Cost (Train): 0.2886034773318025\n",
            "Iteration 7518, Cost (Validation): 0.3470132156422115\n",
            "Iteration 7519, Norm of Gradient: 0.005667332366591315, Cost (Train): 0.28860026563748675\n",
            "Iteration 7519, Cost (Validation): 0.3470138362814593\n",
            "Iteration 7520, Norm of Gradient: 0.005666727863211661, Cost (Train): 0.28859705462826457\n",
            "Iteration 7520, Cost (Validation): 0.3470144572313294\n",
            "Iteration 7521, Norm of Gradient: 0.0056661234862942795, Cost (Train): 0.2885938443039198\n",
            "Iteration 7521, Cost (Validation): 0.34701507849167573\n",
            "Iteration 7522, Norm of Gradient: 0.005665519235796478, Cost (Train): 0.28859063466423607\n",
            "Iteration 7522, Cost (Validation): 0.347015700062352\n",
            "Iteration 7523, Norm of Gradient: 0.005664915111675588, Cost (Train): 0.28858742570899704\n",
            "Iteration 7523, Cost (Validation): 0.3470163219432123\n",
            "Iteration 7524, Norm of Gradient: 0.005664311113888959, Cost (Train): 0.28858421743798685\n",
            "Iteration 7524, Cost (Validation): 0.3470169441341106\n",
            "Iteration 7525, Norm of Gradient: 0.0056637072423939595, Cost (Train): 0.28858100985098933\n",
            "Iteration 7525, Cost (Validation): 0.3470175666349011\n",
            "Iteration 7526, Norm of Gradient: 0.00566310349714798, Cost (Train): 0.2885778029477888\n",
            "Iteration 7526, Cost (Validation): 0.34701818944543783\n",
            "Iteration 7527, Norm of Gradient: 0.005662499878108433, Cost (Train): 0.2885745967281692\n",
            "Iteration 7527, Cost (Validation): 0.34701881256557515\n",
            "Iteration 7528, Norm of Gradient: 0.005661896385232746, Cost (Train): 0.288571391191915\n",
            "Iteration 7528, Cost (Validation): 0.3470194359951673\n",
            "Iteration 7529, Norm of Gradient: 0.005661293018478372, Cost (Train): 0.28856818633881043\n",
            "Iteration 7529, Cost (Validation): 0.3470200597340689\n",
            "Iteration 7530, Norm of Gradient: 0.0056606897778027805, Cost (Train): 0.2885649821686401\n",
            "Iteration 7530, Cost (Validation): 0.34702068378213424\n",
            "Iteration 7531, Norm of Gradient: 0.0056600866631634624, Cost (Train): 0.2885617786811886\n",
            "Iteration 7531, Cost (Validation): 0.34702130813921783\n",
            "Iteration 7532, Norm of Gradient: 0.005659483674517928, Cost (Train): 0.28855857587624034\n",
            "Iteration 7532, Cost (Validation): 0.3470219328051744\n",
            "Iteration 7533, Norm of Gradient: 0.0056588808118237105, Cost (Train): 0.28855537375358037\n",
            "Iteration 7533, Cost (Validation): 0.34702255777985863\n",
            "Iteration 7534, Norm of Gradient: 0.005658278075038358, Cost (Train): 0.2885521723129934\n",
            "Iteration 7534, Cost (Validation): 0.34702318306312524\n",
            "Iteration 7535, Norm of Gradient: 0.005657675464119444, Cost (Train): 0.2885489715542644\n",
            "Iteration 7535, Cost (Validation): 0.347023808654829\n",
            "Iteration 7536, Norm of Gradient: 0.0056570729790245584, Cost (Train): 0.2885457714771784\n",
            "Iteration 7536, Cost (Validation): 0.34702443455482496\n",
            "Iteration 7537, Norm of Gradient: 0.005656470619711315, Cost (Train): 0.2885425720815205\n",
            "Iteration 7537, Cost (Validation): 0.3470250607629681\n",
            "Iteration 7538, Norm of Gradient: 0.005655868386137341, Cost (Train): 0.28853937336707597\n",
            "Iteration 7538, Cost (Validation): 0.3470256872791133\n",
            "Iteration 7539, Norm of Gradient: 0.005655266278260293, Cost (Train): 0.28853617533363013\n",
            "Iteration 7539, Cost (Validation): 0.34702631410311585\n",
            "Iteration 7540, Norm of Gradient: 0.005654664296037836, Cost (Train): 0.28853297798096833\n",
            "Iteration 7540, Cost (Validation): 0.3470269412348309\n",
            "Iteration 7541, Norm of Gradient: 0.0056540624394276675, Cost (Train): 0.2885297813088762\n",
            "Iteration 7541, Cost (Validation): 0.3470275686741136\n",
            "Iteration 7542, Norm of Gradient: 0.0056534607083874935, Cost (Train): 0.28852658531713915\n",
            "Iteration 7542, Cost (Validation): 0.3470281964208195\n",
            "Iteration 7543, Norm of Gradient: 0.00565285910287505, Cost (Train): 0.28852339000554295\n",
            "Iteration 7543, Cost (Validation): 0.3470288244748038\n",
            "Iteration 7544, Norm of Gradient: 0.0056522576228480855, Cost (Train): 0.2885201953738734\n",
            "Iteration 7544, Cost (Validation): 0.3470294528359222\n",
            "Iteration 7545, Norm of Gradient: 0.005651656268264372, Cost (Train): 0.2885170014219165\n",
            "Iteration 7545, Cost (Validation): 0.34703008150403003\n",
            "Iteration 7546, Norm of Gradient: 0.005651055039081701, Cost (Train): 0.288513808149458\n",
            "Iteration 7546, Cost (Validation): 0.3470307104789831\n",
            "Iteration 7547, Norm of Gradient: 0.005650453935257881, Cost (Train): 0.2885106155562841\n",
            "Iteration 7547, Cost (Validation): 0.34703133976063705\n",
            "Iteration 7548, Norm of Gradient: 0.0056498529567507475, Cost (Train): 0.2885074236421808\n",
            "Iteration 7548, Cost (Validation): 0.3470319693488476\n",
            "Iteration 7549, Norm of Gradient: 0.005649252103518149, Cost (Train): 0.28850423240693457\n",
            "Iteration 7549, Cost (Validation): 0.3470325992434707\n",
            "Iteration 7550, Norm of Gradient: 0.0056486513755179555, Cost (Train): 0.2885010418503316\n",
            "Iteration 7550, Cost (Validation): 0.34703322944436216\n",
            "Iteration 7551, Norm of Gradient: 0.00564805077270806, Cost (Train): 0.2884978519721583\n",
            "Iteration 7551, Cost (Validation): 0.347033859951378\n",
            "Iteration 7552, Norm of Gradient: 0.005647450295046369, Cost (Train): 0.28849466277220126\n",
            "Iteration 7552, Cost (Validation): 0.3470344907643742\n",
            "Iteration 7553, Norm of Gradient: 0.005646849942490816, Cost (Train): 0.2884914742502472\n",
            "Iteration 7553, Cost (Validation): 0.347035121883207\n",
            "Iteration 7554, Norm of Gradient: 0.005646249714999352, Cost (Train): 0.2884882864060826\n",
            "Iteration 7554, Cost (Validation): 0.34703575330773256\n",
            "Iteration 7555, Norm of Gradient: 0.005645649612529943, Cost (Train): 0.2884850992394944\n",
            "Iteration 7555, Cost (Validation): 0.3470363850378072\n",
            "Iteration 7556, Norm of Gradient: 0.005645049635040584, Cost (Train): 0.28848191275026946\n",
            "Iteration 7556, Cost (Validation): 0.3470370170732872\n",
            "Iteration 7557, Norm of Gradient: 0.005644449782489282, Cost (Train): 0.2884787269381949\n",
            "Iteration 7557, Cost (Validation): 0.34703764941402904\n",
            "Iteration 7558, Norm of Gradient: 0.005643850054834066, Cost (Train): 0.2884755418030576\n",
            "Iteration 7558, Cost (Validation): 0.3470382820598891\n",
            "Iteration 7559, Norm of Gradient: 0.005643250452032985, Cost (Train): 0.28847235734464477\n",
            "Iteration 7559, Cost (Validation): 0.347038915010724\n",
            "Iteration 7560, Norm of Gradient: 0.005642650974044109, Cost (Train): 0.2884691735627438\n",
            "Iteration 7560, Cost (Validation): 0.34703954826639033\n",
            "Iteration 7561, Norm of Gradient: 0.0056420516208255285, Cost (Train): 0.2884659904571419\n",
            "Iteration 7561, Cost (Validation): 0.34704018182674484\n",
            "Iteration 7562, Norm of Gradient: 0.005641452392335347, Cost (Train): 0.2884628080276265\n",
            "Iteration 7562, Cost (Validation): 0.3470408156916443\n",
            "Iteration 7563, Norm of Gradient: 0.005640853288531697, Cost (Train): 0.2884596262739854\n",
            "Iteration 7563, Cost (Validation): 0.3470414498609455\n",
            "Iteration 7564, Norm of Gradient: 0.005640254309372726, Cost (Train): 0.28845644519600594\n",
            "Iteration 7564, Cost (Validation): 0.3470420843345054\n",
            "Iteration 7565, Norm of Gradient: 0.0056396554548166, Cost (Train): 0.28845326479347594\n",
            "Iteration 7565, Cost (Validation): 0.347042719112181\n",
            "Iteration 7566, Norm of Gradient: 0.0056390567248215066, Cost (Train): 0.28845008506618336\n",
            "Iteration 7566, Cost (Validation): 0.34704335419382926\n",
            "Iteration 7567, Norm of Gradient: 0.005638458119345655, Cost (Train): 0.28844690601391587\n",
            "Iteration 7567, Cost (Validation): 0.3470439895793073\n",
            "Iteration 7568, Norm of Gradient: 0.005637859638347268, Cost (Train): 0.2884437276364617\n",
            "Iteration 7568, Cost (Validation): 0.34704462526847246\n",
            "Iteration 7569, Norm of Gradient: 0.005637261281784596, Cost (Train): 0.2884405499336087\n",
            "Iteration 7569, Cost (Validation): 0.3470452612611819\n",
            "Iteration 7570, Norm of Gradient: 0.005636663049615903, Cost (Train): 0.28843737290514526\n",
            "Iteration 7570, Cost (Validation): 0.34704589755729304\n",
            "Iteration 7571, Norm of Gradient: 0.005636064941799472, Cost (Train): 0.2884341965508595\n",
            "Iteration 7571, Cost (Validation): 0.34704653415666314\n",
            "Iteration 7572, Norm of Gradient: 0.005635466958293615, Cost (Train): 0.2884310208705399\n",
            "Iteration 7572, Cost (Validation): 0.3470471710591498\n",
            "Iteration 7573, Norm of Gradient: 0.00563486909905665, Cost (Train): 0.2884278458639749\n",
            "Iteration 7573, Cost (Validation): 0.34704780826461057\n",
            "Iteration 7574, Norm of Gradient: 0.005634271364046926, Cost (Train): 0.28842467153095297\n",
            "Iteration 7574, Cost (Validation): 0.34704844577290295\n",
            "Iteration 7575, Norm of Gradient: 0.005633673753222805, Cost (Train): 0.28842149787126287\n",
            "Iteration 7575, Cost (Validation): 0.34704908358388475\n",
            "Iteration 7576, Norm of Gradient: 0.005633076266542671, Cost (Train): 0.28841832488469327\n",
            "Iteration 7576, Cost (Validation): 0.34704972169741366\n",
            "Iteration 7577, Norm of Gradient: 0.005632478903964927, Cost (Train): 0.28841515257103295\n",
            "Iteration 7577, Cost (Validation): 0.3470503601133476\n",
            "Iteration 7578, Norm of Gradient: 0.005631881665447995, Cost (Train): 0.28841198093007087\n",
            "Iteration 7578, Cost (Validation): 0.3470509988315444\n",
            "Iteration 7579, Norm of Gradient: 0.005631284550950319, Cost (Train): 0.28840880996159607\n",
            "Iteration 7579, Cost (Validation): 0.34705163785186205\n",
            "Iteration 7580, Norm of Gradient: 0.00563068756043036, Cost (Train): 0.2884056396653976\n",
            "Iteration 7580, Cost (Validation): 0.34705227717415854\n",
            "Iteration 7581, Norm of Gradient: 0.005630090693846599, Cost (Train): 0.2884024700412646\n",
            "Iteration 7581, Cost (Validation): 0.3470529167982921\n",
            "Iteration 7582, Norm of Gradient: 0.005629493951157537, Cost (Train): 0.28839930108898654\n",
            "Iteration 7582, Cost (Validation): 0.3470535567241208\n",
            "Iteration 7583, Norm of Gradient: 0.005628897332321694, Cost (Train): 0.28839613280835263\n",
            "Iteration 7583, Cost (Validation): 0.34705419695150297\n",
            "Iteration 7584, Norm of Gradient: 0.005628300837297612, Cost (Train): 0.2883929651991523\n",
            "Iteration 7584, Cost (Validation): 0.3470548374802968\n",
            "Iteration 7585, Norm of Gradient: 0.005627704466043847, Cost (Train): 0.2883897982611752\n",
            "Iteration 7585, Cost (Validation): 0.34705547831036093\n",
            "Iteration 7586, Norm of Gradient: 0.00562710821851898, Cost (Train): 0.288386631994211\n",
            "Iteration 7586, Cost (Validation): 0.3470561194415535\n",
            "Iteration 7587, Norm of Gradient: 0.0056265120946816065, Cost (Train): 0.2883834663980493\n",
            "Iteration 7587, Cost (Validation): 0.3470567608737334\n",
            "Iteration 7588, Norm of Gradient: 0.005625916094490348, Cost (Train): 0.28838030147248006\n",
            "Iteration 7588, Cost (Validation): 0.347057402606759\n",
            "Iteration 7589, Norm of Gradient: 0.005625320217903838, Cost (Train): 0.28837713721729313\n",
            "Iteration 7589, Cost (Validation): 0.34705804464048895\n",
            "Iteration 7590, Norm of Gradient: 0.005624724464880736, Cost (Train): 0.2883739736322785\n",
            "Iteration 7590, Cost (Validation): 0.3470586869747821\n",
            "Iteration 7591, Norm of Gradient: 0.005624128835379715, Cost (Train): 0.28837081071722626\n",
            "Iteration 7591, Cost (Validation): 0.34705932960949726\n",
            "Iteration 7592, Norm of Gradient: 0.00562353332935947, Cost (Train): 0.2883676484719267\n",
            "Iteration 7592, Cost (Validation): 0.3470599725444933\n",
            "Iteration 7593, Norm of Gradient: 0.005622937946778719, Cost (Train): 0.28836448689617\n",
            "Iteration 7593, Cost (Validation): 0.3470606157796291\n",
            "Iteration 7594, Norm of Gradient: 0.005622342687596193, Cost (Train): 0.2883613259897466\n",
            "Iteration 7594, Cost (Validation): 0.3470612593147637\n",
            "Iteration 7595, Norm of Gradient: 0.005621747551770647, Cost (Train): 0.28835816575244694\n",
            "Iteration 7595, Cost (Validation): 0.34706190314975627\n",
            "Iteration 7596, Norm of Gradient: 0.005621152539260853, Cost (Train): 0.2883550061840614\n",
            "Iteration 7596, Cost (Validation): 0.347062547284466\n",
            "Iteration 7597, Norm of Gradient: 0.005620557650025602, Cost (Train): 0.28835184728438096\n",
            "Iteration 7597, Cost (Validation): 0.347063191718752\n",
            "Iteration 7598, Norm of Gradient: 0.005619962884023705, Cost (Train): 0.28834868905319605\n",
            "Iteration 7598, Cost (Validation): 0.34706383645247363\n",
            "Iteration 7599, Norm of Gradient: 0.0056193682412139945, Cost (Train): 0.2883455314902976\n",
            "Iteration 7599, Cost (Validation): 0.3470644814854902\n",
            "Iteration 7600, Norm of Gradient: 0.0056187737215553195, Cost (Train): 0.2883423745954765\n",
            "Iteration 7600, Cost (Validation): 0.3470651268176613\n",
            "Iteration 7601, Norm of Gradient: 0.005618179325006551, Cost (Train): 0.2883392183685239\n",
            "Iteration 7601, Cost (Validation): 0.3470657724488462\n",
            "Iteration 7602, Norm of Gradient: 0.0056175850515265725, Cost (Train): 0.2883360628092306\n",
            "Iteration 7602, Cost (Validation): 0.34706641837890473\n",
            "Iteration 7603, Norm of Gradient: 0.005616990901074296, Cost (Train): 0.288332907917388\n",
            "Iteration 7603, Cost (Validation): 0.3470670646076964\n",
            "Iteration 7604, Norm of Gradient: 0.005616396873608647, Cost (Train): 0.2883297536927874\n",
            "Iteration 7604, Cost (Validation): 0.34706771113508095\n",
            "Iteration 7605, Norm of Gradient: 0.005615802969088573, Cost (Train): 0.28832660013522\n",
            "Iteration 7605, Cost (Validation): 0.3470683579609182\n",
            "Iteration 7606, Norm of Gradient: 0.005615209187473038, Cost (Train): 0.28832344724447734\n",
            "Iteration 7606, Cost (Validation): 0.34706900508506794\n",
            "Iteration 7607, Norm of Gradient: 0.005614615528721027, Cost (Train): 0.28832029502035095\n",
            "Iteration 7607, Cost (Validation): 0.34706965250739014\n",
            "Iteration 7608, Norm of Gradient: 0.005614021992791543, Cost (Train): 0.28831714346263243\n",
            "Iteration 7608, Cost (Validation): 0.34707030022774477\n",
            "Iteration 7609, Norm of Gradient: 0.005613428579643608, Cost (Train): 0.28831399257111356\n",
            "Iteration 7609, Cost (Validation): 0.3470709482459919\n",
            "Iteration 7610, Norm of Gradient: 0.005612835289236268, Cost (Train): 0.2883108423455862\n",
            "Iteration 7610, Cost (Validation): 0.3470715965619917\n",
            "Iteration 7611, Norm of Gradient: 0.00561224212152858, Cost (Train): 0.288307692785842\n",
            "Iteration 7611, Cost (Validation): 0.3470722451756044\n",
            "Iteration 7612, Norm of Gradient: 0.005611649076479628, Cost (Train): 0.2883045438916733\n",
            "Iteration 7612, Cost (Validation): 0.3470728940866902\n",
            "Iteration 7613, Norm of Gradient: 0.005611056154048509, Cost (Train): 0.28830139566287183\n",
            "Iteration 7613, Cost (Validation): 0.34707354329510937\n",
            "Iteration 7614, Norm of Gradient: 0.005610463354194344, Cost (Train): 0.28829824809923005\n",
            "Iteration 7614, Cost (Validation): 0.3470741928007224\n",
            "Iteration 7615, Norm of Gradient: 0.005609870676876268, Cost (Train): 0.2882951012005401\n",
            "Iteration 7615, Cost (Validation): 0.34707484260338983\n",
            "Iteration 7616, Norm of Gradient: 0.00560927812205344, Cost (Train): 0.2882919549665942\n",
            "Iteration 7616, Cost (Validation): 0.3470754927029721\n",
            "Iteration 7617, Norm of Gradient: 0.005608685689685036, Cost (Train): 0.28828880939718504\n",
            "Iteration 7617, Cost (Validation): 0.34707614309933\n",
            "Iteration 7618, Norm of Gradient: 0.005608093379730252, Cost (Train): 0.2882856644921049\n",
            "Iteration 7618, Cost (Validation): 0.3470767937923239\n",
            "Iteration 7619, Norm of Gradient: 0.0056075011921483, Cost (Train): 0.2882825202511466\n",
            "Iteration 7619, Cost (Validation): 0.34707744478181485\n",
            "Iteration 7620, Norm of Gradient: 0.005606909126898415, Cost (Train): 0.2882793766741027\n",
            "Iteration 7620, Cost (Validation): 0.34707809606766354\n",
            "Iteration 7621, Norm of Gradient: 0.005606317183939847, Cost (Train): 0.28827623376076605\n",
            "Iteration 7621, Cost (Validation): 0.34707874764973085\n",
            "Iteration 7622, Norm of Gradient: 0.005605725363231871, Cost (Train): 0.2882730915109296\n",
            "Iteration 7622, Cost (Validation): 0.34707939952787775\n",
            "Iteration 7623, Norm of Gradient: 0.005605133664733772, Cost (Train): 0.28826994992438615\n",
            "Iteration 7623, Cost (Validation): 0.3470800517019653\n",
            "Iteration 7624, Norm of Gradient: 0.005604542088404866, Cost (Train): 0.2882668090009289\n",
            "Iteration 7624, Cost (Validation): 0.34708070417185466\n",
            "Iteration 7625, Norm of Gradient: 0.005603950634204477, Cost (Train): 0.288263668740351\n",
            "Iteration 7625, Cost (Validation): 0.34708135693740677\n",
            "Iteration 7626, Norm of Gradient: 0.005603359302091953, Cost (Train): 0.28826052914244554\n",
            "Iteration 7626, Cost (Validation): 0.3470820099984831\n",
            "Iteration 7627, Norm of Gradient: 0.005602768092026661, Cost (Train): 0.2882573902070061\n",
            "Iteration 7627, Cost (Validation): 0.3470826633549448\n",
            "Iteration 7628, Norm of Gradient: 0.005602177003967988, Cost (Train): 0.2882542519338258\n",
            "Iteration 7628, Cost (Validation): 0.3470833170066533\n",
            "Iteration 7629, Norm of Gradient: 0.005601586037875334, Cost (Train): 0.28825111432269834\n",
            "Iteration 7629, Cost (Validation): 0.34708397095347004\n",
            "Iteration 7630, Norm of Gradient: 0.005600995193708125, Cost (Train): 0.28824797737341734\n",
            "Iteration 7630, Cost (Validation): 0.3470846251952565\n",
            "Iteration 7631, Norm of Gradient: 0.0056004044714258044, Cost (Train): 0.28824484108577636\n",
            "Iteration 7631, Cost (Validation): 0.34708527973187425\n",
            "Iteration 7632, Norm of Gradient: 0.005599813870987832, Cost (Train): 0.2882417054595691\n",
            "Iteration 7632, Cost (Validation): 0.34708593456318493\n",
            "Iteration 7633, Norm of Gradient: 0.005599223392353686, Cost (Train): 0.28823857049458973\n",
            "Iteration 7633, Cost (Validation): 0.3470865896890504\n",
            "Iteration 7634, Norm of Gradient: 0.005598633035482868, Cost (Train): 0.28823543619063197\n",
            "Iteration 7634, Cost (Validation): 0.34708724510933214\n",
            "Iteration 7635, Norm of Gradient: 0.005598042800334893, Cost (Train): 0.2882323025474898\n",
            "Iteration 7635, Cost (Validation): 0.34708790082389235\n",
            "Iteration 7636, Norm of Gradient: 0.005597452686869302, Cost (Train): 0.28822916956495753\n",
            "Iteration 7636, Cost (Validation): 0.3470885568325926\n",
            "Iteration 7637, Norm of Gradient: 0.005596862695045646, Cost (Train): 0.2882260372428292\n",
            "Iteration 7637, Cost (Validation): 0.34708921313529506\n",
            "Iteration 7638, Norm of Gradient: 0.005596272824823501, Cost (Train): 0.28822290558089925\n",
            "Iteration 7638, Cost (Validation): 0.34708986973186173\n",
            "Iteration 7639, Norm of Gradient: 0.005595683076162463, Cost (Train): 0.28821977457896203\n",
            "Iteration 7639, Cost (Validation): 0.34709052662215484\n",
            "Iteration 7640, Norm of Gradient: 0.0055950934490221396, Cost (Train): 0.288216644236812\n",
            "Iteration 7640, Cost (Validation): 0.34709118380603643\n",
            "Iteration 7641, Norm of Gradient: 0.005594503943362164, Cost (Train): 0.2882135145542436\n",
            "Iteration 7641, Cost (Validation): 0.34709184128336873\n",
            "Iteration 7642, Norm of Gradient: 0.005593914559142185, Cost (Train): 0.28821038553105177\n",
            "Iteration 7642, Cost (Validation): 0.3470924990540143\n",
            "Iteration 7643, Norm of Gradient: 0.005593325296321873, Cost (Train): 0.28820725716703094\n",
            "Iteration 7643, Cost (Validation): 0.34709315711783517\n",
            "Iteration 7644, Norm of Gradient: 0.005592736154860913, Cost (Train): 0.2882041294619761\n",
            "Iteration 7644, Cost (Validation): 0.3470938154746941\n",
            "Iteration 7645, Norm of Gradient: 0.005592147134719013, Cost (Train): 0.28820100241568225\n",
            "Iteration 7645, Cost (Validation): 0.34709447412445354\n",
            "Iteration 7646, Norm of Gradient: 0.0055915582358558955, Cost (Train): 0.2881978760279443\n",
            "Iteration 7646, Cost (Validation): 0.347095133066976\n",
            "Iteration 7647, Norm of Gradient: 0.005590969458231307, Cost (Train): 0.2881947502985573\n",
            "Iteration 7647, Cost (Validation): 0.34709579230212423\n",
            "Iteration 7648, Norm of Gradient: 0.005590380801805008, Cost (Train): 0.2881916252273165\n",
            "Iteration 7648, Cost (Validation): 0.3470964518297609\n",
            "Iteration 7649, Norm of Gradient: 0.0055897922665367784, Cost (Train): 0.28818850081401703\n",
            "Iteration 7649, Cost (Validation): 0.34709711164974877\n",
            "Iteration 7650, Norm of Gradient: 0.005589203852386419, Cost (Train): 0.2881853770584545\n",
            "Iteration 7650, Cost (Validation): 0.34709777176195084\n",
            "Iteration 7651, Norm of Gradient: 0.0055886155593137515, Cost (Train): 0.2881822539604242\n",
            "Iteration 7651, Cost (Validation): 0.3470984321662299\n",
            "Iteration 7652, Norm of Gradient: 0.005588027387278607, Cost (Train): 0.2881791315197216\n",
            "Iteration 7652, Cost (Validation): 0.347099092862449\n",
            "Iteration 7653, Norm of Gradient: 0.0055874393362408455, Cost (Train): 0.28817600973614244\n",
            "Iteration 7653, Cost (Validation): 0.3470997538504712\n",
            "Iteration 7654, Norm of Gradient: 0.005586851406160342, Cost (Train): 0.28817288860948237\n",
            "Iteration 7654, Cost (Validation): 0.3471004151301596\n",
            "Iteration 7655, Norm of Gradient: 0.005586263596996986, Cost (Train): 0.28816976813953726\n",
            "Iteration 7655, Cost (Validation): 0.3471010767013775\n",
            "Iteration 7656, Norm of Gradient: 0.0055856759087106935, Cost (Train): 0.28816664832610284\n",
            "Iteration 7656, Cost (Validation): 0.34710173856398807\n",
            "Iteration 7657, Norm of Gradient: 0.005585088341261391, Cost (Train): 0.2881635291689752\n",
            "Iteration 7657, Cost (Validation): 0.34710240071785464\n",
            "Iteration 7658, Norm of Gradient: 0.00558450089460903, Cost (Train): 0.2881604106679504\n",
            "Iteration 7658, Cost (Validation): 0.34710306316284073\n",
            "Iteration 7659, Norm of Gradient: 0.00558391356871358, Cost (Train): 0.28815729282282454\n",
            "Iteration 7659, Cost (Validation): 0.3471037258988096\n",
            "Iteration 7660, Norm of Gradient: 0.005583326363535022, Cost (Train): 0.28815417563339385\n",
            "Iteration 7660, Cost (Validation): 0.34710438892562506\n",
            "Iteration 7661, Norm of Gradient: 0.005582739279033366, Cost (Train): 0.28815105909945465\n",
            "Iteration 7661, Cost (Validation): 0.3471050522431504\n",
            "Iteration 7662, Norm of Gradient: 0.005582152315168633, Cost (Train): 0.28814794322080334\n",
            "Iteration 7662, Cost (Validation): 0.34710571585124955\n",
            "Iteration 7663, Norm of Gradient: 0.005581565471900866, Cost (Train): 0.28814482799723645\n",
            "Iteration 7663, Cost (Validation): 0.3471063797497861\n",
            "Iteration 7664, Norm of Gradient: 0.005580978749190125, Cost (Train): 0.2881417134285505\n",
            "Iteration 7664, Cost (Validation): 0.3471070439386239\n",
            "Iteration 7665, Norm of Gradient: 0.00558039214699649, Cost (Train): 0.28813859951454235\n",
            "Iteration 7665, Cost (Validation): 0.3471077084176268\n",
            "Iteration 7666, Norm of Gradient: 0.005579805665280057, Cost (Train): 0.28813548625500846\n",
            "Iteration 7666, Cost (Validation): 0.34710837318665877\n",
            "Iteration 7667, Norm of Gradient: 0.005579219304000944, Cost (Train): 0.28813237364974575\n",
            "Iteration 7667, Cost (Validation): 0.3471090382455837\n",
            "Iteration 7668, Norm of Gradient: 0.005578633063119287, Cost (Train): 0.2881292616985513\n",
            "Iteration 7668, Cost (Validation): 0.34710970359426585\n",
            "Iteration 7669, Norm of Gradient: 0.005578046942595236, Cost (Train): 0.288126150401222\n",
            "Iteration 7669, Cost (Validation): 0.34711036923256927\n",
            "Iteration 7670, Norm of Gradient: 0.005577460942388963, Cost (Train): 0.28812303975755504\n",
            "Iteration 7670, Cost (Validation): 0.3471110351603581\n",
            "Iteration 7671, Norm of Gradient: 0.005576875062460661, Cost (Train): 0.2881199297673474\n",
            "Iteration 7671, Cost (Validation): 0.34711170137749664\n",
            "Iteration 7672, Norm of Gradient: 0.005576289302770537, Cost (Train): 0.28811682043039666\n",
            "Iteration 7672, Cost (Validation): 0.3471123678838492\n",
            "Iteration 7673, Norm of Gradient: 0.0055757036632788195, Cost (Train): 0.2881137117464999\n",
            "Iteration 7673, Cost (Validation): 0.34711303467928034\n",
            "Iteration 7674, Norm of Gradient: 0.005575118143945752, Cost (Train): 0.28811060371545477\n",
            "Iteration 7674, Cost (Validation): 0.34711370176365436\n",
            "Iteration 7675, Norm of Gradient: 0.0055745327447316, Cost (Train): 0.2881074963370586\n",
            "Iteration 7675, Cost (Validation): 0.34711436913683585\n",
            "Iteration 7676, Norm of Gradient: 0.0055739474655966455, Cost (Train): 0.2881043896111093\n",
            "Iteration 7676, Cost (Validation): 0.34711503679868944\n",
            "Iteration 7677, Norm of Gradient: 0.00557336230650119, Cost (Train): 0.2881012835374043\n",
            "Iteration 7677, Cost (Validation): 0.34711570474907977\n",
            "Iteration 7678, Norm of Gradient: 0.005572777267405553, Cost (Train): 0.28809817811574157\n",
            "Iteration 7678, Cost (Validation): 0.3471163729878715\n",
            "Iteration 7679, Norm of Gradient: 0.005572192348270069, Cost (Train): 0.2880950733459189\n",
            "Iteration 7679, Cost (Validation): 0.34711704151492956\n",
            "Iteration 7680, Norm of Gradient: 0.005571607549055101, Cost (Train): 0.2880919692277342\n",
            "Iteration 7680, Cost (Validation): 0.3471177103301188\n",
            "Iteration 7681, Norm of Gradient: 0.005571022869721017, Cost (Train): 0.2880888657609857\n",
            "Iteration 7681, Cost (Validation): 0.34711837943330404\n",
            "Iteration 7682, Norm of Gradient: 0.005570438310228213, Cost (Train): 0.2880857629454714\n",
            "Iteration 7682, Cost (Validation): 0.34711904882435035\n",
            "Iteration 7683, Norm of Gradient: 0.005569853870537099, Cost (Train): 0.2880826607809895\n",
            "Iteration 7683, Cost (Validation): 0.34711971850312284\n",
            "Iteration 7684, Norm of Gradient: 0.005569269550608105, Cost (Train): 0.2880795592673384\n",
            "Iteration 7684, Cost (Validation): 0.3471203884694866\n",
            "Iteration 7685, Norm of Gradient: 0.005568685350401679, Cost (Train): 0.28807645840431645\n",
            "Iteration 7685, Cost (Validation): 0.34712105872330684\n",
            "Iteration 7686, Norm of Gradient: 0.005568101269878286, Cost (Train): 0.28807335819172225\n",
            "Iteration 7686, Cost (Validation): 0.34712172926444884\n",
            "Iteration 7687, Norm of Gradient: 0.005567517308998413, Cost (Train): 0.28807025862935415\n",
            "Iteration 7687, Cost (Validation): 0.34712240009277784\n",
            "Iteration 7688, Norm of Gradient: 0.00556693346772256, Cost (Train): 0.288067159717011\n",
            "Iteration 7688, Cost (Validation): 0.34712307120815933\n",
            "Iteration 7689, Norm of Gradient: 0.005566349746011251, Cost (Train): 0.2880640614544914\n",
            "Iteration 7689, Cost (Validation): 0.3471237426104588\n",
            "Iteration 7690, Norm of Gradient: 0.005565766143825024, Cost (Train): 0.2880609638415943\n",
            "Iteration 7690, Cost (Validation): 0.3471244142995417\n",
            "Iteration 7691, Norm of Gradient: 0.005565182661124434, Cost (Train): 0.2880578668781185\n",
            "Iteration 7691, Cost (Validation): 0.3471250862752736\n",
            "Iteration 7692, Norm of Gradient: 0.00556459929787006, Cost (Train): 0.288054770563863\n",
            "Iteration 7692, Cost (Validation): 0.34712575853752026\n",
            "Iteration 7693, Norm of Gradient: 0.005564016054022496, Cost (Train): 0.288051674898627\n",
            "Iteration 7693, Cost (Validation): 0.3471264310861472\n",
            "Iteration 7694, Norm of Gradient: 0.005563432929542355, Cost (Train): 0.28804857988220955\n",
            "Iteration 7694, Cost (Validation): 0.34712710392102053\n",
            "Iteration 7695, Norm of Gradient: 0.005562849924390263, Cost (Train): 0.28804548551441\n",
            "Iteration 7695, Cost (Validation): 0.34712777704200587\n",
            "Iteration 7696, Norm of Gradient: 0.005562267038526875, Cost (Train): 0.28804239179502755\n",
            "Iteration 7696, Cost (Validation): 0.34712845044896923\n",
            "Iteration 7697, Norm of Gradient: 0.005561684271912855, Cost (Train): 0.28803929872386175\n",
            "Iteration 7697, Cost (Validation): 0.34712912414177655\n",
            "Iteration 7698, Norm of Gradient: 0.005561101624508886, Cost (Train): 0.2880362063007121\n",
            "Iteration 7698, Cost (Validation): 0.3471297981202938\n",
            "Iteration 7699, Norm of Gradient: 0.0055605190962756765, Cost (Train): 0.28803311452537816\n",
            "Iteration 7699, Cost (Validation): 0.3471304723843874\n",
            "Iteration 7700, Norm of Gradient: 0.005559936687173944, Cost (Train): 0.2880300233976596\n",
            "Iteration 7700, Cost (Validation): 0.3471311469339233\n",
            "Iteration 7701, Norm of Gradient: 0.005559354397164429, Cost (Train): 0.2880269329173562\n",
            "Iteration 7701, Cost (Validation): 0.3471318217687677\n",
            "Iteration 7702, Norm of Gradient: 0.00555877222620789, Cost (Train): 0.28802384308426787\n",
            "Iteration 7702, Cost (Validation): 0.34713249688878706\n",
            "Iteration 7703, Norm of Gradient: 0.005558190174265102, Cost (Train): 0.2880207538981944\n",
            "Iteration 7703, Cost (Validation): 0.3471331722938477\n",
            "Iteration 7704, Norm of Gradient: 0.005557608241296859, Cost (Train): 0.2880176653589359\n",
            "Iteration 7704, Cost (Validation): 0.347133847983816\n",
            "Iteration 7705, Norm of Gradient: 0.005557026427263976, Cost (Train): 0.2880145774662926\n",
            "Iteration 7705, Cost (Validation): 0.3471345239585587\n",
            "Iteration 7706, Norm of Gradient: 0.005556444732127278, Cost (Train): 0.2880114902200645\n",
            "Iteration 7706, Cost (Validation): 0.3471352002179421\n",
            "Iteration 7707, Norm of Gradient: 0.005555863155847617, Cost (Train): 0.2880084036200519\n",
            "Iteration 7707, Cost (Validation): 0.34713587676183305\n",
            "Iteration 7708, Norm of Gradient: 0.005555281698385859, Cost (Train): 0.28800531766605525\n",
            "Iteration 7708, Cost (Validation): 0.34713655359009815\n",
            "Iteration 7709, Norm of Gradient: 0.005554700359702889, Cost (Train): 0.288002232357875\n",
            "Iteration 7709, Cost (Validation): 0.3471372307026041\n",
            "Iteration 7710, Norm of Gradient: 0.005554119139759609, Cost (Train): 0.2879991476953116\n",
            "Iteration 7710, Cost (Validation): 0.3471379080992179\n",
            "Iteration 7711, Norm of Gradient: 0.00555353803851694, Cost (Train): 0.28799606367816577\n",
            "Iteration 7711, Cost (Validation): 0.3471385857798064\n",
            "Iteration 7712, Norm of Gradient: 0.0055529570559358175, Cost (Train): 0.28799298030623804\n",
            "Iteration 7712, Cost (Validation): 0.3471392637442365\n",
            "Iteration 7713, Norm of Gradient: 0.005552376191977204, Cost (Train): 0.2879898975793293\n",
            "Iteration 7713, Cost (Validation): 0.3471399419923753\n",
            "Iteration 7714, Norm of Gradient: 0.0055517954466020696, Cost (Train): 0.2879868154972405\n",
            "Iteration 7714, Cost (Validation): 0.3471406205240899\n",
            "Iteration 7715, Norm of Gradient: 0.0055512148197714075, Cost (Train): 0.28798373405977246\n",
            "Iteration 7715, Cost (Validation): 0.34714129933924737\n",
            "Iteration 7716, Norm of Gradient: 0.005550634311446231, Cost (Train): 0.28798065326672634\n",
            "Iteration 7716, Cost (Validation): 0.3471419784377151\n",
            "Iteration 7717, Norm of Gradient: 0.005550053921587566, Cost (Train): 0.2879775731179032\n",
            "Iteration 7717, Cost (Validation): 0.3471426578193602\n",
            "Iteration 7718, Norm of Gradient: 0.00554947365015646, Cost (Train): 0.2879744936131041\n",
            "Iteration 7718, Cost (Validation): 0.34714333748405013\n",
            "Iteration 7719, Norm of Gradient: 0.005548893497113979, Cost (Train): 0.2879714147521306\n",
            "Iteration 7719, Cost (Validation): 0.3471440174316523\n",
            "Iteration 7720, Norm of Gradient: 0.005548313462421204, Cost (Train): 0.28796833653478393\n",
            "Iteration 7720, Cost (Validation): 0.3471446976620342\n",
            "Iteration 7721, Norm of Gradient: 0.005547733546039235, Cost (Train): 0.28796525896086556\n",
            "Iteration 7721, Cost (Validation): 0.34714537817506336\n",
            "Iteration 7722, Norm of Gradient: 0.005547153747929194, Cost (Train): 0.2879621820301771\n",
            "Iteration 7722, Cost (Validation): 0.34714605897060735\n",
            "Iteration 7723, Norm of Gradient: 0.005546574068052211, Cost (Train): 0.28795910574252015\n",
            "Iteration 7723, Cost (Validation): 0.347146740048534\n",
            "Iteration 7724, Norm of Gradient: 0.005545994506369446, Cost (Train): 0.2879560300976963\n",
            "Iteration 7724, Cost (Validation): 0.34714742140871097\n",
            "Iteration 7725, Norm of Gradient: 0.005545415062842069, Cost (Train): 0.2879529550955076\n",
            "Iteration 7725, Cost (Validation): 0.34714810305100596\n",
            "Iteration 7726, Norm of Gradient: 0.005544835737431269, Cost (Train): 0.28794988073575567\n",
            "Iteration 7726, Cost (Validation): 0.347148784975287\n",
            "Iteration 7727, Norm of Gradient: 0.005544256530098257, Cost (Train): 0.2879468070182427\n",
            "Iteration 7727, Cost (Validation): 0.347149467181422\n",
            "Iteration 7728, Norm of Gradient: 0.005543677440804255, Cost (Train): 0.2879437339427706\n",
            "Iteration 7728, Cost (Validation): 0.3471501496692788\n",
            "Iteration 7729, Norm of Gradient: 0.005543098469510507, Cost (Train): 0.28794066150914155\n",
            "Iteration 7729, Cost (Validation): 0.3471508324387258\n",
            "Iteration 7730, Norm of Gradient: 0.0055425196161782795, Cost (Train): 0.2879375897171578\n",
            "Iteration 7730, Cost (Validation): 0.34715151548963075\n",
            "Iteration 7731, Norm of Gradient: 0.0055419408807688446, Cost (Train): 0.28793451856662156\n",
            "Iteration 7731, Cost (Validation): 0.3471521988218621\n",
            "Iteration 7732, Norm of Gradient: 0.005541362263243505, Cost (Train): 0.28793144805733534\n",
            "Iteration 7732, Cost (Validation): 0.347152882435288\n",
            "Iteration 7733, Norm of Gradient: 0.005540783763563574, Cost (Train): 0.2879283781891014\n",
            "Iteration 7733, Cost (Validation): 0.3471535663297769\n",
            "Iteration 7734, Norm of Gradient: 0.005540205381690383, Cost (Train): 0.28792530896172247\n",
            "Iteration 7734, Cost (Validation): 0.34715425050519705\n",
            "Iteration 7735, Norm of Gradient: 0.005539627117585284, Cost (Train): 0.28792224037500125\n",
            "Iteration 7735, Cost (Validation): 0.3471549349614169\n",
            "Iteration 7736, Norm of Gradient: 0.005539048971209644, Cost (Train): 0.2879191724287403\n",
            "Iteration 7736, Cost (Validation): 0.34715561969830505\n",
            "Iteration 7737, Norm of Gradient: 0.00553847094252485, Cost (Train): 0.2879161051227425\n",
            "Iteration 7737, Cost (Validation): 0.34715630471573006\n",
            "Iteration 7738, Norm of Gradient: 0.0055378930314923065, Cost (Train): 0.28791303845681077\n",
            "Iteration 7738, Cost (Validation): 0.34715699001356054\n",
            "Iteration 7739, Norm of Gradient: 0.005537315238073435, Cost (Train): 0.287909972430748\n",
            "Iteration 7739, Cost (Validation): 0.3471576755916652\n",
            "Iteration 7740, Norm of Gradient: 0.005536737562229673, Cost (Train): 0.2879069070443573\n",
            "Iteration 7740, Cost (Validation): 0.34715836144991286\n",
            "Iteration 7741, Norm of Gradient: 0.005536160003922478, Cost (Train): 0.28790384229744176\n",
            "Iteration 7741, Cost (Validation): 0.34715904758817234\n",
            "Iteration 7742, Norm of Gradient: 0.005535582563113326, Cost (Train): 0.2879007781898047\n",
            "Iteration 7742, Cost (Validation): 0.34715973400631245\n",
            "Iteration 7743, Norm of Gradient: 0.005535005239763711, Cost (Train): 0.28789771472124925\n",
            "Iteration 7743, Cost (Validation): 0.3471604207042023\n",
            "Iteration 7744, Norm of Gradient: 0.005534428033835139, Cost (Train): 0.287894651891579\n",
            "Iteration 7744, Cost (Validation): 0.3471611076817108\n",
            "Iteration 7745, Norm of Gradient: 0.00553385094528914, Cost (Train): 0.28789158970059725\n",
            "Iteration 7745, Cost (Validation): 0.34716179493870714\n",
            "Iteration 7746, Norm of Gradient: 0.0055332739740872605, Cost (Train): 0.2878885281481077\n",
            "Iteration 7746, Cost (Validation): 0.3471624824750604\n",
            "Iteration 7747, Norm of Gradient: 0.005532697120191062, Cost (Train): 0.2878854672339139\n",
            "Iteration 7747, Cost (Validation): 0.3471631702906398\n",
            "Iteration 7748, Norm of Gradient: 0.005532120383562125, Cost (Train): 0.28788240695781964\n",
            "Iteration 7748, Cost (Validation): 0.34716385838531466\n",
            "Iteration 7749, Norm of Gradient: 0.00553154376416205, Cost (Train): 0.2878793473196286\n",
            "Iteration 7749, Cost (Validation): 0.34716454675895436\n",
            "Iteration 7750, Norm of Gradient: 0.005530967261952453, Cost (Train): 0.2878762883191449\n",
            "Iteration 7750, Cost (Validation): 0.3471652354114282\n",
            "Iteration 7751, Norm of Gradient: 0.005530390876894967, Cost (Train): 0.28787322995617226\n",
            "Iteration 7751, Cost (Validation): 0.34716592434260574\n",
            "Iteration 7752, Norm of Gradient: 0.005529814608951241, Cost (Train): 0.28787017223051475\n",
            "Iteration 7752, Cost (Validation): 0.34716661355235645\n",
            "Iteration 7753, Norm of Gradient: 0.005529238458082948, Cost (Train): 0.28786711514197666\n",
            "Iteration 7753, Cost (Validation): 0.3471673030405501\n",
            "Iteration 7754, Norm of Gradient: 0.005528662424251773, Cost (Train): 0.2878640586903622\n",
            "Iteration 7754, Cost (Validation): 0.34716799280705607\n",
            "Iteration 7755, Norm of Gradient: 0.00552808650741942, Cost (Train): 0.2878610028754755\n",
            "Iteration 7755, Cost (Validation): 0.34716868285174435\n",
            "Iteration 7756, Norm of Gradient: 0.005527510707547609, Cost (Train): 0.2878579476971211\n",
            "Iteration 7756, Cost (Validation): 0.34716937317448454\n",
            "Iteration 7757, Norm of Gradient: 0.0055269350245980816, Cost (Train): 0.28785489315510343\n",
            "Iteration 7757, Cost (Validation): 0.34717006377514664\n",
            "Iteration 7758, Norm of Gradient: 0.0055263594585325945, Cost (Train): 0.287851839249227\n",
            "Iteration 7758, Cost (Validation): 0.3471707546536005\n",
            "Iteration 7759, Norm of Gradient: 0.005525784009312921, Cost (Train): 0.28784878597929653\n",
            "Iteration 7759, Cost (Validation): 0.3471714458097161\n",
            "Iteration 7760, Norm of Gradient: 0.005525208676900853, Cost (Train): 0.2878457333451166\n",
            "Iteration 7760, Cost (Validation): 0.34717213724336343\n",
            "Iteration 7761, Norm of Gradient: 0.005524633461258201, Cost (Train): 0.2878426813464921\n",
            "Iteration 7761, Cost (Validation): 0.3471728289544127\n",
            "Iteration 7762, Norm of Gradient: 0.0055240583623467916, Cost (Train): 0.28783962998322793\n",
            "Iteration 7762, Cost (Validation): 0.3471735209427341\n",
            "Iteration 7763, Norm of Gradient: 0.005523483380128468, Cost (Train): 0.28783657925512907\n",
            "Iteration 7763, Cost (Validation): 0.34717421320819775\n",
            "Iteration 7764, Norm of Gradient: 0.005522908514565094, Cost (Train): 0.28783352916200045\n",
            "Iteration 7764, Cost (Validation): 0.34717490575067395\n",
            "Iteration 7765, Norm of Gradient: 0.005522333765618547, Cost (Train): 0.2878304797036473\n",
            "Iteration 7765, Cost (Validation): 0.34717559857003316\n",
            "Iteration 7766, Norm of Gradient: 0.005521759133250724, Cost (Train): 0.28782743087987467\n",
            "Iteration 7766, Cost (Validation): 0.3471762916661457\n",
            "Iteration 7767, Norm of Gradient: 0.005521184617423541, Cost (Train): 0.2878243826904881\n",
            "Iteration 7767, Cost (Validation): 0.3471769850388821\n",
            "Iteration 7768, Norm of Gradient: 0.00552061021809893, Cost (Train): 0.2878213351352927\n",
            "Iteration 7768, Cost (Validation): 0.347177678688113\n",
            "Iteration 7769, Norm of Gradient: 0.005520035935238837, Cost (Train): 0.28781828821409416\n",
            "Iteration 7769, Cost (Validation): 0.3471783726137089\n",
            "Iteration 7770, Norm of Gradient: 0.005519461768805231, Cost (Train): 0.2878152419266979\n",
            "Iteration 7770, Cost (Validation): 0.3471790668155404\n",
            "Iteration 7771, Norm of Gradient: 0.005518887718760096, Cost (Train): 0.28781219627290955\n",
            "Iteration 7771, Cost (Validation): 0.3471797612934784\n",
            "Iteration 7772, Norm of Gradient: 0.005518313785065433, Cost (Train): 0.28780915125253487\n",
            "Iteration 7772, Cost (Validation): 0.3471804560473936\n",
            "Iteration 7773, Norm of Gradient: 0.005517739967683261, Cost (Train): 0.28780610686537955\n",
            "Iteration 7773, Cost (Validation): 0.3471811510771569\n",
            "Iteration 7774, Norm of Gradient: 0.0055171662665756144, Cost (Train): 0.2878030631112495\n",
            "Iteration 7774, Cost (Validation): 0.3471818463826392\n",
            "Iteration 7775, Norm of Gradient: 0.00551659268170455, Cost (Train): 0.28780001998995064\n",
            "Iteration 7775, Cost (Validation): 0.34718254196371157\n",
            "Iteration 7776, Norm of Gradient: 0.005516019213032137, Cost (Train): 0.287796977501289\n",
            "Iteration 7776, Cost (Validation): 0.34718323782024496\n",
            "Iteration 7777, Norm of Gradient: 0.005515445860520463, Cost (Train): 0.28779393564507083\n",
            "Iteration 7777, Cost (Validation): 0.34718393395211045\n",
            "Iteration 7778, Norm of Gradient: 0.005514872624131634, Cost (Train): 0.2877908944211022\n",
            "Iteration 7778, Cost (Validation): 0.3471846303591794\n",
            "Iteration 7779, Norm of Gradient: 0.005514299503827774, Cost (Train): 0.2877878538291893\n",
            "Iteration 7779, Cost (Validation): 0.3471853270413229\n",
            "Iteration 7780, Norm of Gradient: 0.005513726499571022, Cost (Train): 0.28778481386913857\n",
            "Iteration 7780, Cost (Validation): 0.3471860239984122\n",
            "Iteration 7781, Norm of Gradient: 0.005513153611323537, Cost (Train): 0.2877817745407566\n",
            "Iteration 7781, Cost (Validation): 0.34718672123031874\n",
            "Iteration 7782, Norm of Gradient: 0.0055125808390474925, Cost (Train): 0.2877787358438497\n",
            "Iteration 7782, Cost (Validation): 0.34718741873691406\n",
            "Iteration 7783, Norm of Gradient: 0.005512008182705082, Cost (Train): 0.2877756977782245\n",
            "Iteration 7783, Cost (Validation): 0.34718811651806947\n",
            "Iteration 7784, Norm of Gradient: 0.005511435642258514, Cost (Train): 0.28777266034368776\n",
            "Iteration 7784, Cost (Validation): 0.3471888145736566\n",
            "Iteration 7785, Norm of Gradient: 0.005510863217670015, Cost (Train): 0.28776962354004626\n",
            "Iteration 7785, Cost (Validation): 0.34718951290354705\n",
            "Iteration 7786, Norm of Gradient: 0.00551029090890183, Cost (Train): 0.2877665873671068\n",
            "Iteration 7786, Cost (Validation): 0.3471902115076125\n",
            "Iteration 7787, Norm of Gradient: 0.005509718715916221, Cost (Train): 0.28776355182467617\n",
            "Iteration 7787, Cost (Validation): 0.3471909103857246\n",
            "Iteration 7788, Norm of Gradient: 0.005509146638675463, Cost (Train): 0.2877605169125616\n",
            "Iteration 7788, Cost (Validation): 0.34719160953775524\n",
            "Iteration 7789, Norm of Gradient: 0.005508574677141854, Cost (Train): 0.2877574826305701\n",
            "Iteration 7789, Cost (Validation): 0.3471923089635763\n",
            "Iteration 7790, Norm of Gradient: 0.005508002831277709, Cost (Train): 0.2877544489785088\n",
            "Iteration 7790, Cost (Validation): 0.3471930086630597\n",
            "Iteration 7791, Norm of Gradient: 0.005507431101045354, Cost (Train): 0.2877514159561851\n",
            "Iteration 7791, Cost (Validation): 0.34719370863607735\n",
            "Iteration 7792, Norm of Gradient: 0.00550685948640714, Cost (Train): 0.2877483835634061\n",
            "Iteration 7792, Cost (Validation): 0.34719440888250136\n",
            "Iteration 7793, Norm of Gradient: 0.005506287987325428, Cost (Train): 0.2877453517999794\n",
            "Iteration 7793, Cost (Validation): 0.3471951094022039\n",
            "Iteration 7794, Norm of Gradient: 0.005505716603762603, Cost (Train): 0.28774232066571237\n",
            "Iteration 7794, Cost (Validation): 0.34719581019505696\n",
            "Iteration 7795, Norm of Gradient: 0.005505145335681062, Cost (Train): 0.2877392901604126\n",
            "Iteration 7795, Cost (Validation): 0.34719651126093287\n",
            "Iteration 7796, Norm of Gradient: 0.005504574183043223, Cost (Train): 0.2877362602838878\n",
            "Iteration 7796, Cost (Validation): 0.34719721259970393\n",
            "Iteration 7797, Norm of Gradient: 0.005504003145811516, Cost (Train): 0.2877332310359458\n",
            "Iteration 7797, Cost (Validation): 0.3471979142112426\n",
            "Iteration 7798, Norm of Gradient: 0.0055034322239483945, Cost (Train): 0.28773020241639424\n",
            "Iteration 7798, Cost (Validation): 0.34719861609542113\n",
            "Iteration 7799, Norm of Gradient: 0.005502861417416324, Cost (Train): 0.2877271744250411\n",
            "Iteration 7799, Cost (Validation): 0.3471993182521121\n",
            "Iteration 7800, Norm of Gradient: 0.00550229072617779, Cost (Train): 0.28772414706169436\n",
            "Iteration 7800, Cost (Validation): 0.34720002068118794\n",
            "Iteration 7801, Norm of Gradient: 0.005501720150195294, Cost (Train): 0.28772112032616215\n",
            "Iteration 7801, Cost (Validation): 0.3472007233825213\n",
            "Iteration 7802, Norm of Gradient: 0.005501149689431354, Cost (Train): 0.28771809421825245\n",
            "Iteration 7802, Cost (Validation): 0.34720142635598494\n",
            "Iteration 7803, Norm of Gradient: 0.005500579343848507, Cost (Train): 0.2877150687377736\n",
            "Iteration 7803, Cost (Validation): 0.3472021296014515\n",
            "Iteration 7804, Norm of Gradient: 0.005500009113409305, Cost (Train): 0.28771204388453386\n",
            "Iteration 7804, Cost (Validation): 0.3472028331187939\n",
            "Iteration 7805, Norm of Gradient: 0.005499438998076319, Cost (Train): 0.2877090196583416\n",
            "Iteration 7805, Cost (Validation): 0.3472035369078848\n",
            "Iteration 7806, Norm of Gradient: 0.005498868997812136, Cost (Train): 0.2877059960590054\n",
            "Iteration 7806, Cost (Validation): 0.34720424096859714\n",
            "Iteration 7807, Norm of Gradient: 0.00549829911257936, Cost (Train): 0.28770297308633364\n",
            "Iteration 7807, Cost (Validation): 0.34720494530080404\n",
            "Iteration 7808, Norm of Gradient: 0.005497729342340611, Cost (Train): 0.28769995074013505\n",
            "Iteration 7808, Cost (Validation): 0.34720564990437836\n",
            "Iteration 7809, Norm of Gradient: 0.00549715968705853, Cost (Train): 0.2876969290202184\n",
            "Iteration 7809, Cost (Validation): 0.3472063547791934\n",
            "Iteration 7810, Norm of Gradient: 0.005496590146695768, Cost (Train): 0.28769390792639227\n",
            "Iteration 7810, Cost (Validation): 0.34720705992512213\n",
            "Iteration 7811, Norm of Gradient: 0.005496020721215, Cost (Train): 0.2876908874584656\n",
            "Iteration 7811, Cost (Validation): 0.34720776534203784\n",
            "Iteration 7812, Norm of Gradient: 0.005495451410578917, Cost (Train): 0.2876878676162475\n",
            "Iteration 7812, Cost (Validation): 0.3472084710298137\n",
            "Iteration 7813, Norm of Gradient: 0.0054948822147502215, Cost (Train): 0.28768484839954683\n",
            "Iteration 7813, Cost (Validation): 0.3472091769883233\n",
            "Iteration 7814, Norm of Gradient: 0.005494313133691638, Cost (Train): 0.28768182980817264\n",
            "Iteration 7814, Cost (Validation): 0.3472098832174399\n",
            "Iteration 7815, Norm of Gradient: 0.005493744167365908, Cost (Train): 0.28767881184193433\n",
            "Iteration 7815, Cost (Validation): 0.3472105897170369\n",
            "Iteration 7816, Norm of Gradient: 0.005493175315735787, Cost (Train): 0.287675794500641\n",
            "Iteration 7816, Cost (Validation): 0.34721129648698795\n",
            "Iteration 7817, Norm of Gradient: 0.0054926065787640485, Cost (Train): 0.28767277778410216\n",
            "Iteration 7817, Cost (Validation): 0.3472120035271665\n",
            "Iteration 7818, Norm of Gradient: 0.0054920379564134866, Cost (Train): 0.287669761692127\n",
            "Iteration 7818, Cost (Validation): 0.3472127108374463\n",
            "Iteration 7819, Norm of Gradient: 0.005491469448646907, Cost (Train): 0.2876667462245252\n",
            "Iteration 7819, Cost (Validation): 0.3472134184177011\n",
            "Iteration 7820, Norm of Gradient: 0.005490901055427134, Cost (Train): 0.2876637313811063\n",
            "Iteration 7820, Cost (Validation): 0.3472141262678045\n",
            "Iteration 7821, Norm of Gradient: 0.00549033277671701, Cost (Train): 0.28766071716168\n",
            "Iteration 7821, Cost (Validation): 0.3472148343876305\n",
            "Iteration 7822, Norm of Gradient: 0.005489764612479395, Cost (Train): 0.2876577035660559\n",
            "Iteration 7822, Cost (Validation): 0.3472155427770529\n",
            "Iteration 7823, Norm of Gradient: 0.005489196562677163, Cost (Train): 0.28765469059404397\n",
            "Iteration 7823, Cost (Validation): 0.34721625143594564\n",
            "Iteration 7824, Norm of Gradient: 0.005488628627273205, Cost (Train): 0.2876516782454541\n",
            "Iteration 7824, Cost (Validation): 0.3472169603641828\n",
            "Iteration 7825, Norm of Gradient: 0.005488060806230435, Cost (Train): 0.2876486665200963\n",
            "Iteration 7825, Cost (Validation): 0.34721766956163846\n",
            "Iteration 7826, Norm of Gradient: 0.005487493099511775, Cost (Train): 0.28764565541778064\n",
            "Iteration 7826, Cost (Validation): 0.3472183790281867\n",
            "Iteration 7827, Norm of Gradient: 0.005486925507080169, Cost (Train): 0.28764264493831715\n",
            "Iteration 7827, Cost (Validation): 0.3472190887637017\n",
            "Iteration 7828, Norm of Gradient: 0.005486358028898578, Cost (Train): 0.28763963508151613\n",
            "Iteration 7828, Cost (Validation): 0.3472197987680578\n",
            "Iteration 7829, Norm of Gradient: 0.005485790664929977, Cost (Train): 0.28763662584718785\n",
            "Iteration 7829, Cost (Validation): 0.3472205090411293\n",
            "Iteration 7830, Norm of Gradient: 0.005485223415137361, Cost (Train): 0.2876336172351428\n",
            "Iteration 7830, Cost (Validation): 0.3472212195827905\n",
            "Iteration 7831, Norm of Gradient: 0.005484656279483739, Cost (Train): 0.2876306092451913\n",
            "Iteration 7831, Cost (Validation): 0.3472219303929159\n",
            "Iteration 7832, Norm of Gradient: 0.005484089257932139, Cost (Train): 0.287627601877144\n",
            "Iteration 7832, Cost (Validation): 0.34722264147137993\n",
            "Iteration 7833, Norm of Gradient: 0.005483522350445606, Cost (Train): 0.2876245951308115\n",
            "Iteration 7833, Cost (Validation): 0.3472233528180573\n",
            "Iteration 7834, Norm of Gradient: 0.005482955556987197, Cost (Train): 0.2876215890060044\n",
            "Iteration 7834, Cost (Validation): 0.34722406443282244\n",
            "Iteration 7835, Norm of Gradient: 0.005482388877519994, Cost (Train): 0.2876185835025336\n",
            "Iteration 7835, Cost (Validation): 0.34722477631555015\n",
            "Iteration 7836, Norm of Gradient: 0.005481822312007087, Cost (Train): 0.28761557862021\n",
            "Iteration 7836, Cost (Validation): 0.3472254884661152\n",
            "Iteration 7837, Norm of Gradient: 0.00548125586041159, Cost (Train): 0.2876125743588443\n",
            "Iteration 7837, Cost (Validation): 0.3472262008843923\n",
            "Iteration 7838, Norm of Gradient: 0.005480689522696629, Cost (Train): 0.2876095707182478\n",
            "Iteration 7838, Cost (Validation): 0.3472269135702563\n",
            "Iteration 7839, Norm of Gradient: 0.00548012329882535, Cost (Train): 0.28760656769823134\n",
            "Iteration 7839, Cost (Validation): 0.3472276265235823\n",
            "Iteration 7840, Norm of Gradient: 0.005479557188760912, Cost (Train): 0.28760356529860637\n",
            "Iteration 7840, Cost (Validation): 0.34722833974424505\n",
            "Iteration 7841, Norm of Gradient: 0.005478991192466495, Cost (Train): 0.28760056351918395\n",
            "Iteration 7841, Cost (Validation): 0.3472290532321197\n",
            "Iteration 7842, Norm of Gradient: 0.005478425309905291, Cost (Train): 0.2875975623597754\n",
            "Iteration 7842, Cost (Validation): 0.3472297669870814\n",
            "Iteration 7843, Norm of Gradient: 0.005477859541040515, Cost (Train): 0.28759456182019233\n",
            "Iteration 7843, Cost (Validation): 0.34723048100900533\n",
            "Iteration 7844, Norm of Gradient: 0.005477293885835391, Cost (Train): 0.287591561900246\n",
            "Iteration 7844, Cost (Validation): 0.3472311952977665\n",
            "Iteration 7845, Norm of Gradient: 0.005476728344253167, Cost (Train): 0.287588562599748\n",
            "Iteration 7845, Cost (Validation): 0.3472319098532405\n",
            "Iteration 7846, Norm of Gradient: 0.005476162916257102, Cost (Train): 0.2875855639185102\n",
            "Iteration 7846, Cost (Validation): 0.3472326246753024\n",
            "Iteration 7847, Norm of Gradient: 0.005475597601810476, Cost (Train): 0.2875825658563441\n",
            "Iteration 7847, Cost (Validation): 0.34723333976382775\n",
            "Iteration 7848, Norm of Gradient: 0.00547503240087658, Cost (Train): 0.2875795684130617\n",
            "Iteration 7848, Cost (Validation): 0.3472340551186921\n",
            "Iteration 7849, Norm of Gradient: 0.005474467313418728, Cost (Train): 0.2875765715884747\n",
            "Iteration 7849, Cost (Validation): 0.3472347707397707\n",
            "Iteration 7850, Norm of Gradient: 0.005473902339400247, Cost (Train): 0.28757357538239514\n",
            "Iteration 7850, Cost (Validation): 0.3472354866269394\n",
            "Iteration 7851, Norm of Gradient: 0.005473337478784482, Cost (Train): 0.2875705797946351\n",
            "Iteration 7851, Cost (Validation): 0.3472362027800737\n",
            "Iteration 7852, Norm of Gradient: 0.005472772731534793, Cost (Train): 0.2875675848250066\n",
            "Iteration 7852, Cost (Validation): 0.3472369191990494\n",
            "Iteration 7853, Norm of Gradient: 0.005472208097614558, Cost (Train): 0.28756459047332195\n",
            "Iteration 7853, Cost (Validation): 0.3472376358837421\n",
            "Iteration 7854, Norm of Gradient: 0.0054716435769871724, Cost (Train): 0.28756159673939347\n",
            "Iteration 7854, Cost (Validation): 0.3472383528340279\n",
            "Iteration 7855, Norm of Gradient: 0.005471079169616045, Cost (Train): 0.28755860362303326\n",
            "Iteration 7855, Cost (Validation): 0.3472390700497824\n",
            "Iteration 7856, Norm of Gradient: 0.005470514875464605, Cost (Train): 0.287555611124054\n",
            "Iteration 7856, Cost (Validation): 0.34723978753088164\n",
            "Iteration 7857, Norm of Gradient: 0.005469950694496296, Cost (Train): 0.2875526192422681\n",
            "Iteration 7857, Cost (Validation): 0.3472405052772017\n",
            "Iteration 7858, Norm of Gradient: 0.005469386626674577, Cost (Train): 0.2875496279774882\n",
            "Iteration 7858, Cost (Validation): 0.34724122328861856\n",
            "Iteration 7859, Norm of Gradient: 0.005468822671962927, Cost (Train): 0.28754663732952696\n",
            "Iteration 7859, Cost (Validation): 0.34724194156500837\n",
            "Iteration 7860, Norm of Gradient: 0.005468258830324838, Cost (Train): 0.2875436472981971\n",
            "Iteration 7860, Cost (Validation): 0.3472426601062473\n",
            "Iteration 7861, Norm of Gradient: 0.005467695101723821, Cost (Train): 0.28754065788331146\n",
            "Iteration 7861, Cost (Validation): 0.3472433789122117\n",
            "Iteration 7862, Norm of Gradient: 0.005467131486123401, Cost (Train): 0.28753766908468287\n",
            "Iteration 7862, Cost (Validation): 0.34724409798277767\n",
            "Iteration 7863, Norm of Gradient: 0.005466567983487122, Cost (Train): 0.2875346809021245\n",
            "Iteration 7863, Cost (Validation): 0.34724481731782164\n",
            "Iteration 7864, Norm of Gradient: 0.005466004593778545, Cost (Train): 0.2875316933354492\n",
            "Iteration 7864, Cost (Validation): 0.3472455369172201\n",
            "Iteration 7865, Norm of Gradient: 0.005465441316961243, Cost (Train): 0.2875287063844702\n",
            "Iteration 7865, Cost (Validation): 0.3472462567808496\n",
            "Iteration 7866, Norm of Gradient: 0.005464878152998812, Cost (Train): 0.2875257200490008\n",
            "Iteration 7866, Cost (Validation): 0.34724697690858647\n",
            "Iteration 7867, Norm of Gradient: 0.005464315101854856, Cost (Train): 0.28752273432885417\n",
            "Iteration 7867, Cost (Validation): 0.3472476973003074\n",
            "Iteration 7868, Norm of Gradient: 0.005463752163493007, Cost (Train): 0.2875197492238437\n",
            "Iteration 7868, Cost (Validation): 0.3472484179558891\n",
            "Iteration 7869, Norm of Gradient: 0.005463189337876901, Cost (Train): 0.2875167647337829\n",
            "Iteration 7869, Cost (Validation): 0.3472491388752083\n",
            "Iteration 7870, Norm of Gradient: 0.005462626624970199, Cost (Train): 0.28751378085848517\n",
            "Iteration 7870, Cost (Validation): 0.34724986005814173\n",
            "Iteration 7871, Norm of Gradient: 0.005462064024736576, Cost (Train): 0.2875107975977643\n",
            "Iteration 7871, Cost (Validation): 0.34725058150456606\n",
            "Iteration 7872, Norm of Gradient: 0.00546150153713972, Cost (Train): 0.28750781495143385\n",
            "Iteration 7872, Cost (Validation): 0.3472513032143585\n",
            "Iteration 7873, Norm of Gradient: 0.005460939162143343, Cost (Train): 0.2875048329193076\n",
            "Iteration 7873, Cost (Validation): 0.3472520251873958\n",
            "Iteration 7874, Norm of Gradient: 0.005460376899711165, Cost (Train): 0.28750185150119933\n",
            "Iteration 7874, Cost (Validation): 0.34725274742355505\n",
            "Iteration 7875, Norm of Gradient: 0.005459814749806929, Cost (Train): 0.2874988706969231\n",
            "Iteration 7875, Cost (Validation): 0.3472534699227132\n",
            "Iteration 7876, Norm of Gradient: 0.0054592527123943895, Cost (Train): 0.2874958905062928\n",
            "Iteration 7876, Cost (Validation): 0.3472541926847476\n",
            "Iteration 7877, Norm of Gradient: 0.005458690787437321, Cost (Train): 0.2874929109291225\n",
            "Iteration 7877, Cost (Validation): 0.34725491570953526\n",
            "Iteration 7878, Norm of Gradient: 0.005458128974899512, Cost (Train): 0.28748993196522643\n",
            "Iteration 7878, Cost (Validation): 0.34725563899695355\n",
            "Iteration 7879, Norm of Gradient: 0.005457567274744769, Cost (Train): 0.28748695361441867\n",
            "Iteration 7879, Cost (Validation): 0.3472563625468796\n",
            "Iteration 7880, Norm of Gradient: 0.005457005686936912, Cost (Train): 0.28748397587651364\n",
            "Iteration 7880, Cost (Validation): 0.347257086359191\n",
            "Iteration 7881, Norm of Gradient: 0.005456444211439782, Cost (Train): 0.28748099875132566\n",
            "Iteration 7881, Cost (Validation): 0.3472578104337651\n",
            "Iteration 7882, Norm of Gradient: 0.005455882848217232, Cost (Train): 0.2874780222386693\n",
            "Iteration 7882, Cost (Validation): 0.34725853477047924\n",
            "Iteration 7883, Norm of Gradient: 0.0054553215972331336, Cost (Train): 0.28747504633835896\n",
            "Iteration 7883, Cost (Validation): 0.34725925936921115\n",
            "Iteration 7884, Norm of Gradient: 0.005454760458451374, Cost (Train): 0.2874720710502093\n",
            "Iteration 7884, Cost (Validation): 0.3472599842298383\n",
            "Iteration 7885, Norm of Gradient: 0.005454199431835857, Cost (Train): 0.28746909637403495\n",
            "Iteration 7885, Cost (Validation): 0.3472607093522385\n",
            "Iteration 7886, Norm of Gradient: 0.005453638517350501, Cost (Train): 0.28746612230965074\n",
            "Iteration 7886, Cost (Validation): 0.34726143473628934\n",
            "Iteration 7887, Norm of Gradient: 0.0054530777149592435, Cost (Train): 0.2874631488568716\n",
            "Iteration 7887, Cost (Validation): 0.3472621603818687\n",
            "Iteration 7888, Norm of Gradient: 0.005452517024626039, Cost (Train): 0.28746017601551227\n",
            "Iteration 7888, Cost (Validation): 0.34726288628885427\n",
            "Iteration 7889, Norm of Gradient: 0.0054519564463148505, Cost (Train): 0.2874572037853878\n",
            "Iteration 7889, Cost (Validation): 0.34726361245712417\n",
            "Iteration 7890, Norm of Gradient: 0.005451395979989667, Cost (Train): 0.28745423216631333\n",
            "Iteration 7890, Cost (Validation): 0.3472643388865561\n",
            "Iteration 7891, Norm of Gradient: 0.005450835625614489, Cost (Train): 0.2874512611581041\n",
            "Iteration 7891, Cost (Validation): 0.34726506557702824\n",
            "Iteration 7892, Norm of Gradient: 0.005450275383153333, Cost (Train): 0.2874482907605751\n",
            "Iteration 7892, Cost (Validation): 0.34726579252841866\n",
            "Iteration 7893, Norm of Gradient: 0.005449715252570233, Cost (Train): 0.28744532097354175\n",
            "Iteration 7893, Cost (Validation): 0.3472665197406054\n",
            "Iteration 7894, Norm of Gradient: 0.005449155233829238, Cost (Train): 0.28744235179681943\n",
            "Iteration 7894, Cost (Validation): 0.34726724721346675\n",
            "Iteration 7895, Norm of Gradient: 0.005448595326894414, Cost (Train): 0.2874393832302236\n",
            "Iteration 7895, Cost (Validation): 0.3472679749468809\n",
            "Iteration 7896, Norm of Gradient: 0.0054480355317298446, Cost (Train): 0.28743641527356983\n",
            "Iteration 7896, Cost (Validation): 0.34726870294072615\n",
            "Iteration 7897, Norm of Gradient: 0.005447475848299626, Cost (Train): 0.28743344792667364\n",
            "Iteration 7897, Cost (Validation): 0.3472694311948809\n",
            "Iteration 7898, Norm of Gradient: 0.005446916276567873, Cost (Train): 0.2874304811893507\n",
            "Iteration 7898, Cost (Validation): 0.3472701597092236\n",
            "Iteration 7899, Norm of Gradient: 0.005446356816498719, Cost (Train): 0.28742751506141695\n",
            "Iteration 7899, Cost (Validation): 0.3472708884836326\n",
            "Iteration 7900, Norm of Gradient: 0.005445797468056307, Cost (Train): 0.28742454954268815\n",
            "Iteration 7900, Cost (Validation): 0.3472716175179866\n",
            "Iteration 7901, Norm of Gradient: 0.005445238231204801, Cost (Train): 0.2874215846329801\n",
            "Iteration 7901, Cost (Validation): 0.34727234681216407\n",
            "Iteration 7902, Norm of Gradient: 0.005444679105908381, Cost (Train): 0.2874186203321089\n",
            "Iteration 7902, Cost (Validation): 0.3472730763660437\n",
            "Iteration 7903, Norm of Gradient: 0.005444120092131241, Cost (Train): 0.2874156566398906\n",
            "Iteration 7903, Cost (Validation): 0.3472738061795042\n",
            "Iteration 7904, Norm of Gradient: 0.005443561189837592, Cost (Train): 0.2874126935561414\n",
            "Iteration 7904, Cost (Validation): 0.3472745362524245\n",
            "Iteration 7905, Norm of Gradient: 0.005443002398991662, Cost (Train): 0.28740973108067747\n",
            "Iteration 7905, Cost (Validation): 0.34727526658468316\n",
            "Iteration 7906, Norm of Gradient: 0.005442443719557695, Cost (Train): 0.28740676921331515\n",
            "Iteration 7906, Cost (Validation): 0.3472759971761593\n",
            "Iteration 7907, Norm of Gradient: 0.005441885151499949, Cost (Train): 0.28740380795387066\n",
            "Iteration 7907, Cost (Validation): 0.3472767280267317\n",
            "Iteration 7908, Norm of Gradient: 0.005441326694782701, Cost (Train): 0.2874008473021606\n",
            "Iteration 7908, Cost (Validation): 0.3472774591362795\n",
            "Iteration 7909, Norm of Gradient: 0.0054407683493702405, Cost (Train): 0.2873978872580015\n",
            "Iteration 7909, Cost (Validation): 0.34727819050468167\n",
            "Iteration 7910, Norm of Gradient: 0.0054402101152268775, Cost (Train): 0.28739492782120996\n",
            "Iteration 7910, Cost (Validation): 0.3472789221318173\n",
            "Iteration 7911, Norm of Gradient: 0.005439651992316935, Cost (Train): 0.2873919689916025\n",
            "Iteration 7911, Cost (Validation): 0.3472796540175657\n",
            "Iteration 7912, Norm of Gradient: 0.005439093980604751, Cost (Train): 0.28738901076899614\n",
            "Iteration 7912, Cost (Validation): 0.347280386161806\n",
            "Iteration 7913, Norm of Gradient: 0.005438536080054684, Cost (Train): 0.28738605315320753\n",
            "Iteration 7913, Cost (Validation): 0.34728111856441757\n",
            "Iteration 7914, Norm of Gradient: 0.005437978290631103, Cost (Train): 0.28738309614405366\n",
            "Iteration 7914, Cost (Validation): 0.34728185122527966\n",
            "Iteration 7915, Norm of Gradient: 0.005437420612298398, Cost (Train): 0.28738013974135146\n",
            "Iteration 7915, Cost (Validation): 0.34728258414427177\n",
            "Iteration 7916, Norm of Gradient: 0.005436863045020971, Cost (Train): 0.287377183944918\n",
            "Iteration 7916, Cost (Validation): 0.3472833173212734\n",
            "Iteration 7917, Norm of Gradient: 0.005436305588763243, Cost (Train): 0.28737422875457036\n",
            "Iteration 7917, Cost (Validation): 0.3472840507561638\n",
            "Iteration 7918, Norm of Gradient: 0.00543574824348965, Cost (Train): 0.2873712741701259\n",
            "Iteration 7918, Cost (Validation): 0.34728478444882294\n",
            "Iteration 7919, Norm of Gradient: 0.005435191009164643, Cost (Train): 0.28736832019140185\n",
            "Iteration 7919, Cost (Validation): 0.34728551839913024\n",
            "Iteration 7920, Norm of Gradient: 0.005434633885752689, Cost (Train): 0.28736536681821545\n",
            "Iteration 7920, Cost (Validation): 0.34728625260696544\n",
            "Iteration 7921, Norm of Gradient: 0.005434076873218272, Cost (Train): 0.2873624140503843\n",
            "Iteration 7921, Cost (Validation): 0.34728698707220834\n",
            "Iteration 7922, Norm of Gradient: 0.005433519971525893, Cost (Train): 0.2873594618877259\n",
            "Iteration 7922, Cost (Validation): 0.3472877217947386\n",
            "Iteration 7923, Norm of Gradient: 0.005432963180640065, Cost (Train): 0.2873565103300576\n",
            "Iteration 7923, Cost (Validation): 0.34728845677443626\n",
            "Iteration 7924, Norm of Gradient: 0.005432406500525322, Cost (Train): 0.2873535593771974\n",
            "Iteration 7924, Cost (Validation): 0.34728919201118125\n",
            "Iteration 7925, Norm of Gradient: 0.005431849931146209, Cost (Train): 0.28735060902896287\n",
            "Iteration 7925, Cost (Validation): 0.34728992750485344\n",
            "Iteration 7926, Norm of Gradient: 0.00543129347246729, Cost (Train): 0.2873476592851718\n",
            "Iteration 7926, Cost (Validation): 0.34729066325533287\n",
            "Iteration 7927, Norm of Gradient: 0.005430737124453145, Cost (Train): 0.2873447101456421\n",
            "Iteration 7927, Cost (Validation): 0.3472913992624998\n",
            "Iteration 7928, Norm of Gradient: 0.005430180887068368, Cost (Train): 0.2873417616101918\n",
            "Iteration 7928, Cost (Validation): 0.3472921355262342\n",
            "Iteration 7929, Norm of Gradient: 0.00542962476027757, Cost (Train): 0.2873388136786389\n",
            "Iteration 7929, Cost (Validation): 0.3472928720464164\n",
            "Iteration 7930, Norm of Gradient: 0.005429068744045378, Cost (Train): 0.2873358663508016\n",
            "Iteration 7930, Cost (Validation): 0.34729360882292665\n",
            "Iteration 7931, Norm of Gradient: 0.005428512838336435, Cost (Train): 0.287332919626498\n",
            "Iteration 7931, Cost (Validation): 0.34729434585564534\n",
            "Iteration 7932, Norm of Gradient: 0.005427957043115398, Cost (Train): 0.2873299735055463\n",
            "Iteration 7932, Cost (Validation): 0.3472950831444527\n",
            "Iteration 7933, Norm of Gradient: 0.0054274013583469425, Cost (Train): 0.28732702798776494\n",
            "Iteration 7933, Cost (Validation): 0.3472958206892292\n",
            "Iteration 7934, Norm of Gradient: 0.0054268457839957585, Cost (Train): 0.2873240830729723\n",
            "Iteration 7934, Cost (Validation): 0.34729655848985547\n",
            "Iteration 7935, Norm of Gradient: 0.0054262903200265515, Cost (Train): 0.287321138760987\n",
            "Iteration 7935, Cost (Validation): 0.34729729654621205\n",
            "Iteration 7936, Norm of Gradient: 0.005425734966404042, Cost (Train): 0.2873181950516274\n",
            "Iteration 7936, Cost (Validation): 0.3472980348581794\n",
            "Iteration 7937, Norm of Gradient: 0.00542517972309297, Cost (Train): 0.2873152519447124\n",
            "Iteration 7937, Cost (Validation): 0.34729877342563836\n",
            "Iteration 7938, Norm of Gradient: 0.005424624590058087, Cost (Train): 0.2873123094400605\n",
            "Iteration 7938, Cost (Validation): 0.3472995122484696\n",
            "Iteration 7939, Norm of Gradient: 0.005424069567264164, Cost (Train): 0.28730936753749053\n",
            "Iteration 7939, Cost (Validation): 0.34730025132655384\n",
            "Iteration 7940, Norm of Gradient: 0.0054235146546759835, Cost (Train): 0.2873064262368214\n",
            "Iteration 7940, Cost (Validation): 0.34730099065977205\n",
            "Iteration 7941, Norm of Gradient: 0.005422959852258348, Cost (Train): 0.28730348553787205\n",
            "Iteration 7941, Cost (Validation): 0.34730173024800504\n",
            "Iteration 7942, Norm of Gradient: 0.005422405159976073, Cost (Train): 0.2873005454404614\n",
            "Iteration 7942, Cost (Validation): 0.3473024700911338\n",
            "Iteration 7943, Norm of Gradient: 0.0054218505777939895, Cost (Train): 0.2872976059444088\n",
            "Iteration 7943, Cost (Validation): 0.3473032101890393\n",
            "Iteration 7944, Norm of Gradient: 0.00542129610567695, Cost (Train): 0.28729466704953316\n",
            "Iteration 7944, Cost (Validation): 0.34730395054160274\n",
            "Iteration 7945, Norm of Gradient: 0.005420741743589812, Cost (Train): 0.28729172875565384\n",
            "Iteration 7945, Cost (Validation): 0.34730469114870505\n",
            "Iteration 7946, Norm of Gradient: 0.005420187491497458, Cost (Train): 0.2872887910625901\n",
            "Iteration 7946, Cost (Validation): 0.34730543201022757\n",
            "Iteration 7947, Norm of Gradient: 0.005419633349364784, Cost (Train): 0.28728585397016143\n",
            "Iteration 7947, Cost (Validation): 0.34730617312605144\n",
            "Iteration 7948, Norm of Gradient: 0.005419079317156698, Cost (Train): 0.2872829174781871\n",
            "Iteration 7948, Cost (Validation): 0.347306914496058\n",
            "Iteration 7949, Norm of Gradient: 0.005418525394838129, Cost (Train): 0.2872799815864869\n",
            "Iteration 7949, Cost (Validation): 0.34730765612012865\n",
            "Iteration 7950, Norm of Gradient: 0.005417971582374018, Cost (Train): 0.28727704629488027\n",
            "Iteration 7950, Cost (Validation): 0.3473083979981448\n",
            "Iteration 7951, Norm of Gradient: 0.005417417879729322, Cost (Train): 0.28727411160318694\n",
            "Iteration 7951, Cost (Validation): 0.3473091401299878\n",
            "Iteration 7952, Norm of Gradient: 0.005416864286869016, Cost (Train): 0.2872711775112267\n",
            "Iteration 7952, Cost (Validation): 0.3473098825155393\n",
            "Iteration 7953, Norm of Gradient: 0.0054163108037580876, Cost (Train): 0.2872682440188194\n",
            "Iteration 7953, Cost (Validation): 0.3473106251546808\n",
            "Iteration 7954, Norm of Gradient: 0.0054157574303615435, Cost (Train): 0.28726531112578496\n",
            "Iteration 7954, Cost (Validation): 0.34731136804729396\n",
            "Iteration 7955, Norm of Gradient: 0.005415204166644402, Cost (Train): 0.2872623788319433\n",
            "Iteration 7955, Cost (Validation): 0.34731211119326055\n",
            "Iteration 7956, Norm of Gradient: 0.0054146510125717, Cost (Train): 0.2872594471371145\n",
            "Iteration 7956, Cost (Validation): 0.3473128545924622\n",
            "Iteration 7957, Norm of Gradient: 0.005414097968108491, Cost (Train): 0.2872565160411186\n",
            "Iteration 7957, Cost (Validation): 0.3473135982447807\n",
            "Iteration 7958, Norm of Gradient: 0.005413545033219839, Cost (Train): 0.2872535855437761\n",
            "Iteration 7958, Cost (Validation): 0.34731434215009804\n",
            "Iteration 7959, Norm of Gradient: 0.005412992207870828, Cost (Train): 0.28725065564490704\n",
            "Iteration 7959, Cost (Validation): 0.3473150863082961\n",
            "Iteration 7960, Norm of Gradient: 0.005412439492026558, Cost (Train): 0.2872477263443318\n",
            "Iteration 7960, Cost (Validation): 0.3473158307192568\n",
            "Iteration 7961, Norm of Gradient: 0.005411886885652141, Cost (Train): 0.2872447976418709\n",
            "Iteration 7961, Cost (Validation): 0.3473165753828622\n",
            "Iteration 7962, Norm of Gradient: 0.005411334388712708, Cost (Train): 0.2872418695373447\n",
            "Iteration 7962, Cost (Validation): 0.3473173202989944\n",
            "Iteration 7963, Norm of Gradient: 0.005410782001173403, Cost (Train): 0.287238942030574\n",
            "Iteration 7963, Cost (Validation): 0.3473180654675356\n",
            "Iteration 7964, Norm of Gradient: 0.005410229722999388, Cost (Train): 0.2872360151213793\n",
            "Iteration 7964, Cost (Validation): 0.3473188108883679\n",
            "Iteration 7965, Norm of Gradient: 0.005409677554155839, Cost (Train): 0.2872330888095813\n",
            "Iteration 7965, Cost (Validation): 0.34731955656137353\n",
            "Iteration 7966, Norm of Gradient: 0.005409125494607947, Cost (Train): 0.28723016309500093\n",
            "Iteration 7966, Cost (Validation): 0.3473203024864349\n",
            "Iteration 7967, Norm of Gradient: 0.00540857354432092, Cost (Train): 0.287227237977459\n",
            "Iteration 7967, Cost (Validation): 0.3473210486634344\n",
            "Iteration 7968, Norm of Gradient: 0.00540802170325998, Cost (Train): 0.2872243134567764\n",
            "Iteration 7968, Cost (Validation): 0.3473217950922543\n",
            "Iteration 7969, Norm of Gradient: 0.005407469971390367, Cost (Train): 0.2872213895327743\n",
            "Iteration 7969, Cost (Validation): 0.34732254177277727\n",
            "Iteration 7970, Norm of Gradient: 0.0054069183486773345, Cost (Train): 0.2872184662052737\n",
            "Iteration 7970, Cost (Validation): 0.34732328870488566\n",
            "Iteration 7971, Norm of Gradient: 0.005406366835086151, Cost (Train): 0.28721554347409567\n",
            "Iteration 7971, Cost (Validation): 0.3473240358884622\n",
            "Iteration 7972, Norm of Gradient: 0.005405815430582101, Cost (Train): 0.2872126213390616\n",
            "Iteration 7972, Cost (Validation): 0.3473247833233895\n",
            "Iteration 7973, Norm of Gradient: 0.0054052641351304866, Cost (Train): 0.28720969979999283\n",
            "Iteration 7973, Cost (Validation): 0.34732553100955027\n",
            "Iteration 7974, Norm of Gradient: 0.005404712948696623, Cost (Train): 0.28720677885671064\n",
            "Iteration 7974, Cost (Validation): 0.3473262789468272\n",
            "Iteration 7975, Norm of Gradient: 0.0054041618712458404, Cost (Train): 0.2872038585090365\n",
            "Iteration 7975, Cost (Validation): 0.3473270271351033\n",
            "Iteration 7976, Norm of Gradient: 0.005403610902743487, Cost (Train): 0.28720093875679215\n",
            "Iteration 7976, Cost (Validation): 0.3473277755742611\n",
            "Iteration 7977, Norm of Gradient: 0.005403060043154923, Cost (Train): 0.28719801959979885\n",
            "Iteration 7977, Cost (Validation): 0.34732852426418387\n",
            "Iteration 7978, Norm of Gradient: 0.0054025092924455285, Cost (Train): 0.2871951010378786\n",
            "Iteration 7978, Cost (Validation): 0.34732927320475443\n",
            "Iteration 7979, Norm of Gradient: 0.0054019586505806945, Cost (Train): 0.287192183070853\n",
            "Iteration 7979, Cost (Validation): 0.3473300223958558\n",
            "Iteration 7980, Norm of Gradient: 0.00540140811752583, Cost (Train): 0.2871892656985439\n",
            "Iteration 7980, Cost (Validation): 0.34733077183737116\n",
            "Iteration 7981, Norm of Gradient: 0.005400857693246359, Cost (Train): 0.28718634892077327\n",
            "Iteration 7981, Cost (Validation): 0.3473315215291837\n",
            "Iteration 7982, Norm of Gradient: 0.005400307377707722, Cost (Train): 0.287183432737363\n",
            "Iteration 7982, Cost (Validation): 0.3473322714711764\n",
            "Iteration 7983, Norm of Gradient: 0.005399757170875372, Cost (Train): 0.28718051714813525\n",
            "Iteration 7983, Cost (Validation): 0.3473330216632327\n",
            "Iteration 7984, Norm of Gradient: 0.005399207072714779, Cost (Train): 0.28717760215291194\n",
            "Iteration 7984, Cost (Validation): 0.3473337721052359\n",
            "Iteration 7985, Norm of Gradient: 0.005398657083191429, Cost (Train): 0.28717468775151556\n",
            "Iteration 7985, Cost (Validation): 0.34733452279706933\n",
            "Iteration 7986, Norm of Gradient: 0.005398107202270822, Cost (Train): 0.28717177394376814\n",
            "Iteration 7986, Cost (Validation): 0.34733527373861645\n",
            "Iteration 7987, Norm of Gradient: 0.005397557429918475, Cost (Train): 0.2871688607294921\n",
            "Iteration 7987, Cost (Validation): 0.3473360249297608\n",
            "Iteration 7988, Norm of Gradient: 0.005397007766099919, Cost (Train): 0.2871659481085099\n",
            "Iteration 7988, Cost (Validation): 0.34733677637038574\n",
            "Iteration 7989, Norm of Gradient: 0.0053964582107807, Cost (Train): 0.287163036080644\n",
            "Iteration 7989, Cost (Validation): 0.347337528060375\n",
            "Iteration 7990, Norm of Gradient: 0.005395908763926383, Cost (Train): 0.28716012464571694\n",
            "Iteration 7990, Cost (Validation): 0.3473382799996122\n",
            "Iteration 7991, Norm of Gradient: 0.005395359425502541, Cost (Train): 0.2871572138035514\n",
            "Iteration 7991, Cost (Validation): 0.34733903218798107\n",
            "Iteration 7992, Norm of Gradient: 0.00539481019547477, Cost (Train): 0.2871543035539701\n",
            "Iteration 7992, Cost (Validation): 0.34733978462536536\n",
            "Iteration 7993, Norm of Gradient: 0.005394261073808677, Cost (Train): 0.28715139389679567\n",
            "Iteration 7993, Cost (Validation): 0.3473405373116489\n",
            "Iteration 7994, Norm of Gradient: 0.005393712060469886, Cost (Train): 0.2871484848318511\n",
            "Iteration 7994, Cost (Validation): 0.34734129024671545\n",
            "Iteration 7995, Norm of Gradient: 0.005393163155424033, Cost (Train): 0.2871455763589593\n",
            "Iteration 7995, Cost (Validation): 0.3473420434304491\n",
            "Iteration 7996, Norm of Gradient: 0.005392614358636776, Cost (Train): 0.28714266847794334\n",
            "Iteration 7996, Cost (Validation): 0.34734279686273367\n",
            "Iteration 7997, Norm of Gradient: 0.005392065670073781, Cost (Train): 0.2871397611886262\n",
            "Iteration 7997, Cost (Validation): 0.34734355054345334\n",
            "Iteration 7998, Norm of Gradient: 0.005391517089700733, Cost (Train): 0.287136854490831\n",
            "Iteration 7998, Cost (Validation): 0.3473443044724921\n",
            "Iteration 7999, Norm of Gradient: 0.005390968617483333, Cost (Train): 0.287133948384381\n",
            "Iteration 7999, Cost (Validation): 0.34734505864973414\n",
            "Iteration 8000, Norm of Gradient: 0.005390420253387294, Cost (Train): 0.28713104286909946\n",
            "Iteration 8000, Cost (Validation): 0.34734581307506357\n",
            "Iteration 8001, Norm of Gradient: 0.005389871997378347, Cost (Train): 0.28712813794480985\n",
            "Iteration 8001, Cost (Validation): 0.34734656774836487\n",
            "Iteration 8002, Norm of Gradient: 0.005389323849422237, Cost (Train): 0.28712523361133546\n",
            "Iteration 8002, Cost (Validation): 0.3473473226695221\n",
            "Iteration 8003, Norm of Gradient: 0.005388775809484724, Cost (Train): 0.28712232986849984\n",
            "Iteration 8003, Cost (Validation): 0.3473480778384198\n",
            "Iteration 8004, Norm of Gradient: 0.005388227877531586, Cost (Train): 0.28711942671612667\n",
            "Iteration 8004, Cost (Validation): 0.3473488332549423\n",
            "Iteration 8005, Norm of Gradient: 0.005387680053528613, Cost (Train): 0.2871165241540395\n",
            "Iteration 8005, Cost (Validation): 0.3473495889189741\n",
            "Iteration 8006, Norm of Gradient: 0.005387132337441608, Cost (Train): 0.28711362218206204\n",
            "Iteration 8006, Cost (Validation): 0.3473503448303997\n",
            "Iteration 8007, Norm of Gradient: 0.005386584729236396, Cost (Train): 0.2871107208000182\n",
            "Iteration 8007, Cost (Validation): 0.3473511009891037\n",
            "Iteration 8008, Norm of Gradient: 0.0053860372288788115, Cost (Train): 0.2871078200077317\n",
            "Iteration 8008, Cost (Validation): 0.3473518573949708\n",
            "Iteration 8009, Norm of Gradient: 0.005385489836334708, Cost (Train): 0.2871049198050266\n",
            "Iteration 8009, Cost (Validation): 0.3473526140478857\n",
            "Iteration 8010, Norm of Gradient: 0.005384942551569951, Cost (Train): 0.28710202019172687\n",
            "Iteration 8010, Cost (Validation): 0.347353370947733\n",
            "Iteration 8011, Norm of Gradient: 0.005384395374550421, Cost (Train): 0.2870991211676565\n",
            "Iteration 8011, Cost (Validation): 0.34735412809439764\n",
            "Iteration 8012, Norm of Gradient: 0.005383848305242019, Cost (Train): 0.28709622273263996\n",
            "Iteration 8012, Cost (Validation): 0.3473548854877644\n",
            "Iteration 8013, Norm of Gradient: 0.005383301343610653, Cost (Train): 0.2870933248865011\n",
            "Iteration 8013, Cost (Validation): 0.3473556431277183\n",
            "Iteration 8014, Norm of Gradient: 0.005382754489622252, Cost (Train): 0.2870904276290643\n",
            "Iteration 8014, Cost (Validation): 0.34735640101414417\n",
            "Iteration 8015, Norm of Gradient: 0.005382207743242759, Cost (Train): 0.2870875309601541\n",
            "Iteration 8015, Cost (Validation): 0.3473571591469271\n",
            "Iteration 8016, Norm of Gradient: 0.00538166110443813, Cost (Train): 0.28708463487959496\n",
            "Iteration 8016, Cost (Validation): 0.34735791752595224\n",
            "Iteration 8017, Norm of Gradient: 0.005381114573174339, Cost (Train): 0.2870817393872112\n",
            "Iteration 8017, Cost (Validation): 0.3473586761511046\n",
            "Iteration 8018, Norm of Gradient: 0.005380568149417373, Cost (Train): 0.2870788444828275\n",
            "Iteration 8018, Cost (Validation): 0.3473594350222694\n",
            "Iteration 8019, Norm of Gradient: 0.005380021833133235, Cost (Train): 0.2870759501662685\n",
            "Iteration 8019, Cost (Validation): 0.34736019413933195\n",
            "Iteration 8020, Norm of Gradient: 0.005379475624287943, Cost (Train): 0.28707305643735903\n",
            "Iteration 8020, Cost (Validation): 0.3473609535021775\n",
            "Iteration 8021, Norm of Gradient: 0.005378929522847529, Cost (Train): 0.28707016329592383\n",
            "Iteration 8021, Cost (Validation): 0.3473617131106913\n",
            "Iteration 8022, Norm of Gradient: 0.005378383528778041, Cost (Train): 0.28706727074178773\n",
            "Iteration 8022, Cost (Validation): 0.3473624729647589\n",
            "Iteration 8023, Norm of Gradient: 0.005377837642045543, Cost (Train): 0.28706437877477575\n",
            "Iteration 8023, Cost (Validation): 0.34736323306426564\n",
            "Iteration 8024, Norm of Gradient: 0.0053772918626161115, Cost (Train): 0.2870614873947129\n",
            "Iteration 8024, Cost (Validation): 0.3473639934090972\n",
            "Iteration 8025, Norm of Gradient: 0.005376746190455841, Cost (Train): 0.2870585966014242\n",
            "Iteration 8025, Cost (Validation): 0.34736475399913896\n",
            "Iteration 8026, Norm of Gradient: 0.005376200625530838, Cost (Train): 0.2870557063947349\n",
            "Iteration 8026, Cost (Validation): 0.34736551483427663\n",
            "Iteration 8027, Norm of Gradient: 0.0053756551678072265, Cost (Train): 0.28705281677447014\n",
            "Iteration 8027, Cost (Validation): 0.3473662759143959\n",
            "Iteration 8028, Norm of Gradient: 0.005375109817251145, Cost (Train): 0.2870499277404553\n",
            "Iteration 8028, Cost (Validation): 0.3473670372393825\n",
            "Iteration 8029, Norm of Gradient: 0.005374564573828745, Cost (Train): 0.28704703929251574\n",
            "Iteration 8029, Cost (Validation): 0.3473677988091221\n",
            "Iteration 8030, Norm of Gradient: 0.005374019437506194, Cost (Train): 0.28704415143047696\n",
            "Iteration 8030, Cost (Validation): 0.3473685606235007\n",
            "Iteration 8031, Norm of Gradient: 0.0053734744082496785, Cost (Train): 0.28704126415416437\n",
            "Iteration 8031, Cost (Validation): 0.347369322682404\n",
            "Iteration 8032, Norm of Gradient: 0.005372929486025392, Cost (Train): 0.2870383774634036\n",
            "Iteration 8032, Cost (Validation): 0.34737008498571814\n",
            "Iteration 8033, Norm of Gradient: 0.0053723846707995505, Cost (Train): 0.2870354913580204\n",
            "Iteration 8033, Cost (Validation): 0.34737084753332903\n",
            "Iteration 8034, Norm of Gradient: 0.005371839962538381, Cost (Train): 0.2870326058378404\n",
            "Iteration 8034, Cost (Validation): 0.3473716103251227\n",
            "Iteration 8035, Norm of Gradient: 0.005371295361208126, Cost (Train): 0.28702972090268947\n",
            "Iteration 8035, Cost (Validation): 0.3473723733609853\n",
            "Iteration 8036, Norm of Gradient: 0.005370750866775042, Cost (Train): 0.28702683655239336\n",
            "Iteration 8036, Cost (Validation): 0.34737313664080305\n",
            "Iteration 8037, Norm of Gradient: 0.005370206479205402, Cost (Train): 0.28702395278677817\n",
            "Iteration 8037, Cost (Validation): 0.34737390016446196\n",
            "Iteration 8038, Norm of Gradient: 0.005369662198465495, Cost (Train): 0.28702106960566987\n",
            "Iteration 8038, Cost (Validation): 0.3473746639318485\n",
            "Iteration 8039, Norm of Gradient: 0.0053691180245216226, Cost (Train): 0.28701818700889453\n",
            "Iteration 8039, Cost (Validation): 0.3473754279428489\n",
            "Iteration 8040, Norm of Gradient: 0.005368573957340101, Cost (Train): 0.28701530499627836\n",
            "Iteration 8040, Cost (Validation): 0.3473761921973496\n",
            "Iteration 8041, Norm of Gradient: 0.005368029996887264, Cost (Train): 0.2870124235676475\n",
            "Iteration 8041, Cost (Validation): 0.34737695669523694\n",
            "Iteration 8042, Norm of Gradient: 0.005367486143129458, Cost (Train): 0.28700954272282836\n",
            "Iteration 8042, Cost (Validation): 0.3473777214363975\n",
            "Iteration 8043, Norm of Gradient: 0.0053669423960330444, Cost (Train): 0.2870066624616472\n",
            "Iteration 8043, Cost (Validation): 0.3473784864207177\n",
            "Iteration 8044, Norm of Gradient: 0.0053663987555644, Cost (Train): 0.28700378278393057\n",
            "Iteration 8044, Cost (Validation): 0.34737925164808425\n",
            "Iteration 8045, Norm of Gradient: 0.005365855221689917, Cost (Train): 0.2870009036895049\n",
            "Iteration 8045, Cost (Validation): 0.3473800171183837\n",
            "Iteration 8046, Norm of Gradient: 0.005365311794376003, Cost (Train): 0.2869980251781968\n",
            "Iteration 8046, Cost (Validation): 0.34738078283150287\n",
            "Iteration 8047, Norm of Gradient: 0.005364768473589075, Cost (Train): 0.28699514724983294\n",
            "Iteration 8047, Cost (Validation): 0.34738154878732835\n",
            "Iteration 8048, Norm of Gradient: 0.005364225259295573, Cost (Train): 0.2869922699042401\n",
            "Iteration 8048, Cost (Validation): 0.34738231498574706\n",
            "Iteration 8049, Norm of Gradient: 0.005363682151461948, Cost (Train): 0.286989393141245\n",
            "Iteration 8049, Cost (Validation): 0.3473830814266458\n",
            "Iteration 8050, Norm of Gradient: 0.005363139150054665, Cost (Train): 0.28698651696067456\n",
            "Iteration 8050, Cost (Validation): 0.3473838481099116\n",
            "Iteration 8051, Norm of Gradient: 0.005362596255040202, Cost (Train): 0.2869836413623556\n",
            "Iteration 8051, Cost (Validation): 0.3473846150354313\n",
            "Iteration 8052, Norm of Gradient: 0.005362053466385058, Cost (Train): 0.2869807663461153\n",
            "Iteration 8052, Cost (Validation): 0.34738538220309184\n",
            "Iteration 8053, Norm of Gradient: 0.005361510784055742, Cost (Train): 0.28697789191178064\n",
            "Iteration 8053, Cost (Validation): 0.3473861496127805\n",
            "Iteration 8054, Norm of Gradient: 0.005360968208018777, Cost (Train): 0.28697501805917874\n",
            "Iteration 8054, Cost (Validation): 0.3473869172643843\n",
            "Iteration 8055, Norm of Gradient: 0.005360425738240705, Cost (Train): 0.28697214478813693\n",
            "Iteration 8055, Cost (Validation): 0.34738768515779045\n",
            "Iteration 8056, Norm of Gradient: 0.005359883374688079, Cost (Train): 0.28696927209848244\n",
            "Iteration 8056, Cost (Validation): 0.34738845329288615\n",
            "Iteration 8057, Norm of Gradient: 0.005359341117327469, Cost (Train): 0.28696639999004264\n",
            "Iteration 8057, Cost (Validation): 0.3473892216695587\n",
            "Iteration 8058, Norm of Gradient: 0.005358798966125458, Cost (Train): 0.286963528462645\n",
            "Iteration 8058, Cost (Validation): 0.34738999028769546\n",
            "Iteration 8059, Norm of Gradient: 0.005358256921048647, Cost (Train): 0.28696065751611693\n",
            "Iteration 8059, Cost (Validation): 0.34739075914718376\n",
            "Iteration 8060, Norm of Gradient: 0.005357714982063645, Cost (Train): 0.2869577871502861\n",
            "Iteration 8060, Cost (Validation): 0.3473915282479111\n",
            "Iteration 8061, Norm of Gradient: 0.005357173149137084, Cost (Train): 0.2869549173649801\n",
            "Iteration 8061, Cost (Validation): 0.3473922975897649\n",
            "Iteration 8062, Norm of Gradient: 0.005356631422235605, Cost (Train): 0.28695204816002656\n",
            "Iteration 8062, Cost (Validation): 0.3473930671726329\n",
            "Iteration 8063, Norm of Gradient: 0.005356089801325865, Cost (Train): 0.28694917953525345\n",
            "Iteration 8063, Cost (Validation): 0.3473938369964026\n",
            "Iteration 8064, Norm of Gradient: 0.005355548286374539, Cost (Train): 0.2869463114904885\n",
            "Iteration 8064, Cost (Validation): 0.34739460706096154\n",
            "Iteration 8065, Norm of Gradient: 0.005355006877348309, Cost (Train): 0.28694344402555966\n",
            "Iteration 8065, Cost (Validation): 0.3473953773661975\n",
            "Iteration 8066, Norm of Gradient: 0.005354465574213881, Cost (Train): 0.2869405771402949\n",
            "Iteration 8066, Cost (Validation): 0.34739614791199835\n",
            "Iteration 8067, Norm of Gradient: 0.00535392437693797, Cost (Train): 0.2869377108345224\n",
            "Iteration 8067, Cost (Validation): 0.34739691869825184\n",
            "Iteration 8068, Norm of Gradient: 0.005353383285487305, Cost (Train): 0.2869348451080701\n",
            "Iteration 8068, Cost (Validation): 0.34739768972484575\n",
            "Iteration 8069, Norm of Gradient: 0.005352842299828634, Cost (Train): 0.2869319799607664\n",
            "Iteration 8069, Cost (Validation): 0.34739846099166816\n",
            "Iteration 8070, Norm of Gradient: 0.005352301419928715, Cost (Train): 0.2869291153924395\n",
            "Iteration 8070, Cost (Validation): 0.34739923249860694\n",
            "Iteration 8071, Norm of Gradient: 0.005351760645754323, Cost (Train): 0.2869262514029176\n",
            "Iteration 8071, Cost (Validation): 0.3474000042455501\n",
            "Iteration 8072, Norm of Gradient: 0.005351219977272249, Cost (Train): 0.2869233879920293\n",
            "Iteration 8072, Cost (Validation): 0.3474007762323858\n",
            "Iteration 8073, Norm of Gradient: 0.005350679414449297, Cost (Train): 0.28692052515960315\n",
            "Iteration 8073, Cost (Validation): 0.3474015484590022\n",
            "Iteration 8074, Norm of Gradient: 0.005350138957252283, Cost (Train): 0.2869176629054674\n",
            "Iteration 8074, Cost (Validation): 0.34740232092528733\n",
            "Iteration 8075, Norm of Gradient: 0.005349598605648042, Cost (Train): 0.28691480122945096\n",
            "Iteration 8075, Cost (Validation): 0.34740309363112953\n",
            "Iteration 8076, Norm of Gradient: 0.005349058359603422, Cost (Train): 0.2869119401313824\n",
            "Iteration 8076, Cost (Validation): 0.3474038665764172\n",
            "Iteration 8077, Norm of Gradient: 0.005348518219085284, Cost (Train): 0.2869090796110904\n",
            "Iteration 8077, Cost (Validation): 0.34740463976103847\n",
            "Iteration 8078, Norm of Gradient: 0.005347978184060507, Cost (Train): 0.286906219668404\n",
            "Iteration 8078, Cost (Validation): 0.3474054131848819\n",
            "Iteration 8079, Norm of Gradient: 0.005347438254495981, Cost (Train): 0.28690336030315194\n",
            "Iteration 8079, Cost (Validation): 0.34740618684783586\n",
            "Iteration 8080, Norm of Gradient: 0.0053468984303586126, Cost (Train): 0.2869005015151633\n",
            "Iteration 8080, Cost (Validation): 0.3474069607497889\n",
            "Iteration 8081, Norm of Gradient: 0.005346358711615321, Cost (Train): 0.2868976433042669\n",
            "Iteration 8081, Cost (Validation): 0.3474077348906295\n",
            "Iteration 8082, Norm of Gradient: 0.005345819098233045, Cost (Train): 0.28689478567029214\n",
            "Iteration 8082, Cost (Validation): 0.3474085092702463\n",
            "Iteration 8083, Norm of Gradient: 0.00534527959017873, Cost (Train): 0.2868919286130681\n",
            "Iteration 8083, Cost (Validation): 0.3474092838885281\n",
            "Iteration 8084, Norm of Gradient: 0.005344740187419342, Cost (Train): 0.28688907213242393\n",
            "Iteration 8084, Cost (Validation): 0.3474100587453633\n",
            "Iteration 8085, Norm of Gradient: 0.0053442008899218605, Cost (Train): 0.286886216228189\n",
            "Iteration 8085, Cost (Validation): 0.34741083384064103\n",
            "Iteration 8086, Norm of Gradient: 0.005343661697653278, Cost (Train): 0.28688336090019273\n",
            "Iteration 8086, Cost (Validation): 0.34741160917424985\n",
            "Iteration 8087, Norm of Gradient: 0.005343122610580602, Cost (Train): 0.28688050614826455\n",
            "Iteration 8087, Cost (Validation): 0.3474123847460787\n",
            "Iteration 8088, Norm of Gradient: 0.0053425836286708555, Cost (Train): 0.286877651972234\n",
            "Iteration 8088, Cost (Validation): 0.34741316055601656\n",
            "Iteration 8089, Norm of Gradient: 0.005342044751891074, Cost (Train): 0.28687479837193075\n",
            "Iteration 8089, Cost (Validation): 0.3474139366039523\n",
            "Iteration 8090, Norm of Gradient: 0.00534150598020831, Cost (Train): 0.2868719453471843\n",
            "Iteration 8090, Cost (Validation): 0.34741471288977505\n",
            "Iteration 8091, Norm of Gradient: 0.0053409673135896285, Cost (Train): 0.28686909289782453\n",
            "Iteration 8091, Cost (Validation): 0.3474154894133738\n",
            "Iteration 8092, Norm of Gradient: 0.005340428752002109, Cost (Train): 0.2868662410236811\n",
            "Iteration 8092, Cost (Validation): 0.3474162661746378\n",
            "Iteration 8093, Norm of Gradient: 0.005339890295412848, Cost (Train): 0.28686338972458403\n",
            "Iteration 8093, Cost (Validation): 0.3474170431734561\n",
            "Iteration 8094, Norm of Gradient: 0.005339351943788952, Cost (Train): 0.2868605390003631\n",
            "Iteration 8094, Cost (Validation): 0.34741782040971797\n",
            "Iteration 8095, Norm of Gradient: 0.005338813697097546, Cost (Train): 0.28685768885084856\n",
            "Iteration 8095, Cost (Validation): 0.34741859788331275\n",
            "Iteration 8096, Norm of Gradient: 0.0053382755553057675, Cost (Train): 0.28685483927587024\n",
            "Iteration 8096, Cost (Validation): 0.34741937559412983\n",
            "Iteration 8097, Norm of Gradient: 0.005337737518380768, Cost (Train): 0.28685199027525843\n",
            "Iteration 8097, Cost (Validation): 0.3474201535420584\n",
            "Iteration 8098, Norm of Gradient: 0.0053371995862897155, Cost (Train): 0.2868491418488433\n",
            "Iteration 8098, Cost (Validation): 0.3474209317269881\n",
            "Iteration 8099, Norm of Gradient: 0.0053366617589997905, Cost (Train): 0.28684629399645517\n",
            "Iteration 8099, Cost (Validation): 0.34742171014880835\n",
            "Iteration 8100, Norm of Gradient: 0.005336124036478189, Cost (Train): 0.28684344671792433\n",
            "Iteration 8100, Cost (Validation): 0.3474224888074086\n",
            "Iteration 8101, Norm of Gradient: 0.005335586418692119, Cost (Train): 0.28684060001308126\n",
            "Iteration 8101, Cost (Validation): 0.3474232677026786\n",
            "Iteration 8102, Norm of Gradient: 0.005335048905608807, Cost (Train): 0.28683775388175636\n",
            "Iteration 8102, Cost (Validation): 0.3474240468345081\n",
            "Iteration 8103, Norm of Gradient: 0.00533451149719549, Cost (Train): 0.2868349083237804\n",
            "Iteration 8103, Cost (Validation): 0.34742482620278653\n",
            "Iteration 8104, Norm of Gradient: 0.005333974193419421, Cost (Train): 0.2868320633389837\n",
            "Iteration 8104, Cost (Validation): 0.3474256058074037\n",
            "Iteration 8105, Norm of Gradient: 0.0053334369942478685, Cost (Train): 0.28682921892719726\n",
            "Iteration 8105, Cost (Validation): 0.3474263856482496\n",
            "Iteration 8106, Norm of Gradient: 0.005332899899648113, Cost (Train): 0.28682637508825165\n",
            "Iteration 8106, Cost (Validation): 0.347427165725214\n",
            "Iteration 8107, Norm of Gradient: 0.0053323629095874524, Cost (Train): 0.2868235318219778\n",
            "Iteration 8107, Cost (Validation): 0.3474279460381867\n",
            "Iteration 8108, Norm of Gradient: 0.005331826024033194, Cost (Train): 0.2868206891282066\n",
            "Iteration 8108, Cost (Validation): 0.3474287265870578\n",
            "Iteration 8109, Norm of Gradient: 0.005331289242952666, Cost (Train): 0.28681784700676893\n",
            "Iteration 8109, Cost (Validation): 0.3474295073717173\n",
            "Iteration 8110, Norm of Gradient: 0.005330752566313203, Cost (Train): 0.28681500545749594\n",
            "Iteration 8110, Cost (Validation): 0.3474302883920552\n",
            "Iteration 8111, Norm of Gradient: 0.005330215994082163, Cost (Train): 0.2868121644802188\n",
            "Iteration 8111, Cost (Validation): 0.3474310696479616\n",
            "Iteration 8112, Norm of Gradient: 0.00532967952622691, Cost (Train): 0.28680932407476845\n",
            "Iteration 8112, Cost (Validation): 0.3474318511393268\n",
            "Iteration 8113, Norm of Gradient: 0.005329143162714827, Cost (Train): 0.2868064842409764\n",
            "Iteration 8113, Cost (Validation): 0.34743263286604087\n",
            "Iteration 8114, Norm of Gradient: 0.005328606903513309, Cost (Train): 0.2868036449786738\n",
            "Iteration 8114, Cost (Validation): 0.34743341482799417\n",
            "Iteration 8115, Norm of Gradient: 0.005328070748589769, Cost (Train): 0.286800806287692\n",
            "Iteration 8115, Cost (Validation): 0.3474341970250771\n",
            "Iteration 8116, Norm of Gradient: 0.00532753469791163, Cost (Train): 0.28679796816786257\n",
            "Iteration 8116, Cost (Validation): 0.3474349794571798\n",
            "Iteration 8117, Norm of Gradient: 0.005326998751446331, Cost (Train): 0.28679513061901696\n",
            "Iteration 8117, Cost (Validation): 0.3474357621241929\n",
            "Iteration 8118, Norm of Gradient: 0.005326462909161325, Cost (Train): 0.28679229364098685\n",
            "Iteration 8118, Cost (Validation): 0.34743654502600685\n",
            "Iteration 8119, Norm of Gradient: 0.00532592717102408, Cost (Train): 0.2867894572336038\n",
            "Iteration 8119, Cost (Validation): 0.3474373281625121\n",
            "Iteration 8120, Norm of Gradient: 0.005325391537002077, Cost (Train): 0.2867866213966995\n",
            "Iteration 8120, Cost (Validation): 0.3474381115335994\n",
            "Iteration 8121, Norm of Gradient: 0.0053248560070628115, Cost (Train): 0.2867837861301058\n",
            "Iteration 8121, Cost (Validation): 0.34743889513915915\n",
            "Iteration 8122, Norm of Gradient: 0.005324320581173794, Cost (Train): 0.2867809514336547\n",
            "Iteration 8122, Cost (Validation): 0.3474396789790821\n",
            "Iteration 8123, Norm of Gradient: 0.005323785259302549, Cost (Train): 0.2867781173071778\n",
            "Iteration 8123, Cost (Validation): 0.3474404630532592\n",
            "Iteration 8124, Norm of Gradient: 0.0053232500414166165, Cost (Train): 0.2867752837505074\n",
            "Iteration 8124, Cost (Validation): 0.34744124736158094\n",
            "Iteration 8125, Norm of Gradient: 0.005322714927483545, Cost (Train): 0.2867724507634754\n",
            "Iteration 8125, Cost (Validation): 0.3474420319039383\n",
            "Iteration 8126, Norm of Gradient: 0.005322179917470905, Cost (Train): 0.286769618345914\n",
            "Iteration 8126, Cost (Validation): 0.3474428166802222\n",
            "Iteration 8127, Norm of Gradient: 0.005321645011346276, Cost (Train): 0.28676678649765536\n",
            "Iteration 8127, Cost (Validation): 0.34744360169032357\n",
            "Iteration 8128, Norm of Gradient: 0.005321110209077253, Cost (Train): 0.2867639552185317\n",
            "Iteration 8128, Cost (Validation): 0.3474443869341334\n",
            "Iteration 8129, Norm of Gradient: 0.005320575510631446, Cost (Train): 0.28676112450837554\n",
            "Iteration 8129, Cost (Validation): 0.3474451724115426\n",
            "Iteration 8130, Norm of Gradient: 0.0053200409159764785, Cost (Train): 0.28675829436701905\n",
            "Iteration 8130, Cost (Validation): 0.3474459581224424\n",
            "Iteration 8131, Norm of Gradient: 0.005319506425079988, Cost (Train): 0.28675546479429487\n",
            "Iteration 8131, Cost (Validation): 0.34744674406672393\n",
            "Iteration 8132, Norm of Gradient: 0.005318972037909626, Cost (Train): 0.2867526357900354\n",
            "Iteration 8132, Cost (Validation): 0.34744753024427827\n",
            "Iteration 8133, Norm of Gradient: 0.005318437754433058, Cost (Train): 0.28674980735407335\n",
            "Iteration 8133, Cost (Validation): 0.3474483166549969\n",
            "Iteration 8134, Norm of Gradient: 0.005317903574617965, Cost (Train): 0.28674697948624145\n",
            "Iteration 8134, Cost (Validation): 0.34744910329877093\n",
            "Iteration 8135, Norm of Gradient: 0.005317369498432039, Cost (Train): 0.28674415218637217\n",
            "Iteration 8135, Cost (Validation): 0.34744989017549177\n",
            "Iteration 8136, Norm of Gradient: 0.005316835525842992, Cost (Train): 0.28674132545429865\n",
            "Iteration 8136, Cost (Validation): 0.3474506772850508\n",
            "Iteration 8137, Norm of Gradient: 0.005316301656818543, Cost (Train): 0.28673849928985357\n",
            "Iteration 8137, Cost (Validation): 0.3474514646273395\n",
            "Iteration 8138, Norm of Gradient: 0.00531576789132643, Cost (Train): 0.28673567369286995\n",
            "Iteration 8138, Cost (Validation): 0.3474522522022494\n",
            "Iteration 8139, Norm of Gradient: 0.005315234229334402, Cost (Train): 0.28673284866318066\n",
            "Iteration 8139, Cost (Validation): 0.3474530400096719\n",
            "Iteration 8140, Norm of Gradient: 0.0053147006708102245, Cost (Train): 0.28673002420061905\n",
            "Iteration 8140, Cost (Validation): 0.3474538280494988\n",
            "Iteration 8141, Norm of Gradient: 0.005314167215721677, Cost (Train): 0.28672720030501797\n",
            "Iteration 8141, Cost (Validation): 0.34745461632162167\n",
            "Iteration 8142, Norm of Gradient: 0.0053136338640365505, Cost (Train): 0.2867243769762108\n",
            "Iteration 8142, Cost (Validation): 0.3474554048259321\n",
            "Iteration 8143, Norm of Gradient: 0.005313100615722652, Cost (Train): 0.28672155421403084\n",
            "Iteration 8143, Cost (Validation): 0.3474561935623221\n",
            "Iteration 8144, Norm of Gradient: 0.005312567470747803, Cost (Train): 0.28671873201831133\n",
            "Iteration 8144, Cost (Validation): 0.3474569825306833\n",
            "Iteration 8145, Norm of Gradient: 0.005312034429079839, Cost (Train): 0.28671591038888566\n",
            "Iteration 8145, Cost (Validation): 0.3474577717309075\n",
            "Iteration 8146, Norm of Gradient: 0.0053115014906866065, Cost (Train): 0.2867130893255875\n",
            "Iteration 8146, Cost (Validation): 0.34745856116288687\n",
            "Iteration 8147, Norm of Gradient: 0.00531096865553597, Cost (Train): 0.28671026882825024\n",
            "Iteration 8147, Cost (Validation): 0.3474593508265131\n",
            "Iteration 8148, Norm of Gradient: 0.005310435923595806, Cost (Train): 0.2867074488967075\n",
            "Iteration 8148, Cost (Validation): 0.34746014072167825\n",
            "Iteration 8149, Norm of Gradient: 0.005309903294834006, Cost (Train): 0.28670462953079306\n",
            "Iteration 8149, Cost (Validation): 0.3474609308482745\n",
            "Iteration 8150, Norm of Gradient: 0.005309370769218472, Cost (Train): 0.28670181073034057\n",
            "Iteration 8150, Cost (Validation): 0.34746172120619384\n",
            "Iteration 8151, Norm of Gradient: 0.005308838346717125, Cost (Train): 0.2866989924951839\n",
            "Iteration 8151, Cost (Validation): 0.34746251179532855\n",
            "Iteration 8152, Norm of Gradient: 0.005308306027297899, Cost (Train): 0.28669617482515697\n",
            "Iteration 8152, Cost (Validation): 0.3474633026155707\n",
            "Iteration 8153, Norm of Gradient: 0.0053077738109287365, Cost (Train): 0.28669335772009363\n",
            "Iteration 8153, Cost (Validation): 0.34746409366681263\n",
            "Iteration 8154, Norm of Gradient: 0.005307241697577604, Cost (Train): 0.28669054117982795\n",
            "Iteration 8154, Cost (Validation): 0.34746488494894673\n",
            "Iteration 8155, Norm of Gradient: 0.005306709687212471, Cost (Train): 0.28668772520419405\n",
            "Iteration 8155, Cost (Validation): 0.34746567646186516\n",
            "Iteration 8156, Norm of Gradient: 0.005306177779801329, Cost (Train): 0.2866849097930261\n",
            "Iteration 8156, Cost (Validation): 0.34746646820546045\n",
            "Iteration 8157, Norm of Gradient: 0.005305645975312181, Cost (Train): 0.28668209494615815\n",
            "Iteration 8157, Cost (Validation): 0.3474672601796252\n",
            "Iteration 8158, Norm of Gradient: 0.00530511427371304, Cost (Train): 0.28667928066342463\n",
            "Iteration 8158, Cost (Validation): 0.34746805238425166\n",
            "Iteration 8159, Norm of Gradient: 0.00530458267497194, Cost (Train): 0.28667646694465987\n",
            "Iteration 8159, Cost (Validation): 0.3474688448192326\n",
            "Iteration 8160, Norm of Gradient: 0.0053040511790569235, Cost (Train): 0.2866736537896983\n",
            "Iteration 8160, Cost (Validation): 0.3474696374844605\n",
            "Iteration 8161, Norm of Gradient: 0.0053035197859360485, Cost (Train): 0.2866708411983744\n",
            "Iteration 8161, Cost (Validation): 0.3474704303798281\n",
            "Iteration 8162, Norm of Gradient: 0.005302988495577388, Cost (Train): 0.2866680291705227\n",
            "Iteration 8162, Cost (Validation): 0.34747122350522813\n",
            "Iteration 8163, Norm of Gradient: 0.0053024573079490265, Cost (Train): 0.28666521770597775\n",
            "Iteration 8163, Cost (Validation): 0.34747201686055323\n",
            "Iteration 8164, Norm of Gradient: 0.005301926223019065, Cost (Train): 0.28666240680457433\n",
            "Iteration 8164, Cost (Validation): 0.34747281044569633\n",
            "Iteration 8165, Norm of Gradient: 0.005301395240755619, Cost (Train): 0.2866595964661472\n",
            "Iteration 8165, Cost (Validation): 0.3474736042605504\n",
            "Iteration 8166, Norm of Gradient: 0.005300864361126813, Cost (Train): 0.28665678669053113\n",
            "Iteration 8166, Cost (Validation): 0.3474743983050081\n",
            "Iteration 8167, Norm of Gradient: 0.00530033358410079, Cost (Train): 0.28665397747756105\n",
            "Iteration 8167, Cost (Validation): 0.3474751925789625\n",
            "Iteration 8168, Norm of Gradient: 0.005299802909645705, Cost (Train): 0.28665116882707187\n",
            "Iteration 8168, Cost (Validation): 0.3474759870823067\n",
            "Iteration 8169, Norm of Gradient: 0.005299272337729728, Cost (Train): 0.2866483607388986\n",
            "Iteration 8169, Cost (Validation): 0.34747678181493363\n",
            "Iteration 8170, Norm of Gradient: 0.005298741868321039, Cost (Train): 0.2866455532128763\n",
            "Iteration 8170, Cost (Validation): 0.3474775767767365\n",
            "Iteration 8171, Norm of Gradient: 0.005298211501387839, Cost (Train): 0.2866427462488403\n",
            "Iteration 8171, Cost (Validation): 0.3474783719676085\n",
            "Iteration 8172, Norm of Gradient: 0.005297681236898335, Cost (Train): 0.2866399398466256\n",
            "Iteration 8172, Cost (Validation): 0.3474791673874428\n",
            "Iteration 8173, Norm of Gradient: 0.005297151074820755, Cost (Train): 0.28663713400606755\n",
            "Iteration 8173, Cost (Validation): 0.34747996303613266\n",
            "Iteration 8174, Norm of Gradient: 0.005296621015123333, Cost (Train): 0.2866343287270015\n",
            "Iteration 8174, Cost (Validation): 0.34748075891357133\n",
            "Iteration 8175, Norm of Gradient: 0.0052960910577743246, Cost (Train): 0.2866315240092629\n",
            "Iteration 8175, Cost (Validation): 0.3474815550196523\n",
            "Iteration 8176, Norm of Gradient: 0.005295561202741994, Cost (Train): 0.2866287198526872\n",
            "Iteration 8176, Cost (Validation): 0.3474823513542689\n",
            "Iteration 8177, Norm of Gradient: 0.005295031449994622, Cost (Train): 0.28662591625710987\n",
            "Iteration 8177, Cost (Validation): 0.34748314791731466\n",
            "Iteration 8178, Norm of Gradient: 0.0052945017995005, Cost (Train): 0.28662311322236667\n",
            "Iteration 8178, Cost (Validation): 0.347483944708683\n",
            "Iteration 8179, Norm of Gradient: 0.005293972251227938, Cost (Train): 0.2866203107482933\n",
            "Iteration 8179, Cost (Validation): 0.34748474172826754\n",
            "Iteration 8180, Norm of Gradient: 0.005293442805145253, Cost (Train): 0.2866175088347253\n",
            "Iteration 8180, Cost (Validation): 0.34748553897596196\n",
            "Iteration 8181, Norm of Gradient: 0.005292913461220783, Cost (Train): 0.2866147074814986\n",
            "Iteration 8181, Cost (Validation): 0.34748633645165977\n",
            "Iteration 8182, Norm of Gradient: 0.0052923842194228755, Cost (Train): 0.28661190668844916\n",
            "Iteration 8182, Cost (Validation): 0.3474871341552548\n",
            "Iteration 8183, Norm of Gradient: 0.005291855079719892, Cost (Train): 0.28660910645541277\n",
            "Iteration 8183, Cost (Validation): 0.3474879320866408\n",
            "Iteration 8184, Norm of Gradient: 0.005291326042080209, Cost (Train): 0.28660630678222554\n",
            "Iteration 8184, Cost (Validation): 0.34748873024571153\n",
            "Iteration 8185, Norm of Gradient: 0.005290797106472215, Cost (Train): 0.2866035076687235\n",
            "Iteration 8185, Cost (Validation): 0.34748952863236093\n",
            "Iteration 8186, Norm of Gradient: 0.005290268272864314, Cost (Train): 0.2866007091147428\n",
            "Iteration 8186, Cost (Validation): 0.3474903272464828\n",
            "Iteration 8187, Norm of Gradient: 0.005289739541224925, Cost (Train): 0.2865979111201196\n",
            "Iteration 8187, Cost (Validation): 0.34749112608797117\n",
            "Iteration 8188, Norm of Gradient: 0.0052892109115224755, Cost (Train): 0.2865951136846903\n",
            "Iteration 8188, Cost (Validation): 0.3474919251567201\n",
            "Iteration 8189, Norm of Gradient: 0.005288682383725411, Cost (Train): 0.2865923168082911\n",
            "Iteration 8189, Cost (Validation): 0.3474927244526236\n",
            "Iteration 8190, Norm of Gradient: 0.005288153957802189, Cost (Train): 0.28658952049075853\n",
            "Iteration 8190, Cost (Validation): 0.3474935239755757\n",
            "Iteration 8191, Norm of Gradient: 0.005287625633721284, Cost (Train): 0.2865867247319289\n",
            "Iteration 8191, Cost (Validation): 0.34749432372547073\n",
            "Iteration 8192, Norm of Gradient: 0.005287097411451177, Cost (Train): 0.28658392953163886\n",
            "Iteration 8192, Cost (Validation): 0.3474951237022027\n",
            "Iteration 8193, Norm of Gradient: 0.005286569290960372, Cost (Train): 0.28658113488972503\n",
            "Iteration 8193, Cost (Validation): 0.3474959239056661\n",
            "Iteration 8194, Norm of Gradient: 0.005286041272217377, Cost (Train): 0.28657834080602396\n",
            "Iteration 8194, Cost (Validation): 0.347496724335755\n",
            "Iteration 8195, Norm of Gradient: 0.005285513355190722, Cost (Train): 0.28657554728037254\n",
            "Iteration 8195, Cost (Validation): 0.34749752499236386\n",
            "Iteration 8196, Norm of Gradient: 0.005284985539848944, Cost (Train): 0.28657275431260737\n",
            "Iteration 8196, Cost (Validation): 0.34749832587538715\n",
            "Iteration 8197, Norm of Gradient: 0.005284457826160598, Cost (Train): 0.2865699619025655\n",
            "Iteration 8197, Cost (Validation): 0.3474991269847192\n",
            "Iteration 8198, Norm of Gradient: 0.005283930214094251, Cost (Train): 0.2865671700500837\n",
            "Iteration 8198, Cost (Validation): 0.3474999283202546\n",
            "Iteration 8199, Norm of Gradient: 0.005283402703618485, Cost (Train): 0.2865643787549991\n",
            "Iteration 8199, Cost (Validation): 0.3475007298818879\n",
            "Iteration 8200, Norm of Gradient: 0.005282875294701893, Cost (Train): 0.28656158801714876\n",
            "Iteration 8200, Cost (Validation): 0.3475015316695137\n",
            "Iteration 8201, Norm of Gradient: 0.0052823479873130835, Cost (Train): 0.2865587978363696\n",
            "Iteration 8201, Cost (Validation): 0.3475023336830266\n",
            "Iteration 8202, Norm of Gradient: 0.005281820781420679, Cost (Train): 0.28655600821249905\n",
            "Iteration 8202, Cost (Validation): 0.3475031359223213\n",
            "Iteration 8203, Norm of Gradient: 0.0052812936769933115, Cost (Train): 0.2865532191453742\n",
            "Iteration 8203, Cost (Validation): 0.34750393838729254\n",
            "Iteration 8204, Norm of Gradient: 0.005280766673999634, Cost (Train): 0.2865504306348324\n",
            "Iteration 8204, Cost (Validation): 0.3475047410778352\n",
            "Iteration 8205, Norm of Gradient: 0.0052802397724083065, Cost (Train): 0.28654764268071115\n",
            "Iteration 8205, Cost (Validation): 0.34750554399384415\n",
            "Iteration 8206, Norm of Gradient: 0.005279712972188006, Cost (Train): 0.28654485528284773\n",
            "Iteration 8206, Cost (Validation): 0.3475063471352141\n",
            "Iteration 8207, Norm of Gradient: 0.00527918627330742, Cost (Train): 0.2865420684410798\n",
            "Iteration 8207, Cost (Validation): 0.3475071505018403\n",
            "Iteration 8208, Norm of Gradient: 0.005278659675735254, Cost (Train): 0.2865392821552448\n",
            "Iteration 8208, Cost (Validation): 0.34750795409361734\n",
            "Iteration 8209, Norm of Gradient: 0.005278133179440222, Cost (Train): 0.28653649642518053\n",
            "Iteration 8209, Cost (Validation): 0.3475087579104406\n",
            "Iteration 8210, Norm of Gradient: 0.005277606784391056, Cost (Train): 0.2865337112507247\n",
            "Iteration 8210, Cost (Validation): 0.34750956195220506\n",
            "Iteration 8211, Norm of Gradient: 0.005277080490556499, Cost (Train): 0.2865309266317149\n",
            "Iteration 8211, Cost (Validation): 0.3475103662188058\n",
            "Iteration 8212, Norm of Gradient: 0.005276554297905308, Cost (Train): 0.2865281425679891\n",
            "Iteration 8212, Cost (Validation): 0.34751117071013815\n",
            "Iteration 8213, Norm of Gradient: 0.0052760282064062535, Cost (Train): 0.28652535905938514\n",
            "Iteration 8213, Cost (Validation): 0.3475119754260972\n",
            "Iteration 8214, Norm of Gradient: 0.0052755022160281195, Cost (Train): 0.286522576105741\n",
            "Iteration 8214, Cost (Validation): 0.34751278036657834\n",
            "Iteration 8215, Norm of Gradient: 0.005274976326739705, Cost (Train): 0.28651979370689484\n",
            "Iteration 8215, Cost (Validation): 0.3475135855314768\n",
            "Iteration 8216, Norm of Gradient: 0.005274450538509818, Cost (Train): 0.28651701186268463\n",
            "Iteration 8216, Cost (Validation): 0.34751439092068814\n",
            "Iteration 8217, Norm of Gradient: 0.005273924851307287, Cost (Train): 0.28651423057294856\n",
            "Iteration 8217, Cost (Validation): 0.3475151965341076\n",
            "Iteration 8218, Norm of Gradient: 0.005273399265100945, Cost (Train): 0.28651144983752486\n",
            "Iteration 8218, Cost (Validation): 0.34751600237163077\n",
            "Iteration 8219, Norm of Gradient: 0.005272873779859649, Cost (Train): 0.2865086696562518\n",
            "Iteration 8219, Cost (Validation): 0.3475168084331531\n",
            "Iteration 8220, Norm of Gradient: 0.005272348395552261, Cost (Train): 0.28650589002896776\n",
            "Iteration 8220, Cost (Validation): 0.34751761471857023\n",
            "Iteration 8221, Norm of Gradient: 0.005271823112147658, Cost (Train): 0.28650311095551123\n",
            "Iteration 8221, Cost (Validation): 0.3475184212277779\n",
            "Iteration 8222, Norm of Gradient: 0.0052712979296147345, Cost (Train): 0.2865003324357206\n",
            "Iteration 8222, Cost (Validation): 0.3475192279606716\n",
            "Iteration 8223, Norm of Gradient: 0.0052707728479223954, Cost (Train): 0.2864975544694344\n",
            "Iteration 8223, Cost (Validation): 0.3475200349171471\n",
            "Iteration 8224, Norm of Gradient: 0.005270247867039557, Cost (Train): 0.2864947770564914\n",
            "Iteration 8224, Cost (Validation): 0.3475208420971002\n",
            "Iteration 8225, Norm of Gradient: 0.005269722986935154, Cost (Train): 0.2864920001967301\n",
            "Iteration 8225, Cost (Validation): 0.3475216495004267\n",
            "Iteration 8226, Norm of Gradient: 0.005269198207578131, Cost (Train): 0.2864892238899893\n",
            "Iteration 8226, Cost (Validation): 0.3475224571270225\n",
            "Iteration 8227, Norm of Gradient: 0.005268673528937446, Cost (Train): 0.2864864481361079\n",
            "Iteration 8227, Cost (Validation): 0.3475232649767836\n",
            "Iteration 8228, Norm of Gradient: 0.005268148950982074, Cost (Train): 0.28648367293492466\n",
            "Iteration 8228, Cost (Validation): 0.3475240730496058\n",
            "Iteration 8229, Norm of Gradient: 0.005267624473680996, Cost (Train): 0.2864808982862786\n",
            "Iteration 8229, Cost (Validation): 0.34752488134538523\n",
            "Iteration 8230, Norm of Gradient: 0.005267100097003215, Cost (Train): 0.2864781241900087\n",
            "Iteration 8230, Cost (Validation): 0.34752568986401783\n",
            "Iteration 8231, Norm of Gradient: 0.005266575820917741, Cost (Train): 0.286475350645954\n",
            "Iteration 8231, Cost (Validation): 0.34752649860539986\n",
            "Iteration 8232, Norm of Gradient: 0.005266051645393601, Cost (Train): 0.28647257765395356\n",
            "Iteration 8232, Cost (Validation): 0.3475273075694274\n",
            "Iteration 8233, Norm of Gradient: 0.005265527570399836, Cost (Train): 0.28646980521384674\n",
            "Iteration 8233, Cost (Validation): 0.3475281167559967\n",
            "Iteration 8234, Norm of Gradient: 0.005265003595905494, Cost (Train): 0.28646703332547274\n",
            "Iteration 8234, Cost (Validation): 0.3475289261650039\n",
            "Iteration 8235, Norm of Gradient: 0.005264479721879645, Cost (Train): 0.28646426198867087\n",
            "Iteration 8235, Cost (Validation): 0.3475297357963455\n",
            "Iteration 8236, Norm of Gradient: 0.005263955948291366, Cost (Train): 0.2864614912032804\n",
            "Iteration 8236, Cost (Validation): 0.3475305456499177\n",
            "Iteration 8237, Norm of Gradient: 0.005263432275109749, Cost (Train): 0.2864587209691409\n",
            "Iteration 8237, Cost (Validation): 0.34753135572561705\n",
            "Iteration 8238, Norm of Gradient: 0.005262908702303902, Cost (Train): 0.2864559512860919\n",
            "Iteration 8238, Cost (Validation): 0.34753216602333975\n",
            "Iteration 8239, Norm of Gradient: 0.005262385229842942, Cost (Train): 0.28645318215397303\n",
            "Iteration 8239, Cost (Validation): 0.34753297654298254\n",
            "Iteration 8240, Norm of Gradient: 0.005261861857696002, Cost (Train): 0.2864504135726238\n",
            "Iteration 8240, Cost (Validation): 0.34753378728444184\n",
            "Iteration 8241, Norm of Gradient: 0.005261338585832227, Cost (Train): 0.286447645541884\n",
            "Iteration 8241, Cost (Validation): 0.3475345982476143\n",
            "Iteration 8242, Norm of Gradient: 0.00526081541422078, Cost (Train): 0.28644487806159336\n",
            "Iteration 8242, Cost (Validation): 0.3475354094323965\n",
            "Iteration 8243, Norm of Gradient: 0.005260292342830828, Cost (Train): 0.28644211113159174\n",
            "Iteration 8243, Cost (Validation): 0.34753622083868524\n",
            "Iteration 8244, Norm of Gradient: 0.005259769371631558, Cost (Train): 0.286439344751719\n",
            "Iteration 8244, Cost (Validation): 0.3475370324663771\n",
            "Iteration 8245, Norm of Gradient: 0.00525924650059217, Cost (Train): 0.2864365789218152\n",
            "Iteration 8245, Cost (Validation): 0.347537844315369\n",
            "Iteration 8246, Norm of Gradient: 0.005258723729681875, Cost (Train): 0.28643381364172027\n",
            "Iteration 8246, Cost (Validation): 0.3475386563855577\n",
            "Iteration 8247, Norm of Gradient: 0.005258201058869899, Cost (Train): 0.2864310489112743\n",
            "Iteration 8247, Cost (Validation): 0.34753946867684016\n",
            "Iteration 8248, Norm of Gradient: 0.005257678488125482, Cost (Train): 0.2864282847303175\n",
            "Iteration 8248, Cost (Validation): 0.34754028118911323\n",
            "Iteration 8249, Norm of Gradient: 0.005257156017417872, Cost (Train): 0.28642552109869013\n",
            "Iteration 8249, Cost (Validation): 0.34754109392227395\n",
            "Iteration 8250, Norm of Gradient: 0.005256633646716337, Cost (Train): 0.2864227580162323\n",
            "Iteration 8250, Cost (Validation): 0.3475419068762192\n",
            "Iteration 8251, Norm of Gradient: 0.005256111375990154, Cost (Train): 0.2864199954827845\n",
            "Iteration 8251, Cost (Validation): 0.34754272005084624\n",
            "Iteration 8252, Norm of Gradient: 0.005255589205208616, Cost (Train): 0.28641723349818715\n",
            "Iteration 8252, Cost (Validation): 0.34754353344605204\n",
            "Iteration 8253, Norm of Gradient: 0.005255067134341025, Cost (Train): 0.28641447206228066\n",
            "Iteration 8253, Cost (Validation): 0.347544347061734\n",
            "Iteration 8254, Norm of Gradient: 0.005254545163356701, Cost (Train): 0.28641171117490555\n",
            "Iteration 8254, Cost (Validation): 0.347545160897789\n",
            "Iteration 8255, Norm of Gradient: 0.005254023292224974, Cost (Train): 0.2864089508359024\n",
            "Iteration 8255, Cost (Validation): 0.34754597495411454\n",
            "Iteration 8256, Norm of Gradient: 0.00525350152091519, Cost (Train): 0.28640619104511195\n",
            "Iteration 8256, Cost (Validation): 0.34754678923060783\n",
            "Iteration 8257, Norm of Gradient: 0.0052529798493967045, Cost (Train): 0.286403431802375\n",
            "Iteration 8257, Cost (Validation): 0.34754760372716637\n",
            "Iteration 8258, Norm of Gradient: 0.005252458277638889, Cost (Train): 0.2864006731075321\n",
            "Iteration 8258, Cost (Validation): 0.3475484184436874\n",
            "Iteration 8259, Norm of Gradient: 0.005251936805611126, Cost (Train): 0.2863979149604243\n",
            "Iteration 8259, Cost (Validation): 0.34754923338006843\n",
            "Iteration 8260, Norm of Gradient: 0.005251415433282814, Cost (Train): 0.2863951573608925\n",
            "Iteration 8260, Cost (Validation): 0.34755004853620697\n",
            "Iteration 8261, Norm of Gradient: 0.005250894160623363, Cost (Train): 0.2863924003087776\n",
            "Iteration 8261, Cost (Validation): 0.34755086391200063\n",
            "Iteration 8262, Norm of Gradient: 0.005250372987602195, Cost (Train): 0.28638964380392073\n",
            "Iteration 8262, Cost (Validation): 0.34755167950734683\n",
            "Iteration 8263, Norm of Gradient: 0.005249851914188746, Cost (Train): 0.2863868878461629\n",
            "Iteration 8263, Cost (Validation): 0.34755249532214344\n",
            "Iteration 8264, Norm of Gradient: 0.0052493309403524695, Cost (Train): 0.2863841324353453\n",
            "Iteration 8264, Cost (Validation): 0.347553311356288\n",
            "Iteration 8265, Norm of Gradient: 0.005248810066062822, Cost (Train): 0.2863813775713092\n",
            "Iteration 8265, Cost (Validation): 0.34755412760967824\n",
            "Iteration 8266, Norm of Gradient: 0.005248289291289283, Cost (Train): 0.2863786232538959\n",
            "Iteration 8266, Cost (Validation): 0.34755494408221205\n",
            "Iteration 8267, Norm of Gradient: 0.005247768616001342, Cost (Train): 0.28637586948294674\n",
            "Iteration 8267, Cost (Validation): 0.3475557607737872\n",
            "Iteration 8268, Norm of Gradient: 0.005247248040168498, Cost (Train): 0.2863731162583032\n",
            "Iteration 8268, Cost (Validation): 0.34755657768430154\n",
            "Iteration 8269, Norm of Gradient: 0.005246727563760268, Cost (Train): 0.28637036357980666\n",
            "Iteration 8269, Cost (Validation): 0.3475573948136531\n",
            "Iteration 8270, Norm of Gradient: 0.00524620718674618, Cost (Train): 0.2863676114472988\n",
            "Iteration 8270, Cost (Validation): 0.3475582121617397\n",
            "Iteration 8271, Norm of Gradient: 0.005245686909095774, Cost (Train): 0.28636485986062105\n",
            "Iteration 8271, Cost (Validation): 0.3475590297284596\n",
            "Iteration 8272, Norm of Gradient: 0.005245166730778603, Cost (Train): 0.28636210881961516\n",
            "Iteration 8272, Cost (Validation): 0.3475598475137106\n",
            "Iteration 8273, Norm of Gradient: 0.005244646651764239, Cost (Train): 0.28635935832412296\n",
            "Iteration 8273, Cost (Validation): 0.347560665517391\n",
            "Iteration 8274, Norm of Gradient: 0.005244126672022258, Cost (Train): 0.2863566083739862\n",
            "Iteration 8274, Cost (Validation): 0.3475614837393989\n",
            "Iteration 8275, Norm of Gradient: 0.005243606791522255, Cost (Train): 0.2863538589690467\n",
            "Iteration 8275, Cost (Validation): 0.34756230217963247\n",
            "Iteration 8276, Norm of Gradient: 0.005243087010233837, Cost (Train): 0.2863511101091464\n",
            "Iteration 8276, Cost (Validation): 0.34756312083799007\n",
            "Iteration 8277, Norm of Gradient: 0.005242567328126622, Cost (Train): 0.2863483617941273\n",
            "Iteration 8277, Cost (Validation): 0.34756393971436994\n",
            "Iteration 8278, Norm of Gradient: 0.005242047745170244, Cost (Train): 0.28634561402383146\n",
            "Iteration 8278, Cost (Validation): 0.3475647588086704\n",
            "Iteration 8279, Norm of Gradient: 0.005241528261334347, Cost (Train): 0.286342866798101\n",
            "Iteration 8279, Cost (Validation): 0.3475655781207899\n",
            "Iteration 8280, Norm of Gradient: 0.005241008876588589, Cost (Train): 0.28634012011677806\n",
            "Iteration 8280, Cost (Validation): 0.3475663976506269\n",
            "Iteration 8281, Norm of Gradient: 0.005240489590902644, Cost (Train): 0.2863373739797049\n",
            "Iteration 8281, Cost (Validation): 0.3475672173980799\n",
            "Iteration 8282, Norm of Gradient: 0.0052399704042461965, Cost (Train): 0.28633462838672386\n",
            "Iteration 8282, Cost (Validation): 0.34756803736304737\n",
            "Iteration 8283, Norm of Gradient: 0.005239451316588942, Cost (Train): 0.28633188333767723\n",
            "Iteration 8283, Cost (Validation): 0.347568857545428\n",
            "Iteration 8284, Norm of Gradient: 0.005238932327900589, Cost (Train): 0.2863291388324075\n",
            "Iteration 8284, Cost (Validation): 0.3475696779451203\n",
            "Iteration 8285, Norm of Gradient: 0.005238413438150866, Cost (Train): 0.28632639487075706\n",
            "Iteration 8285, Cost (Validation): 0.34757049856202304\n",
            "Iteration 8286, Norm of Gradient: 0.005237894647309507, Cost (Train): 0.2863236514525686\n",
            "Iteration 8286, Cost (Validation): 0.347571319396035\n",
            "Iteration 8287, Norm of Gradient: 0.005237375955346261, Cost (Train): 0.28632090857768466\n",
            "Iteration 8287, Cost (Validation): 0.34757214044705476\n",
            "Iteration 8288, Norm of Gradient: 0.00523685736223089, Cost (Train): 0.2863181662459479\n",
            "Iteration 8288, Cost (Validation): 0.3475729617149813\n",
            "Iteration 8289, Norm of Gradient: 0.005236338867933171, Cost (Train): 0.2863154244572011\n",
            "Iteration 8289, Cost (Validation): 0.3475737831997135\n",
            "Iteration 8290, Norm of Gradient: 0.005235820472422891, Cost (Train): 0.2863126832112871\n",
            "Iteration 8290, Cost (Validation): 0.3475746049011501\n",
            "Iteration 8291, Norm of Gradient: 0.005235302175669851, Cost (Train): 0.2863099425080486\n",
            "Iteration 8291, Cost (Validation): 0.3475754268191903\n",
            "Iteration 8292, Norm of Gradient: 0.005234783977643867, Cost (Train): 0.2863072023473287\n",
            "Iteration 8292, Cost (Validation): 0.347576248953733\n",
            "Iteration 8293, Norm of Gradient: 0.005234265878314764, Cost (Train): 0.28630446272897037\n",
            "Iteration 8293, Cost (Validation): 0.34757707130467724\n",
            "Iteration 8294, Norm of Gradient: 0.005233747877652381, Cost (Train): 0.28630172365281653\n",
            "Iteration 8294, Cost (Validation): 0.3475778938719221\n",
            "Iteration 8295, Norm of Gradient: 0.0052332299756265716, Cost (Train): 0.28629898511871044\n",
            "Iteration 8295, Cost (Validation): 0.34757871665536677\n",
            "Iteration 8296, Norm of Gradient: 0.005232712172207204, Cost (Train): 0.28629624712649526\n",
            "Iteration 8296, Cost (Validation): 0.3475795396549104\n",
            "Iteration 8297, Norm of Gradient: 0.005232194467364153, Cost (Train): 0.286293509676014\n",
            "Iteration 8297, Cost (Validation): 0.34758036287045235\n",
            "Iteration 8298, Norm of Gradient: 0.005231676861067313, Cost (Train): 0.28629077276711035\n",
            "Iteration 8298, Cost (Validation): 0.34758118630189183\n",
            "Iteration 8299, Norm of Gradient: 0.0052311593532865875, Cost (Train): 0.28628803639962735\n",
            "Iteration 8299, Cost (Validation): 0.34758200994912813\n",
            "Iteration 8300, Norm of Gradient: 0.005230641943991893, Cost (Train): 0.28628530057340856\n",
            "Iteration 8300, Cost (Validation): 0.34758283381206073\n",
            "Iteration 8301, Norm of Gradient: 0.00523012463315316, Cost (Train): 0.2862825652882975\n",
            "Iteration 8301, Cost (Validation): 0.3475836578905889\n",
            "Iteration 8302, Norm of Gradient: 0.005229607420740331, Cost (Train): 0.2862798305441375\n",
            "Iteration 8302, Cost (Validation): 0.3475844821846123\n",
            "Iteration 8303, Norm of Gradient: 0.0052290903067233625, Cost (Train): 0.28627709634077253\n",
            "Iteration 8303, Cost (Validation): 0.34758530669403037\n",
            "Iteration 8304, Norm of Gradient: 0.005228573291072224, Cost (Train): 0.28627436267804596\n",
            "Iteration 8304, Cost (Validation): 0.34758613141874267\n",
            "Iteration 8305, Norm of Gradient: 0.005228056373756894, Cost (Train): 0.2862716295558016\n",
            "Iteration 8305, Cost (Validation): 0.3475869563586488\n",
            "Iteration 8306, Norm of Gradient: 0.005227539554747369, Cost (Train): 0.2862688969738833\n",
            "Iteration 8306, Cost (Validation): 0.3475877815136484\n",
            "Iteration 8307, Norm of Gradient: 0.005227022834013657, Cost (Train): 0.28626616493213497\n",
            "Iteration 8307, Cost (Validation): 0.3475886068836413\n",
            "Iteration 8308, Norm of Gradient: 0.005226506211525775, Cost (Train): 0.2862634334304003\n",
            "Iteration 8308, Cost (Validation): 0.34758943246852714\n",
            "Iteration 8309, Norm of Gradient: 0.005225989687253759, Cost (Train): 0.2862607024685235\n",
            "Iteration 8309, Cost (Validation): 0.3475902582682057\n",
            "Iteration 8310, Norm of Gradient: 0.00522547326116765, Cost (Train): 0.2862579720463485\n",
            "Iteration 8310, Cost (Validation): 0.34759108428257685\n",
            "Iteration 8311, Norm of Gradient: 0.005224956933237511, Cost (Train): 0.28625524216371945\n",
            "Iteration 8311, Cost (Validation): 0.3475919105115406\n",
            "Iteration 8312, Norm of Gradient: 0.005224440703433409, Cost (Train): 0.28625251282048053\n",
            "Iteration 8312, Cost (Validation): 0.34759273695499676\n",
            "Iteration 8313, Norm of Gradient: 0.005223924571725432, Cost (Train): 0.2862497840164759\n",
            "Iteration 8313, Cost (Validation): 0.3475935636128454\n",
            "Iteration 8314, Norm of Gradient: 0.005223408538083674, Cost (Train): 0.2862470557515498\n",
            "Iteration 8314, Cost (Validation): 0.34759439048498636\n",
            "Iteration 8315, Norm of Gradient: 0.005222892602478245, Cost (Train): 0.2862443280255467\n",
            "Iteration 8315, Cost (Validation): 0.3475952175713199\n",
            "Iteration 8316, Norm of Gradient: 0.005222376764879266, Cost (Train): 0.2862416008383111\n",
            "Iteration 8316, Cost (Validation): 0.3475960448717461\n",
            "Iteration 8317, Norm of Gradient: 0.005221861025256873, Cost (Train): 0.28623887418968724\n",
            "Iteration 8317, Cost (Validation): 0.34759687238616516\n",
            "Iteration 8318, Norm of Gradient: 0.005221345383581214, Cost (Train): 0.2862361480795199\n",
            "Iteration 8318, Cost (Validation): 0.34759770011447716\n",
            "Iteration 8319, Norm of Gradient: 0.005220829839822449, Cost (Train): 0.28623342250765343\n",
            "Iteration 8319, Cost (Validation): 0.3475985280565826\n",
            "Iteration 8320, Norm of Gradient: 0.005220314393950751, Cost (Train): 0.28623069747393254\n",
            "Iteration 8320, Cost (Validation): 0.3475993562123815\n",
            "Iteration 8321, Norm of Gradient: 0.005219799045936305, Cost (Train): 0.2862279729782022\n",
            "Iteration 8321, Cost (Validation): 0.34760018458177444\n",
            "Iteration 8322, Norm of Gradient: 0.005219283795749311, Cost (Train): 0.2862252490203068\n",
            "Iteration 8322, Cost (Validation): 0.3476010131646617\n",
            "Iteration 8323, Norm of Gradient: 0.005218768643359979, Cost (Train): 0.28622252560009154\n",
            "Iteration 8323, Cost (Validation): 0.34760184196094374\n",
            "Iteration 8324, Norm of Gradient: 0.005218253588738533, Cost (Train): 0.2862198027174011\n",
            "Iteration 8324, Cost (Validation): 0.34760267097052105\n",
            "Iteration 8325, Norm of Gradient: 0.005217738631855211, Cost (Train): 0.28621708037208055\n",
            "Iteration 8325, Cost (Validation): 0.3476035001932942\n",
            "Iteration 8326, Norm of Gradient: 0.005217223772680259, Cost (Train): 0.2862143585639748\n",
            "Iteration 8326, Cost (Validation): 0.3476043296291638\n",
            "Iteration 8327, Norm of Gradient: 0.0052167090111839435, Cost (Train): 0.28621163729292914\n",
            "Iteration 8327, Cost (Validation): 0.3476051592780304\n",
            "Iteration 8328, Norm of Gradient: 0.005216194347336536, Cost (Train): 0.28620891655878844\n",
            "Iteration 8328, Cost (Validation): 0.3476059891397947\n",
            "Iteration 8329, Norm of Gradient: 0.005215679781108324, Cost (Train): 0.2862061963613981\n",
            "Iteration 8329, Cost (Validation): 0.3476068192143573\n",
            "Iteration 8330, Norm of Gradient: 0.005215165312469608, Cost (Train): 0.2862034767006035\n",
            "Iteration 8330, Cost (Validation): 0.34760764950161926\n",
            "Iteration 8331, Norm of Gradient: 0.005214650941390702, Cost (Train): 0.28620075757624963\n",
            "Iteration 8331, Cost (Validation): 0.34760848000148115\n",
            "Iteration 8332, Norm of Gradient: 0.005214136667841928, Cost (Train): 0.2861980389881822\n",
            "Iteration 8332, Cost (Validation): 0.3476093107138438\n",
            "Iteration 8333, Norm of Gradient: 0.005213622491793627, Cost (Train): 0.28619532093624644\n",
            "Iteration 8333, Cost (Validation): 0.34761014163860837\n",
            "Iteration 8334, Norm of Gradient: 0.005213108413216147, Cost (Train): 0.28619260342028807\n",
            "Iteration 8334, Cost (Validation): 0.34761097277567554\n",
            "Iteration 8335, Norm of Gradient: 0.005212594432079853, Cost (Train): 0.28618988644015253\n",
            "Iteration 8335, Cost (Validation): 0.3476118041249464\n",
            "Iteration 8336, Norm of Gradient: 0.00521208054835512, Cost (Train): 0.28618716999568544\n",
            "Iteration 8336, Cost (Validation): 0.347612635686322\n",
            "Iteration 8337, Norm of Gradient: 0.005211566762012337, Cost (Train): 0.2861844540867326\n",
            "Iteration 8337, Cost (Validation): 0.34761346745970345\n",
            "Iteration 8338, Norm of Gradient: 0.005211053073021904, Cost (Train): 0.2861817387131397\n",
            "Iteration 8338, Cost (Validation): 0.3476142994449918\n",
            "Iteration 8339, Norm of Gradient: 0.005210539481354235, Cost (Train): 0.2861790238747526\n",
            "Iteration 8339, Cost (Validation): 0.3476151316420883\n",
            "Iteration 8340, Norm of Gradient: 0.005210025986979756, Cost (Train): 0.28617630957141704\n",
            "Iteration 8340, Cost (Validation): 0.3476159640508941\n",
            "Iteration 8341, Norm of Gradient: 0.005209512589868906, Cost (Train): 0.2861735958029792\n",
            "Iteration 8341, Cost (Validation): 0.34761679667131057\n",
            "Iteration 8342, Norm of Gradient: 0.005208999289992135, Cost (Train): 0.2861708825692849\n",
            "Iteration 8342, Cost (Validation): 0.34761762950323893\n",
            "Iteration 8343, Norm of Gradient: 0.005208486087319909, Cost (Train): 0.2861681698701803\n",
            "Iteration 8343, Cost (Validation): 0.3476184625465806\n",
            "Iteration 8344, Norm of Gradient: 0.005207972981822703, Cost (Train): 0.28616545770551144\n",
            "Iteration 8344, Cost (Validation): 0.34761929580123685\n",
            "Iteration 8345, Norm of Gradient: 0.005207459973471006, Cost (Train): 0.28616274607512454\n",
            "Iteration 8345, Cost (Validation): 0.3476201292671093\n",
            "Iteration 8346, Norm of Gradient: 0.005206947062235319, Cost (Train): 0.28616003497886583\n",
            "Iteration 8346, Cost (Validation): 0.3476209629440993\n",
            "Iteration 8347, Norm of Gradient: 0.005206434248086159, Cost (Train): 0.2861573244165817\n",
            "Iteration 8347, Cost (Validation): 0.34762179683210853\n",
            "Iteration 8348, Norm of Gradient: 0.005205921530994049, Cost (Train): 0.28615461438811834\n",
            "Iteration 8348, Cost (Validation): 0.34762263093103846\n",
            "Iteration 8349, Norm of Gradient: 0.005205408910929529, Cost (Train): 0.2861519048933224\n",
            "Iteration 8349, Cost (Validation): 0.3476234652407908\n",
            "Iteration 8350, Norm of Gradient: 0.005204896387863151, Cost (Train): 0.2861491959320402\n",
            "Iteration 8350, Cost (Validation): 0.3476242997612671\n",
            "Iteration 8351, Norm of Gradient: 0.005204383961765479, Cost (Train): 0.2861464875041183\n",
            "Iteration 8351, Cost (Validation): 0.3476251344923692\n",
            "Iteration 8352, Norm of Gradient: 0.005203871632607091, Cost (Train): 0.28614377960940335\n",
            "Iteration 8352, Cost (Validation): 0.34762596943399887\n",
            "Iteration 8353, Norm of Gradient: 0.005203359400358574, Cost (Train): 0.28614107224774205\n",
            "Iteration 8353, Cost (Validation): 0.3476268045860578\n",
            "Iteration 8354, Norm of Gradient: 0.005202847264990532, Cost (Train): 0.2861383654189811\n",
            "Iteration 8354, Cost (Validation): 0.347627639948448\n",
            "Iteration 8355, Norm of Gradient: 0.005202335226473577, Cost (Train): 0.28613565912296723\n",
            "Iteration 8355, Cost (Validation): 0.34762847552107123\n",
            "Iteration 8356, Norm of Gradient: 0.005201823284778334, Cost (Train): 0.28613295335954736\n",
            "Iteration 8356, Cost (Validation): 0.3476293113038295\n",
            "Iteration 8357, Norm of Gradient: 0.005201311439875447, Cost (Train): 0.2861302481285684\n",
            "Iteration 8357, Cost (Validation): 0.34763014729662484\n",
            "Iteration 8358, Norm of Gradient: 0.005200799691735564, Cost (Train): 0.2861275434298772\n",
            "Iteration 8358, Cost (Validation): 0.34763098349935917\n",
            "Iteration 8359, Norm of Gradient: 0.005200288040329349, Cost (Train): 0.286124839263321\n",
            "Iteration 8359, Cost (Validation): 0.3476318199119347\n",
            "Iteration 8360, Norm of Gradient: 0.00519977648562748, Cost (Train): 0.2861221356287466\n",
            "Iteration 8360, Cost (Validation): 0.3476326565342534\n",
            "Iteration 8361, Norm of Gradient: 0.005199265027600645, Cost (Train): 0.28611943252600147\n",
            "Iteration 8361, Cost (Validation): 0.3476334933662177\n",
            "Iteration 8362, Norm of Gradient: 0.005198753666219544, Cost (Train): 0.28611672995493265\n",
            "Iteration 8362, Cost (Validation): 0.3476343304077295\n",
            "Iteration 8363, Norm of Gradient: 0.005198242401454894, Cost (Train): 0.28611402791538737\n",
            "Iteration 8363, Cost (Validation): 0.34763516765869124\n",
            "Iteration 8364, Norm of Gradient: 0.005197731233277417, Cost (Train): 0.2861113264072131\n",
            "Iteration 8364, Cost (Validation): 0.3476360051190052\n",
            "Iteration 8365, Norm of Gradient: 0.005197220161657855, Cost (Train): 0.28610862543025717\n",
            "Iteration 8365, Cost (Validation): 0.34763684278857376\n",
            "Iteration 8366, Norm of Gradient: 0.005196709186566957, Cost (Train): 0.2861059249843669\n",
            "Iteration 8366, Cost (Validation): 0.3476376806672993\n",
            "Iteration 8367, Norm of Gradient: 0.005196198307975487, Cost (Train): 0.28610322506938995\n",
            "Iteration 8367, Cost (Validation): 0.3476385187550841\n",
            "Iteration 8368, Norm of Gradient: 0.005195687525854222, Cost (Train): 0.28610052568517386\n",
            "Iteration 8368, Cost (Validation): 0.3476393570518309\n",
            "Iteration 8369, Norm of Gradient: 0.005195176840173947, Cost (Train): 0.2860978268315662\n",
            "Iteration 8369, Cost (Validation): 0.3476401955574421\n",
            "Iteration 8370, Norm of Gradient: 0.005194666250905467, Cost (Train): 0.28609512850841484\n",
            "Iteration 8370, Cost (Validation): 0.3476410342718202\n",
            "Iteration 8371, Norm of Gradient: 0.00519415575801959, Cost (Train): 0.2860924307155673\n",
            "Iteration 8371, Cost (Validation): 0.34764187319486795\n",
            "Iteration 8372, Norm of Gradient: 0.005193645361487144, Cost (Train): 0.2860897334528715\n",
            "Iteration 8372, Cost (Validation): 0.347642712326488\n",
            "Iteration 8373, Norm of Gradient: 0.005193135061278967, Cost (Train): 0.28608703672017527\n",
            "Iteration 8373, Cost (Validation): 0.347643551666583\n",
            "Iteration 8374, Norm of Gradient: 0.005192624857365907, Cost (Train): 0.2860843405173266\n",
            "Iteration 8374, Cost (Validation): 0.34764439121505575\n",
            "Iteration 8375, Norm of Gradient: 0.005192114749718829, Cost (Train): 0.2860816448441734\n",
            "Iteration 8375, Cost (Validation): 0.347645230971809\n",
            "Iteration 8376, Norm of Gradient: 0.005191604738308606, Cost (Train): 0.2860789497005638\n",
            "Iteration 8376, Cost (Validation): 0.3476460709367456\n",
            "Iteration 8377, Norm of Gradient: 0.005191094823106125, Cost (Train): 0.2860762550863459\n",
            "Iteration 8377, Cost (Validation): 0.34764691110976836\n",
            "Iteration 8378, Norm of Gradient: 0.005190585004082285, Cost (Train): 0.2860735610013678\n",
            "Iteration 8378, Cost (Validation): 0.3476477514907805\n",
            "Iteration 8379, Norm of Gradient: 0.0051900752812079985, Cost (Train): 0.2860708674454777\n",
            "Iteration 8379, Cost (Validation): 0.3476485920796847\n",
            "Iteration 8380, Norm of Gradient: 0.00518956565445419, Cost (Train): 0.28606817441852406\n",
            "Iteration 8380, Cost (Validation): 0.347649432876384\n",
            "Iteration 8381, Norm of Gradient: 0.005189056123791794, Cost (Train): 0.2860654819203551\n",
            "Iteration 8381, Cost (Validation): 0.34765027388078157\n",
            "Iteration 8382, Norm of Gradient: 0.0051885466891917606, Cost (Train): 0.2860627899508192\n",
            "Iteration 8382, Cost (Validation): 0.3476511150927805\n",
            "Iteration 8383, Norm of Gradient: 0.005188037350625049, Cost (Train): 0.2860600985097649\n",
            "Iteration 8383, Cost (Validation): 0.347651956512284\n",
            "Iteration 8384, Norm of Gradient: 0.005187528108062635, Cost (Train): 0.2860574075970407\n",
            "Iteration 8384, Cost (Validation): 0.34765279813919503\n",
            "Iteration 8385, Norm of Gradient: 0.0051870189614755015, Cost (Train): 0.28605471721249515\n",
            "Iteration 8385, Cost (Validation): 0.34765363997341714\n",
            "Iteration 8386, Norm of Gradient: 0.005186509910834647, Cost (Train): 0.28605202735597685\n",
            "Iteration 8386, Cost (Validation): 0.34765448201485344\n",
            "Iteration 8387, Norm of Gradient: 0.005186000956111081, Cost (Train): 0.28604933802733473\n",
            "Iteration 8387, Cost (Validation): 0.3476553242634073\n",
            "Iteration 8388, Norm of Gradient: 0.005185492097275828, Cost (Train): 0.2860466492264173\n",
            "Iteration 8388, Cost (Validation): 0.3476561667189821\n",
            "Iteration 8389, Norm of Gradient: 0.005184983334299919, Cost (Train): 0.2860439609530735\n",
            "Iteration 8389, Cost (Validation): 0.3476570093814812\n",
            "Iteration 8390, Norm of Gradient: 0.005184474667154404, Cost (Train): 0.28604127320715217\n",
            "Iteration 8390, Cost (Validation): 0.34765785225080814\n",
            "Iteration 8391, Norm of Gradient: 0.0051839660958103405, Cost (Train): 0.28603858598850224\n",
            "Iteration 8391, Cost (Validation): 0.3476586953268664\n",
            "Iteration 8392, Norm of Gradient: 0.005183457620238799, Cost (Train): 0.2860358992969728\n",
            "Iteration 8392, Cost (Validation): 0.34765953860955945\n",
            "Iteration 8393, Norm of Gradient: 0.005182949240410864, Cost (Train): 0.28603321313241287\n",
            "Iteration 8393, Cost (Validation): 0.3476603820987911\n",
            "Iteration 8394, Norm of Gradient: 0.00518244095629763, Cost (Train): 0.28603052749467145\n",
            "Iteration 8394, Cost (Validation): 0.34766122579446473\n",
            "Iteration 8395, Norm of Gradient: 0.005181932767870207, Cost (Train): 0.28602784238359785\n",
            "Iteration 8395, Cost (Validation): 0.3476620696964843\n",
            "Iteration 8396, Norm of Gradient: 0.0051814246750997136, Cost (Train): 0.2860251577990413\n",
            "Iteration 8396, Cost (Validation): 0.3476629138047532\n",
            "Iteration 8397, Norm of Gradient: 0.005180916677957281, Cost (Train): 0.28602247374085105\n",
            "Iteration 8397, Cost (Validation): 0.34766375811917555\n",
            "Iteration 8398, Norm of Gradient: 0.0051804087764140555, Cost (Train): 0.2860197902088764\n",
            "Iteration 8398, Cost (Validation): 0.347664602639655\n",
            "Iteration 8399, Norm of Gradient: 0.005179900970441193, Cost (Train): 0.28601710720296686\n",
            "Iteration 8399, Cost (Validation): 0.3476654473660953\n",
            "Iteration 8400, Norm of Gradient: 0.005179393260009862, Cost (Train): 0.28601442472297195\n",
            "Iteration 8400, Cost (Validation): 0.34766629229840057\n",
            "Iteration 8401, Norm of Gradient: 0.005178885645091244, Cost (Train): 0.2860117427687411\n",
            "Iteration 8401, Cost (Validation): 0.3476671374364746\n",
            "Iteration 8402, Norm of Gradient: 0.005178378125656533, Cost (Train): 0.28600906134012377\n",
            "Iteration 8402, Cost (Validation): 0.34766798278022154\n",
            "Iteration 8403, Norm of Gradient: 0.005177870701676932, Cost (Train): 0.28600638043696996\n",
            "Iteration 8403, Cost (Validation): 0.34766882832954527\n",
            "Iteration 8404, Norm of Gradient: 0.00517736337312366, Cost (Train): 0.286003700059129\n",
            "Iteration 8404, Cost (Validation): 0.34766967408434984\n",
            "Iteration 8405, Norm of Gradient: 0.005176856139967947, Cost (Train): 0.2860010202064509\n",
            "Iteration 8405, Cost (Validation): 0.3476705200445396\n",
            "Iteration 8406, Norm of Gradient: 0.005176349002181034, Cost (Train): 0.2859983408787855\n",
            "Iteration 8406, Cost (Validation): 0.3476713662100185\n",
            "Iteration 8407, Norm of Gradient: 0.005175841959734175, Cost (Train): 0.2859956620759825\n",
            "Iteration 8407, Cost (Validation): 0.34767221258069086\n",
            "Iteration 8408, Norm of Gradient: 0.005175335012598637, Cost (Train): 0.285992983797892\n",
            "Iteration 8408, Cost (Validation): 0.3476730591564609\n",
            "Iteration 8409, Norm of Gradient: 0.005174828160745696, Cost (Train): 0.28599030604436393\n",
            "Iteration 8409, Cost (Validation): 0.34767390593723296\n",
            "Iteration 8410, Norm of Gradient: 0.005174321404146643, Cost (Train): 0.28598762881524836\n",
            "Iteration 8410, Cost (Validation): 0.3476747529229113\n",
            "Iteration 8411, Norm of Gradient: 0.005173814742772783, Cost (Train): 0.28598495211039543\n",
            "Iteration 8411, Cost (Validation): 0.34767560011340043\n",
            "Iteration 8412, Norm of Gradient: 0.005173308176595427, Cost (Train): 0.2859822759296553\n",
            "Iteration 8412, Cost (Validation): 0.34767644750860466\n",
            "Iteration 8413, Norm of Gradient: 0.0051728017055859025, Cost (Train): 0.28597960027287816\n",
            "Iteration 8413, Cost (Validation): 0.3476772951084285\n",
            "Iteration 8414, Norm of Gradient: 0.005172295329715549, Cost (Train): 0.2859769251399144\n",
            "Iteration 8414, Cost (Validation): 0.3476781429127765\n",
            "Iteration 8415, Norm of Gradient: 0.005171789048955716, Cost (Train): 0.28597425053061437\n",
            "Iteration 8415, Cost (Validation): 0.3476789909215532\n",
            "Iteration 8416, Norm of Gradient: 0.005171282863277768, Cost (Train): 0.28597157644482835\n",
            "Iteration 8416, Cost (Validation): 0.3476798391346632\n",
            "Iteration 8417, Norm of Gradient: 0.005170776772653078, Cost (Train): 0.28596890288240695\n",
            "Iteration 8417, Cost (Validation): 0.3476806875520112\n",
            "Iteration 8418, Norm of Gradient: 0.005170270777053034, Cost (Train): 0.28596622984320064\n",
            "Iteration 8418, Cost (Validation): 0.3476815361735018\n",
            "Iteration 8419, Norm of Gradient: 0.005169764876449035, Cost (Train): 0.28596355732705997\n",
            "Iteration 8419, Cost (Validation): 0.3476823849990398\n",
            "Iteration 8420, Norm of Gradient: 0.005169259070812492, Cost (Train): 0.2859608853338357\n",
            "Iteration 8420, Cost (Validation): 0.34768323402853\n",
            "Iteration 8421, Norm of Gradient: 0.005168753360114827, Cost (Train): 0.2859582138633784\n",
            "Iteration 8421, Cost (Validation): 0.34768408326187716\n",
            "Iteration 8422, Norm of Gradient: 0.005168247744327476, Cost (Train): 0.2859555429155389\n",
            "Iteration 8422, Cost (Validation): 0.34768493269898615\n",
            "Iteration 8423, Norm of Gradient: 0.005167742223421886, Cost (Train): 0.285952872490168\n",
            "Iteration 8423, Cost (Validation): 0.34768578233976194\n",
            "Iteration 8424, Norm of Gradient: 0.005167236797369515, Cost (Train): 0.2859502025871167\n",
            "Iteration 8424, Cost (Validation): 0.3476866321841095\n",
            "Iteration 8425, Norm of Gradient: 0.005166731466141838, Cost (Train): 0.28594753320623567\n",
            "Iteration 8425, Cost (Validation): 0.3476874822319337\n",
            "Iteration 8426, Norm of Gradient: 0.0051662262297103335, Cost (Train): 0.28594486434737615\n",
            "Iteration 8426, Cost (Validation): 0.3476883324831397\n",
            "Iteration 8427, Norm of Gradient: 0.0051657210880465, Cost (Train): 0.28594219601038906\n",
            "Iteration 8427, Cost (Validation): 0.3476891829376325\n",
            "Iteration 8428, Norm of Gradient: 0.005165216041121842, Cost (Train): 0.28593952819512564\n",
            "Iteration 8428, Cost (Validation): 0.3476900335953173\n",
            "Iteration 8429, Norm of Gradient: 0.005164711088907881, Cost (Train): 0.28593686090143694\n",
            "Iteration 8429, Cost (Validation): 0.34769088445609925\n",
            "Iteration 8430, Norm of Gradient: 0.005164206231376146, Cost (Train): 0.28593419412917426\n",
            "Iteration 8430, Cost (Validation): 0.3476917355198835\n",
            "Iteration 8431, Norm of Gradient: 0.005163701468498182, Cost (Train): 0.2859315278781888\n",
            "Iteration 8431, Cost (Validation): 0.34769258678657544\n",
            "Iteration 8432, Norm of Gradient: 0.005163196800245542, Cost (Train): 0.28592886214833196\n",
            "Iteration 8432, Cost (Validation): 0.34769343825608023\n",
            "Iteration 8433, Norm of Gradient: 0.0051626922265897955, Cost (Train): 0.2859261969394551\n",
            "Iteration 8433, Cost (Validation): 0.34769428992830326\n",
            "Iteration 8434, Norm of Gradient: 0.0051621877475025195, Cost (Train): 0.28592353225140976\n",
            "Iteration 8434, Cost (Validation): 0.34769514180315\n",
            "Iteration 8435, Norm of Gradient: 0.005161683362955306, Cost (Train): 0.28592086808404743\n",
            "Iteration 8435, Cost (Validation): 0.34769599388052574\n",
            "Iteration 8436, Norm of Gradient: 0.005161179072919757, Cost (Train): 0.2859182044372196\n",
            "Iteration 8436, Cost (Validation): 0.347696846160336\n",
            "Iteration 8437, Norm of Gradient: 0.005160674877367488, Cost (Train): 0.28591554131077807\n",
            "Iteration 8437, Cost (Validation): 0.3476976986424864\n",
            "Iteration 8438, Norm of Gradient: 0.005160170776270126, Cost (Train): 0.2859128787045743\n",
            "Iteration 8438, Cost (Validation): 0.3476985513268824\n",
            "Iteration 8439, Norm of Gradient: 0.00515966676959931, Cost (Train): 0.2859102166184603\n",
            "Iteration 8439, Cost (Validation): 0.3476994042134296\n",
            "Iteration 8440, Norm of Gradient: 0.005159162857326689, Cost (Train): 0.2859075550522876\n",
            "Iteration 8440, Cost (Validation): 0.3477002573020337\n",
            "Iteration 8441, Norm of Gradient: 0.005158659039423926, Cost (Train): 0.2859048940059084\n",
            "Iteration 8441, Cost (Validation): 0.3477011105926003\n",
            "Iteration 8442, Norm of Gradient: 0.005158155315862697, Cost (Train): 0.2859022334791744\n",
            "Iteration 8442, Cost (Validation): 0.34770196408503523\n",
            "Iteration 8443, Norm of Gradient: 0.005157651686614686, Cost (Train): 0.2858995734719374\n",
            "Iteration 8443, Cost (Validation): 0.34770281777924417\n",
            "Iteration 8444, Norm of Gradient: 0.0051571481516515935, Cost (Train): 0.28589691398404987\n",
            "Iteration 8444, Cost (Validation): 0.34770367167513305\n",
            "Iteration 8445, Norm of Gradient: 0.005156644710945128, Cost (Train): 0.28589425501536364\n",
            "Iteration 8445, Cost (Validation): 0.3477045257726077\n",
            "Iteration 8446, Norm of Gradient: 0.005156141364467011, Cost (Train): 0.28589159656573093\n",
            "Iteration 8446, Cost (Validation): 0.3477053800715738\n",
            "Iteration 8447, Norm of Gradient: 0.0051556381121889774, Cost (Train): 0.28588893863500386\n",
            "Iteration 8447, Cost (Validation): 0.34770623457193767\n",
            "Iteration 8448, Norm of Gradient: 0.0051551349540827724, Cost (Train): 0.28588628122303483\n",
            "Iteration 8448, Cost (Validation): 0.3477070892736051\n",
            "Iteration 8449, Norm of Gradient: 0.0051546318901201545, Cost (Train): 0.2858836243296761\n",
            "Iteration 8449, Cost (Validation): 0.3477079441764821\n",
            "Iteration 8450, Norm of Gradient: 0.005154128920272891, Cost (Train): 0.28588096795478\n",
            "Iteration 8450, Cost (Validation): 0.3477087992804748\n",
            "Iteration 8451, Norm of Gradient: 0.005153626044512766, Cost (Train): 0.2858783120981991\n",
            "Iteration 8451, Cost (Validation): 0.3477096545854893\n",
            "Iteration 8452, Norm of Gradient: 0.00515312326281157, Cost (Train): 0.28587565675978577\n",
            "Iteration 8452, Cost (Validation): 0.3477105100914318\n",
            "Iteration 8453, Norm of Gradient: 0.00515262057514111, Cost (Train): 0.28587300193939263\n",
            "Iteration 8453, Cost (Validation): 0.3477113657982083\n",
            "Iteration 8454, Norm of Gradient: 0.0051521179814732025, Cost (Train): 0.28587034763687236\n",
            "Iteration 8454, Cost (Validation): 0.34771222170572536\n",
            "Iteration 8455, Norm of Gradient: 0.005151615481779674, Cost (Train): 0.28586769385207744\n",
            "Iteration 8455, Cost (Validation): 0.3477130778138891\n",
            "Iteration 8456, Norm of Gradient: 0.005151113076032366, Cost (Train): 0.28586504058486084\n",
            "Iteration 8456, Cost (Validation): 0.34771393412260587\n",
            "Iteration 8457, Norm of Gradient: 0.005150610764203131, Cost (Train): 0.28586238783507517\n",
            "Iteration 8457, Cost (Validation): 0.347714790631782\n",
            "Iteration 8458, Norm of Gradient: 0.005150108546263833, Cost (Train): 0.2858597356025733\n",
            "Iteration 8458, Cost (Validation): 0.34771564734132404\n",
            "Iteration 8459, Norm of Gradient: 0.0051496064221863485, Cost (Train): 0.28585708388720815\n",
            "Iteration 8459, Cost (Validation): 0.3477165042511383\n",
            "Iteration 8460, Norm of Gradient: 0.005149104391942563, Cost (Train): 0.28585443268883265\n",
            "Iteration 8460, Cost (Validation): 0.3477173613611313\n",
            "Iteration 8461, Norm of Gradient: 0.005148602455504378, Cost (Train): 0.2858517820072999\n",
            "Iteration 8461, Cost (Validation): 0.3477182186712097\n",
            "Iteration 8462, Norm of Gradient: 0.005148100612843703, Cost (Train): 0.28584913184246286\n",
            "Iteration 8462, Cost (Validation): 0.34771907618128\n",
            "Iteration 8463, Norm of Gradient: 0.0051475988639324635, Cost (Train): 0.2858464821941747\n",
            "Iteration 8463, Cost (Validation): 0.34771993389124883\n",
            "Iteration 8464, Norm of Gradient: 0.005147097208742591, Cost (Train): 0.2858438330622887\n",
            "Iteration 8464, Cost (Validation): 0.3477207918010229\n",
            "Iteration 8465, Norm of Gradient: 0.005146595647246032, Cost (Train): 0.2858411844466579\n",
            "Iteration 8465, Cost (Validation): 0.34772164991050886\n",
            "Iteration 8466, Norm of Gradient: 0.005146094179414747, Cost (Train): 0.2858385363471358\n",
            "Iteration 8466, Cost (Validation): 0.34772250821961354\n",
            "Iteration 8467, Norm of Gradient: 0.005145592805220704, Cost (Train): 0.28583588876357563\n",
            "Iteration 8467, Cost (Validation): 0.34772336672824367\n",
            "Iteration 8468, Norm of Gradient: 0.005145091524635887, Cost (Train): 0.2858332416958309\n",
            "Iteration 8468, Cost (Validation): 0.3477242254363061\n",
            "Iteration 8469, Norm of Gradient: 0.005144590337632285, Cost (Train): 0.28583059514375503\n",
            "Iteration 8469, Cost (Validation): 0.34772508434370786\n",
            "Iteration 8470, Norm of Gradient: 0.005144089244181908, Cost (Train): 0.2858279491072015\n",
            "Iteration 8470, Cost (Validation): 0.3477259434503556\n",
            "Iteration 8471, Norm of Gradient: 0.00514358824425677, Cost (Train): 0.2858253035860239\n",
            "Iteration 8471, Cost (Validation): 0.3477268027561566\n",
            "Iteration 8472, Norm of Gradient: 0.005143087337828901, Cost (Train): 0.285822658580076\n",
            "Iteration 8472, Cost (Validation): 0.34772766226101764\n",
            "Iteration 8473, Norm of Gradient: 0.00514258652487034, Cost (Train): 0.28582001408921137\n",
            "Iteration 8473, Cost (Validation): 0.3477285219648459\n",
            "Iteration 8474, Norm of Gradient: 0.00514208580535314, Cost (Train): 0.2858173701132838\n",
            "Iteration 8474, Cost (Validation): 0.34772938186754837\n",
            "Iteration 8475, Norm of Gradient: 0.005141585179249364, Cost (Train): 0.2858147266521471\n",
            "Iteration 8475, Cost (Validation): 0.34773024196903224\n",
            "Iteration 8476, Norm of Gradient: 0.005141084646531087, Cost (Train): 0.28581208370565514\n",
            "Iteration 8476, Cost (Validation): 0.3477311022692048\n",
            "Iteration 8477, Norm of Gradient: 0.0051405842071703955, Cost (Train): 0.28580944127366187\n",
            "Iteration 8477, Cost (Validation): 0.347731962767973\n",
            "Iteration 8478, Norm of Gradient: 0.00514008386113939, Cost (Train): 0.2858067993560212\n",
            "Iteration 8478, Cost (Validation): 0.3477328234652444\n",
            "Iteration 8479, Norm of Gradient: 0.005139583608410179, Cost (Train): 0.2858041579525873\n",
            "Iteration 8479, Cost (Validation): 0.34773368436092617\n",
            "Iteration 8480, Norm of Gradient: 0.0051390834489548854, Cost (Train): 0.2858015170632141\n",
            "Iteration 8480, Cost (Validation): 0.34773454545492566\n",
            "Iteration 8481, Norm of Gradient: 0.005138583382745644, Cost (Train): 0.2857988766877559\n",
            "Iteration 8481, Cost (Validation): 0.34773540674715026\n",
            "Iteration 8482, Norm of Gradient: 0.005138083409754597, Cost (Train): 0.2857962368260668\n",
            "Iteration 8482, Cost (Validation): 0.34773626823750736\n",
            "Iteration 8483, Norm of Gradient: 0.005137583529953905, Cost (Train): 0.28579359747800115\n",
            "Iteration 8483, Cost (Validation): 0.34773712992590455\n",
            "Iteration 8484, Norm of Gradient: 0.005137083743315733, Cost (Train): 0.28579095864341314\n",
            "Iteration 8484, Cost (Validation): 0.3477379918122493\n",
            "Iteration 8485, Norm of Gradient: 0.005136584049812264, Cost (Train): 0.28578832032215734\n",
            "Iteration 8485, Cost (Validation): 0.3477388538964491\n",
            "Iteration 8486, Norm of Gradient: 0.0051360844494156895, Cost (Train): 0.28578568251408804\n",
            "Iteration 8486, Cost (Validation): 0.34773971617841165\n",
            "Iteration 8487, Norm of Gradient: 0.005135584942098212, Cost (Train): 0.28578304521905973\n",
            "Iteration 8487, Cost (Validation): 0.34774057865804453\n",
            "Iteration 8488, Norm of Gradient: 0.0051350855278320465, Cost (Train): 0.2857804084369271\n",
            "Iteration 8488, Cost (Validation): 0.3477414413352554\n",
            "Iteration 8489, Norm of Gradient: 0.005134586206589421, Cost (Train): 0.2857777721675445\n",
            "Iteration 8489, Cost (Validation): 0.34774230420995217\n",
            "Iteration 8490, Norm of Gradient: 0.0051340869783425705, Cost (Train): 0.2857751364107669\n",
            "Iteration 8490, Cost (Validation): 0.34774316728204235\n",
            "Iteration 8491, Norm of Gradient: 0.005133587843063748, Cost (Train): 0.2857725011664488\n",
            "Iteration 8491, Cost (Validation): 0.34774403055143394\n",
            "Iteration 8492, Norm of Gradient: 0.005133088800725215, Cost (Train): 0.285769866434445\n",
            "Iteration 8492, Cost (Validation): 0.34774489401803466\n",
            "Iteration 8493, Norm of Gradient: 0.0051325898512992445, Cost (Train): 0.28576723221461037\n",
            "Iteration 8493, Cost (Validation): 0.34774575768175253\n",
            "Iteration 8494, Norm of Gradient: 0.005132090994758118, Cost (Train): 0.2857645985067998\n",
            "Iteration 8494, Cost (Validation): 0.34774662154249536\n",
            "Iteration 8495, Norm of Gradient: 0.005131592231074134, Cost (Train): 0.2857619653108682\n",
            "Iteration 8495, Cost (Validation): 0.3477474856001712\n",
            "Iteration 8496, Norm of Gradient: 0.005131093560219601, Cost (Train): 0.2857593326266707\n",
            "Iteration 8496, Cost (Validation): 0.34774834985468817\n",
            "Iteration 8497, Norm of Gradient: 0.005130594982166837, Cost (Train): 0.28575670045406215\n",
            "Iteration 8497, Cost (Validation): 0.3477492143059541\n",
            "Iteration 8498, Norm of Gradient: 0.005130096496888173, Cost (Train): 0.2857540687928977\n",
            "Iteration 8498, Cost (Validation): 0.34775007895387733\n",
            "Iteration 8499, Norm of Gradient: 0.005129598104355952, Cost (Train): 0.2857514376430327\n",
            "Iteration 8499, Cost (Validation): 0.34775094379836585\n",
            "Iteration 8500, Norm of Gradient: 0.005129099804542527, Cost (Train): 0.2857488070043221\n",
            "Iteration 8500, Cost (Validation): 0.3477518088393278\n",
            "Iteration 8501, Norm of Gradient: 0.005128601597420263, Cost (Train): 0.28574617687662135\n",
            "Iteration 8501, Cost (Validation): 0.3477526740766717\n",
            "Iteration 8502, Norm of Gradient: 0.00512810348296154, Cost (Train): 0.28574354725978585\n",
            "Iteration 8502, Cost (Validation): 0.34775353951030546\n",
            "Iteration 8503, Norm of Gradient: 0.005127605461138743, Cost (Train): 0.2857409181536708\n",
            "Iteration 8503, Cost (Validation): 0.34775440514013756\n",
            "Iteration 8504, Norm of Gradient: 0.0051271075319242725, Cost (Train): 0.2857382895581317\n",
            "Iteration 8504, Cost (Validation): 0.34775527096607645\n",
            "Iteration 8505, Norm of Gradient: 0.005126609695290542, Cost (Train): 0.28573566147302415\n",
            "Iteration 8505, Cost (Validation): 0.3477561369880304\n",
            "Iteration 8506, Norm of Gradient: 0.005126111951209973, Cost (Train): 0.28573303389820365\n",
            "Iteration 8506, Cost (Validation): 0.34775700320590774\n",
            "Iteration 8507, Norm of Gradient: 0.005125614299654998, Cost (Train): 0.28573040683352574\n",
            "Iteration 8507, Cost (Validation): 0.3477578696196172\n",
            "Iteration 8508, Norm of Gradient: 0.005125116740598069, Cost (Train): 0.28572778027884627\n",
            "Iteration 8508, Cost (Validation): 0.34775873622906717\n",
            "Iteration 8509, Norm of Gradient: 0.005124619274011637, Cost (Train): 0.28572515423402084\n",
            "Iteration 8509, Cost (Validation): 0.34775960303416625\n",
            "Iteration 8510, Norm of Gradient: 0.0051241218998681745, Cost (Train): 0.2857225286989052\n",
            "Iteration 8510, Cost (Validation): 0.3477604700348229\n",
            "Iteration 8511, Norm of Gradient: 0.005123624618140159, Cost (Train): 0.28571990367335526\n",
            "Iteration 8511, Cost (Validation): 0.3477613372309459\n",
            "Iteration 8512, Norm of Gradient: 0.0051231274288000865, Cost (Train): 0.28571727915722683\n",
            "Iteration 8512, Cost (Validation): 0.3477622046224439\n",
            "Iteration 8513, Norm of Gradient: 0.005122630331820457, Cost (Train): 0.285714655150376\n",
            "Iteration 8513, Cost (Validation): 0.3477630722092257\n",
            "Iteration 8514, Norm of Gradient: 0.005122133327173787, Cost (Train): 0.2857120316526586\n",
            "Iteration 8514, Cost (Validation): 0.3477639399912\n",
            "Iteration 8515, Norm of Gradient: 0.005121636414832602, Cost (Train): 0.28570940866393085\n",
            "Iteration 8515, Cost (Validation): 0.3477648079682756\n",
            "Iteration 8516, Norm of Gradient: 0.00512113959476944, Cost (Train): 0.2857067861840489\n",
            "Iteration 8516, Cost (Validation): 0.3477656761403614\n",
            "Iteration 8517, Norm of Gradient: 0.005120642866956848, Cost (Train): 0.2857041642128687\n",
            "Iteration 8517, Cost (Validation): 0.3477665445073662\n",
            "Iteration 8518, Norm of Gradient: 0.0051201462313673896, Cost (Train): 0.2857015427502465\n",
            "Iteration 8518, Cost (Validation): 0.347767413069199\n",
            "Iteration 8519, Norm of Gradient: 0.005119649687973635, Cost (Train): 0.28569892179603884\n",
            "Iteration 8519, Cost (Validation): 0.34776828182576874\n",
            "Iteration 8520, Norm of Gradient: 0.0051191532367481675, Cost (Train): 0.2856963013501019\n",
            "Iteration 8520, Cost (Validation): 0.3477691507769846\n",
            "Iteration 8521, Norm of Gradient: 0.0051186568776635825, Cost (Train): 0.28569368141229196\n",
            "Iteration 8521, Cost (Validation): 0.34777001992275536\n",
            "Iteration 8522, Norm of Gradient: 0.005118160610692485, Cost (Train): 0.2856910619824657\n",
            "Iteration 8522, Cost (Validation): 0.34777088926299027\n",
            "Iteration 8523, Norm of Gradient: 0.005117664435807494, Cost (Train): 0.28568844306047947\n",
            "Iteration 8523, Cost (Validation): 0.3477717587975985\n",
            "Iteration 8524, Norm of Gradient: 0.005117168352981238, Cost (Train): 0.28568582464618975\n",
            "Iteration 8524, Cost (Validation): 0.34777262852648916\n",
            "Iteration 8525, Norm of Gradient: 0.005116672362186357, Cost (Train): 0.2856832067394534\n",
            "Iteration 8525, Cost (Validation): 0.3477734984495715\n",
            "Iteration 8526, Norm of Gradient: 0.005116176463395502, Cost (Train): 0.28568058934012697\n",
            "Iteration 8526, Cost (Validation): 0.34777436856675487\n",
            "Iteration 8527, Norm of Gradient: 0.005115680656581336, Cost (Train): 0.2856779724480671\n",
            "Iteration 8527, Cost (Validation): 0.3477752388779483\n",
            "Iteration 8528, Norm of Gradient: 0.005115184941716535, Cost (Train): 0.28567535606313066\n",
            "Iteration 8528, Cost (Validation): 0.34777610938306147\n",
            "Iteration 8529, Norm of Gradient: 0.005114689318773784, Cost (Train): 0.28567274018517447\n",
            "Iteration 8529, Cost (Validation): 0.3477769800820036\n",
            "Iteration 8530, Norm of Gradient: 0.005114193787725779, Cost (Train): 0.2856701248140555\n",
            "Iteration 8530, Cost (Validation): 0.3477778509746841\n",
            "Iteration 8531, Norm of Gradient: 0.005113698348545231, Cost (Train): 0.2856675099496305\n",
            "Iteration 8531, Cost (Validation): 0.3477787220610125\n",
            "Iteration 8532, Norm of Gradient: 0.005113203001204856, Cost (Train): 0.28566489559175673\n",
            "Iteration 8532, Cost (Validation): 0.3477795933408983\n",
            "Iteration 8533, Norm of Gradient: 0.005112707745677388, Cost (Train): 0.285662281740291\n",
            "Iteration 8533, Cost (Validation): 0.347780464814251\n",
            "Iteration 8534, Norm of Gradient: 0.005112212581935569, Cost (Train): 0.28565966839509055\n",
            "Iteration 8534, Cost (Validation): 0.3477813364809802\n",
            "Iteration 8535, Norm of Gradient: 0.005111717509952152, Cost (Train): 0.2856570555560126\n",
            "Iteration 8535, Cost (Validation): 0.34778220834099555\n",
            "Iteration 8536, Norm of Gradient: 0.0051112225296999015, Cost (Train): 0.28565444322291433\n",
            "Iteration 8536, Cost (Validation): 0.3477830803942068\n",
            "Iteration 8537, Norm of Gradient: 0.005110727641151598, Cost (Train): 0.28565183139565287\n",
            "Iteration 8537, Cost (Validation): 0.3477839526405236\n",
            "Iteration 8538, Norm of Gradient: 0.005110232844280023, Cost (Train): 0.28564922007408583\n",
            "Iteration 8538, Cost (Validation): 0.34778482507985564\n",
            "Iteration 8539, Norm of Gradient: 0.00510973813905798, Cost (Train): 0.2856466092580703\n",
            "Iteration 8539, Cost (Validation): 0.34778569771211276\n",
            "Iteration 8540, Norm of Gradient: 0.005109243525458278, Cost (Train): 0.285643998947464\n",
            "Iteration 8540, Cost (Validation): 0.34778657053720485\n",
            "Iteration 8541, Norm of Gradient: 0.005108749003453738, Cost (Train): 0.28564138914212417\n",
            "Iteration 8541, Cost (Validation): 0.34778744355504176\n",
            "Iteration 8542, Norm of Gradient: 0.005108254573017194, Cost (Train): 0.2856387798419086\n",
            "Iteration 8542, Cost (Validation): 0.34778831676553335\n",
            "Iteration 8543, Norm of Gradient: 0.005107760234121488, Cost (Train): 0.2856361710466747\n",
            "Iteration 8543, Cost (Validation): 0.3477891901685896\n",
            "Iteration 8544, Norm of Gradient: 0.005107265986739478, Cost (Train): 0.28563356275628027\n",
            "Iteration 8544, Cost (Validation): 0.3477900637641206\n",
            "Iteration 8545, Norm of Gradient: 0.0051067718308440285, Cost (Train): 0.28563095497058305\n",
            "Iteration 8545, Cost (Validation): 0.3477909375520363\n",
            "Iteration 8546, Norm of Gradient: 0.005106277766408019, Cost (Train): 0.28562834768944056\n",
            "Iteration 8546, Cost (Validation): 0.3477918115322467\n",
            "Iteration 8547, Norm of Gradient: 0.005105783793404337, Cost (Train): 0.2856257409127109\n",
            "Iteration 8547, Cost (Validation): 0.3477926857046621\n",
            "Iteration 8548, Norm of Gradient: 0.005105289911805885, Cost (Train): 0.28562313464025185\n",
            "Iteration 8548, Cost (Validation): 0.3477935600691926\n",
            "Iteration 8549, Norm of Gradient: 0.005104796121585573, Cost (Train): 0.28562052887192135\n",
            "Iteration 8549, Cost (Validation): 0.3477944346257484\n",
            "Iteration 8550, Norm of Gradient: 0.005104302422716324, Cost (Train): 0.28561792360757743\n",
            "Iteration 8550, Cost (Validation): 0.3477953093742397\n",
            "Iteration 8551, Norm of Gradient: 0.005103808815171072, Cost (Train): 0.2856153188470781\n",
            "Iteration 8551, Cost (Validation): 0.3477961843145768\n",
            "Iteration 8552, Norm of Gradient: 0.005103315298922764, Cost (Train): 0.2856127145902815\n",
            "Iteration 8552, Cost (Validation): 0.34779705944667005\n",
            "Iteration 8553, Norm of Gradient: 0.005102821873944353, Cost (Train): 0.2856101108370456\n",
            "Iteration 8553, Cost (Validation): 0.3477979347704298\n",
            "Iteration 8554, Norm of Gradient: 0.005102328540208811, Cost (Train): 0.28560750758722886\n",
            "Iteration 8554, Cost (Validation): 0.3477988102857665\n",
            "Iteration 8555, Norm of Gradient: 0.005101835297689113, Cost (Train): 0.2856049048406894\n",
            "Iteration 8555, Cost (Validation): 0.3477996859925905\n",
            "Iteration 8556, Norm of Gradient: 0.005101342146358251, Cost (Train): 0.2856023025972856\n",
            "Iteration 8556, Cost (Validation): 0.3478005618908123\n",
            "Iteration 8557, Norm of Gradient: 0.005100849086189227, Cost (Train): 0.2855997008568758\n",
            "Iteration 8557, Cost (Validation): 0.3478014379803425\n",
            "Iteration 8558, Norm of Gradient: 0.005100356117155052, Cost (Train): 0.2855970996193185\n",
            "Iteration 8558, Cost (Validation): 0.34780231426109165\n",
            "Iteration 8559, Norm of Gradient: 0.00509986323922875, Cost (Train): 0.2855944988844721\n",
            "Iteration 8559, Cost (Validation): 0.34780319073297034\n",
            "Iteration 8560, Norm of Gradient: 0.005099370452383355, Cost (Train): 0.28559189865219514\n",
            "Iteration 8560, Cost (Validation): 0.34780406739588915\n",
            "Iteration 8561, Norm of Gradient: 0.0050988777565919175, Cost (Train): 0.28558929892234625\n",
            "Iteration 8561, Cost (Validation): 0.3478049442497589\n",
            "Iteration 8562, Norm of Gradient: 0.0050983851518274885, Cost (Train): 0.28558669969478406\n",
            "Iteration 8562, Cost (Validation): 0.3478058212944902\n",
            "Iteration 8563, Norm of Gradient: 0.005097892638063139, Cost (Train): 0.2855841009693672\n",
            "Iteration 8563, Cost (Validation): 0.3478066985299939\n",
            "Iteration 8564, Norm of Gradient: 0.005097400215271949, Cost (Train): 0.28558150274595456\n",
            "Iteration 8564, Cost (Validation): 0.34780757595618084\n",
            "Iteration 8565, Norm of Gradient: 0.005096907883427007, Cost (Train): 0.28557890502440486\n",
            "Iteration 8565, Cost (Validation): 0.3478084535729617\n",
            "Iteration 8566, Norm of Gradient: 0.005096415642501418, Cost (Train): 0.28557630780457705\n",
            "Iteration 8566, Cost (Validation): 0.3478093313802475\n",
            "Iteration 8567, Norm of Gradient: 0.005095923492468292, Cost (Train): 0.2855737110863299\n",
            "Iteration 8567, Cost (Validation): 0.3478102093779492\n",
            "Iteration 8568, Norm of Gradient: 0.005095431433300754, Cost (Train): 0.28557111486952247\n",
            "Iteration 8568, Cost (Validation): 0.3478110875659777\n",
            "Iteration 8569, Norm of Gradient: 0.005094939464971938, Cost (Train): 0.28556851915401377\n",
            "Iteration 8569, Cost (Validation): 0.34781196594424396\n",
            "Iteration 8570, Norm of Gradient: 0.005094447587454993, Cost (Train): 0.28556592393966296\n",
            "Iteration 8570, Cost (Validation): 0.34781284451265915\n",
            "Iteration 8571, Norm of Gradient: 0.005093955800723074, Cost (Train): 0.285563329226329\n",
            "Iteration 8571, Cost (Validation): 0.34781372327113425\n",
            "Iteration 8572, Norm of Gradient: 0.00509346410474935, Cost (Train): 0.2855607350138712\n",
            "Iteration 8572, Cost (Validation): 0.3478146022195805\n",
            "Iteration 8573, Norm of Gradient: 0.005092972499506999, Cost (Train): 0.28555814130214885\n",
            "Iteration 8573, Cost (Validation): 0.347815481357909\n",
            "Iteration 8574, Norm of Gradient: 0.005092480984969214, Cost (Train): 0.2855555480910211\n",
            "Iteration 8574, Cost (Validation): 0.347816360686031\n",
            "Iteration 8575, Norm of Gradient: 0.005091989561109195, Cost (Train): 0.2855529553803474\n",
            "Iteration 8575, Cost (Validation): 0.3478172402038576\n",
            "Iteration 8576, Norm of Gradient: 0.005091498227900154, Cost (Train): 0.28555036316998716\n",
            "Iteration 8576, Cost (Validation): 0.34781811991130035\n",
            "Iteration 8577, Norm of Gradient: 0.005091006985315318, Cost (Train): 0.2855477714597998\n",
            "Iteration 8577, Cost (Validation): 0.34781899980827036\n",
            "Iteration 8578, Norm of Gradient: 0.00509051583332792, Cost (Train): 0.28554518024964487\n",
            "Iteration 8578, Cost (Validation): 0.3478198798946792\n",
            "Iteration 8579, Norm of Gradient: 0.005090024771911205, Cost (Train): 0.28554258953938183\n",
            "Iteration 8579, Cost (Validation): 0.3478207601704381\n",
            "Iteration 8580, Norm of Gradient: 0.00508953380103843, Cost (Train): 0.2855399993288705\n",
            "Iteration 8580, Cost (Validation): 0.3478216406354585\n",
            "Iteration 8581, Norm of Gradient: 0.005089042920682864, Cost (Train): 0.2855374096179703\n",
            "Iteration 8581, Cost (Validation): 0.34782252128965213\n",
            "Iteration 8582, Norm of Gradient: 0.005088552130817786, Cost (Train): 0.2855348204065411\n",
            "Iteration 8582, Cost (Validation): 0.34782340213293017\n",
            "Iteration 8583, Norm of Gradient: 0.005088061431416487, Cost (Train): 0.28553223169444275\n",
            "Iteration 8583, Cost (Validation): 0.34782428316520453\n",
            "Iteration 8584, Norm of Gradient: 0.005087570822452266, Cost (Train): 0.285529643481535\n",
            "Iteration 8584, Cost (Validation): 0.3478251643863866\n",
            "Iteration 8585, Norm of Gradient: 0.005087080303898436, Cost (Train): 0.2855270557676776\n",
            "Iteration 8585, Cost (Validation): 0.3478260457963881\n",
            "Iteration 8586, Norm of Gradient: 0.0050865898757283205, Cost (Train): 0.28552446855273084\n",
            "Iteration 8586, Cost (Validation): 0.34782692739512067\n",
            "Iteration 8587, Norm of Gradient: 0.0050860995379152524, Cost (Train): 0.2855218818365544\n",
            "Iteration 8587, Cost (Validation): 0.3478278091824962\n",
            "Iteration 8588, Norm of Gradient: 0.005085609290432579, Cost (Train): 0.28551929561900846\n",
            "Iteration 8588, Cost (Validation): 0.34782869115842624\n",
            "Iteration 8589, Norm of Gradient: 0.005085119133253654, Cost (Train): 0.28551670989995315\n",
            "Iteration 8589, Cost (Validation): 0.3478295733228227\n",
            "Iteration 8590, Norm of Gradient: 0.005084629066351846, Cost (Train): 0.2855141246792485\n",
            "Iteration 8590, Cost (Validation): 0.34783045567559745\n",
            "Iteration 8591, Norm of Gradient: 0.005084139089700534, Cost (Train): 0.2855115399567549\n",
            "Iteration 8591, Cost (Validation): 0.3478313382166624\n",
            "Iteration 8592, Norm of Gradient: 0.005083649203273105, Cost (Train): 0.2855089557323325\n",
            "Iteration 8592, Cost (Validation): 0.34783222094592936\n",
            "Iteration 8593, Norm of Gradient: 0.00508315940704296, Cost (Train): 0.2855063720058416\n",
            "Iteration 8593, Cost (Validation): 0.3478331038633104\n",
            "Iteration 8594, Norm of Gradient: 0.005082669700983511, Cost (Train): 0.2855037887771426\n",
            "Iteration 8594, Cost (Validation): 0.34783398696871753\n",
            "Iteration 8595, Norm of Gradient: 0.005082180085068179, Cost (Train): 0.28550120604609597\n",
            "Iteration 8595, Cost (Validation): 0.3478348702620627\n",
            "Iteration 8596, Norm of Gradient: 0.005081690559270397, Cost (Train): 0.28549862381256214\n",
            "Iteration 8596, Cost (Validation): 0.34783575374325815\n",
            "Iteration 8597, Norm of Gradient: 0.005081201123563609, Cost (Train): 0.28549604207640167\n",
            "Iteration 8597, Cost (Validation): 0.3478366374122159\n",
            "Iteration 8598, Norm of Gradient: 0.005080711777921268, Cost (Train): 0.2854934608374751\n",
            "Iteration 8598, Cost (Validation): 0.34783752126884804\n",
            "Iteration 8599, Norm of Gradient: 0.005080222522316844, Cost (Train): 0.28549088009564316\n",
            "Iteration 8599, Cost (Validation): 0.347838405313067\n",
            "Iteration 8600, Norm of Gradient: 0.005079733356723811, Cost (Train): 0.2854882998507665\n",
            "Iteration 8600, Cost (Validation): 0.34783928954478477\n",
            "Iteration 8601, Norm of Gradient: 0.005079244281115658, Cost (Train): 0.28548572010270584\n",
            "Iteration 8601, Cost (Validation): 0.34784017396391376\n",
            "Iteration 8602, Norm of Gradient: 0.005078755295465881, Cost (Train): 0.2854831408513219\n",
            "Iteration 8602, Cost (Validation): 0.3478410585703664\n",
            "Iteration 8603, Norm of Gradient: 0.005078266399747992, Cost (Train): 0.28548056209647565\n",
            "Iteration 8603, Cost (Validation): 0.34784194336405483\n",
            "Iteration 8604, Norm of Gradient: 0.0050777775939355105, Cost (Train): 0.285477983838028\n",
            "Iteration 8604, Cost (Validation): 0.3478428283448916\n",
            "Iteration 8605, Norm of Gradient: 0.005077288878001969, Cost (Train): 0.28547540607583993\n",
            "Iteration 8605, Cost (Validation): 0.34784371351278914\n",
            "Iteration 8606, Norm of Gradient: 0.005076800251920907, Cost (Train): 0.2854728288097723\n",
            "Iteration 8606, Cost (Validation): 0.3478445988676599\n",
            "Iteration 8607, Norm of Gradient: 0.005076311715665881, Cost (Train): 0.28547025203968635\n",
            "Iteration 8607, Cost (Validation): 0.3478454844094164\n",
            "Iteration 8608, Norm of Gradient: 0.005075823269210453, Cost (Train): 0.2854676757654431\n",
            "Iteration 8608, Cost (Validation): 0.3478463701379712\n",
            "Iteration 8609, Norm of Gradient: 0.005075334912528196, Cost (Train): 0.28546509998690367\n",
            "Iteration 8609, Cost (Validation): 0.34784725605323685\n",
            "Iteration 8610, Norm of Gradient: 0.0050748466455927, Cost (Train): 0.28546252470392947\n",
            "Iteration 8610, Cost (Validation): 0.3478481421551261\n",
            "Iteration 8611, Norm of Gradient: 0.005074358468377559, Cost (Train): 0.2854599499163815\n",
            "Iteration 8611, Cost (Validation): 0.34784902844355153\n",
            "Iteration 8612, Norm of Gradient: 0.0050738703808563814, Cost (Train): 0.28545737562412143\n",
            "Iteration 8612, Cost (Validation): 0.34784991491842593\n",
            "Iteration 8613, Norm of Gradient: 0.005073382383002784, Cost (Train): 0.2854548018270104\n",
            "Iteration 8613, Cost (Validation): 0.347850801579662\n",
            "Iteration 8614, Norm of Gradient: 0.005072894474790398, Cost (Train): 0.28545222852491\n",
            "Iteration 8614, Cost (Validation): 0.34785168842717257\n",
            "Iteration 8615, Norm of Gradient: 0.005072406656192863, Cost (Train): 0.28544965571768144\n",
            "Iteration 8615, Cost (Validation): 0.3478525754608705\n",
            "Iteration 8616, Norm of Gradient: 0.005071918927183829, Cost (Train): 0.2854470834051866\n",
            "Iteration 8616, Cost (Validation): 0.3478534626806686\n",
            "Iteration 8617, Norm of Gradient: 0.005071431287736957, Cost (Train): 0.28544451158728695\n",
            "Iteration 8617, Cost (Validation): 0.34785435008647975\n",
            "Iteration 8618, Norm of Gradient: 0.005070943737825922, Cost (Train): 0.28544194026384406\n",
            "Iteration 8618, Cost (Validation): 0.34785523767821697\n",
            "Iteration 8619, Norm of Gradient: 0.0050704562774244065, Cost (Train): 0.2854393694347197\n",
            "Iteration 8619, Cost (Validation): 0.3478561254557933\n",
            "Iteration 8620, Norm of Gradient: 0.005069968906506104, Cost (Train): 0.28543679909977565\n",
            "Iteration 8620, Cost (Validation): 0.3478570134191217\n",
            "Iteration 8621, Norm of Gradient: 0.00506948162504472, Cost (Train): 0.28543422925887363\n",
            "Iteration 8621, Cost (Validation): 0.3478579015681152\n",
            "Iteration 8622, Norm of Gradient: 0.0050689944330139704, Cost (Train): 0.28543165991187563\n",
            "Iteration 8622, Cost (Validation): 0.3478587899026869\n",
            "Iteration 8623, Norm of Gradient: 0.005068507330387581, Cost (Train): 0.28542909105864345\n",
            "Iteration 8623, Cost (Validation): 0.3478596784227501\n",
            "Iteration 8624, Norm of Gradient: 0.005068020317139289, Cost (Train): 0.2854265226990391\n",
            "Iteration 8624, Cost (Validation): 0.3478605671282179\n",
            "Iteration 8625, Norm of Gradient: 0.0050675333932428445, Cost (Train): 0.2854239548329246\n",
            "Iteration 8625, Cost (Validation): 0.34786145601900353\n",
            "Iteration 8626, Norm of Gradient: 0.005067046558672006, Cost (Train): 0.28542138746016205\n",
            "Iteration 8626, Cost (Validation): 0.3478623450950202\n",
            "Iteration 8627, Norm of Gradient: 0.005066559813400541, Cost (Train): 0.28541882058061346\n",
            "Iteration 8627, Cost (Validation): 0.3478632343561812\n",
            "Iteration 8628, Norm of Gradient: 0.005066073157402232, Cost (Train): 0.28541625419414113\n",
            "Iteration 8628, Cost (Validation): 0.34786412380240006\n",
            "Iteration 8629, Norm of Gradient: 0.005065586590650871, Cost (Train): 0.2854136883006072\n",
            "Iteration 8629, Cost (Validation): 0.34786501343358994\n",
            "Iteration 8630, Norm of Gradient: 0.005065100113120257, Cost (Train): 0.285411122899874\n",
            "Iteration 8630, Cost (Validation): 0.3478659032496643\n",
            "Iteration 8631, Norm of Gradient: 0.0050646137247842065, Cost (Train): 0.2854085579918039\n",
            "Iteration 8631, Cost (Validation): 0.3478667932505367\n",
            "Iteration 8632, Norm of Gradient: 0.00506412742561654, Cost (Train): 0.2854059935762592\n",
            "Iteration 8632, Cost (Validation): 0.34786768343612057\n",
            "Iteration 8633, Norm of Gradient: 0.005063641215591094, Cost (Train): 0.28540342965310245\n",
            "Iteration 8633, Cost (Validation): 0.34786857380632946\n",
            "Iteration 8634, Norm of Gradient: 0.005063155094681714, Cost (Train): 0.285400866222196\n",
            "Iteration 8634, Cost (Validation): 0.34786946436107685\n",
            "Iteration 8635, Norm of Gradient: 0.005062669062862252, Cost (Train): 0.28539830328340243\n",
            "Iteration 8635, Cost (Validation): 0.34787035510027653\n",
            "Iteration 8636, Norm of Gradient: 0.005062183120106578, Cost (Train): 0.2853957408365845\n",
            "Iteration 8636, Cost (Validation): 0.3478712460238421\n",
            "Iteration 8637, Norm of Gradient: 0.005061697266388568, Cost (Train): 0.28539317888160465\n",
            "Iteration 8637, Cost (Validation): 0.3478721371316872\n",
            "Iteration 8638, Norm of Gradient: 0.00506121150168211, Cost (Train): 0.28539061741832566\n",
            "Iteration 8638, Cost (Validation): 0.3478730284237255\n",
            "Iteration 8639, Norm of Gradient: 0.005060725825961103, Cost (Train): 0.2853880564466103\n",
            "Iteration 8639, Cost (Validation): 0.34787391989987093\n",
            "Iteration 8640, Norm of Gradient: 0.005060240239199457, Cost (Train): 0.2853854959663213\n",
            "Iteration 8640, Cost (Validation): 0.34787481156003724\n",
            "Iteration 8641, Norm of Gradient: 0.00505975474137109, Cost (Train): 0.2853829359773217\n",
            "Iteration 8641, Cost (Validation): 0.34787570340413815\n",
            "Iteration 8642, Norm of Gradient: 0.005059269332449933, Cost (Train): 0.28538037647947423\n",
            "Iteration 8642, Cost (Validation): 0.34787659543208765\n",
            "Iteration 8643, Norm of Gradient: 0.005058784012409929, Cost (Train): 0.2853778174726419\n",
            "Iteration 8643, Cost (Validation): 0.3478774876437998\n",
            "Iteration 8644, Norm of Gradient: 0.005058298781225029, Cost (Train): 0.28537525895668775\n",
            "Iteration 8644, Cost (Validation): 0.34787838003918825\n",
            "Iteration 8645, Norm of Gradient: 0.0050578136388691975, Cost (Train): 0.2853727009314748\n",
            "Iteration 8645, Cost (Validation): 0.3478792726181672\n",
            "Iteration 8646, Norm of Gradient: 0.005057328585316405, Cost (Train): 0.2853701433968663\n",
            "Iteration 8646, Cost (Validation): 0.3478801653806507\n",
            "Iteration 8647, Norm of Gradient: 0.005056843620540639, Cost (Train): 0.28536758635272524\n",
            "Iteration 8647, Cost (Validation): 0.3478810583265528\n",
            "Iteration 8648, Norm of Gradient: 0.005056358744515891, Cost (Train): 0.2853650297989149\n",
            "Iteration 8648, Cost (Validation): 0.34788195145578754\n",
            "Iteration 8649, Norm of Gradient: 0.005055873957216168, Cost (Train): 0.28536247373529866\n",
            "Iteration 8649, Cost (Validation): 0.34788284476826914\n",
            "Iteration 8650, Norm of Gradient: 0.005055389258615487, Cost (Train): 0.2853599181617398\n",
            "Iteration 8650, Cost (Validation): 0.34788373826391183\n",
            "Iteration 8651, Norm of Gradient: 0.005054904648687872, Cost (Train): 0.28535736307810156\n",
            "Iteration 8651, Cost (Validation): 0.3478846319426298\n",
            "Iteration 8652, Norm of Gradient: 0.005054420127407364, Cost (Train): 0.2853548084842476\n",
            "Iteration 8652, Cost (Validation): 0.34788552580433735\n",
            "Iteration 8653, Norm of Gradient: 0.005053935694748007, Cost (Train): 0.28535225438004125\n",
            "Iteration 8653, Cost (Validation): 0.34788641984894864\n",
            "Iteration 8654, Norm of Gradient: 0.005053451350683863, Cost (Train): 0.285349700765346\n",
            "Iteration 8654, Cost (Validation): 0.34788731407637824\n",
            "Iteration 8655, Norm of Gradient: 0.005052967095188999, Cost (Train): 0.2853471476400256\n",
            "Iteration 8655, Cost (Validation): 0.3478882084865404\n",
            "Iteration 8656, Norm of Gradient: 0.005052482928237496, Cost (Train): 0.2853445950039436\n",
            "Iteration 8656, Cost (Validation): 0.34788910307934956\n",
            "Iteration 8657, Norm of Gradient: 0.005051998849803444, Cost (Train): 0.2853420428569637\n",
            "Iteration 8657, Cost (Validation): 0.34788999785472025\n",
            "Iteration 8658, Norm of Gradient: 0.005051514859860944, Cost (Train): 0.28533949119894964\n",
            "Iteration 8658, Cost (Validation): 0.34789089281256685\n",
            "Iteration 8659, Norm of Gradient: 0.005051030958384108, Cost (Train): 0.28533694002976523\n",
            "Iteration 8659, Cost (Validation): 0.34789178795280395\n",
            "Iteration 8660, Norm of Gradient: 0.005050547145347058, Cost (Train): 0.28533438934927424\n",
            "Iteration 8660, Cost (Validation): 0.34789268327534617\n",
            "Iteration 8661, Norm of Gradient: 0.005050063420723927, Cost (Train): 0.2853318391573407\n",
            "Iteration 8661, Cost (Validation): 0.3478935787801081\n",
            "Iteration 8662, Norm of Gradient: 0.005049579784488857, Cost (Train): 0.2853292894538285\n",
            "Iteration 8662, Cost (Validation): 0.34789447446700433\n",
            "Iteration 8663, Norm of Gradient: 0.005049096236616003, Cost (Train): 0.2853267402386016\n",
            "Iteration 8663, Cost (Validation): 0.3478953703359497\n",
            "Iteration 8664, Norm of Gradient: 0.005048612777079532, Cost (Train): 0.28532419151152416\n",
            "Iteration 8664, Cost (Validation): 0.3478962663868588\n",
            "Iteration 8665, Norm of Gradient: 0.005048129405853614, Cost (Train): 0.2853216432724602\n",
            "Iteration 8665, Cost (Validation): 0.3478971626196464\n",
            "Iteration 8666, Norm of Gradient: 0.00504764612291244, Cost (Train): 0.2853190955212738\n",
            "Iteration 8666, Cost (Validation): 0.3478980590342274\n",
            "Iteration 8667, Norm of Gradient: 0.005047162928230202, Cost (Train): 0.28531654825782937\n",
            "Iteration 8667, Cost (Validation): 0.34789895563051654\n",
            "Iteration 8668, Norm of Gradient: 0.0050466798217811085, Cost (Train): 0.28531400148199104\n",
            "Iteration 8668, Cost (Validation): 0.3478998524084288\n",
            "Iteration 8669, Norm of Gradient: 0.005046196803539377, Cost (Train): 0.2853114551936231\n",
            "Iteration 8669, Cost (Validation): 0.34790074936787907\n",
            "Iteration 8670, Norm of Gradient: 0.0050457138734792345, Cost (Train): 0.28530890939259007\n",
            "Iteration 8670, Cost (Validation): 0.34790164650878225\n",
            "Iteration 8671, Norm of Gradient: 0.005045231031574921, Cost (Train): 0.2853063640787562\n",
            "Iteration 8671, Cost (Validation): 0.34790254383105335\n",
            "Iteration 8672, Norm of Gradient: 0.005044748277800683, Cost (Train): 0.28530381925198595\n",
            "Iteration 8672, Cost (Validation): 0.3479034413346075\n",
            "Iteration 8673, Norm of Gradient: 0.005044265612130781, Cost (Train): 0.2853012749121439\n",
            "Iteration 8673, Cost (Validation): 0.3479043390193596\n",
            "Iteration 8674, Norm of Gradient: 0.0050437830345394855, Cost (Train): 0.2852987310590947\n",
            "Iteration 8674, Cost (Validation): 0.34790523688522496\n",
            "Iteration 8675, Norm of Gradient: 0.005043300545001077, Cost (Train): 0.2852961876927028\n",
            "Iteration 8675, Cost (Validation): 0.3479061349321186\n",
            "Iteration 8676, Norm of Gradient: 0.005042818143489845, Cost (Train): 0.2852936448128331\n",
            "Iteration 8676, Cost (Validation): 0.3479070331599557\n",
            "Iteration 8677, Norm of Gradient: 0.0050423358299800936, Cost (Train): 0.2852911024193501\n",
            "Iteration 8677, Cost (Validation): 0.3479079315686516\n",
            "Iteration 8678, Norm of Gradient: 0.00504185360444613, Cost (Train): 0.28528856051211865\n",
            "Iteration 8678, Cost (Validation): 0.34790883015812135\n",
            "Iteration 8679, Norm of Gradient: 0.005041371466862282, Cost (Train): 0.2852860190910036\n",
            "Iteration 8679, Cost (Validation): 0.3479097289282805\n",
            "Iteration 8680, Norm of Gradient: 0.005040889417202879, Cost (Train): 0.28528347815586985\n",
            "Iteration 8680, Cost (Validation): 0.3479106278790442\n",
            "Iteration 8681, Norm of Gradient: 0.005040407455442265, Cost (Train): 0.2852809377065823\n",
            "Iteration 8681, Cost (Validation): 0.3479115270103279\n",
            "Iteration 8682, Norm of Gradient: 0.005039925581554794, Cost (Train): 0.2852783977430059\n",
            "Iteration 8682, Cost (Validation): 0.34791242632204705\n",
            "Iteration 8683, Norm of Gradient: 0.0050394437955148304, Cost (Train): 0.2852758582650058\n",
            "Iteration 8683, Cost (Validation): 0.34791332581411705\n",
            "Iteration 8684, Norm of Gradient: 0.00503896209729675, Cost (Train): 0.28527331927244687\n",
            "Iteration 8684, Cost (Validation): 0.34791422548645334\n",
            "Iteration 8685, Norm of Gradient: 0.005038480486874935, Cost (Train): 0.28527078076519435\n",
            "Iteration 8685, Cost (Validation): 0.34791512533897156\n",
            "Iteration 8686, Norm of Gradient: 0.005037998964223784, Cost (Train): 0.2852682427431136\n",
            "Iteration 8686, Cost (Validation): 0.34791602537158717\n",
            "Iteration 8687, Norm of Gradient: 0.0050375175293177026, Cost (Train): 0.2852657052060696\n",
            "Iteration 8687, Cost (Validation): 0.34791692558421594\n",
            "Iteration 8688, Norm of Gradient: 0.005037036182131106, Cost (Train): 0.2852631681539277\n",
            "Iteration 8688, Cost (Validation): 0.3479178259767732\n",
            "Iteration 8689, Norm of Gradient: 0.005036554922638422, Cost (Train): 0.2852606315865533\n",
            "Iteration 8689, Cost (Validation): 0.34791872654917494\n",
            "Iteration 8690, Norm of Gradient: 0.005036073750814088, Cost (Train): 0.28525809550381176\n",
            "Iteration 8690, Cost (Validation): 0.3479196273013367\n",
            "Iteration 8691, Norm of Gradient: 0.005035592666632551, Cost (Train): 0.28525555990556856\n",
            "Iteration 8691, Cost (Validation): 0.3479205282331743\n",
            "Iteration 8692, Norm of Gradient: 0.005035111670068269, Cost (Train): 0.2852530247916892\n",
            "Iteration 8692, Cost (Validation): 0.34792142934460346\n",
            "Iteration 8693, Norm of Gradient: 0.0050346307610957125, Cost (Train): 0.2852504901620391\n",
            "Iteration 8693, Cost (Validation): 0.3479223306355401\n",
            "Iteration 8694, Norm of Gradient: 0.005034149939689359, Cost (Train): 0.28524795601648395\n",
            "Iteration 8694, Cost (Validation): 0.3479232321059001\n",
            "Iteration 8695, Norm of Gradient: 0.0050336692058236985, Cost (Train): 0.2852454223548894\n",
            "Iteration 8695, Cost (Validation): 0.3479241337555992\n",
            "Iteration 8696, Norm of Gradient: 0.00503318855947323, Cost (Train): 0.28524288917712115\n",
            "Iteration 8696, Cost (Validation): 0.34792503558455345\n",
            "Iteration 8697, Norm of Gradient: 0.005032708000612464, Cost (Train): 0.2852403564830449\n",
            "Iteration 8697, Cost (Validation): 0.34792593759267887\n",
            "Iteration 8698, Norm of Gradient: 0.005032227529215923, Cost (Train): 0.2852378242725264\n",
            "Iteration 8698, Cost (Validation): 0.3479268397798915\n",
            "Iteration 8699, Norm of Gradient: 0.005031747145258134, Cost (Train): 0.2852352925454316\n",
            "Iteration 8699, Cost (Validation): 0.3479277421461072\n",
            "Iteration 8700, Norm of Gradient: 0.005031266848713641, Cost (Train): 0.2852327613016265\n",
            "Iteration 8700, Cost (Validation): 0.34792864469124224\n",
            "Iteration 8701, Norm of Gradient: 0.005030786639556997, Cost (Train): 0.28523023054097674\n",
            "Iteration 8701, Cost (Validation): 0.34792954741521265\n",
            "Iteration 8702, Norm of Gradient: 0.00503030651776276, Cost (Train): 0.28522770026334854\n",
            "Iteration 8702, Cost (Validation): 0.3479304503179347\n",
            "Iteration 8703, Norm of Gradient: 0.005029826483305504, Cost (Train): 0.28522517046860796\n",
            "Iteration 8703, Cost (Validation): 0.34793135339932446\n",
            "Iteration 8704, Norm of Gradient: 0.005029346536159815, Cost (Train): 0.285222641156621\n",
            "Iteration 8704, Cost (Validation): 0.3479322566592983\n",
            "Iteration 8705, Norm of Gradient: 0.005028866676300282, Cost (Train): 0.28522011232725397\n",
            "Iteration 8705, Cost (Validation): 0.3479331600977724\n",
            "Iteration 8706, Norm of Gradient: 0.00502838690370151, Cost (Train): 0.2852175839803729\n",
            "Iteration 8706, Cost (Validation): 0.34793406371466307\n",
            "Iteration 8707, Norm of Gradient: 0.005027907218338112, Cost (Train): 0.28521505611584397\n",
            "Iteration 8707, Cost (Validation): 0.34793496750988673\n",
            "Iteration 8708, Norm of Gradient: 0.005027427620184713, Cost (Train): 0.28521252873353375\n",
            "Iteration 8708, Cost (Validation): 0.3479358714833598\n",
            "Iteration 8709, Norm of Gradient: 0.005026948109215946, Cost (Train): 0.2852100018333085\n",
            "Iteration 8709, Cost (Validation): 0.3479367756349986\n",
            "Iteration 8710, Norm of Gradient: 0.0050264686854064565, Cost (Train): 0.28520747541503455\n",
            "Iteration 8710, Cost (Validation): 0.3479376799647196\n",
            "Iteration 8711, Norm of Gradient: 0.005025989348730902, Cost (Train): 0.28520494947857844\n",
            "Iteration 8711, Cost (Validation): 0.34793858447243936\n",
            "Iteration 8712, Norm of Gradient: 0.005025510099163942, Cost (Train): 0.2852024240238066\n",
            "Iteration 8712, Cost (Validation): 0.34793948915807443\n",
            "Iteration 8713, Norm of Gradient: 0.0050250309366802574, Cost (Train): 0.2851998990505857\n",
            "Iteration 8713, Cost (Validation): 0.3479403940215413\n",
            "Iteration 8714, Norm of Gradient: 0.0050245518612545325, Cost (Train): 0.28519737455878214\n",
            "Iteration 8714, Cost (Validation): 0.3479412990627567\n",
            "Iteration 8715, Norm of Gradient: 0.005024072872861462, Cost (Train): 0.28519485054826277\n",
            "Iteration 8715, Cost (Validation): 0.3479422042816372\n",
            "Iteration 8716, Norm of Gradient: 0.005023593971475754, Cost (Train): 0.2851923270188943\n",
            "Iteration 8716, Cost (Validation): 0.3479431096780995\n",
            "Iteration 8717, Norm of Gradient: 0.005023115157072125, Cost (Train): 0.2851898039705434\n",
            "Iteration 8717, Cost (Validation): 0.3479440152520603\n",
            "Iteration 8718, Norm of Gradient: 0.005022636429625301, Cost (Train): 0.28518728140307686\n",
            "Iteration 8718, Cost (Validation): 0.34794492100343644\n",
            "Iteration 8719, Norm of Gradient: 0.0050221577891100206, Cost (Train): 0.2851847593163616\n",
            "Iteration 8719, Cost (Validation): 0.34794582693214465\n",
            "Iteration 8720, Norm of Gradient: 0.005021679235501031, Cost (Train): 0.2851822377102646\n",
            "Iteration 8720, Cost (Validation): 0.34794673303810175\n",
            "Iteration 8721, Norm of Gradient: 0.005021200768773089, Cost (Train): 0.2851797165846527\n",
            "Iteration 8721, Cost (Validation): 0.3479476393212247\n",
            "Iteration 8722, Norm of Gradient: 0.005020722388900965, Cost (Train): 0.2851771959393929\n",
            "Iteration 8722, Cost (Validation): 0.3479485457814303\n",
            "Iteration 8723, Norm of Gradient: 0.005020244095859436, Cost (Train): 0.28517467577435235\n",
            "Iteration 8723, Cost (Validation): 0.34794945241863556\n",
            "Iteration 8724, Norm of Gradient: 0.005019765889623288, Cost (Train): 0.2851721560893981\n",
            "Iteration 8724, Cost (Validation): 0.34795035923275747\n",
            "Iteration 8725, Norm of Gradient: 0.005019287770167324, Cost (Train): 0.2851696368843974\n",
            "Iteration 8725, Cost (Validation): 0.34795126622371303\n",
            "Iteration 8726, Norm of Gradient: 0.00501880973746635, Cost (Train): 0.28516711815921736\n",
            "Iteration 8726, Cost (Validation): 0.34795217339141926\n",
            "Iteration 8727, Norm of Gradient: 0.005018331791495187, Cost (Train): 0.2851645999137253\n",
            "Iteration 8727, Cost (Validation): 0.34795308073579334\n",
            "Iteration 8728, Norm of Gradient: 0.005017853932228663, Cost (Train): 0.28516208214778843\n",
            "Iteration 8728, Cost (Validation): 0.3479539882567524\n",
            "Iteration 8729, Norm of Gradient: 0.00501737615964162, Cost (Train): 0.28515956486127425\n",
            "Iteration 8729, Cost (Validation): 0.3479548959542135\n",
            "Iteration 8730, Norm of Gradient: 0.005016898473708905, Cost (Train): 0.28515704805405007\n",
            "Iteration 8730, Cost (Validation): 0.3479558038280939\n",
            "Iteration 8731, Norm of Gradient: 0.005016420874405379, Cost (Train): 0.28515453172598343\n",
            "Iteration 8731, Cost (Validation): 0.34795671187831084\n",
            "Iteration 8732, Norm of Gradient: 0.005015943361705913, Cost (Train): 0.28515201587694183\n",
            "Iteration 8732, Cost (Validation): 0.34795762010478165\n",
            "Iteration 8733, Norm of Gradient: 0.005015465935585388, Cost (Train): 0.28514950050679283\n",
            "Iteration 8733, Cost (Validation): 0.3479585285074236\n",
            "Iteration 8734, Norm of Gradient: 0.0050149885960186925, Cost (Train): 0.2851469856154039\n",
            "Iteration 8734, Cost (Validation): 0.347959437086154\n",
            "Iteration 8735, Norm of Gradient: 0.005014511342980729, Cost (Train): 0.2851444712026429\n",
            "Iteration 8735, Cost (Validation): 0.34796034584089036\n",
            "Iteration 8736, Norm of Gradient: 0.0050140341764464075, Cost (Train): 0.2851419572683775\n",
            "Iteration 8736, Cost (Validation): 0.34796125477155\n",
            "Iteration 8737, Norm of Gradient: 0.00501355709639065, Cost (Train): 0.28513944381247536\n",
            "Iteration 8737, Cost (Validation): 0.34796216387805035\n",
            "Iteration 8738, Norm of Gradient: 0.005013080102788387, Cost (Train): 0.2851369308348044\n",
            "Iteration 8738, Cost (Validation): 0.34796307316030906\n",
            "Iteration 8739, Norm of Gradient: 0.00501260319561456, Cost (Train): 0.28513441833523245\n",
            "Iteration 8739, Cost (Validation): 0.3479639826182435\n",
            "Iteration 8740, Norm of Gradient: 0.0050121263748441214, Cost (Train): 0.28513190631362734\n",
            "Iteration 8740, Cost (Validation): 0.3479648922517714\n",
            "Iteration 8741, Norm of Gradient: 0.005011649640452033, Cost (Train): 0.28512939476985716\n",
            "Iteration 8741, Cost (Validation): 0.34796580206081024\n",
            "Iteration 8742, Norm of Gradient: 0.0050111729924132665, Cost (Train): 0.2851268837037899\n",
            "Iteration 8742, Cost (Validation): 0.3479667120452777\n",
            "Iteration 8743, Norm of Gradient: 0.005010696430702803, Cost (Train): 0.28512437311529354\n",
            "Iteration 8743, Cost (Validation): 0.34796762220509153\n",
            "Iteration 8744, Norm of Gradient: 0.005010219955295636, Cost (Train): 0.28512186300423625\n",
            "Iteration 8744, Cost (Validation): 0.34796853254016935\n",
            "Iteration 8745, Norm of Gradient: 0.005009743566166768, Cost (Train): 0.28511935337048616\n",
            "Iteration 8745, Cost (Validation): 0.34796944305042893\n",
            "Iteration 8746, Norm of Gradient: 0.0050092672632912096, Cost (Train): 0.28511684421391154\n",
            "Iteration 8746, Cost (Validation): 0.3479703537357882\n",
            "Iteration 8747, Norm of Gradient: 0.005008791046643985, Cost (Train): 0.28511433553438054\n",
            "Iteration 8747, Cost (Validation): 0.34797126459616473\n",
            "Iteration 8748, Norm of Gradient: 0.005008314916200127, Cost (Train): 0.28511182733176155\n",
            "Iteration 8748, Cost (Validation): 0.34797217563147664\n",
            "Iteration 8749, Norm of Gradient: 0.005007838871934677, Cost (Train): 0.28510931960592295\n",
            "Iteration 8749, Cost (Validation): 0.34797308684164174\n",
            "Iteration 8750, Norm of Gradient: 0.005007362913822689, Cost (Train): 0.2851068123567331\n",
            "Iteration 8750, Cost (Validation): 0.3479739982265779\n",
            "Iteration 8751, Norm of Gradient: 0.0050068870418392255, Cost (Train): 0.2851043055840604\n",
            "Iteration 8751, Cost (Validation): 0.34797490978620327\n",
            "Iteration 8752, Norm of Gradient: 0.00500641125595936, Cost (Train): 0.28510179928777346\n",
            "Iteration 8752, Cost (Validation): 0.34797582152043555\n",
            "Iteration 8753, Norm of Gradient: 0.0050059355561581755, Cost (Train): 0.2850992934677409\n",
            "Iteration 8753, Cost (Validation): 0.3479767334291931\n",
            "Iteration 8754, Norm of Gradient: 0.005005459942410765, Cost (Train): 0.28509678812383116\n",
            "Iteration 8754, Cost (Validation): 0.3479776455123939\n",
            "Iteration 8755, Norm of Gradient: 0.005004984414692232, Cost (Train): 0.28509428325591296\n",
            "Iteration 8755, Cost (Validation): 0.34797855776995595\n",
            "Iteration 8756, Norm of Gradient: 0.005004508972977688, Cost (Train): 0.28509177886385506\n",
            "Iteration 8756, Cost (Validation): 0.34797947020179765\n",
            "Iteration 8757, Norm of Gradient: 0.00500403361724226, Cost (Train): 0.28508927494752606\n",
            "Iteration 8757, Cost (Validation): 0.34798038280783694\n",
            "Iteration 8758, Norm of Gradient: 0.00500355834746108, Cost (Train): 0.2850867715067949\n",
            "Iteration 8758, Cost (Validation): 0.3479812955879922\n",
            "Iteration 8759, Norm of Gradient: 0.00500308316360929, Cost (Train): 0.2850842685415305\n",
            "Iteration 8759, Cost (Validation): 0.3479822085421816\n",
            "Iteration 8760, Norm of Gradient: 0.0050026080656620465, Cost (Train): 0.2850817660516016\n",
            "Iteration 8760, Cost (Validation): 0.3479831216703235\n",
            "Iteration 8761, Norm of Gradient: 0.0050021330535945105, Cost (Train): 0.2850792640368772\n",
            "Iteration 8761, Cost (Validation): 0.3479840349723364\n",
            "Iteration 8762, Norm of Gradient: 0.005001658127381857, Cost (Train): 0.28507676249722647\n",
            "Iteration 8762, Cost (Validation): 0.3479849484481384\n",
            "Iteration 8763, Norm of Gradient: 0.005001183286999269, Cost (Train): 0.28507426143251813\n",
            "Iteration 8763, Cost (Validation): 0.347985862097648\n",
            "Iteration 8764, Norm of Gradient: 0.005000708532421941, Cost (Train): 0.28507176084262154\n",
            "Iteration 8764, Cost (Validation): 0.34798677592078364\n",
            "Iteration 8765, Norm of Gradient: 0.005000233863625076, Cost (Train): 0.2850692607274058\n",
            "Iteration 8765, Cost (Validation): 0.3479876899174639\n",
            "Iteration 8766, Norm of Gradient: 0.004999759280583888, Cost (Train): 0.28506676108674\n",
            "Iteration 8766, Cost (Validation): 0.3479886040876072\n",
            "Iteration 8767, Norm of Gradient: 0.004999284783273601, Cost (Train): 0.28506426192049356\n",
            "Iteration 8767, Cost (Validation): 0.3479895184311321\n",
            "Iteration 8768, Norm of Gradient: 0.004998810371669449, Cost (Train): 0.2850617632285356\n",
            "Iteration 8768, Cost (Validation): 0.3479904329479573\n",
            "Iteration 8769, Norm of Gradient: 0.004998336045746676, Cost (Train): 0.2850592650107357\n",
            "Iteration 8769, Cost (Validation): 0.3479913476380013\n",
            "Iteration 8770, Norm of Gradient: 0.004997861805480535, Cost (Train): 0.28505676726696294\n",
            "Iteration 8770, Cost (Validation): 0.3479922625011828\n",
            "Iteration 8771, Norm of Gradient: 0.00499738765084629, Cost (Train): 0.285054269997087\n",
            "Iteration 8771, Cost (Validation): 0.34799317753742054\n",
            "Iteration 8772, Norm of Gradient: 0.004996913581819215, Cost (Train): 0.2850517732009773\n",
            "Iteration 8772, Cost (Validation): 0.3479940927466331\n",
            "Iteration 8773, Norm of Gradient: 0.004996439598374593, Cost (Train): 0.2850492768785034\n",
            "Iteration 8773, Cost (Validation): 0.3479950081287395\n",
            "Iteration 8774, Norm of Gradient: 0.00499596570048772, Cost (Train): 0.28504678102953485\n",
            "Iteration 8774, Cost (Validation): 0.3479959236836584\n",
            "Iteration 8775, Norm of Gradient: 0.004995491888133897, Cost (Train): 0.28504428565394135\n",
            "Iteration 8775, Cost (Validation): 0.3479968394113087\n",
            "Iteration 8776, Norm of Gradient: 0.004995018161288439, Cost (Train): 0.2850417907515925\n",
            "Iteration 8776, Cost (Validation): 0.3479977553116092\n",
            "Iteration 8777, Norm of Gradient: 0.004994544519926668, Cost (Train): 0.2850392963223582\n",
            "Iteration 8777, Cost (Validation): 0.3479986713844789\n",
            "Iteration 8778, Norm of Gradient: 0.00499407096402392, Cost (Train): 0.2850368023661081\n",
            "Iteration 8778, Cost (Validation): 0.3479995876298367\n",
            "Iteration 8779, Norm of Gradient: 0.004993597493555537, Cost (Train): 0.28503430888271203\n",
            "Iteration 8779, Cost (Validation): 0.3480005040476016\n",
            "Iteration 8780, Norm of Gradient: 0.004993124108496874, Cost (Train): 0.28503181587204\n",
            "Iteration 8780, Cost (Validation): 0.3480014206376926\n",
            "Iteration 8781, Norm of Gradient: 0.004992650808823293, Cost (Train): 0.2850293233339618\n",
            "Iteration 8781, Cost (Validation): 0.3480023374000288\n",
            "Iteration 8782, Norm of Gradient: 0.004992177594510166, Cost (Train): 0.2850268312683475\n",
            "Iteration 8782, Cost (Validation): 0.3480032543345293\n",
            "Iteration 8783, Norm of Gradient: 0.004991704465532878, Cost (Train): 0.2850243396750671\n",
            "Iteration 8783, Cost (Validation): 0.3480041714411132\n",
            "Iteration 8784, Norm of Gradient: 0.004991231421866823, Cost (Train): 0.28502184855399065\n",
            "Iteration 8784, Cost (Validation): 0.3480050887196997\n",
            "Iteration 8785, Norm of Gradient: 0.0049907584634874035, Cost (Train): 0.28501935790498834\n",
            "Iteration 8785, Cost (Validation): 0.34800600617020794\n",
            "Iteration 8786, Norm of Gradient: 0.00499028559037003, Cost (Train): 0.2850168677279303\n",
            "Iteration 8786, Cost (Validation): 0.3480069237925572\n",
            "Iteration 8787, Norm of Gradient: 0.0049898128024901305, Cost (Train): 0.2850143780226868\n",
            "Iteration 8787, Cost (Validation): 0.34800784158666676\n",
            "Iteration 8788, Norm of Gradient: 0.0049893400998231325, Cost (Train): 0.2850118887891281\n",
            "Iteration 8788, Cost (Validation): 0.3480087595524559\n",
            "Iteration 8789, Norm of Gradient: 0.004988867482344483, Cost (Train): 0.2850094000271245\n",
            "Iteration 8789, Cost (Validation): 0.3480096776898441\n",
            "Iteration 8790, Norm of Gradient: 0.004988394950029631, Cost (Train): 0.2850069117365464\n",
            "Iteration 8790, Cost (Validation): 0.34801059599875056\n",
            "Iteration 8791, Norm of Gradient: 0.004987922502854042, Cost (Train): 0.2850044239172642\n",
            "Iteration 8791, Cost (Validation): 0.3480115144790948\n",
            "Iteration 8792, Norm of Gradient: 0.004987450140793186, Cost (Train): 0.2850019365691484\n",
            "Iteration 8792, Cost (Validation): 0.3480124331307962\n",
            "Iteration 8793, Norm of Gradient: 0.004986977863822547, Cost (Train): 0.2849994496920695\n",
            "Iteration 8793, Cost (Validation): 0.3480133519537743\n",
            "Iteration 8794, Norm of Gradient: 0.004986505671917617, Cost (Train): 0.2849969632858981\n",
            "Iteration 8794, Cost (Validation): 0.3480142709479486\n",
            "Iteration 8795, Norm of Gradient: 0.004986033565053897, Cost (Train): 0.2849944773505047\n",
            "Iteration 8795, Cost (Validation): 0.3480151901132388\n",
            "Iteration 8796, Norm of Gradient: 0.004985561543206899, Cost (Train): 0.28499199188576013\n",
            "Iteration 8796, Cost (Validation): 0.34801610944956424\n",
            "Iteration 8797, Norm of Gradient: 0.004985089606352145, Cost (Train): 0.2849895068915349\n",
            "Iteration 8797, Cost (Validation): 0.3480170289568448\n",
            "Iteration 8798, Norm of Gradient: 0.004984617754465168, Cost (Train): 0.2849870223677\n",
            "Iteration 8798, Cost (Validation): 0.3480179486349999\n",
            "Iteration 8799, Norm of Gradient: 0.004984145987521506, Cost (Train): 0.2849845383141261\n",
            "Iteration 8799, Cost (Validation): 0.3480188684839494\n",
            "Iteration 8800, Norm of Gradient: 0.004983674305496713, Cost (Train): 0.2849820547306841\n",
            "Iteration 8800, Cost (Validation): 0.34801978850361304\n",
            "Iteration 8801, Norm of Gradient: 0.00498320270836635, Cost (Train): 0.28497957161724485\n",
            "Iteration 8801, Cost (Validation): 0.3480207086939106\n",
            "Iteration 8802, Norm of Gradient: 0.004982731196105985, Cost (Train): 0.2849770889736794\n",
            "Iteration 8802, Cost (Validation): 0.3480216290547618\n",
            "Iteration 8803, Norm of Gradient: 0.004982259768691204, Cost (Train): 0.28497460679985864\n",
            "Iteration 8803, Cost (Validation): 0.3480225495860865\n",
            "Iteration 8804, Norm of Gradient: 0.004981788426097593, Cost (Train): 0.2849721250956537\n",
            "Iteration 8804, Cost (Validation): 0.3480234702878047\n",
            "Iteration 8805, Norm of Gradient: 0.004981317168300754, Cost (Train): 0.28496964386093565\n",
            "Iteration 8805, Cost (Validation): 0.34802439115983624\n",
            "Iteration 8806, Norm of Gradient: 0.004980845995276297, Cost (Train): 0.2849671630955757\n",
            "Iteration 8806, Cost (Validation): 0.34802531220210103\n",
            "Iteration 8807, Norm of Gradient: 0.0049803749069998425, Cost (Train): 0.284964682799445\n",
            "Iteration 8807, Cost (Validation): 0.3480262334145191\n",
            "Iteration 8808, Norm of Gradient: 0.00497990390344702, Cost (Train): 0.28496220297241476\n",
            "Iteration 8808, Cost (Validation): 0.34802715479701046\n",
            "Iteration 8809, Norm of Gradient: 0.004979432984593468, Cost (Train): 0.28495972361435623\n",
            "Iteration 8809, Cost (Validation): 0.3480280763494951\n",
            "Iteration 8810, Norm of Gradient: 0.004978962150414838, Cost (Train): 0.28495724472514095\n",
            "Iteration 8810, Cost (Validation): 0.34802899807189325\n",
            "Iteration 8811, Norm of Gradient: 0.004978491400886788, Cost (Train): 0.28495476630464006\n",
            "Iteration 8811, Cost (Validation): 0.3480299199641249\n",
            "Iteration 8812, Norm of Gradient: 0.004978020735984985, Cost (Train): 0.28495228835272524\n",
            "Iteration 8812, Cost (Validation): 0.34803084202611034\n",
            "Iteration 8813, Norm of Gradient: 0.004977550155685112, Cost (Train): 0.28494981086926774\n",
            "Iteration 8813, Cost (Validation): 0.3480317642577697\n",
            "Iteration 8814, Norm of Gradient: 0.004977079659962854, Cost (Train): 0.28494733385413923\n",
            "Iteration 8814, Cost (Validation): 0.34803268665902326\n",
            "Iteration 8815, Norm of Gradient: 0.004976609248793911, Cost (Train): 0.28494485730721125\n",
            "Iteration 8815, Cost (Validation): 0.34803360922979126\n",
            "Iteration 8816, Norm of Gradient: 0.00497613892215399, Cost (Train): 0.2849423812283555\n",
            "Iteration 8816, Cost (Validation): 0.34803453196999395\n",
            "Iteration 8817, Norm of Gradient: 0.004975668680018809, Cost (Train): 0.28493990561744353\n",
            "Iteration 8817, Cost (Validation): 0.3480354548795517\n",
            "Iteration 8818, Norm of Gradient: 0.004975198522364096, Cost (Train): 0.28493743047434716\n",
            "Iteration 8818, Cost (Validation): 0.3480363779583849\n",
            "Iteration 8819, Norm of Gradient: 0.004974728449165587, Cost (Train): 0.2849349557989382\n",
            "Iteration 8819, Cost (Validation): 0.34803730120641396\n",
            "Iteration 8820, Norm of Gradient: 0.00497425846039903, Cost (Train): 0.2849324815910884\n",
            "Iteration 8820, Cost (Validation): 0.3480382246235593\n",
            "Iteration 8821, Norm of Gradient: 0.004973788556040183, Cost (Train): 0.28493000785066963\n",
            "Iteration 8821, Cost (Validation): 0.3480391482097415\n",
            "Iteration 8822, Norm of Gradient: 0.00497331873606481, Cost (Train): 0.28492753457755376\n",
            "Iteration 8822, Cost (Validation): 0.3480400719648809\n",
            "Iteration 8823, Norm of Gradient: 0.0049728490004486885, Cost (Train): 0.28492506177161286\n",
            "Iteration 8823, Cost (Validation): 0.3480409958888982\n",
            "Iteration 8824, Norm of Gradient: 0.0049723793491676035, Cost (Train): 0.28492258943271886\n",
            "Iteration 8824, Cost (Validation): 0.3480419199817139\n",
            "Iteration 8825, Norm of Gradient: 0.004971909782197352, Cost (Train): 0.28492011756074387\n",
            "Iteration 8825, Cost (Validation): 0.3480428442432487\n",
            "Iteration 8826, Norm of Gradient: 0.004971440299513738, Cost (Train): 0.28491764615556\n",
            "Iteration 8826, Cost (Validation): 0.34804376867342324\n",
            "Iteration 8827, Norm of Gradient: 0.004970970901092577, Cost (Train): 0.28491517521703935\n",
            "Iteration 8827, Cost (Validation): 0.3480446932721581\n",
            "Iteration 8828, Norm of Gradient: 0.004970501586909694, Cost (Train): 0.2849127047450542\n",
            "Iteration 8828, Cost (Validation): 0.34804561803937406\n",
            "Iteration 8829, Norm of Gradient: 0.004970032356940924, Cost (Train): 0.2849102347394767\n",
            "Iteration 8829, Cost (Validation): 0.348046542974992\n",
            "Iteration 8830, Norm of Gradient: 0.004969563211162109, Cost (Train): 0.2849077652001793\n",
            "Iteration 8830, Cost (Validation): 0.3480474680789325\n",
            "Iteration 8831, Norm of Gradient: 0.004969094149549105, Cost (Train): 0.2849052961270342\n",
            "Iteration 8831, Cost (Validation): 0.34804839335111665\n",
            "Iteration 8832, Norm of Gradient: 0.004968625172077773, Cost (Train): 0.28490282751991386\n",
            "Iteration 8832, Cost (Validation): 0.34804931879146506\n",
            "Iteration 8833, Norm of Gradient: 0.0049681562787239906, Cost (Train): 0.2849003593786907\n",
            "Iteration 8833, Cost (Validation): 0.3480502443998988\n",
            "Iteration 8834, Norm of Gradient: 0.004967687469463635, Cost (Train): 0.2848978917032373\n",
            "Iteration 8834, Cost (Validation): 0.34805117017633874\n",
            "Iteration 8835, Norm of Gradient: 0.0049672187442726026, Cost (Train): 0.28489542449342603\n",
            "Iteration 8835, Cost (Validation): 0.34805209612070587\n",
            "Iteration 8836, Norm of Gradient: 0.0049667501031267925, Cost (Train): 0.2848929577491296\n",
            "Iteration 8836, Cost (Validation): 0.34805302223292117\n",
            "Iteration 8837, Norm of Gradient: 0.004966281546002121, Cost (Train): 0.2848904914702206\n",
            "Iteration 8837, Cost (Validation): 0.34805394851290566\n",
            "Iteration 8838, Norm of Gradient: 0.004965813072874505, Cost (Train): 0.2848880256565717\n",
            "Iteration 8838, Cost (Validation): 0.3480548749605805\n",
            "Iteration 8839, Norm of Gradient: 0.004965344683719878, Cost (Train): 0.2848855603080557\n",
            "Iteration 8839, Cost (Validation): 0.34805580157586674\n",
            "Iteration 8840, Norm of Gradient: 0.004964876378514178, Cost (Train): 0.28488309542454526\n",
            "Iteration 8840, Cost (Validation): 0.34805672835868556\n",
            "Iteration 8841, Norm of Gradient: 0.00496440815723336, Cost (Train): 0.2848806310059133\n",
            "Iteration 8841, Cost (Validation): 0.348057655308958\n",
            "Iteration 8842, Norm of Gradient: 0.004963940019853379, Cost (Train): 0.2848781670520327\n",
            "Iteration 8842, Cost (Validation): 0.3480585824266055\n",
            "Iteration 8843, Norm of Gradient: 0.00496347196635021, Cost (Train): 0.2848757035627763\n",
            "Iteration 8843, Cost (Validation): 0.34805950971154914\n",
            "Iteration 8844, Norm of Gradient: 0.004963003996699827, Cost (Train): 0.2848732405380171\n",
            "Iteration 8844, Cost (Validation): 0.34806043716371027\n",
            "Iteration 8845, Norm of Gradient: 0.0049625361108782216, Cost (Train): 0.28487077797762805\n",
            "Iteration 8845, Cost (Validation): 0.3480613647830102\n",
            "Iteration 8846, Norm of Gradient: 0.004962068308861392, Cost (Train): 0.2848683158814823\n",
            "Iteration 8846, Cost (Validation): 0.3480622925693702\n",
            "Iteration 8847, Norm of Gradient: 0.004961600590625345, Cost (Train): 0.28486585424945293\n",
            "Iteration 8847, Cost (Validation): 0.3480632205227118\n",
            "Iteration 8848, Norm of Gradient: 0.004961132956146099, Cost (Train): 0.2848633930814131\n",
            "Iteration 8848, Cost (Validation): 0.3480641486429564\n",
            "Iteration 8849, Norm of Gradient: 0.004960665405399681, Cost (Train): 0.284860932377236\n",
            "Iteration 8849, Cost (Validation): 0.3480650769300254\n",
            "Iteration 8850, Norm of Gradient: 0.004960197938362129, Cost (Train): 0.2848584721367948\n",
            "Iteration 8850, Cost (Validation): 0.34806600538384025\n",
            "Iteration 8851, Norm of Gradient: 0.004959730555009489, Cost (Train): 0.28485601235996305\n",
            "Iteration 8851, Cost (Validation): 0.3480669340043226\n",
            "Iteration 8852, Norm of Gradient: 0.004959263255317816, Cost (Train): 0.2848535530466138\n",
            "Iteration 8852, Cost (Validation): 0.34806786279139384\n",
            "Iteration 8853, Norm of Gradient: 0.004958796039263176, Cost (Train): 0.2848510941966205\n",
            "Iteration 8853, Cost (Validation): 0.34806879174497574\n",
            "Iteration 8854, Norm of Gradient: 0.0049583289068216435, Cost (Train): 0.28484863580985675\n",
            "Iteration 8854, Cost (Validation): 0.34806972086498983\n",
            "Iteration 8855, Norm of Gradient: 0.004957861857969304, Cost (Train): 0.2848461778861959\n",
            "Iteration 8855, Cost (Validation): 0.3480706501513578\n",
            "Iteration 8856, Norm of Gradient: 0.004957394892682252, Cost (Train): 0.2848437204255115\n",
            "Iteration 8856, Cost (Validation): 0.34807157960400137\n",
            "Iteration 8857, Norm of Gradient: 0.00495692801093659, Cost (Train): 0.2848412634276771\n",
            "Iteration 8857, Cost (Validation): 0.3480725092228422\n",
            "Iteration 8858, Norm of Gradient: 0.004956461212708435, Cost (Train): 0.2848388068925664\n",
            "Iteration 8858, Cost (Validation): 0.34807343900780213\n",
            "Iteration 8859, Norm of Gradient: 0.0049559944979739035, Cost (Train): 0.28483635082005304\n",
            "Iteration 8859, Cost (Validation): 0.3480743689588029\n",
            "Iteration 8860, Norm of Gradient: 0.004955527866709135, Cost (Train): 0.28483389521001073\n",
            "Iteration 8860, Cost (Validation): 0.34807529907576645\n",
            "Iteration 8861, Norm of Gradient: 0.004955061318890265, Cost (Train): 0.2848314400623133\n",
            "Iteration 8861, Cost (Validation): 0.34807622935861454\n",
            "Iteration 8862, Norm of Gradient: 0.0049545948544934505, Cost (Train): 0.28482898537683443\n",
            "Iteration 8862, Cost (Validation): 0.34807715980726917\n",
            "Iteration 8863, Norm of Gradient: 0.004954128473494849, Cost (Train): 0.2848265311534481\n",
            "Iteration 8863, Cost (Validation): 0.34807809042165216\n",
            "Iteration 8864, Norm of Gradient: 0.004953662175870632, Cost (Train): 0.2848240773920282\n",
            "Iteration 8864, Cost (Validation): 0.34807902120168566\n",
            "Iteration 8865, Norm of Gradient: 0.004953195961596979, Cost (Train): 0.2848216240924486\n",
            "Iteration 8865, Cost (Validation): 0.3480799521472915\n",
            "Iteration 8866, Norm of Gradient: 0.004952729830650081, Cost (Train): 0.2848191712545834\n",
            "Iteration 8866, Cost (Validation): 0.34808088325839176\n",
            "Iteration 8867, Norm of Gradient: 0.004952263783006137, Cost (Train): 0.2848167188783066\n",
            "Iteration 8867, Cost (Validation): 0.3480818145349086\n",
            "Iteration 8868, Norm of Gradient: 0.0049517978186413535, Cost (Train): 0.2848142669634924\n",
            "Iteration 8868, Cost (Validation): 0.3480827459767641\n",
            "Iteration 8869, Norm of Gradient: 0.00495133193753195, Cost (Train): 0.28481181551001467\n",
            "Iteration 8869, Cost (Validation): 0.34808367758388037\n",
            "Iteration 8870, Norm of Gradient: 0.0049508661396541545, Cost (Train): 0.28480936451774785\n",
            "Iteration 8870, Cost (Validation): 0.3480846093561796\n",
            "Iteration 8871, Norm of Gradient: 0.004950400424984204, Cost (Train): 0.28480691398656616\n",
            "Iteration 8871, Cost (Validation): 0.348085541293584\n",
            "Iteration 8872, Norm of Gradient: 0.004949934793498343, Cost (Train): 0.2848044639163438\n",
            "Iteration 8872, Cost (Validation): 0.34808647339601584\n",
            "Iteration 8873, Norm of Gradient: 0.00494946924517283, Cost (Train): 0.2848020143069551\n",
            "Iteration 8873, Cost (Validation): 0.3480874056633974\n",
            "Iteration 8874, Norm of Gradient: 0.004949003779983929, Cost (Train): 0.2847995651582745\n",
            "Iteration 8874, Cost (Validation): 0.34808833809565104\n",
            "Iteration 8875, Norm of Gradient: 0.004948538397907916, Cost (Train): 0.2847971164701763\n",
            "Iteration 8875, Cost (Validation): 0.34808927069269896\n",
            "Iteration 8876, Norm of Gradient: 0.004948073098921074, Cost (Train): 0.28479466824253513\n",
            "Iteration 8876, Cost (Validation): 0.3480902034544637\n",
            "Iteration 8877, Norm of Gradient: 0.004947607882999698, Cost (Train): 0.28479222047522545\n",
            "Iteration 8877, Cost (Validation): 0.34809113638086764\n",
            "Iteration 8878, Norm of Gradient: 0.004947142750120091, Cost (Train): 0.28478977316812176\n",
            "Iteration 8878, Cost (Validation): 0.3480920694718332\n",
            "Iteration 8879, Norm of Gradient: 0.004946677700258566, Cost (Train): 0.2847873263210987\n",
            "Iteration 8879, Cost (Validation): 0.3480930027272829\n",
            "Iteration 8880, Norm of Gradient: 0.004946212733391444, Cost (Train): 0.28478487993403095\n",
            "Iteration 8880, Cost (Validation): 0.34809393614713924\n",
            "Iteration 8881, Norm of Gradient: 0.004945747849495058, Cost (Train): 0.2847824340067932\n",
            "Iteration 8881, Cost (Validation): 0.34809486973132475\n",
            "Iteration 8882, Norm of Gradient: 0.004945283048545748, Cost (Train): 0.2847799885392602\n",
            "Iteration 8882, Cost (Validation): 0.34809580347976204\n",
            "Iteration 8883, Norm of Gradient: 0.004944818330519865, Cost (Train): 0.2847775435313068\n",
            "Iteration 8883, Cost (Validation): 0.3480967373923738\n",
            "Iteration 8884, Norm of Gradient: 0.004944353695393771, Cost (Train): 0.28477509898280773\n",
            "Iteration 8884, Cost (Validation): 0.3480976714690826\n",
            "Iteration 8885, Norm of Gradient: 0.00494388914314383, Cost (Train): 0.28477265489363796\n",
            "Iteration 8885, Cost (Validation): 0.34809860570981116\n",
            "Iteration 8886, Norm of Gradient: 0.004943424673746426, Cost (Train): 0.2847702112636723\n",
            "Iteration 8886, Cost (Validation): 0.34809954011448224\n",
            "Iteration 8887, Norm of Gradient: 0.004942960287177944, Cost (Train): 0.284767768092786\n",
            "Iteration 8887, Cost (Validation): 0.3481004746830185\n",
            "Iteration 8888, Norm of Gradient: 0.004942495983414783, Cost (Train): 0.28476532538085375\n",
            "Iteration 8888, Cost (Validation): 0.3481014094153429\n",
            "Iteration 8889, Norm of Gradient: 0.004942031762433349, Cost (Train): 0.2847628831277509\n",
            "Iteration 8889, Cost (Validation): 0.34810234431137815\n",
            "Iteration 8890, Norm of Gradient: 0.0049415676242100595, Cost (Train): 0.2847604413333524\n",
            "Iteration 8890, Cost (Validation): 0.34810327937104707\n",
            "Iteration 8891, Norm of Gradient: 0.004941103568721339, Cost (Train): 0.2847579999975335\n",
            "Iteration 8891, Cost (Validation): 0.3481042145942726\n",
            "Iteration 8892, Norm of Gradient: 0.004940639595943624, Cost (Train): 0.28475555912016937\n",
            "Iteration 8892, Cost (Validation): 0.3481051499809778\n",
            "Iteration 8893, Norm of Gradient: 0.004940175705853357, Cost (Train): 0.2847531187011354\n",
            "Iteration 8893, Cost (Validation): 0.3481060855310854\n",
            "Iteration 8894, Norm of Gradient: 0.004939711898426994, Cost (Train): 0.2847506787403067\n",
            "Iteration 8894, Cost (Validation): 0.3481070212445186\n",
            "Iteration 8895, Norm of Gradient: 0.004939248173640996, Cost (Train): 0.2847482392375587\n",
            "Iteration 8895, Cost (Validation): 0.3481079571212003\n",
            "Iteration 8896, Norm of Gradient: 0.004938784531471838, Cost (Train): 0.2847458001927668\n",
            "Iteration 8896, Cost (Validation): 0.3481088931610536\n",
            "Iteration 8897, Norm of Gradient: 0.004938320971896, Cost (Train): 0.2847433616058065\n",
            "Iteration 8897, Cost (Validation): 0.34810982936400164\n",
            "Iteration 8898, Norm of Gradient: 0.004937857494889974, Cost (Train): 0.28474092347655316\n",
            "Iteration 8898, Cost (Validation): 0.3481107657299675\n",
            "Iteration 8899, Norm of Gradient: 0.004937394100430262, Cost (Train): 0.2847384858048825\n",
            "Iteration 8899, Cost (Validation): 0.34811170225887433\n",
            "Iteration 8900, Norm of Gradient: 0.004936930788493371, Cost (Train): 0.28473604859067\n",
            "Iteration 8900, Cost (Validation): 0.3481126389506454\n",
            "Iteration 8901, Norm of Gradient: 0.004936467559055822, Cost (Train): 0.28473361183379114\n",
            "Iteration 8901, Cost (Validation): 0.3481135758052039\n",
            "Iteration 8902, Norm of Gradient: 0.004936004412094145, Cost (Train): 0.2847311755341219\n",
            "Iteration 8902, Cost (Validation): 0.3481145128224732\n",
            "Iteration 8903, Norm of Gradient: 0.0049355413475848766, Cost (Train): 0.2847287396915378\n",
            "Iteration 8903, Cost (Validation): 0.3481154500023763\n",
            "Iteration 8904, Norm of Gradient: 0.004935078365504563, Cost (Train): 0.2847263043059146\n",
            "Iteration 8904, Cost (Validation): 0.34811638734483685\n",
            "Iteration 8905, Norm of Gradient: 0.004934615465829762, Cost (Train): 0.2847238693771283\n",
            "Iteration 8905, Cost (Validation): 0.34811732484977814\n",
            "Iteration 8906, Norm of Gradient: 0.004934152648537041, Cost (Train): 0.28472143490505447\n",
            "Iteration 8906, Cost (Validation): 0.34811826251712347\n",
            "Iteration 8907, Norm of Gradient: 0.004933689913602972, Cost (Train): 0.28471900088956925\n",
            "Iteration 8907, Cost (Validation): 0.3481192003467964\n",
            "Iteration 8908, Norm of Gradient: 0.004933227261004142, Cost (Train): 0.2847165673305484\n",
            "Iteration 8908, Cost (Validation): 0.3481201383387203\n",
            "Iteration 8909, Norm of Gradient: 0.004932764690717143, Cost (Train): 0.2847141342278682\n",
            "Iteration 8909, Cost (Validation): 0.3481210764928187\n",
            "Iteration 8910, Norm of Gradient: 0.004932302202718582, Cost (Train): 0.2847117015814044\n",
            "Iteration 8910, Cost (Validation): 0.3481220148090152\n",
            "Iteration 8911, Norm of Gradient: 0.004931839796985068, Cost (Train): 0.2847092693910333\n",
            "Iteration 8911, Cost (Validation): 0.34812295328723325\n",
            "Iteration 8912, Norm of Gradient: 0.004931377473493224, Cost (Train): 0.2847068376566309\n",
            "Iteration 8912, Cost (Validation): 0.34812389192739657\n",
            "Iteration 8913, Norm of Gradient: 0.0049309152322196785, Cost (Train): 0.2847044063780734\n",
            "Iteration 8913, Cost (Validation): 0.34812483072942874\n",
            "Iteration 8914, Norm of Gradient: 0.004930453073141077, Cost (Train): 0.2847019755552371\n",
            "Iteration 8914, Cost (Validation): 0.34812576969325343\n",
            "Iteration 8915, Norm of Gradient: 0.004929990996234064, Cost (Train): 0.2846995451879981\n",
            "Iteration 8915, Cost (Validation): 0.3481267088187943\n",
            "Iteration 8916, Norm of Gradient: 0.004929529001475301, Cost (Train): 0.28469711527623304\n",
            "Iteration 8916, Cost (Validation): 0.34812764810597524\n",
            "Iteration 8917, Norm of Gradient: 0.0049290670888414555, Cost (Train): 0.284694685819818\n",
            "Iteration 8917, Cost (Validation): 0.3481285875547198\n",
            "Iteration 8918, Norm of Gradient: 0.004928605258309204, Cost (Train): 0.2846922568186295\n",
            "Iteration 8918, Cost (Validation): 0.348129527164952\n",
            "Iteration 8919, Norm of Gradient: 0.004928143509855235, Cost (Train): 0.2846898282725438\n",
            "Iteration 8919, Cost (Validation): 0.34813046693659555\n",
            "Iteration 8920, Norm of Gradient: 0.004927681843456243, Cost (Train): 0.28468740018143773\n",
            "Iteration 8920, Cost (Validation): 0.3481314068695743\n",
            "Iteration 8921, Norm of Gradient: 0.004927220259088934, Cost (Train): 0.28468497254518765\n",
            "Iteration 8921, Cost (Validation): 0.3481323469638121\n",
            "Iteration 8922, Norm of Gradient: 0.004926758756730022, Cost (Train): 0.2846825453636702\n",
            "Iteration 8922, Cost (Validation): 0.3481332872192332\n",
            "Iteration 8923, Norm of Gradient: 0.00492629733635623, Cost (Train): 0.2846801186367619\n",
            "Iteration 8923, Cost (Validation): 0.3481342276357613\n",
            "Iteration 8924, Norm of Gradient: 0.0049258359979442905, Cost (Train): 0.2846776923643397\n",
            "Iteration 8924, Cost (Validation): 0.34813516821332036\n",
            "Iteration 8925, Norm of Gradient: 0.004925374741470946, Cost (Train): 0.28467526654628006\n",
            "Iteration 8925, Cost (Validation): 0.34813610895183456\n",
            "Iteration 8926, Norm of Gradient: 0.00492491356691295, Cost (Train): 0.28467284118245983\n",
            "Iteration 8926, Cost (Validation): 0.34813704985122795\n",
            "Iteration 8927, Norm of Gradient: 0.00492445247424706, Cost (Train): 0.2846704162727559\n",
            "Iteration 8927, Cost (Validation): 0.34813799091142467\n",
            "Iteration 8928, Norm of Gradient: 0.004923991463450047, Cost (Train): 0.2846679918170451\n",
            "Iteration 8928, Cost (Validation): 0.3481389321323487\n",
            "Iteration 8929, Norm of Gradient: 0.00492353053449869, Cost (Train): 0.2846655678152043\n",
            "Iteration 8929, Cost (Validation): 0.34813987351392434\n",
            "Iteration 8930, Norm of Gradient: 0.004923069687369775, Cost (Train): 0.28466314426711053\n",
            "Iteration 8930, Cost (Validation): 0.34814081505607575\n",
            "Iteration 8931, Norm of Gradient: 0.004922608922040103, Cost (Train): 0.28466072117264074\n",
            "Iteration 8931, Cost (Validation): 0.3481417567587272\n",
            "Iteration 8932, Norm of Gradient: 0.004922148238486478, Cost (Train): 0.2846582985316719\n",
            "Iteration 8932, Cost (Validation): 0.3481426986218029\n",
            "Iteration 8933, Norm of Gradient: 0.0049216876366857155, Cost (Train): 0.2846558763440813\n",
            "Iteration 8933, Cost (Validation): 0.34814364064522724\n",
            "Iteration 8934, Norm of Gradient: 0.004921227116614643, Cost (Train): 0.2846534546097459\n",
            "Iteration 8934, Cost (Validation): 0.34814458282892446\n",
            "Iteration 8935, Norm of Gradient: 0.0049207666782500915, Cost (Train): 0.284651033328543\n",
            "Iteration 8935, Cost (Validation): 0.348145525172819\n",
            "Iteration 8936, Norm of Gradient: 0.0049203063215689045, Cost (Train): 0.28464861250034984\n",
            "Iteration 8936, Cost (Validation): 0.3481464676768352\n",
            "Iteration 8937, Norm of Gradient: 0.004919846046547936, Cost (Train): 0.2846461921250436\n",
            "Iteration 8937, Cost (Validation): 0.3481474103408976\n",
            "Iteration 8938, Norm of Gradient: 0.0049193858531640456, Cost (Train): 0.2846437722025016\n",
            "Iteration 8938, Cost (Validation): 0.3481483531649305\n",
            "Iteration 8939, Norm of Gradient: 0.004918925741394107, Cost (Train): 0.28464135273260116\n",
            "Iteration 8939, Cost (Validation): 0.34814929614885853\n",
            "Iteration 8940, Norm of Gradient: 0.004918465711214997, Cost (Train): 0.2846389337152199\n",
            "Iteration 8940, Cost (Validation): 0.34815023929260625\n",
            "Iteration 8941, Norm of Gradient: 0.004918005762603606, Cost (Train): 0.28463651515023514\n",
            "Iteration 8941, Cost (Validation): 0.34815118259609806\n",
            "Iteration 8942, Norm of Gradient: 0.004917545895536832, Cost (Train): 0.2846340970375244\n",
            "Iteration 8942, Cost (Validation): 0.3481521260592588\n",
            "Iteration 8943, Norm of Gradient: 0.004917086109991583, Cost (Train): 0.2846316793769651\n",
            "Iteration 8943, Cost (Validation): 0.34815306968201293\n",
            "Iteration 8944, Norm of Gradient: 0.004916626405944773, Cost (Train): 0.2846292621684351\n",
            "Iteration 8944, Cost (Validation): 0.34815401346428515\n",
            "Iteration 8945, Norm of Gradient: 0.004916166783373331, Cost (Train): 0.28462684541181177\n",
            "Iteration 8945, Cost (Validation): 0.34815495740600017\n",
            "Iteration 8946, Norm of Gradient: 0.004915707242254189, Cost (Train): 0.284624429106973\n",
            "Iteration 8946, Cost (Validation): 0.34815590150708275\n",
            "Iteration 8947, Norm of Gradient: 0.004915247782564292, Cost (Train): 0.2846220132537963\n",
            "Iteration 8947, Cost (Validation): 0.3481568457674576\n",
            "Iteration 8948, Norm of Gradient: 0.004914788404280593, Cost (Train): 0.28461959785215973\n",
            "Iteration 8948, Cost (Validation): 0.3481577901870495\n",
            "Iteration 8949, Norm of Gradient: 0.004914329107380054, Cost (Train): 0.2846171829019408\n",
            "Iteration 8949, Cost (Validation): 0.34815873476578335\n",
            "Iteration 8950, Norm of Gradient: 0.004913869891839644, Cost (Train): 0.2846147684030176\n",
            "Iteration 8950, Cost (Validation): 0.348159679503584\n",
            "Iteration 8951, Norm of Gradient: 0.004913410757636348, Cost (Train): 0.2846123543552679\n",
            "Iteration 8951, Cost (Validation): 0.3481606244003764\n",
            "Iteration 8952, Norm of Gradient: 0.00491295170474715, Cost (Train): 0.2846099407585697\n",
            "Iteration 8952, Cost (Validation): 0.34816156945608534\n",
            "Iteration 8953, Norm of Gradient: 0.004912492733149053, Cost (Train): 0.28460752761280106\n",
            "Iteration 8953, Cost (Validation): 0.3481625146706359\n",
            "Iteration 8954, Norm of Gradient: 0.004912033842819062, Cost (Train): 0.28460511491783985\n",
            "Iteration 8954, Cost (Validation): 0.348163460043953\n",
            "Iteration 8955, Norm of Gradient: 0.004911575033734193, Cost (Train): 0.28460270267356436\n",
            "Iteration 8955, Cost (Validation): 0.3481644055759617\n",
            "Iteration 8956, Norm of Gradient: 0.004911116305871472, Cost (Train): 0.28460029087985256\n",
            "Iteration 8956, Cost (Validation): 0.3481653512665871\n",
            "Iteration 8957, Norm of Gradient: 0.004910657659207937, Cost (Train): 0.2845978795365827\n",
            "Iteration 8957, Cost (Validation): 0.3481662971157542\n",
            "Iteration 8958, Norm of Gradient: 0.004910199093720628, Cost (Train): 0.28459546864363305\n",
            "Iteration 8958, Cost (Validation): 0.3481672431233882\n",
            "Iteration 8959, Norm of Gradient: 0.0049097406093865995, Cost (Train): 0.2845930582008818\n",
            "Iteration 8959, Cost (Validation): 0.3481681892894143\n",
            "Iteration 8960, Norm of Gradient: 0.004909282206182914, Cost (Train): 0.2845906482082073\n",
            "Iteration 8960, Cost (Validation): 0.34816913561375756\n",
            "Iteration 8961, Norm of Gradient: 0.004908823884086641, Cost (Train): 0.2845882386654879\n",
            "Iteration 8961, Cost (Validation): 0.34817008209634326\n",
            "Iteration 8962, Norm of Gradient: 0.004908365643074861, Cost (Train): 0.284585829572602\n",
            "Iteration 8962, Cost (Validation): 0.3481710287370967\n",
            "Iteration 8963, Norm of Gradient: 0.004907907483124664, Cost (Train): 0.284583420929428\n",
            "Iteration 8963, Cost (Validation): 0.34817197553594315\n",
            "Iteration 8964, Norm of Gradient: 0.004907449404213148, Cost (Train): 0.2845810127358444\n",
            "Iteration 8964, Cost (Validation): 0.3481729224928079\n",
            "Iteration 8965, Norm of Gradient: 0.00490699140631742, Cost (Train): 0.2845786049917298\n",
            "Iteration 8965, Cost (Validation): 0.3481738696076163\n",
            "Iteration 8966, Norm of Gradient: 0.004906533489414596, Cost (Train): 0.28457619769696274\n",
            "Iteration 8966, Cost (Validation): 0.34817481688029384\n",
            "Iteration 8967, Norm of Gradient: 0.004906075653481801, Cost (Train): 0.2845737908514219\n",
            "Iteration 8967, Cost (Validation): 0.34817576431076586\n",
            "Iteration 8968, Norm of Gradient: 0.00490561789849617, Cost (Train): 0.28457138445498587\n",
            "Iteration 8968, Cost (Validation): 0.3481767118989578\n",
            "Iteration 8969, Norm of Gradient: 0.004905160224434846, Cost (Train): 0.2845689785075333\n",
            "Iteration 8969, Cost (Validation): 0.34817765964479513\n",
            "Iteration 8970, Norm of Gradient: 0.004904702631274982, Cost (Train): 0.2845665730089431\n",
            "Iteration 8970, Cost (Validation): 0.34817860754820346\n",
            "Iteration 8971, Norm of Gradient: 0.004904245118993738, Cost (Train): 0.28456416795909406\n",
            "Iteration 8971, Cost (Validation): 0.34817955560910824\n",
            "Iteration 8972, Norm of Gradient: 0.004903787687568285, Cost (Train): 0.2845617633578649\n",
            "Iteration 8972, Cost (Validation): 0.34818050382743515\n",
            "Iteration 8973, Norm of Gradient: 0.004903330336975803, Cost (Train): 0.2845593592051347\n",
            "Iteration 8973, Cost (Validation): 0.34818145220310964\n",
            "Iteration 8974, Norm of Gradient: 0.004902873067193479, Cost (Train): 0.28455695550078225\n",
            "Iteration 8974, Cost (Validation): 0.3481824007360575\n",
            "Iteration 8975, Norm of Gradient: 0.004902415878198512, Cost (Train): 0.2845545522446865\n",
            "Iteration 8975, Cost (Validation): 0.3481833494262044\n",
            "Iteration 8976, Norm of Gradient: 0.004901958769968105, Cost (Train): 0.28455214943672663\n",
            "Iteration 8976, Cost (Validation): 0.348184298273476\n",
            "Iteration 8977, Norm of Gradient: 0.00490150174247948, Cost (Train): 0.28454974707678155\n",
            "Iteration 8977, Cost (Validation): 0.3481852472777981\n",
            "Iteration 8978, Norm of Gradient: 0.004901044795709854, Cost (Train): 0.2845473451647305\n",
            "Iteration 8978, Cost (Validation): 0.34818619643909643\n",
            "Iteration 8979, Norm of Gradient: 0.004900587929636465, Cost (Train): 0.28454494370045263\n",
            "Iteration 8979, Cost (Validation): 0.3481871457572968\n",
            "Iteration 8980, Norm of Gradient: 0.004900131144236552, Cost (Train): 0.284542542683827\n",
            "Iteration 8980, Cost (Validation): 0.348188095232325\n",
            "Iteration 8981, Norm of Gradient: 0.004899674439487368, Cost (Train): 0.2845401421147331\n",
            "Iteration 8981, Cost (Validation): 0.3481890448641072\n",
            "Iteration 8982, Norm of Gradient: 0.004899217815366174, Cost (Train): 0.28453774199305004\n",
            "Iteration 8982, Cost (Validation): 0.34818999465256895\n",
            "Iteration 8983, Norm of Gradient: 0.004898761271850237, Cost (Train): 0.2845353423186571\n",
            "Iteration 8983, Cost (Validation): 0.3481909445976363\n",
            "Iteration 8984, Norm of Gradient: 0.004898304808916836, Cost (Train): 0.28453294309143384\n",
            "Iteration 8984, Cost (Validation): 0.3481918946992353\n",
            "Iteration 8985, Norm of Gradient: 0.004897848426543259, Cost (Train): 0.2845305443112596\n",
            "Iteration 8985, Cost (Validation): 0.3481928449572918\n",
            "Iteration 8986, Norm of Gradient: 0.004897392124706802, Cost (Train): 0.2845281459780139\n",
            "Iteration 8986, Cost (Validation): 0.348193795371732\n",
            "Iteration 8987, Norm of Gradient: 0.004896935903384766, Cost (Train): 0.28452574809157616\n",
            "Iteration 8987, Cost (Validation): 0.34819474594248184\n",
            "Iteration 8988, Norm of Gradient: 0.00489647976255447, Cost (Train): 0.284523350651826\n",
            "Iteration 8988, Cost (Validation): 0.3481956966694676\n",
            "Iteration 8989, Norm of Gradient: 0.004896023702193233, Cost (Train): 0.284520953658643\n",
            "Iteration 8989, Cost (Validation): 0.3481966475526152\n",
            "Iteration 8990, Norm of Gradient: 0.00489556772227839, Cost (Train): 0.2845185571119068\n",
            "Iteration 8990, Cost (Validation): 0.3481975985918509\n",
            "Iteration 8991, Norm of Gradient: 0.004895111822787278, Cost (Train): 0.2845161610114972\n",
            "Iteration 8991, Cost (Validation): 0.3481985497871009\n",
            "Iteration 8992, Norm of Gradient: 0.00489465600369725, Cost (Train): 0.2845137653572938\n",
            "Iteration 8992, Cost (Validation): 0.34819950113829146\n",
            "Iteration 8993, Norm of Gradient: 0.004894200264985662, Cost (Train): 0.2845113701491764\n",
            "Iteration 8993, Cost (Validation): 0.34820045264534877\n",
            "Iteration 8994, Norm of Gradient: 0.004893744606629882, Cost (Train): 0.2845089753870249\n",
            "Iteration 8994, Cost (Validation): 0.3482014043081991\n",
            "Iteration 8995, Norm of Gradient: 0.004893289028607287, Cost (Train): 0.284506581070719\n",
            "Iteration 8995, Cost (Validation): 0.34820235612676886\n",
            "Iteration 8996, Norm of Gradient: 0.004892833530895261, Cost (Train): 0.2845041872001389\n",
            "Iteration 8996, Cost (Validation): 0.34820330810098443\n",
            "Iteration 8997, Norm of Gradient: 0.004892378113471199, Cost (Train): 0.28450179377516427\n",
            "Iteration 8997, Cost (Validation): 0.3482042602307721\n",
            "Iteration 8998, Norm of Gradient: 0.004891922776312501, Cost (Train): 0.2844994007956752\n",
            "Iteration 8998, Cost (Validation): 0.3482052125160584\n",
            "Iteration 8999, Norm of Gradient: 0.004891467519396584, Cost (Train): 0.28449700826155183\n",
            "Iteration 8999, Cost (Validation): 0.3482061649567697\n",
            "Iteration 9000, Norm of Gradient: 0.004891012342700865, Cost (Train): 0.28449461617267413\n",
            "Iteration 9000, Cost (Validation): 0.34820711755283246\n",
            "Iteration 9001, Norm of Gradient: 0.004890557246202774, Cost (Train): 0.2844922245289223\n",
            "Iteration 9001, Cost (Validation): 0.34820807030417333\n",
            "Iteration 9002, Norm of Gradient: 0.004890102229879749, Cost (Train): 0.2844898333301765\n",
            "Iteration 9002, Cost (Validation): 0.3482090232107187\n",
            "Iteration 9003, Norm of Gradient: 0.00488964729370924, Cost (Train): 0.2844874425763169\n",
            "Iteration 9003, Cost (Validation): 0.34820997627239525\n",
            "Iteration 9004, Norm of Gradient: 0.0048891924376686996, Cost (Train): 0.2844850522672238\n",
            "Iteration 9004, Cost (Validation): 0.3482109294891296\n",
            "Iteration 9005, Norm of Gradient: 0.004888737661735596, Cost (Train): 0.28448266240277753\n",
            "Iteration 9005, Cost (Validation): 0.34821188286084837\n",
            "Iteration 9006, Norm of Gradient: 0.0048882829658873994, Cost (Train): 0.2844802729828585\n",
            "Iteration 9006, Cost (Validation): 0.3482128363874782\n",
            "Iteration 9007, Norm of Gradient: 0.0048878283501015965, Cost (Train): 0.2844778840073469\n",
            "Iteration 9007, Cost (Validation): 0.34821379006894576\n",
            "Iteration 9008, Norm of Gradient: 0.004887373814355675, Cost (Train): 0.28447549547612333\n",
            "Iteration 9008, Cost (Validation): 0.34821474390517804\n",
            "Iteration 9009, Norm of Gradient: 0.004886919358627139, Cost (Train): 0.28447310738906834\n",
            "Iteration 9009, Cost (Validation): 0.3482156978961015\n",
            "Iteration 9010, Norm of Gradient: 0.004886464982893495, Cost (Train): 0.2844707197460622\n",
            "Iteration 9010, Cost (Validation): 0.34821665204164315\n",
            "Iteration 9011, Norm of Gradient: 0.004886010687132262, Cost (Train): 0.2844683325469857\n",
            "Iteration 9011, Cost (Validation): 0.34821760634172977\n",
            "Iteration 9012, Norm of Gradient: 0.004885556471320967, Cost (Train): 0.28446594579171947\n",
            "Iteration 9012, Cost (Validation): 0.34821856079628816\n",
            "Iteration 9013, Norm of Gradient: 0.004885102335437145, Cost (Train): 0.28446355948014396\n",
            "Iteration 9013, Cost (Validation): 0.34821951540524526\n",
            "Iteration 9014, Norm of Gradient: 0.004884648279458341, Cost (Train): 0.28446117361214013\n",
            "Iteration 9014, Cost (Validation): 0.34822047016852814\n",
            "Iteration 9015, Norm of Gradient: 0.004884194303362109, Cost (Train): 0.2844587881875885\n",
            "Iteration 9015, Cost (Validation): 0.3482214250860636\n",
            "Iteration 9016, Norm of Gradient: 0.00488374040712601, Cost (Train): 0.28445640320636995\n",
            "Iteration 9016, Cost (Validation): 0.3482223801577786\n",
            "Iteration 9017, Norm of Gradient: 0.004883286590727615, Cost (Train): 0.2844540186683654\n",
            "Iteration 9017, Cost (Validation): 0.34822333538360034\n",
            "Iteration 9018, Norm of Gradient: 0.004882832854144505, Cost (Train): 0.28445163457345546\n",
            "Iteration 9018, Cost (Validation): 0.3482242907634558\n",
            "Iteration 9019, Norm of Gradient: 0.004882379197354267, Cost (Train): 0.28444925092152146\n",
            "Iteration 9019, Cost (Validation): 0.348225246297272\n",
            "Iteration 9020, Norm of Gradient: 0.004881925620334498, Cost (Train): 0.284446867712444\n",
            "Iteration 9020, Cost (Validation): 0.34822620198497617\n",
            "Iteration 9021, Norm of Gradient: 0.004881472123062806, Cost (Train): 0.2844444849461042\n",
            "Iteration 9021, Cost (Validation): 0.3482271578264954\n",
            "Iteration 9022, Norm of Gradient: 0.004881018705516805, Cost (Train): 0.28444210262238323\n",
            "Iteration 9022, Cost (Validation): 0.3482281138217569\n",
            "Iteration 9023, Norm of Gradient: 0.004880565367674118, Cost (Train): 0.284439720741162\n",
            "Iteration 9023, Cost (Validation): 0.34822906997068787\n",
            "Iteration 9024, Norm of Gradient: 0.004880112109512378, Cost (Train): 0.28443733930232173\n",
            "Iteration 9024, Cost (Validation): 0.3482300262732156\n",
            "Iteration 9025, Norm of Gradient: 0.004879658931009226, Cost (Train): 0.2844349583057436\n",
            "Iteration 9025, Cost (Validation): 0.34823098272926734\n",
            "Iteration 9026, Norm of Gradient: 0.004879205832142311, Cost (Train): 0.2844325777513089\n",
            "Iteration 9026, Cost (Validation): 0.3482319393387704\n",
            "Iteration 9027, Norm of Gradient: 0.004878752812889294, Cost (Train): 0.2844301976388988\n",
            "Iteration 9027, Cost (Validation): 0.3482328961016521\n",
            "Iteration 9028, Norm of Gradient: 0.00487829987322784, Cost (Train): 0.2844278179683948\n",
            "Iteration 9028, Cost (Validation): 0.3482338530178398\n",
            "Iteration 9029, Norm of Gradient: 0.0048778470131356265, Cost (Train): 0.2844254387396779\n",
            "Iteration 9029, Cost (Validation): 0.34823481008726104\n",
            "Iteration 9030, Norm of Gradient: 0.0048773942325903395, Cost (Train): 0.28442305995262973\n",
            "Iteration 9030, Cost (Validation): 0.34823576730984324\n",
            "Iteration 9031, Norm of Gradient: 0.004876941531569671, Cost (Train): 0.2844206816071317\n",
            "Iteration 9031, Cost (Validation): 0.3482367246855137\n",
            "Iteration 9032, Norm of Gradient: 0.004876488910051323, Cost (Train): 0.28441830370306537\n",
            "Iteration 9032, Cost (Validation): 0.34823768221420004\n",
            "Iteration 9033, Norm of Gradient: 0.004876036368013009, Cost (Train): 0.2844159262403122\n",
            "Iteration 9033, Cost (Validation): 0.34823863989582976\n",
            "Iteration 9034, Norm of Gradient: 0.004875583905432446, Cost (Train): 0.28441354921875367\n",
            "Iteration 9034, Cost (Validation): 0.34823959773033053\n",
            "Iteration 9035, Norm of Gradient: 0.004875131522287365, Cost (Train): 0.2844111726382715\n",
            "Iteration 9035, Cost (Validation): 0.3482405557176298\n",
            "Iteration 9036, Norm of Gradient: 0.0048746792185555025, Cost (Train): 0.2844087964987474\n",
            "Iteration 9036, Cost (Validation): 0.3482415138576553\n",
            "Iteration 9037, Norm of Gradient: 0.0048742269942146045, Cost (Train): 0.2844064208000629\n",
            "Iteration 9037, Cost (Validation): 0.34824247215033455\n",
            "Iteration 9038, Norm of Gradient: 0.004873774849242424, Cost (Train): 0.2844040455420999\n",
            "Iteration 9038, Cost (Validation): 0.3482434305955954\n",
            "Iteration 9039, Norm of Gradient: 0.004873322783616727, Cost (Train): 0.28440167072474015\n",
            "Iteration 9039, Cost (Validation): 0.34824438919336553\n",
            "Iteration 9040, Norm of Gradient: 0.0048728707973152855, Cost (Train): 0.2843992963478654\n",
            "Iteration 9040, Cost (Validation): 0.3482453479435727\n",
            "Iteration 9041, Norm of Gradient: 0.004872418890315878, Cost (Train): 0.2843969224113576\n",
            "Iteration 9041, Cost (Validation): 0.3482463068461446\n",
            "Iteration 9042, Norm of Gradient: 0.0048719670625962976, Cost (Train): 0.28439454891509874\n",
            "Iteration 9042, Cost (Validation): 0.34824726590100913\n",
            "Iteration 9043, Norm of Gradient: 0.004871515314134339, Cost (Train): 0.2843921758589707\n",
            "Iteration 9043, Cost (Validation): 0.34824822510809417\n",
            "Iteration 9044, Norm of Gradient: 0.004871063644907812, Cost (Train): 0.28438980324285534\n",
            "Iteration 9044, Cost (Validation): 0.34824918446732755\n",
            "Iteration 9045, Norm of Gradient: 0.00487061205489453, Cost (Train): 0.28438743106663494\n",
            "Iteration 9045, Cost (Validation): 0.34825014397863713\n",
            "Iteration 9046, Norm of Gradient: 0.004870160544072317, Cost (Train): 0.2843850593301915\n",
            "Iteration 9046, Cost (Validation): 0.348251103641951\n",
            "Iteration 9047, Norm of Gradient: 0.004869709112419009, Cost (Train): 0.28438268803340705\n",
            "Iteration 9047, Cost (Validation): 0.3482520634571969\n",
            "Iteration 9048, Norm of Gradient: 0.004869257759912446, Cost (Train): 0.28438031717616397\n",
            "Iteration 9048, Cost (Validation): 0.3482530234243031\n",
            "Iteration 9049, Norm of Gradient: 0.004868806486530475, Cost (Train): 0.28437794675834427\n",
            "Iteration 9049, Cost (Validation): 0.3482539835431974\n",
            "Iteration 9050, Norm of Gradient: 0.00486835529225096, Cost (Train): 0.28437557677983033\n",
            "Iteration 9050, Cost (Validation): 0.34825494381380806\n",
            "Iteration 9051, Norm of Gradient: 0.004867904177051766, Cost (Train): 0.2843732072405045\n",
            "Iteration 9051, Cost (Validation): 0.34825590423606306\n",
            "Iteration 9052, Norm of Gradient: 0.004867453140910769, Cost (Train): 0.284370838140249\n",
            "Iteration 9052, Cost (Validation): 0.34825686480989054\n",
            "Iteration 9053, Norm of Gradient: 0.0048670021838058556, Cost (Train): 0.2843684694789463\n",
            "Iteration 9053, Cost (Validation): 0.3482578255352187\n",
            "Iteration 9054, Norm of Gradient: 0.004866551305714917, Cost (Train): 0.2843661012564789\n",
            "Iteration 9054, Cost (Validation): 0.3482587864119758\n",
            "Iteration 9055, Norm of Gradient: 0.004866100506615856, Cost (Train): 0.2843637334727291\n",
            "Iteration 9055, Cost (Validation): 0.3482597474400898\n",
            "Iteration 9056, Norm of Gradient: 0.004865649786486586, Cost (Train): 0.28436136612757945\n",
            "Iteration 9056, Cost (Validation): 0.3482607086194892\n",
            "Iteration 9057, Norm of Gradient: 0.004865199145305022, Cost (Train): 0.2843589992209127\n",
            "Iteration 9057, Cost (Validation): 0.34826166995010227\n",
            "Iteration 9058, Norm of Gradient: 0.004864748583049095, Cost (Train): 0.28435663275261125\n",
            "Iteration 9058, Cost (Validation): 0.3482626314318572\n",
            "Iteration 9059, Norm of Gradient: 0.00486429809969674, Cost (Train): 0.2843542667225578\n",
            "Iteration 9059, Cost (Validation): 0.34826359306468246\n",
            "Iteration 9060, Norm of Gradient: 0.004863847695225904, Cost (Train): 0.2843519011306352\n",
            "Iteration 9060, Cost (Validation): 0.3482645548485063\n",
            "Iteration 9061, Norm of Gradient: 0.00486339736961454, Cost (Train): 0.2843495359767259\n",
            "Iteration 9061, Cost (Validation): 0.3482655167832573\n",
            "Iteration 9062, Norm of Gradient: 0.0048629471228406095, Cost (Train): 0.2843471712607129\n",
            "Iteration 9062, Cost (Validation): 0.34826647886886375\n",
            "Iteration 9063, Norm of Gradient: 0.004862496954882086, Cost (Train): 0.2843448069824789\n",
            "Iteration 9063, Cost (Validation): 0.3482674411052542\n",
            "Iteration 9064, Norm of Gradient: 0.004862046865716947, Cost (Train): 0.2843424431419068\n",
            "Iteration 9064, Cost (Validation): 0.3482684034923571\n",
            "Iteration 9065, Norm of Gradient: 0.004861596855323181, Cost (Train): 0.28434007973887954\n",
            "Iteration 9065, Cost (Validation): 0.348269366030101\n",
            "Iteration 9066, Norm of Gradient: 0.004861146923678784, Cost (Train): 0.28433771677328007\n",
            "Iteration 9066, Cost (Validation): 0.34827032871841446\n",
            "Iteration 9067, Norm of Gradient: 0.004860697070761763, Cost (Train): 0.2843353542449913\n",
            "Iteration 9067, Cost (Validation): 0.34827129155722614\n",
            "Iteration 9068, Norm of Gradient: 0.004860247296550132, Cost (Train): 0.2843329921538962\n",
            "Iteration 9068, Cost (Validation): 0.3482722545464646\n",
            "Iteration 9069, Norm of Gradient: 0.0048597976010219136, Cost (Train): 0.28433063049987806\n",
            "Iteration 9069, Cost (Validation): 0.34827321768605846\n",
            "Iteration 9070, Norm of Gradient: 0.004859347984155138, Cost (Train): 0.2843282692828199\n",
            "Iteration 9070, Cost (Validation): 0.34827418097593643\n",
            "Iteration 9071, Norm of Gradient: 0.004858898445927845, Cost (Train): 0.2843259085026048\n",
            "Iteration 9071, Cost (Validation): 0.34827514441602736\n",
            "Iteration 9072, Norm of Gradient: 0.004858448986318084, Cost (Train): 0.28432354815911604\n",
            "Iteration 9072, Cost (Validation): 0.3482761080062598\n",
            "Iteration 9073, Norm of Gradient: 0.00485799960530391, Cost (Train): 0.28432118825223684\n",
            "Iteration 9073, Cost (Validation): 0.3482770717465627\n",
            "Iteration 9074, Norm of Gradient: 0.00485755030286339, Cost (Train): 0.28431882878185055\n",
            "Iteration 9074, Cost (Validation): 0.34827803563686477\n",
            "Iteration 9075, Norm of Gradient: 0.004857101078974597, Cost (Train): 0.2843164697478404\n",
            "Iteration 9075, Cost (Validation): 0.34827899967709486\n",
            "Iteration 9076, Norm of Gradient: 0.0048566519336156145, Cost (Train): 0.28431411115008987\n",
            "Iteration 9076, Cost (Validation): 0.34827996386718185\n",
            "Iteration 9077, Norm of Gradient: 0.004856202866764531, Cost (Train): 0.28431175298848216\n",
            "Iteration 9077, Cost (Validation): 0.3482809282070547\n",
            "Iteration 9078, Norm of Gradient: 0.004855753878399449, Cost (Train): 0.28430939526290105\n",
            "Iteration 9078, Cost (Validation): 0.3482818926966423\n",
            "Iteration 9079, Norm of Gradient: 0.004855304968498475, Cost (Train): 0.28430703797322976\n",
            "Iteration 9079, Cost (Validation): 0.34828285733587355\n",
            "Iteration 9080, Norm of Gradient: 0.0048548561370397245, Cost (Train): 0.284304681119352\n",
            "Iteration 9080, Cost (Validation): 0.34828382212467757\n",
            "Iteration 9081, Norm of Gradient: 0.004854407384001326, Cost (Train): 0.2843023247011512\n",
            "Iteration 9081, Cost (Validation): 0.3482847870629834\n",
            "Iteration 9082, Norm of Gradient: 0.00485395870936141, Cost (Train): 0.28429996871851115\n",
            "Iteration 9082, Cost (Validation): 0.3482857521507199\n",
            "Iteration 9083, Norm of Gradient: 0.00485351011309812, Cost (Train): 0.2842976131713154\n",
            "Iteration 9083, Cost (Validation): 0.3482867173878163\n",
            "Iteration 9084, Norm of Gradient: 0.004853061595189607, Cost (Train): 0.28429525805944783\n",
            "Iteration 9084, Cost (Validation): 0.3482876827742017\n",
            "Iteration 9085, Norm of Gradient: 0.004852613155614029, Cost (Train): 0.28429290338279195\n",
            "Iteration 9085, Cost (Validation): 0.34828864830980527\n",
            "Iteration 9086, Norm of Gradient: 0.004852164794349554, Cost (Train): 0.2842905491412317\n",
            "Iteration 9086, Cost (Validation): 0.3482896139945562\n",
            "Iteration 9087, Norm of Gradient: 0.004851716511374359, Cost (Train): 0.2842881953346509\n",
            "Iteration 9087, Cost (Validation): 0.34829057982838363\n",
            "Iteration 9088, Norm of Gradient: 0.0048512683066666284, Cost (Train): 0.2842858419629334\n",
            "Iteration 9088, Cost (Validation): 0.3482915458112169\n",
            "Iteration 9089, Norm of Gradient: 0.004850820180204553, Cost (Train): 0.28428348902596323\n",
            "Iteration 9089, Cost (Validation): 0.3482925119429851\n",
            "Iteration 9090, Norm of Gradient: 0.004850372131966338, Cost (Train): 0.2842811365236242\n",
            "Iteration 9090, Cost (Validation): 0.3482934782236177\n",
            "Iteration 9091, Norm of Gradient: 0.004849924161930193, Cost (Train): 0.28427878445580035\n",
            "Iteration 9091, Cost (Validation): 0.3482944446530441\n",
            "Iteration 9092, Norm of Gradient: 0.004849476270074332, Cost (Train): 0.2842764328223758\n",
            "Iteration 9092, Cost (Validation): 0.34829541123119345\n",
            "Iteration 9093, Norm of Gradient: 0.004849028456376988, Cost (Train): 0.28427408162323464\n",
            "Iteration 9093, Cost (Validation): 0.3482963779579953\n",
            "Iteration 9094, Norm of Gradient: 0.0048485807208163935, Cost (Train): 0.28427173085826096\n",
            "Iteration 9094, Cost (Validation): 0.34829734483337893\n",
            "Iteration 9095, Norm of Gradient: 0.004848133063370793, Cost (Train): 0.28426938052733886\n",
            "Iteration 9095, Cost (Validation): 0.34829831185727395\n",
            "Iteration 9096, Norm of Gradient: 0.004847685484018438, Cost (Train): 0.28426703063035275\n",
            "Iteration 9096, Cost (Validation): 0.3482992790296098\n",
            "Iteration 9097, Norm of Gradient: 0.004847237982737592, Cost (Train): 0.2842646811671867\n",
            "Iteration 9097, Cost (Validation): 0.3483002463503159\n",
            "Iteration 9098, Norm of Gradient: 0.00484679055950652, Cost (Train): 0.2842623321377251\n",
            "Iteration 9098, Cost (Validation): 0.34830121381932194\n",
            "Iteration 9099, Norm of Gradient: 0.004846343214303505, Cost (Train): 0.28425998354185233\n",
            "Iteration 9099, Cost (Validation): 0.3483021814365575\n",
            "Iteration 9100, Norm of Gradient: 0.00484589594710683, Cost (Train): 0.2842576353794527\n",
            "Iteration 9100, Cost (Validation): 0.348303149201952\n",
            "Iteration 9101, Norm of Gradient: 0.00484544875789479, Cost (Train): 0.28425528765041064\n",
            "Iteration 9101, Cost (Validation): 0.3483041171154352\n",
            "Iteration 9102, Norm of Gradient: 0.004845001646645688, Cost (Train): 0.28425294035461063\n",
            "Iteration 9102, Cost (Validation): 0.3483050851769368\n",
            "Iteration 9103, Norm of Gradient: 0.004844554613337836, Cost (Train): 0.28425059349193726\n",
            "Iteration 9103, Cost (Validation): 0.34830605338638654\n",
            "Iteration 9104, Norm of Gradient: 0.0048441076579495545, Cost (Train): 0.284248247062275\n",
            "Iteration 9104, Cost (Validation): 0.348307021743714\n",
            "Iteration 9105, Norm of Gradient: 0.00484366078045917, Cost (Train): 0.28424590106550834\n",
            "Iteration 9105, Cost (Validation): 0.34830799024884895\n",
            "Iteration 9106, Norm of Gradient: 0.004843213980845021, Cost (Train): 0.2842435555015221\n",
            "Iteration 9106, Cost (Validation): 0.3483089589017213\n",
            "Iteration 9107, Norm of Gradient: 0.004842767259085453, Cost (Train): 0.28424121037020095\n",
            "Iteration 9107, Cost (Validation): 0.3483099277022608\n",
            "Iteration 9108, Norm of Gradient: 0.004842320615158818, Cost (Train): 0.2842388656714295\n",
            "Iteration 9108, Cost (Validation): 0.3483108966503974\n",
            "Iteration 9109, Norm of Gradient: 0.004841874049043478, Cost (Train): 0.2842365214050926\n",
            "Iteration 9109, Cost (Validation): 0.3483118657460608\n",
            "Iteration 9110, Norm of Gradient: 0.004841427560717806, Cost (Train): 0.284234177571075\n",
            "Iteration 9110, Cost (Validation): 0.34831283498918103\n",
            "Iteration 9111, Norm of Gradient: 0.004840981150160178, Cost (Train): 0.28423183416926157\n",
            "Iteration 9111, Cost (Validation): 0.348313804379688\n",
            "Iteration 9112, Norm of Gradient: 0.0048405348173489815, Cost (Train): 0.28422949119953717\n",
            "Iteration 9112, Cost (Validation): 0.34831477391751176\n",
            "Iteration 9113, Norm of Gradient: 0.004840088562262613, Cost (Train): 0.2842271486617868\n",
            "Iteration 9113, Cost (Validation): 0.34831574360258216\n",
            "Iteration 9114, Norm of Gradient: 0.004839642384879477, Cost (Train): 0.2842248065558953\n",
            "Iteration 9114, Cost (Validation): 0.3483167134348293\n",
            "Iteration 9115, Norm of Gradient: 0.004839196285177985, Cost (Train): 0.28422246488174785\n",
            "Iteration 9115, Cost (Validation): 0.3483176834141833\n",
            "Iteration 9116, Norm of Gradient: 0.004838750263136557, Cost (Train): 0.2842201236392294\n",
            "Iteration 9116, Cost (Validation): 0.3483186535405742\n",
            "Iteration 9117, Norm of Gradient: 0.004838304318733623, Cost (Train): 0.284217782828225\n",
            "Iteration 9117, Cost (Validation): 0.34831962381393217\n",
            "Iteration 9118, Norm of Gradient: 0.00483785845194762, Cost (Train): 0.2842154424486199\n",
            "Iteration 9118, Cost (Validation): 0.3483205942341872\n",
            "Iteration 9119, Norm of Gradient: 0.004837412662756994, Cost (Train): 0.2842131025002992\n",
            "Iteration 9119, Cost (Validation): 0.3483215648012697\n",
            "Iteration 9120, Norm of Gradient: 0.004836966951140199, Cost (Train): 0.2842107629831482\n",
            "Iteration 9120, Cost (Validation): 0.3483225355151097\n",
            "Iteration 9121, Norm of Gradient: 0.004836521317075698, Cost (Train): 0.28420842389705203\n",
            "Iteration 9121, Cost (Validation): 0.3483235063756375\n",
            "Iteration 9122, Norm of Gradient: 0.004836075760541963, Cost (Train): 0.28420608524189606\n",
            "Iteration 9122, Cost (Validation): 0.3483244773827835\n",
            "Iteration 9123, Norm of Gradient: 0.0048356302815174695, Cost (Train): 0.2842037470175658\n",
            "Iteration 9123, Cost (Validation): 0.34832544853647784\n",
            "Iteration 9124, Norm of Gradient: 0.004835184879980707, Cost (Train): 0.2842014092239464\n",
            "Iteration 9124, Cost (Validation): 0.3483264198366509\n",
            "Iteration 9125, Norm of Gradient: 0.004834739555910174, Cost (Train): 0.2841990718609233\n",
            "Iteration 9125, Cost (Validation): 0.3483273912832331\n",
            "Iteration 9126, Norm of Gradient: 0.004834294309284371, Cost (Train): 0.28419673492838216\n",
            "Iteration 9126, Cost (Validation): 0.34832836287615476\n",
            "Iteration 9127, Norm of Gradient: 0.004833849140081812, Cost (Train): 0.2841943984262083\n",
            "Iteration 9127, Cost (Validation): 0.3483293346153464\n",
            "Iteration 9128, Norm of Gradient: 0.0048334040482810176, Cost (Train): 0.2841920623542874\n",
            "Iteration 9128, Cost (Validation): 0.34833030650073826\n",
            "Iteration 9129, Norm of Gradient: 0.004832959033860517, Cost (Train): 0.2841897267125049\n",
            "Iteration 9129, Cost (Validation): 0.34833127853226103\n",
            "Iteration 9130, Norm of Gradient: 0.004832514096798848, Cost (Train): 0.2841873915007467\n",
            "Iteration 9130, Cost (Validation): 0.34833225070984525\n",
            "Iteration 9131, Norm of Gradient: 0.004832069237074557, Cost (Train): 0.2841850567188982\n",
            "Iteration 9131, Cost (Validation): 0.3483332230334213\n",
            "Iteration 9132, Norm of Gradient: 0.0048316244546661976, Cost (Train): 0.2841827223668452\n",
            "Iteration 9132, Cost (Validation): 0.3483341955029198\n",
            "Iteration 9133, Norm of Gradient: 0.0048311797495523305, Cost (Train): 0.2841803884444736\n",
            "Iteration 9133, Cost (Validation): 0.3483351681182714\n",
            "Iteration 9134, Norm of Gradient: 0.004830735121711528, Cost (Train): 0.2841780549516689\n",
            "Iteration 9134, Cost (Validation): 0.34833614087940673\n",
            "Iteration 9135, Norm of Gradient: 0.00483029057112237, Cost (Train): 0.28417572188831736\n",
            "Iteration 9135, Cost (Validation): 0.3483371137862564\n",
            "Iteration 9136, Norm of Gradient: 0.004829846097763442, Cost (Train): 0.2841733892543045\n",
            "Iteration 9136, Cost (Validation): 0.34833808683875117\n",
            "Iteration 9137, Norm of Gradient: 0.004829401701613342, Cost (Train): 0.28417105704951645\n",
            "Iteration 9137, Cost (Validation): 0.3483390600368218\n",
            "Iteration 9138, Norm of Gradient: 0.004828957382650671, Cost (Train): 0.284168725273839\n",
            "Iteration 9138, Cost (Validation): 0.3483400333803988\n",
            "Iteration 9139, Norm of Gradient: 0.004828513140854043, Cost (Train): 0.28416639392715837\n",
            "Iteration 9139, Cost (Validation): 0.34834100686941327\n",
            "Iteration 9140, Norm of Gradient: 0.004828068976202077, Cost (Train): 0.2841640630093605\n",
            "Iteration 9140, Cost (Validation): 0.3483419805037959\n",
            "Iteration 9141, Norm of Gradient: 0.004827624888673403, Cost (Train): 0.2841617325203314\n",
            "Iteration 9141, Cost (Validation): 0.3483429542834774\n",
            "Iteration 9142, Norm of Gradient: 0.004827180878246657, Cost (Train): 0.28415940245995736\n",
            "Iteration 9142, Cost (Validation): 0.34834392820838883\n",
            "Iteration 9143, Norm of Gradient: 0.004826736944900486, Cost (Train): 0.28415707282812447\n",
            "Iteration 9143, Cost (Validation): 0.348344902278461\n",
            "Iteration 9144, Norm of Gradient: 0.00482629308861354, Cost (Train): 0.2841547436247189\n",
            "Iteration 9144, Cost (Validation): 0.34834587649362486\n",
            "Iteration 9145, Norm of Gradient: 0.0048258493093644855, Cost (Train): 0.2841524148496269\n",
            "Iteration 9145, Cost (Validation): 0.3483468508538114\n",
            "Iteration 9146, Norm of Gradient: 0.004825405607131988, Cost (Train): 0.28415008650273493\n",
            "Iteration 9146, Cost (Validation): 0.34834782535895153\n",
            "Iteration 9147, Norm of Gradient: 0.004824961981894729, Cost (Train): 0.28414775858392916\n",
            "Iteration 9147, Cost (Validation): 0.34834880000897633\n",
            "Iteration 9148, Norm of Gradient: 0.004824518433631392, Cost (Train): 0.28414543109309603\n",
            "Iteration 9148, Cost (Validation): 0.34834977480381685\n",
            "Iteration 9149, Norm of Gradient: 0.004824074962320674, Cost (Train): 0.28414310403012194\n",
            "Iteration 9149, Cost (Validation): 0.34835074974340413\n",
            "Iteration 9150, Norm of Gradient: 0.004823631567941277, Cost (Train): 0.2841407773948933\n",
            "Iteration 9150, Cost (Validation): 0.3483517248276694\n",
            "Iteration 9151, Norm of Gradient: 0.004823188250471912, Cost (Train): 0.28413845118729675\n",
            "Iteration 9151, Cost (Validation): 0.3483527000565437\n",
            "Iteration 9152, Norm of Gradient: 0.004822745009891299, Cost (Train): 0.2841361254072187\n",
            "Iteration 9152, Cost (Validation): 0.34835367542995804\n",
            "Iteration 9153, Norm of Gradient: 0.004822301846178166, Cost (Train): 0.28413380005454575\n",
            "Iteration 9153, Cost (Validation): 0.34835465094784396\n",
            "Iteration 9154, Norm of Gradient: 0.004821858759311248, Cost (Train): 0.2841314751291646\n",
            "Iteration 9154, Cost (Validation): 0.3483556266101324\n",
            "Iteration 9155, Norm of Gradient: 0.004821415749269288, Cost (Train): 0.28412915063096184\n",
            "Iteration 9155, Cost (Validation): 0.34835660241675487\n",
            "Iteration 9156, Norm of Gradient: 0.00482097281603104, Cost (Train): 0.2841268265598242\n",
            "Iteration 9156, Cost (Validation): 0.3483575783676424\n",
            "Iteration 9157, Norm of Gradient: 0.004820529959575264, Cost (Train): 0.28412450291563845\n",
            "Iteration 9157, Cost (Validation): 0.34835855446272646\n",
            "Iteration 9158, Norm of Gradient: 0.004820087179880728, Cost (Train): 0.2841221796982914\n",
            "Iteration 9158, Cost (Validation): 0.34835953070193837\n",
            "Iteration 9159, Norm of Gradient: 0.00481964447692621, Cost (Train): 0.2841198569076697\n",
            "Iteration 9159, Cost (Validation): 0.3483605070852095\n",
            "Iteration 9160, Norm of Gradient: 0.004819201850690494, Cost (Train): 0.2841175345436605\n",
            "Iteration 9160, Cost (Validation): 0.3483614836124713\n",
            "Iteration 9161, Norm of Gradient: 0.004818759301152374, Cost (Train): 0.28411521260615047\n",
            "Iteration 9161, Cost (Validation): 0.3483624602836552\n",
            "Iteration 9162, Norm of Gradient: 0.00481831682829065, Cost (Train): 0.2841128910950266\n",
            "Iteration 9162, Cost (Validation): 0.3483634370986925\n",
            "Iteration 9163, Norm of Gradient: 0.004817874432084134, Cost (Train): 0.284110570010176\n",
            "Iteration 9163, Cost (Validation): 0.34836441405751495\n",
            "Iteration 9164, Norm of Gradient: 0.004817432112511641, Cost (Train): 0.28410824935148565\n",
            "Iteration 9164, Cost (Validation): 0.34836539116005383\n",
            "Iteration 9165, Norm of Gradient: 0.004816989869551998, Cost (Train): 0.2841059291188425\n",
            "Iteration 9165, Cost (Validation): 0.34836636840624086\n",
            "Iteration 9166, Norm of Gradient: 0.00481654770318404, Cost (Train): 0.2841036093121339\n",
            "Iteration 9166, Cost (Validation): 0.34836734579600753\n",
            "Iteration 9167, Norm of Gradient: 0.004816105613386607, Cost (Train): 0.28410128993124684\n",
            "Iteration 9167, Cost (Validation): 0.3483683233292856\n",
            "Iteration 9168, Norm of Gradient: 0.004815663600138553, Cost (Train): 0.2840989709760685\n",
            "Iteration 9168, Cost (Validation): 0.3483693010060065\n",
            "Iteration 9169, Norm of Gradient: 0.004815221663418734, Cost (Train): 0.2840966524464862\n",
            "Iteration 9169, Cost (Validation): 0.348370278826102\n",
            "Iteration 9170, Norm of Gradient: 0.004814779803206017, Cost (Train): 0.28409433434238707\n",
            "Iteration 9170, Cost (Validation): 0.3483712567895038\n",
            "Iteration 9171, Norm of Gradient: 0.004814338019479276, Cost (Train): 0.28409201666365863\n",
            "Iteration 9171, Cost (Validation): 0.3483722348961437\n",
            "Iteration 9172, Norm of Gradient: 0.004813896312217397, Cost (Train): 0.28408969941018813\n",
            "Iteration 9172, Cost (Validation): 0.3483732131459534\n",
            "Iteration 9173, Norm of Gradient: 0.004813454681399268, Cost (Train): 0.28408738258186295\n",
            "Iteration 9173, Cost (Validation): 0.3483741915388646\n",
            "Iteration 9174, Norm of Gradient: 0.004813013127003789, Cost (Train): 0.2840850661785705\n",
            "Iteration 9174, Cost (Validation): 0.3483751700748093\n",
            "Iteration 9175, Norm of Gradient: 0.00481257164900987, Cost (Train): 0.28408275020019835\n",
            "Iteration 9175, Cost (Validation): 0.3483761487537192\n",
            "Iteration 9176, Norm of Gradient: 0.0048121302473964215, Cost (Train): 0.2840804346466341\n",
            "Iteration 9176, Cost (Validation): 0.3483771275755262\n",
            "Iteration 9177, Norm of Gradient: 0.004811688922142373, Cost (Train): 0.284078119517765\n",
            "Iteration 9177, Cost (Validation): 0.3483781065401622\n",
            "Iteration 9178, Norm of Gradient: 0.004811247673226652, Cost (Train): 0.28407580481347894\n",
            "Iteration 9178, Cost (Validation): 0.3483790856475592\n",
            "Iteration 9179, Norm of Gradient: 0.004810806500628199, Cost (Train): 0.28407349053366343\n",
            "Iteration 9179, Cost (Validation): 0.3483800648976491\n",
            "Iteration 9180, Norm of Gradient: 0.004810365404325965, Cost (Train): 0.2840711766782062\n",
            "Iteration 9180, Cost (Validation): 0.34838104429036393\n",
            "Iteration 9181, Norm of Gradient: 0.004809924384298902, Cost (Train): 0.2840688632469949\n",
            "Iteration 9181, Cost (Validation): 0.3483820238256357\n",
            "Iteration 9182, Norm of Gradient: 0.004809483440525977, Cost (Train): 0.28406655023991745\n",
            "Iteration 9182, Cost (Validation): 0.3483830035033965\n",
            "Iteration 9183, Norm of Gradient: 0.004809042572986161, Cost (Train): 0.2840642376568615\n",
            "Iteration 9183, Cost (Validation): 0.34838398332357834\n",
            "Iteration 9184, Norm of Gradient: 0.004808601781658434, Cost (Train): 0.28406192549771486\n",
            "Iteration 9184, Cost (Validation): 0.34838496328611335\n",
            "Iteration 9185, Norm of Gradient: 0.0048081610665217855, Cost (Train): 0.2840596137623656\n",
            "Iteration 9185, Cost (Validation): 0.3483859433909337\n",
            "Iteration 9186, Norm of Gradient: 0.004807720427555213, Cost (Train): 0.2840573024507015\n",
            "Iteration 9186, Cost (Validation): 0.3483869236379715\n",
            "Iteration 9187, Norm of Gradient: 0.00480727986473772, Cost (Train): 0.28405499156261055\n",
            "Iteration 9187, Cost (Validation): 0.348387904027159\n",
            "Iteration 9188, Norm of Gradient: 0.004806839378048318, Cost (Train): 0.2840526810979807\n",
            "Iteration 9188, Cost (Validation): 0.3483888845584284\n",
            "Iteration 9189, Norm of Gradient: 0.004806398967466029, Cost (Train): 0.2840503710567001\n",
            "Iteration 9189, Cost (Validation): 0.34838986523171206\n",
            "Iteration 9190, Norm of Gradient: 0.004805958632969883, Cost (Train): 0.2840480614386568\n",
            "Iteration 9190, Cost (Validation): 0.3483908460469421\n",
            "Iteration 9191, Norm of Gradient: 0.0048055183745389154, Cost (Train): 0.28404575224373896\n",
            "Iteration 9191, Cost (Validation): 0.3483918270040509\n",
            "Iteration 9192, Norm of Gradient: 0.004805078192152172, Cost (Train): 0.28404344347183463\n",
            "Iteration 9192, Cost (Validation): 0.34839280810297085\n",
            "Iteration 9193, Norm of Gradient: 0.004804638085788705, Cost (Train): 0.28404113512283213\n",
            "Iteration 9193, Cost (Validation): 0.34839378934363424\n",
            "Iteration 9194, Norm of Gradient: 0.004804198055427576, Cost (Train): 0.28403882719661966\n",
            "Iteration 9194, Cost (Validation): 0.3483947707259736\n",
            "Iteration 9195, Norm of Gradient: 0.004803758101047855, Cost (Train): 0.2840365196930855\n",
            "Iteration 9195, Cost (Validation): 0.34839575224992114\n",
            "Iteration 9196, Norm of Gradient: 0.004803318222628617, Cost (Train): 0.284034212612118\n",
            "Iteration 9196, Cost (Validation): 0.3483967339154095\n",
            "Iteration 9197, Norm of Gradient: 0.0048028784201489495, Cost (Train): 0.2840319059536055\n",
            "Iteration 9197, Cost (Validation): 0.34839771572237105\n",
            "Iteration 9198, Norm of Gradient: 0.004802438693587944, Cost (Train): 0.2840295997174363\n",
            "Iteration 9198, Cost (Validation): 0.34839869767073844\n",
            "Iteration 9199, Norm of Gradient: 0.004801999042924705, Cost (Train): 0.2840272939034991\n",
            "Iteration 9199, Cost (Validation): 0.34839967976044417\n",
            "Iteration 9200, Norm of Gradient: 0.0048015594681383375, Cost (Train): 0.28402498851168223\n",
            "Iteration 9200, Cost (Validation): 0.3484006619914207\n",
            "Iteration 9201, Norm of Gradient: 0.0048011199692079625, Cost (Train): 0.28402268354187415\n",
            "Iteration 9201, Cost (Validation): 0.3484016443636008\n",
            "Iteration 9202, Norm of Gradient: 0.0048006805461127035, Cost (Train): 0.2840203789939636\n",
            "Iteration 9202, Cost (Validation): 0.3484026268769171\n",
            "Iteration 9203, Norm of Gradient: 0.004800241198831694, Cost (Train): 0.28401807486783903\n",
            "Iteration 9203, Cost (Validation): 0.348403609531302\n",
            "Iteration 9204, Norm of Gradient: 0.004799801927344076, Cost (Train): 0.2840157711633891\n",
            "Iteration 9204, Cost (Validation): 0.3484045923266885\n",
            "Iteration 9205, Norm of Gradient: 0.004799362731628998, Cost (Train): 0.2840134678805027\n",
            "Iteration 9205, Cost (Validation): 0.34840557526300925\n",
            "Iteration 9206, Norm of Gradient: 0.004798923611665619, Cost (Train): 0.28401116501906826\n",
            "Iteration 9206, Cost (Validation): 0.3484065583401969\n",
            "Iteration 9207, Norm of Gradient: 0.004798484567433104, Cost (Train): 0.28400886257897473\n",
            "Iteration 9207, Cost (Validation): 0.3484075415581842\n",
            "Iteration 9208, Norm of Gradient: 0.004798045598910627, Cost (Train): 0.28400656056011087\n",
            "Iteration 9208, Cost (Validation): 0.3484085249169041\n",
            "Iteration 9209, Norm of Gradient: 0.004797606706077366, Cost (Train): 0.2840042589623656\n",
            "Iteration 9209, Cost (Validation): 0.34840950841628937\n",
            "Iteration 9210, Norm of Gradient: 0.0047971678889125145, Cost (Train): 0.28400195778562765\n",
            "Iteration 9210, Cost (Validation): 0.34841049205627284\n",
            "Iteration 9211, Norm of Gradient: 0.004796729147395268, Cost (Train): 0.2839996570297861\n",
            "Iteration 9211, Cost (Validation): 0.34841147583678744\n",
            "Iteration 9212, Norm of Gradient: 0.0047962904815048325, Cost (Train): 0.2839973566947298\n",
            "Iteration 9212, Cost (Validation): 0.3484124597577661\n",
            "Iteration 9213, Norm of Gradient: 0.0047958518912204215, Cost (Train): 0.28399505678034775\n",
            "Iteration 9213, Cost (Validation): 0.3484134438191417\n",
            "Iteration 9214, Norm of Gradient: 0.004795413376521255, Cost (Train): 0.2839927572865291\n",
            "Iteration 9214, Cost (Validation): 0.3484144280208472\n",
            "Iteration 9215, Norm of Gradient: 0.0047949749373865655, Cost (Train): 0.28399045821316293\n",
            "Iteration 9215, Cost (Validation): 0.34841541236281576\n",
            "Iteration 9216, Norm of Gradient: 0.004794536573795586, Cost (Train): 0.28398815956013823\n",
            "Iteration 9216, Cost (Validation): 0.3484163968449803\n",
            "Iteration 9217, Norm of Gradient: 0.004794098285727566, Cost (Train): 0.2839858613273443\n",
            "Iteration 9217, Cost (Validation): 0.3484173814672739\n",
            "Iteration 9218, Norm of Gradient: 0.004793660073161754, Cost (Train): 0.2839835635146703\n",
            "Iteration 9218, Cost (Validation): 0.34841836622962974\n",
            "Iteration 9219, Norm of Gradient: 0.004793221936077415, Cost (Train): 0.28398126612200547\n",
            "Iteration 9219, Cost (Validation): 0.3484193511319808\n",
            "Iteration 9220, Norm of Gradient: 0.004792783874453818, Cost (Train): 0.28397896914923904\n",
            "Iteration 9220, Cost (Validation): 0.3484203361742603\n",
            "Iteration 9221, Norm of Gradient: 0.004792345888270237, Cost (Train): 0.28397667259626047\n",
            "Iteration 9221, Cost (Validation): 0.3484213213564014\n",
            "Iteration 9222, Norm of Gradient: 0.0047919079775059605, Cost (Train): 0.283974376462959\n",
            "Iteration 9222, Cost (Validation): 0.34842230667833735\n",
            "Iteration 9223, Norm of Gradient: 0.004791470142140279, Cost (Train): 0.28397208074922403\n",
            "Iteration 9223, Cost (Validation): 0.3484232921400014\n",
            "Iteration 9224, Norm of Gradient: 0.004791032382152495, Cost (Train): 0.28396978545494506\n",
            "Iteration 9224, Cost (Validation): 0.3484242777413267\n",
            "Iteration 9225, Norm of Gradient: 0.004790594697521917, Cost (Train): 0.28396749058001164\n",
            "Iteration 9225, Cost (Validation): 0.34842526348224667\n",
            "Iteration 9226, Norm of Gradient: 0.0047901570882278605, Cost (Train): 0.28396519612431315\n",
            "Iteration 9226, Cost (Validation): 0.3484262493626945\n",
            "Iteration 9227, Norm of Gradient: 0.004789719554249652, Cost (Train): 0.2839629020877392\n",
            "Iteration 9227, Cost (Validation): 0.3484272353826036\n",
            "Iteration 9228, Norm of Gradient: 0.004789282095566622, Cost (Train): 0.28396060847017934\n",
            "Iteration 9228, Cost (Validation): 0.3484282215419075\n",
            "Iteration 9229, Norm of Gradient: 0.004788844712158113, Cost (Train): 0.2839583152715234\n",
            "Iteration 9229, Cost (Validation): 0.3484292078405394\n",
            "Iteration 9230, Norm of Gradient: 0.004788407404003472, Cost (Train): 0.28395602249166085\n",
            "Iteration 9230, Cost (Validation): 0.34843019427843275\n",
            "Iteration 9231, Norm of Gradient: 0.0047879701710820555, Cost (Train): 0.28395373013048153\n",
            "Iteration 9231, Cost (Validation): 0.34843118085552116\n",
            "Iteration 9232, Norm of Gradient: 0.004787533013373229, Cost (Train): 0.2839514381878751\n",
            "Iteration 9232, Cost (Validation): 0.34843216757173795\n",
            "Iteration 9233, Norm of Gradient: 0.004787095930856363, Cost (Train): 0.28394914666373144\n",
            "Iteration 9233, Cost (Validation): 0.3484331544270168\n",
            "Iteration 9234, Norm of Gradient: 0.004786658923510838, Cost (Train): 0.2839468555579404\n",
            "Iteration 9234, Cost (Validation): 0.3484341414212912\n",
            "Iteration 9235, Norm of Gradient: 0.004786221991316043, Cost (Train): 0.28394456487039177\n",
            "Iteration 9235, Cost (Validation): 0.3484351285544946\n",
            "Iteration 9236, Norm of Gradient: 0.004785785134251374, Cost (Train): 0.28394227460097554\n",
            "Iteration 9236, Cost (Validation): 0.34843611582656087\n",
            "Iteration 9237, Norm of Gradient: 0.004785348352296231, Cost (Train): 0.2839399847495817\n",
            "Iteration 9237, Cost (Validation): 0.3484371032374234\n",
            "Iteration 9238, Norm of Gradient: 0.00478491164543003, Cost (Train): 0.2839376953161001\n",
            "Iteration 9238, Cost (Validation): 0.34843809078701593\n",
            "Iteration 9239, Norm of Gradient: 0.004784475013632187, Cost (Train): 0.2839354063004209\n",
            "Iteration 9239, Cost (Validation): 0.3484390784752722\n",
            "Iteration 9240, Norm of Gradient: 0.004784038456882132, Cost (Train): 0.2839331177024341\n",
            "Iteration 9240, Cost (Validation): 0.34844006630212593\n",
            "Iteration 9241, Norm of Gradient: 0.004783601975159298, Cost (Train): 0.28393082952202997\n",
            "Iteration 9241, Cost (Validation): 0.3484410542675108\n",
            "Iteration 9242, Norm of Gradient: 0.00478316556844313, Cost (Train): 0.28392854175909843\n",
            "Iteration 9242, Cost (Validation): 0.3484420423713606\n",
            "Iteration 9243, Norm of Gradient: 0.004782729236713078, Cost (Train): 0.28392625441352987\n",
            "Iteration 9243, Cost (Validation): 0.34844303061360926\n",
            "Iteration 9244, Norm of Gradient: 0.0047822929799486, Cost (Train): 0.2839239674852143\n",
            "Iteration 9244, Cost (Validation): 0.3484440189941904\n",
            "Iteration 9245, Norm of Gradient: 0.0047818567981291644, Cost (Train): 0.28392168097404225\n",
            "Iteration 9245, Cost (Validation): 0.34844500751303803\n",
            "Iteration 9246, Norm of Gradient: 0.004781420691234243, Cost (Train): 0.28391939487990386\n",
            "Iteration 9246, Cost (Validation): 0.34844599617008604\n",
            "Iteration 9247, Norm of Gradient: 0.0047809846592433225, Cost (Train): 0.2839171092026896\n",
            "Iteration 9247, Cost (Validation): 0.34844698496526827\n",
            "Iteration 9248, Norm of Gradient: 0.004780548702135888, Cost (Train): 0.2839148239422896\n",
            "Iteration 9248, Cost (Validation): 0.3484479738985187\n",
            "Iteration 9249, Norm of Gradient: 0.004780112819891441, Cost (Train): 0.28391253909859465\n",
            "Iteration 9249, Cost (Validation): 0.3484489629697714\n",
            "Iteration 9250, Norm of Gradient: 0.0047796770124894855, Cost (Train): 0.283910254671495\n",
            "Iteration 9250, Cost (Validation): 0.3484499521789602\n",
            "Iteration 9251, Norm of Gradient: 0.004779241279909536, Cost (Train): 0.28390797066088114\n",
            "Iteration 9251, Cost (Validation): 0.34845094152601924\n",
            "Iteration 9252, Norm of Gradient: 0.004778805622131112, Cost (Train): 0.2839056870666437\n",
            "Iteration 9252, Cost (Validation): 0.34845193101088257\n",
            "Iteration 9253, Norm of Gradient: 0.004778370039133745, Cost (Train): 0.28390340388867336\n",
            "Iteration 9253, Cost (Validation): 0.34845292063348426\n",
            "Iteration 9254, Norm of Gradient: 0.004777934530896973, Cost (Train): 0.2839011211268604\n",
            "Iteration 9254, Cost (Validation): 0.34845391039375834\n",
            "Iteration 9255, Norm of Gradient: 0.004777499097400339, Cost (Train): 0.2838988387810958\n",
            "Iteration 9255, Cost (Validation): 0.34845490029163917\n",
            "Iteration 9256, Norm of Gradient: 0.004777063738623396, Cost (Train): 0.2838965568512702\n",
            "Iteration 9256, Cost (Validation): 0.3484558903270607\n",
            "Iteration 9257, Norm of Gradient: 0.004776628454545705, Cost (Train): 0.28389427533727424\n",
            "Iteration 9257, Cost (Validation): 0.34845688049995727\n",
            "Iteration 9258, Norm of Gradient: 0.004776193245146834, Cost (Train): 0.28389199423899875\n",
            "Iteration 9258, Cost (Validation): 0.3484578708102631\n",
            "Iteration 9259, Norm of Gradient: 0.004775758110406358, Cost (Train): 0.28388971355633463\n",
            "Iteration 9259, Cost (Validation): 0.3484588612579123\n",
            "Iteration 9260, Norm of Gradient: 0.004775323050303864, Cost (Train): 0.2838874332891726\n",
            "Iteration 9260, Cost (Validation): 0.3484598518428393\n",
            "Iteration 9261, Norm of Gradient: 0.004774888064818942, Cost (Train): 0.2838851534374036\n",
            "Iteration 9261, Cost (Validation): 0.3484608425649784\n",
            "Iteration 9262, Norm of Gradient: 0.0047744531539311895, Cost (Train): 0.2838828740009186\n",
            "Iteration 9262, Cost (Validation): 0.34846183342426396\n",
            "Iteration 9263, Norm of Gradient: 0.004774018317620218, Cost (Train): 0.2838805949796086\n",
            "Iteration 9263, Cost (Validation): 0.3484628244206302\n",
            "Iteration 9264, Norm of Gradient: 0.004773583555865639, Cost (Train): 0.2838783163733645\n",
            "Iteration 9264, Cost (Validation): 0.3484638155540117\n",
            "Iteration 9265, Norm of Gradient: 0.004773148868647077, Cost (Train): 0.2838760381820775\n",
            "Iteration 9265, Cost (Validation): 0.34846480682434267\n",
            "Iteration 9266, Norm of Gradient: 0.004772714255944162, Cost (Train): 0.2838737604056386\n",
            "Iteration 9266, Cost (Validation): 0.3484657982315578\n",
            "Iteration 9267, Norm of Gradient: 0.0047722797177365325, Cost (Train): 0.28387148304393894\n",
            "Iteration 9267, Cost (Validation): 0.3484667897755915\n",
            "Iteration 9268, Norm of Gradient: 0.0047718452540038364, Cost (Train): 0.2838692060968697\n",
            "Iteration 9268, Cost (Validation): 0.3484677814563782\n",
            "Iteration 9269, Norm of Gradient: 0.004771410864725724, Cost (Train): 0.28386692956432213\n",
            "Iteration 9269, Cost (Validation): 0.34846877327385245\n",
            "Iteration 9270, Norm of Gradient: 0.00477097654988186, Cost (Train): 0.28386465344618733\n",
            "Iteration 9270, Cost (Validation): 0.34846976522794887\n",
            "Iteration 9271, Norm of Gradient: 0.004770542309451913, Cost (Train): 0.28386237774235673\n",
            "Iteration 9271, Cost (Validation): 0.348470757318602\n",
            "Iteration 9272, Norm of Gradient: 0.00477010814341556, Cost (Train): 0.2838601024527216\n",
            "Iteration 9272, Cost (Validation): 0.3484717495457465\n",
            "Iteration 9273, Norm of Gradient: 0.004769674051752486, Cost (Train): 0.2838578275771733\n",
            "Iteration 9273, Cost (Validation): 0.34847274190931704\n",
            "Iteration 9274, Norm of Gradient: 0.004769240034442383, Cost (Train): 0.2838555531156032\n",
            "Iteration 9274, Cost (Validation): 0.34847373440924817\n",
            "Iteration 9275, Norm of Gradient: 0.004768806091464952, Cost (Train): 0.2838532790679029\n",
            "Iteration 9275, Cost (Validation): 0.3484747270454747\n",
            "Iteration 9276, Norm of Gradient: 0.004768372222799901, Cost (Train): 0.28385100543396363\n",
            "Iteration 9276, Cost (Validation): 0.34847571981793146\n",
            "Iteration 9277, Norm of Gradient: 0.004767938428426948, Cost (Train): 0.28384873221367707\n",
            "Iteration 9277, Cost (Validation): 0.34847671272655295\n",
            "Iteration 9278, Norm of Gradient: 0.004767504708325813, Cost (Train): 0.28384645940693476\n",
            "Iteration 9278, Cost (Validation): 0.34847770577127424\n",
            "Iteration 9279, Norm of Gradient: 0.0047670710624762285, Cost (Train): 0.28384418701362824\n",
            "Iteration 9279, Cost (Validation): 0.3484786989520298\n",
            "Iteration 9280, Norm of Gradient: 0.004766637490857935, Cost (Train): 0.2838419150336492\n",
            "Iteration 9280, Cost (Validation): 0.34847969226875486\n",
            "Iteration 9281, Norm of Gradient: 0.004766203993450678, Cost (Train): 0.28383964346688934\n",
            "Iteration 9281, Cost (Validation): 0.348480685721384\n",
            "Iteration 9282, Norm of Gradient: 0.004765770570234211, Cost (Train): 0.28383737231324024\n",
            "Iteration 9282, Cost (Validation): 0.3484816793098523\n",
            "Iteration 9283, Norm of Gradient: 0.004765337221188299, Cost (Train): 0.2838351015725938\n",
            "Iteration 9283, Cost (Validation): 0.3484826730340946\n",
            "Iteration 9284, Norm of Gradient: 0.004764903946292709, Cost (Train): 0.28383283124484165\n",
            "Iteration 9284, Cost (Validation): 0.34848366689404575\n",
            "Iteration 9285, Norm of Gradient: 0.00476447074552722, Cost (Train): 0.2838305613298757\n",
            "Iteration 9285, Cost (Validation): 0.348484660889641\n",
            "Iteration 9286, Norm of Gradient: 0.004764037618871618, Cost (Train): 0.2838282918275879\n",
            "Iteration 9286, Cost (Validation): 0.34848565502081513\n",
            "Iteration 9287, Norm of Gradient: 0.004763604566305694, Cost (Train): 0.28382602273787\n",
            "Iteration 9287, Cost (Validation): 0.3484866492875032\n",
            "Iteration 9288, Norm of Gradient: 0.004763171587809249, Cost (Train): 0.2838237540606141\n",
            "Iteration 9288, Cost (Validation): 0.3484876436896404\n",
            "Iteration 9289, Norm of Gradient: 0.0047627386833620925, Cost (Train): 0.2838214857957121\n",
            "Iteration 9289, Cost (Validation): 0.34848863822716175\n",
            "Iteration 9290, Norm of Gradient: 0.004762305852944041, Cost (Train): 0.2838192179430559\n",
            "Iteration 9290, Cost (Validation): 0.3484896329000024\n",
            "Iteration 9291, Norm of Gradient: 0.004761873096534915, Cost (Train): 0.28381695050253775\n",
            "Iteration 9291, Cost (Validation): 0.3484906277080974\n",
            "Iteration 9292, Norm of Gradient: 0.0047614404141145495, Cost (Train): 0.2838146834740496\n",
            "Iteration 9292, Cost (Validation): 0.34849162265138195\n",
            "Iteration 9293, Norm of Gradient: 0.004761007805662781, Cost (Train): 0.2838124168574837\n",
            "Iteration 9293, Cost (Validation): 0.3484926177297914\n",
            "Iteration 9294, Norm of Gradient: 0.004760575271159458, Cost (Train): 0.28381015065273213\n",
            "Iteration 9294, Cost (Validation): 0.3484936129432608\n",
            "Iteration 9295, Norm of Gradient: 0.004760142810584434, Cost (Train): 0.28380788485968717\n",
            "Iteration 9295, Cost (Validation): 0.3484946082917255\n",
            "Iteration 9296, Norm of Gradient: 0.00475971042391757, Cost (Train): 0.283805619478241\n",
            "Iteration 9296, Cost (Validation): 0.3484956037751208\n",
            "Iteration 9297, Norm of Gradient: 0.0047592781111387384, Cost (Train): 0.283803354508286\n",
            "Iteration 9297, Cost (Validation): 0.34849659939338196\n",
            "Iteration 9298, Norm of Gradient: 0.004758845872227813, Cost (Train): 0.28380108994971437\n",
            "Iteration 9298, Cost (Validation): 0.34849759514644435\n",
            "Iteration 9299, Norm of Gradient: 0.004758413707164681, Cost (Train): 0.2837988258024186\n",
            "Iteration 9299, Cost (Validation): 0.3484985910342433\n",
            "Iteration 9300, Norm of Gradient: 0.004757981615929236, Cost (Train): 0.28379656206629106\n",
            "Iteration 9300, Cost (Validation): 0.3484995870567142\n",
            "Iteration 9301, Norm of Gradient: 0.004757549598501376, Cost (Train): 0.28379429874122414\n",
            "Iteration 9301, Cost (Validation): 0.34850058321379257\n",
            "Iteration 9302, Norm of Gradient: 0.004757117654861009, Cost (Train): 0.28379203582711027\n",
            "Iteration 9302, Cost (Validation): 0.34850157950541377\n",
            "Iteration 9303, Norm of Gradient: 0.004756685784988051, Cost (Train): 0.28378977332384214\n",
            "Iteration 9303, Cost (Validation): 0.34850257593151335\n",
            "Iteration 9304, Norm of Gradient: 0.004756253988862427, Cost (Train): 0.2837875112313122\n",
            "Iteration 9304, Cost (Validation): 0.3485035724920267\n",
            "Iteration 9305, Norm of Gradient: 0.004755822266464065, Cost (Train): 0.2837852495494129\n",
            "Iteration 9305, Cost (Validation): 0.34850456918688943\n",
            "Iteration 9306, Norm of Gradient: 0.004755390617772904, Cost (Train): 0.2837829882780372\n",
            "Iteration 9306, Cost (Validation): 0.3485055660160371\n",
            "Iteration 9307, Norm of Gradient: 0.004754959042768892, Cost (Train): 0.2837807274170776\n",
            "Iteration 9307, Cost (Validation): 0.34850656297940535\n",
            "Iteration 9308, Norm of Gradient: 0.00475452754143198, Cost (Train): 0.28377846696642667\n",
            "Iteration 9308, Cost (Validation): 0.34850756007692973\n",
            "Iteration 9309, Norm of Gradient: 0.00475409611374213, Cost (Train): 0.2837762069259774\n",
            "Iteration 9309, Cost (Validation): 0.3485085573085458\n",
            "Iteration 9310, Norm of Gradient: 0.004753664759679312, Cost (Train): 0.28377394729562244\n",
            "Iteration 9310, Cost (Validation): 0.3485095546741894\n",
            "Iteration 9311, Norm of Gradient: 0.004753233479223499, Cost (Train): 0.2837716880752545\n",
            "Iteration 9311, Cost (Validation): 0.3485105521737961\n",
            "Iteration 9312, Norm of Gradient: 0.004752802272354681, Cost (Train): 0.2837694292647667\n",
            "Iteration 9312, Cost (Validation): 0.34851154980730176\n",
            "Iteration 9313, Norm of Gradient: 0.004752371139052846, Cost (Train): 0.28376717086405184\n",
            "Iteration 9313, Cost (Validation): 0.348512547574642\n",
            "Iteration 9314, Norm of Gradient: 0.004751940079297991, Cost (Train): 0.2837649128730027\n",
            "Iteration 9314, Cost (Validation): 0.3485135454757526\n",
            "Iteration 9315, Norm of Gradient: 0.004751509093070127, Cost (Train): 0.28376265529151246\n",
            "Iteration 9315, Cost (Validation): 0.34851454351056943\n",
            "Iteration 9316, Norm of Gradient: 0.0047510781803492665, Cost (Train): 0.2837603981194741\n",
            "Iteration 9316, Cost (Validation): 0.3485155416790283\n",
            "Iteration 9317, Norm of Gradient: 0.004750647341115431, Cost (Train): 0.2837581413567805\n",
            "Iteration 9317, Cost (Validation): 0.34851653998106513\n",
            "Iteration 9318, Norm of Gradient: 0.0047502165753486505, Cost (Train): 0.283755885003325\n",
            "Iteration 9318, Cost (Validation): 0.34851753841661565\n",
            "Iteration 9319, Norm of Gradient: 0.004749785883028962, Cost (Train): 0.2837536290590005\n",
            "Iteration 9319, Cost (Validation): 0.34851853698561597\n",
            "Iteration 9320, Norm of Gradient: 0.004749355264136412, Cost (Train): 0.2837513735237003\n",
            "Iteration 9320, Cost (Validation): 0.3485195356880019\n",
            "Iteration 9321, Norm of Gradient: 0.00474892471865105, Cost (Train): 0.28374911839731753\n",
            "Iteration 9321, Cost (Validation): 0.34852053452370946\n",
            "Iteration 9322, Norm of Gradient: 0.004748494246552936, Cost (Train): 0.2837468636797455\n",
            "Iteration 9322, Cost (Validation): 0.3485215334926746\n",
            "Iteration 9323, Norm of Gradient: 0.004748063847822139, Cost (Train): 0.2837446093708774\n",
            "Iteration 9323, Cost (Validation): 0.34852253259483346\n",
            "Iteration 9324, Norm of Gradient: 0.004747633522438734, Cost (Train): 0.28374235547060667\n",
            "Iteration 9324, Cost (Validation): 0.348523531830122\n",
            "Iteration 9325, Norm of Gradient: 0.004747203270382801, Cost (Train): 0.2837401019788265\n",
            "Iteration 9325, Cost (Validation): 0.3485245311984763\n",
            "Iteration 9326, Norm of Gradient: 0.004746773091634433, Cost (Train): 0.2837378488954304\n",
            "Iteration 9326, Cost (Validation): 0.3485255306998325\n",
            "Iteration 9327, Norm of Gradient: 0.004746342986173726, Cost (Train): 0.2837355962203118\n",
            "Iteration 9327, Cost (Validation): 0.3485265303341267\n",
            "Iteration 9328, Norm of Gradient: 0.004745912953980785, Cost (Train): 0.28373334395336414\n",
            "Iteration 9328, Cost (Validation): 0.3485275301012951\n",
            "Iteration 9329, Norm of Gradient: 0.004745482995035724, Cost (Train): 0.28373109209448094\n",
            "Iteration 9329, Cost (Validation): 0.3485285300012738\n",
            "Iteration 9330, Norm of Gradient: 0.004745053109318659, Cost (Train): 0.2837288406435557\n",
            "Iteration 9330, Cost (Validation): 0.34852953003399917\n",
            "Iteration 9331, Norm of Gradient: 0.004744623296809723, Cost (Train): 0.28372658960048186\n",
            "Iteration 9331, Cost (Validation): 0.34853053019940733\n",
            "Iteration 9332, Norm of Gradient: 0.004744193557489049, Cost (Train): 0.2837243389651534\n",
            "Iteration 9332, Cost (Validation): 0.34853153049743457\n",
            "Iteration 9333, Norm of Gradient: 0.004743763891336779, Cost (Train): 0.2837220887374637\n",
            "Iteration 9333, Cost (Validation): 0.34853253092801717\n",
            "Iteration 9334, Norm of Gradient: 0.004743334298333065, Cost (Train): 0.28371983891730657\n",
            "Iteration 9334, Cost (Validation): 0.34853353149109145\n",
            "Iteration 9335, Norm of Gradient: 0.004742904778458063, Cost (Train): 0.28371758950457565\n",
            "Iteration 9335, Cost (Validation): 0.34853453218659386\n",
            "Iteration 9336, Norm of Gradient: 0.004742475331691938, Cost (Train): 0.2837153404991648\n",
            "Iteration 9336, Cost (Validation): 0.34853553301446066\n",
            "Iteration 9337, Norm of Gradient: 0.004742045958014865, Cost (Train): 0.28371309190096783\n",
            "Iteration 9337, Cost (Validation): 0.34853653397462836\n",
            "Iteration 9338, Norm of Gradient: 0.004741616657407023, Cost (Train): 0.2837108437098784\n",
            "Iteration 9338, Cost (Validation): 0.34853753506703333\n",
            "Iteration 9339, Norm of Gradient: 0.0047411874298485986, Cost (Train): 0.2837085959257906\n",
            "Iteration 9339, Cost (Validation): 0.34853853629161197\n",
            "Iteration 9340, Norm of Gradient: 0.004740758275319791, Cost (Train): 0.2837063485485984\n",
            "Iteration 9340, Cost (Validation): 0.34853953764830087\n",
            "Iteration 9341, Norm of Gradient: 0.004740329193800797, Cost (Train): 0.2837041015781954\n",
            "Iteration 9341, Cost (Validation): 0.3485405391370365\n",
            "Iteration 9342, Norm of Gradient: 0.004739900185271831, Cost (Train): 0.283701855014476\n",
            "Iteration 9342, Cost (Validation): 0.34854154075775534\n",
            "Iteration 9343, Norm of Gradient: 0.004739471249713109, Cost (Train): 0.283699608857334\n",
            "Iteration 9343, Cost (Validation): 0.348542542510394\n",
            "Iteration 9344, Norm of Gradient: 0.004739042387104858, Cost (Train): 0.2836973631066636\n",
            "Iteration 9344, Cost (Validation): 0.34854354439488916\n",
            "Iteration 9345, Norm of Gradient: 0.004738613597427309, Cost (Train): 0.28369511776235873\n",
            "Iteration 9345, Cost (Validation): 0.3485445464111774\n",
            "Iteration 9346, Norm of Gradient: 0.004738184880660704, Cost (Train): 0.2836928728243137\n",
            "Iteration 9346, Cost (Validation): 0.3485455485591951\n",
            "Iteration 9347, Norm of Gradient: 0.0047377562367852885, Cost (Train): 0.28369062829242264\n",
            "Iteration 9347, Cost (Validation): 0.34854655083887937\n",
            "Iteration 9348, Norm of Gradient: 0.004737327665781318, Cost (Train): 0.28368838416657965\n",
            "Iteration 9348, Cost (Validation): 0.34854755325016673\n",
            "Iteration 9349, Norm of Gradient: 0.004736899167629057, Cost (Train): 0.2836861404466792\n",
            "Iteration 9349, Cost (Validation): 0.3485485557929938\n",
            "Iteration 9350, Norm of Gradient: 0.004736470742308773, Cost (Train): 0.28368389713261555\n",
            "Iteration 9350, Cost (Validation): 0.34854955846729746\n",
            "Iteration 9351, Norm of Gradient: 0.004736042389800745, Cost (Train): 0.2836816542242829\n",
            "Iteration 9351, Cost (Validation): 0.3485505612730144\n",
            "Iteration 9352, Norm of Gradient: 0.004735614110085257, Cost (Train): 0.2836794117215757\n",
            "Iteration 9352, Cost (Validation): 0.3485515642100815\n",
            "Iteration 9353, Norm of Gradient: 0.004735185903142602, Cost (Train): 0.28367716962438827\n",
            "Iteration 9353, Cost (Validation): 0.34855256727843553\n",
            "Iteration 9354, Norm of Gradient: 0.004734757768953079, Cost (Train): 0.2836749279326151\n",
            "Iteration 9354, Cost (Validation): 0.34855357047801344\n",
            "Iteration 9355, Norm of Gradient: 0.004734329707496997, Cost (Train): 0.2836726866461508\n",
            "Iteration 9355, Cost (Validation): 0.34855457380875204\n",
            "Iteration 9356, Norm of Gradient: 0.004733901718754668, Cost (Train): 0.2836704457648897\n",
            "Iteration 9356, Cost (Validation): 0.34855557727058833\n",
            "Iteration 9357, Norm of Gradient: 0.004733473802706417, Cost (Train): 0.28366820528872644\n",
            "Iteration 9357, Cost (Validation): 0.34855658086345914\n",
            "Iteration 9358, Norm of Gradient: 0.004733045959332572, Cost (Train): 0.2836659652175557\n",
            "Iteration 9358, Cost (Validation): 0.3485575845873015\n",
            "Iteration 9359, Norm of Gradient: 0.004732618188613469, Cost (Train): 0.28366372555127195\n",
            "Iteration 9359, Cost (Validation): 0.3485585884420524\n",
            "Iteration 9360, Norm of Gradient: 0.0047321904905294546, Cost (Train): 0.2836614862897699\n",
            "Iteration 9360, Cost (Validation): 0.3485595924276489\n",
            "Iteration 9361, Norm of Gradient: 0.004731762865060879, Cost (Train): 0.28365924743294435\n",
            "Iteration 9361, Cost (Validation): 0.348560596544028\n",
            "Iteration 9362, Norm of Gradient: 0.004731335312188102, Cost (Train): 0.2836570089806899\n",
            "Iteration 9362, Cost (Validation): 0.3485616007911267\n",
            "Iteration 9363, Norm of Gradient: 0.004730907831891492, Cost (Train): 0.2836547709329015\n",
            "Iteration 9363, Cost (Validation): 0.34856260516888227\n",
            "Iteration 9364, Norm of Gradient: 0.004730480424151419, Cost (Train): 0.28365253328947376\n",
            "Iteration 9364, Cost (Validation): 0.34856360967723177\n",
            "Iteration 9365, Norm of Gradient: 0.004730053088948268, Cost (Train): 0.2836502960503017\n",
            "Iteration 9365, Cost (Validation): 0.34856461431611224\n",
            "Iteration 9366, Norm of Gradient: 0.004729625826262425, Cost (Train): 0.2836480592152801\n",
            "Iteration 9366, Cost (Validation): 0.3485656190854611\n",
            "Iteration 9367, Norm of Gradient: 0.004729198636074289, Cost (Train): 0.28364582278430395\n",
            "Iteration 9367, Cost (Validation): 0.3485666239852153\n",
            "Iteration 9368, Norm of Gradient: 0.004728771518364263, Cost (Train): 0.2836435867572682\n",
            "Iteration 9368, Cost (Validation): 0.3485676290153123\n",
            "Iteration 9369, Norm of Gradient: 0.004728344473112756, Cost (Train): 0.28364135113406785\n",
            "Iteration 9369, Cost (Validation): 0.3485686341756892\n",
            "Iteration 9370, Norm of Gradient: 0.004727917500300188, Cost (Train): 0.283639115914598\n",
            "Iteration 9370, Cost (Validation): 0.3485696394662833\n",
            "Iteration 9371, Norm of Gradient: 0.004727490599906984, Cost (Train): 0.2836368810987536\n",
            "Iteration 9371, Cost (Validation): 0.34857064488703204\n",
            "Iteration 9372, Norm of Gradient: 0.00472706377191358, Cost (Train): 0.28363464668642985\n",
            "Iteration 9372, Cost (Validation): 0.34857165043787264\n",
            "Iteration 9373, Norm of Gradient: 0.004726637016300412, Cost (Train): 0.28363241267752193\n",
            "Iteration 9373, Cost (Validation): 0.34857265611874244\n",
            "Iteration 9374, Norm of Gradient: 0.004726210333047932, Cost (Train): 0.28363017907192495\n",
            "Iteration 9374, Cost (Validation): 0.34857366192957906\n",
            "Iteration 9375, Norm of Gradient: 0.004725783722136593, Cost (Train): 0.28362794586953405\n",
            "Iteration 9375, Cost (Validation): 0.34857466787031965\n",
            "Iteration 9376, Norm of Gradient: 0.004725357183546858, Cost (Train): 0.28362571307024476\n",
            "Iteration 9376, Cost (Validation): 0.34857567394090183\n",
            "Iteration 9377, Norm of Gradient: 0.004724930717259196, Cost (Train): 0.2836234806739522\n",
            "Iteration 9377, Cost (Validation): 0.3485766801412629\n",
            "Iteration 9378, Norm of Gradient: 0.004724504323254086, Cost (Train): 0.2836212486805516\n",
            "Iteration 9378, Cost (Validation): 0.34857768647134063\n",
            "Iteration 9379, Norm of Gradient: 0.004724078001512012, Cost (Train): 0.28361901708993853\n",
            "Iteration 9379, Cost (Validation): 0.34857869293107235\n",
            "Iteration 9380, Norm of Gradient: 0.004723651752013468, Cost (Train): 0.28361678590200834\n",
            "Iteration 9380, Cost (Validation): 0.34857969952039564\n",
            "Iteration 9381, Norm of Gradient: 0.004723225574738951, Cost (Train): 0.28361455511665645\n",
            "Iteration 9381, Cost (Validation): 0.34858070623924803\n",
            "Iteration 9382, Norm of Gradient: 0.004722799469668968, Cost (Train): 0.28361232473377834\n",
            "Iteration 9382, Cost (Validation): 0.3485817130875673\n",
            "Iteration 9383, Norm of Gradient: 0.0047223734367840345, Cost (Train): 0.28361009475326954\n",
            "Iteration 9383, Cost (Validation): 0.3485827200652909\n",
            "Iteration 9384, Norm of Gradient: 0.00472194747606467, Cost (Train): 0.2836078651750256\n",
            "Iteration 9384, Cost (Validation): 0.34858372717235664\n",
            "Iteration 9385, Norm of Gradient: 0.004721521587491404, Cost (Train): 0.2836056359989421\n",
            "Iteration 9385, Cost (Validation): 0.3485847344087021\n",
            "Iteration 9386, Norm of Gradient: 0.004721095771044774, Cost (Train): 0.2836034072249148\n",
            "Iteration 9386, Cost (Validation): 0.3485857417742649\n",
            "Iteration 9387, Norm of Gradient: 0.004720670026705322, Cost (Train): 0.2836011788528392\n",
            "Iteration 9387, Cost (Validation): 0.34858674926898303\n",
            "Iteration 9388, Norm of Gradient: 0.004720244354453598, Cost (Train): 0.283598950882611\n",
            "Iteration 9388, Cost (Validation): 0.34858775689279403\n",
            "Iteration 9389, Norm of Gradient: 0.004719818754270162, Cost (Train): 0.28359672331412605\n",
            "Iteration 9389, Cost (Validation): 0.3485887646456358\n",
            "Iteration 9390, Norm of Gradient: 0.004719393226135578, Cost (Train): 0.28359449614728\n",
            "Iteration 9390, Cost (Validation): 0.3485897725274463\n",
            "Iteration 9391, Norm of Gradient: 0.004718967770030418, Cost (Train): 0.28359226938196885\n",
            "Iteration 9391, Cost (Validation): 0.348590780538163\n",
            "Iteration 9392, Norm of Gradient: 0.004718542385935264, Cost (Train): 0.28359004301808827\n",
            "Iteration 9392, Cost (Validation): 0.34859178867772406\n",
            "Iteration 9393, Norm of Gradient: 0.004718117073830704, Cost (Train): 0.28358781705553426\n",
            "Iteration 9393, Cost (Validation): 0.3485927969460674\n",
            "Iteration 9394, Norm of Gradient: 0.004717691833697329, Cost (Train): 0.28358559149420265\n",
            "Iteration 9394, Cost (Validation): 0.34859380534313067\n",
            "Iteration 9395, Norm of Gradient: 0.004717266665515743, Cost (Train): 0.2835833663339895\n",
            "Iteration 9395, Cost (Validation): 0.34859481386885205\n",
            "Iteration 9396, Norm of Gradient: 0.004716841569266555, Cost (Train): 0.2835811415747907\n",
            "Iteration 9396, Cost (Validation): 0.34859582252316945\n",
            "Iteration 9397, Norm of Gradient: 0.004716416544930381, Cost (Train): 0.2835789172165024\n",
            "Iteration 9397, Cost (Validation): 0.34859683130602087\n",
            "Iteration 9398, Norm of Gradient: 0.004715991592487845, Cost (Train): 0.28357669325902063\n",
            "Iteration 9398, Cost (Validation): 0.34859784021734436\n",
            "Iteration 9399, Norm of Gradient: 0.004715566711919581, Cost (Train): 0.28357446970224137\n",
            "Iteration 9399, Cost (Validation): 0.34859884925707796\n",
            "Iteration 9400, Norm of Gradient: 0.004715141903206222, Cost (Train): 0.28357224654606095\n",
            "Iteration 9400, Cost (Validation): 0.3485998584251598\n",
            "Iteration 9401, Norm of Gradient: 0.004714717166328416, Cost (Train): 0.28357002379037544\n",
            "Iteration 9401, Cost (Validation): 0.3486008677215278\n",
            "Iteration 9402, Norm of Gradient: 0.004714292501266817, Cost (Train): 0.28356780143508103\n",
            "Iteration 9402, Cost (Validation): 0.3486018771461203\n",
            "Iteration 9403, Norm of Gradient: 0.004713867908002084, Cost (Train): 0.28356557948007416\n",
            "Iteration 9403, Cost (Validation): 0.3486028866988753\n",
            "Iteration 9404, Norm of Gradient: 0.004713443386514884, Cost (Train): 0.2835633579252509\n",
            "Iteration 9404, Cost (Validation): 0.3486038963797311\n",
            "Iteration 9405, Norm of Gradient: 0.004713018936785893, Cost (Train): 0.2835611367705077\n",
            "Iteration 9405, Cost (Validation): 0.3486049061886259\n",
            "Iteration 9406, Norm of Gradient: 0.004712594558795792, Cost (Train): 0.2835589160157409\n",
            "Iteration 9406, Cost (Validation): 0.3486059161254979\n",
            "Iteration 9407, Norm of Gradient: 0.004712170252525269, Cost (Train): 0.28355669566084685\n",
            "Iteration 9407, Cost (Validation): 0.34860692619028544\n",
            "Iteration 9408, Norm of Gradient: 0.004711746017955022, Cost (Train): 0.28355447570572195\n",
            "Iteration 9408, Cost (Validation): 0.34860793638292664\n",
            "Iteration 9409, Norm of Gradient: 0.004711321855065755, Cost (Train): 0.28355225615026286\n",
            "Iteration 9409, Cost (Validation): 0.3486089467033599\n",
            "Iteration 9410, Norm of Gradient: 0.004710897763838178, Cost (Train): 0.28355003699436593\n",
            "Iteration 9410, Cost (Validation): 0.3486099571515236\n",
            "Iteration 9411, Norm of Gradient: 0.00471047374425301, Cost (Train): 0.2835478182379278\n",
            "Iteration 9411, Cost (Validation): 0.3486109677273561\n",
            "Iteration 9412, Norm of Gradient: 0.004710049796290973, Cost (Train): 0.2835455998808449\n",
            "Iteration 9412, Cost (Validation): 0.3486119784307958\n",
            "Iteration 9413, Norm of Gradient: 0.004709625919932805, Cost (Train): 0.28354338192301404\n",
            "Iteration 9413, Cost (Validation): 0.348612989261781\n",
            "Iteration 9414, Norm of Gradient: 0.004709202115159242, Cost (Train): 0.28354116436433163\n",
            "Iteration 9414, Cost (Validation): 0.3486140002202503\n",
            "Iteration 9415, Norm of Gradient: 0.004708778381951034, Cost (Train): 0.28353894720469464\n",
            "Iteration 9415, Cost (Validation): 0.3486150113061421\n",
            "Iteration 9416, Norm of Gradient: 0.004708354720288931, Cost (Train): 0.28353673044399963\n",
            "Iteration 9416, Cost (Validation): 0.34861602251939483\n",
            "Iteration 9417, Norm of Gradient: 0.004707931130153698, Cost (Train): 0.28353451408214336\n",
            "Iteration 9417, Cost (Validation): 0.34861703385994725\n",
            "Iteration 9418, Norm of Gradient: 0.004707507611526103, Cost (Train): 0.28353229811902275\n",
            "Iteration 9418, Cost (Validation): 0.3486180453277376\n",
            "Iteration 9419, Norm of Gradient: 0.004707084164386921, Cost (Train): 0.28353008255453443\n",
            "Iteration 9419, Cost (Validation): 0.3486190569227047\n",
            "Iteration 9420, Norm of Gradient: 0.004706660788716937, Cost (Train): 0.28352786738857544\n",
            "Iteration 9420, Cost (Validation): 0.3486200686447869\n",
            "Iteration 9421, Norm of Gradient: 0.0047062374844969385, Cost (Train): 0.2835256526210427\n",
            "Iteration 9421, Cost (Validation): 0.3486210804939231\n",
            "Iteration 9422, Norm of Gradient: 0.004705814251707728, Cost (Train): 0.28352343825183296\n",
            "Iteration 9422, Cost (Validation): 0.34862209247005177\n",
            "Iteration 9423, Norm of Gradient: 0.004705391090330104, Cost (Train): 0.2835212242808434\n",
            "Iteration 9423, Cost (Validation): 0.3486231045731117\n",
            "Iteration 9424, Norm of Gradient: 0.0047049680003448825, Cost (Train): 0.283519010707971\n",
            "Iteration 9424, Cost (Validation): 0.3486241168030415\n",
            "Iteration 9425, Norm of Gradient: 0.004704544981732881, Cost (Train): 0.2835167975331127\n",
            "Iteration 9425, Cost (Validation): 0.3486251291597799\n",
            "Iteration 9426, Norm of Gradient: 0.004704122034474927, Cost (Train): 0.28351458475616576\n",
            "Iteration 9426, Cost (Validation): 0.3486261416432657\n",
            "Iteration 9427, Norm of Gradient: 0.004703699158551853, Cost (Train): 0.2835123723770272\n",
            "Iteration 9427, Cost (Validation): 0.3486271542534377\n",
            "Iteration 9428, Norm of Gradient: 0.0047032763539445, Cost (Train): 0.2835101603955942\n",
            "Iteration 9428, Cost (Validation): 0.3486281669902347\n",
            "Iteration 9429, Norm of Gradient: 0.004702853620633715, Cost (Train): 0.2835079488117639\n",
            "Iteration 9429, Cost (Validation): 0.3486291798535955\n",
            "Iteration 9430, Norm of Gradient: 0.004702430958600354, Cost (Train): 0.2835057376254336\n",
            "Iteration 9430, Cost (Validation): 0.34863019284345903\n",
            "Iteration 9431, Norm of Gradient: 0.00470200836782528, Cost (Train): 0.28350352683650054\n",
            "Iteration 9431, Cost (Validation): 0.34863120595976405\n",
            "Iteration 9432, Norm of Gradient: 0.004701585848289363, Cost (Train): 0.283501316444862\n",
            "Iteration 9432, Cost (Validation): 0.3486322192024495\n",
            "Iteration 9433, Norm of Gradient: 0.004701163399973476, Cost (Train): 0.2834991064504155\n",
            "Iteration 9433, Cost (Validation): 0.3486332325714545\n",
            "Iteration 9434, Norm of Gradient: 0.0047007410228585055, Cost (Train): 0.2834968968530581\n",
            "Iteration 9434, Cost (Validation): 0.3486342460667178\n",
            "Iteration 9435, Norm of Gradient: 0.004700318716925341, Cost (Train): 0.2834946876526875\n",
            "Iteration 9435, Cost (Validation): 0.3486352596881783\n",
            "Iteration 9436, Norm of Gradient: 0.004699896482154882, Cost (Train): 0.283492478849201\n",
            "Iteration 9436, Cost (Validation): 0.34863627343577536\n",
            "Iteration 9437, Norm of Gradient: 0.004699474318528032, Cost (Train): 0.28349027044249614\n",
            "Iteration 9437, Cost (Validation): 0.34863728730944776\n",
            "Iteration 9438, Norm of Gradient: 0.004699052226025705, Cost (Train): 0.28348806243247043\n",
            "Iteration 9438, Cost (Validation): 0.34863830130913465\n",
            "Iteration 9439, Norm of Gradient: 0.004698630204628819, Cost (Train): 0.2834858548190214\n",
            "Iteration 9439, Cost (Validation): 0.3486393154347751\n",
            "Iteration 9440, Norm of Gradient: 0.0046982082543183, Cost (Train): 0.2834836476020467\n",
            "Iteration 9440, Cost (Validation): 0.3486403296863082\n",
            "Iteration 9441, Norm of Gradient: 0.004697786375075084, Cost (Train): 0.2834814407814439\n",
            "Iteration 9441, Cost (Validation): 0.3486413440636732\n",
            "Iteration 9442, Norm of Gradient: 0.00469736456688011, Cost (Train): 0.2834792343571107\n",
            "Iteration 9442, Cost (Validation): 0.34864235856680914\n",
            "Iteration 9443, Norm of Gradient: 0.004696942829714327, Cost (Train): 0.2834770283289447\n",
            "Iteration 9443, Cost (Validation): 0.3486433731956552\n",
            "Iteration 9444, Norm of Gradient: 0.004696521163558688, Cost (Train): 0.28347482269684376\n",
            "Iteration 9444, Cost (Validation): 0.3486443879501507\n",
            "Iteration 9445, Norm of Gradient: 0.004696099568394159, Cost (Train): 0.28347261746070573\n",
            "Iteration 9445, Cost (Validation): 0.3486454028302349\n",
            "Iteration 9446, Norm of Gradient: 0.004695678044201707, Cost (Train): 0.28347041262042816\n",
            "Iteration 9446, Cost (Validation): 0.34864641783584693\n",
            "Iteration 9447, Norm of Gradient: 0.004695256590962308, Cost (Train): 0.2834682081759092\n",
            "Iteration 9447, Cost (Validation): 0.34864743296692624\n",
            "Iteration 9448, Norm of Gradient: 0.004694835208656946, Cost (Train): 0.28346600412704637\n",
            "Iteration 9448, Cost (Validation): 0.348648448223412\n",
            "Iteration 9449, Norm of Gradient: 0.004694413897266612, Cost (Train): 0.28346380047373787\n",
            "Iteration 9449, Cost (Validation): 0.34864946360524374\n",
            "Iteration 9450, Norm of Gradient: 0.004693992656772306, Cost (Train): 0.28346159721588166\n",
            "Iteration 9450, Cost (Validation): 0.3486504791123607\n",
            "Iteration 9451, Norm of Gradient: 0.004693571487155029, Cost (Train): 0.28345939435337547\n",
            "Iteration 9451, Cost (Validation): 0.34865149474470225\n",
            "Iteration 9452, Norm of Gradient: 0.004693150388395793, Cost (Train): 0.28345719188611757\n",
            "Iteration 9452, Cost (Validation): 0.34865251050220786\n",
            "Iteration 9453, Norm of Gradient: 0.004692729360475621, Cost (Train): 0.2834549898140059\n",
            "Iteration 9453, Cost (Validation): 0.3486535263848171\n",
            "Iteration 9454, Norm of Gradient: 0.004692308403375535, Cost (Train): 0.28345278813693864\n",
            "Iteration 9454, Cost (Validation): 0.3486545423924693\n",
            "Iteration 9455, Norm of Gradient: 0.004691887517076571, Cost (Train): 0.28345058685481384\n",
            "Iteration 9455, Cost (Validation): 0.34865555852510394\n",
            "Iteration 9456, Norm of Gradient: 0.004691466701559769, Cost (Train): 0.28344838596752964\n",
            "Iteration 9456, Cost (Validation): 0.3486565747826606\n",
            "Iteration 9457, Norm of Gradient: 0.004691045956806175, Cost (Train): 0.2834461854749844\n",
            "Iteration 9457, Cost (Validation): 0.34865759116507883\n",
            "Iteration 9458, Norm of Gradient: 0.004690625282796844, Cost (Train): 0.28344398537707616\n",
            "Iteration 9458, Cost (Validation): 0.3486586076722982\n",
            "Iteration 9459, Norm of Gradient: 0.004690204679512837, Cost (Train): 0.28344178567370326\n",
            "Iteration 9459, Cost (Validation): 0.3486596243042584\n",
            "Iteration 9460, Norm of Gradient: 0.004689784146935226, Cost (Train): 0.2834395863647641\n",
            "Iteration 9460, Cost (Validation): 0.3486606410608989\n",
            "Iteration 9461, Norm of Gradient: 0.004689363685045082, Cost (Train): 0.28343738745015695\n",
            "Iteration 9461, Cost (Validation): 0.3486616579421594\n",
            "Iteration 9462, Norm of Gradient: 0.004688943293823491, Cost (Train): 0.2834351889297802\n",
            "Iteration 9462, Cost (Validation): 0.34866267494797964\n",
            "Iteration 9463, Norm of Gradient: 0.0046885229732515415, Cost (Train): 0.28343299080353224\n",
            "Iteration 9463, Cost (Validation): 0.3486636920782993\n",
            "Iteration 9464, Norm of Gradient: 0.004688102723310331, Cost (Train): 0.28343079307131147\n",
            "Iteration 9464, Cost (Validation): 0.34866470933305815\n",
            "Iteration 9465, Norm of Gradient: 0.004687682543980962, Cost (Train): 0.2834285957330166\n",
            "Iteration 9465, Cost (Validation): 0.34866572671219576\n",
            "Iteration 9466, Norm of Gradient: 0.004687262435244548, Cost (Train): 0.2834263987885458\n",
            "Iteration 9466, Cost (Validation): 0.34866674421565214\n",
            "Iteration 9467, Norm of Gradient: 0.004686842397082205, Cost (Train): 0.28342420223779796\n",
            "Iteration 9467, Cost (Validation): 0.34866776184336695\n",
            "Iteration 9468, Norm of Gradient: 0.004686422429475058, Cost (Train): 0.2834220060806715\n",
            "Iteration 9468, Cost (Validation): 0.34866877959528003\n",
            "Iteration 9469, Norm of Gradient: 0.004686002532404241, Cost (Train): 0.28341981031706504\n",
            "Iteration 9469, Cost (Validation): 0.34866979747133137\n",
            "Iteration 9470, Norm of Gradient: 0.004685582705850891, Cost (Train): 0.2834176149468773\n",
            "Iteration 9470, Cost (Validation): 0.3486708154714606\n",
            "Iteration 9471, Norm of Gradient: 0.004685162949796155, Cost (Train): 0.28341541997000697\n",
            "Iteration 9471, Cost (Validation): 0.34867183359560794\n",
            "Iteration 9472, Norm of Gradient: 0.004684743264221186, Cost (Train): 0.28341322538635283\n",
            "Iteration 9472, Cost (Validation): 0.34867285184371305\n",
            "Iteration 9473, Norm of Gradient: 0.004684323649107146, Cost (Train): 0.2834110311958135\n",
            "Iteration 9473, Cost (Validation): 0.34867387021571605\n",
            "Iteration 9474, Norm of Gradient: 0.004683904104435199, Cost (Train): 0.283408837398288\n",
            "Iteration 9474, Cost (Validation): 0.3486748887115568\n",
            "Iteration 9475, Norm of Gradient: 0.004683484630186523, Cost (Train): 0.2834066439936748\n",
            "Iteration 9475, Cost (Validation): 0.3486759073311754\n",
            "Iteration 9476, Norm of Gradient: 0.004683065226342297, Cost (Train): 0.2834044509818733\n",
            "Iteration 9476, Cost (Validation): 0.34867692607451184\n",
            "Iteration 9477, Norm of Gradient: 0.004682645892883709, Cost (Train): 0.283402258362782\n",
            "Iteration 9477, Cost (Validation): 0.3486779449415062\n",
            "Iteration 9478, Norm of Gradient: 0.004682226629791954, Cost (Train): 0.28340006613630003\n",
            "Iteration 9478, Cost (Validation): 0.3486789639320986\n",
            "Iteration 9479, Norm of Gradient: 0.0046818074370482375, Cost (Train): 0.2833978743023263\n",
            "Iteration 9479, Cost (Validation): 0.34867998304622905\n",
            "Iteration 9480, Norm of Gradient: 0.004681388314633764, Cost (Train): 0.2833956828607599\n",
            "Iteration 9480, Cost (Validation): 0.3486810022838377\n",
            "Iteration 9481, Norm of Gradient: 0.004680969262529755, Cost (Train): 0.28339349181149975\n",
            "Iteration 9481, Cost (Validation): 0.3486820216448648\n",
            "Iteration 9482, Norm of Gradient: 0.00468055028071743, Cost (Train): 0.28339130115444505\n",
            "Iteration 9482, Cost (Validation): 0.3486830411292504\n",
            "Iteration 9483, Norm of Gradient: 0.00468013136917802, Cost (Train): 0.2833891108894949\n",
            "Iteration 9483, Cost (Validation): 0.34868406073693486\n",
            "Iteration 9484, Norm of Gradient: 0.004679712527892764, Cost (Train): 0.2833869210165485\n",
            "Iteration 9484, Cost (Validation): 0.3486850804678583\n",
            "Iteration 9485, Norm of Gradient: 0.004679293756842905, Cost (Train): 0.2833847315355049\n",
            "Iteration 9485, Cost (Validation): 0.34868610032196096\n",
            "Iteration 9486, Norm of Gradient: 0.004678875056009694, Cost (Train): 0.28338254244626343\n",
            "Iteration 9486, Cost (Validation): 0.34868712029918325\n",
            "Iteration 9487, Norm of Gradient: 0.00467845642537439, Cost (Train): 0.28338035374872345\n",
            "Iteration 9487, Cost (Validation): 0.3486881403994654\n",
            "Iteration 9488, Norm of Gradient: 0.004678037864918259, Cost (Train): 0.28337816544278405\n",
            "Iteration 9488, Cost (Validation): 0.3486891606227477\n",
            "Iteration 9489, Norm of Gradient: 0.00467761937462257, Cost (Train): 0.28337597752834465\n",
            "Iteration 9489, Cost (Validation): 0.34869018096897064\n",
            "Iteration 9490, Norm of Gradient: 0.004677200954468605, Cost (Train): 0.28337379000530466\n",
            "Iteration 9490, Cost (Validation): 0.3486912014380744\n",
            "Iteration 9491, Norm of Gradient: 0.004676782604437651, Cost (Train): 0.28337160287356356\n",
            "Iteration 9491, Cost (Validation): 0.3486922220299996\n",
            "Iteration 9492, Norm of Gradient: 0.0046763643245109985, Cost (Train): 0.2833694161330205\n",
            "Iteration 9492, Cost (Validation): 0.34869324274468655\n",
            "Iteration 9493, Norm of Gradient: 0.004675946114669948, Cost (Train): 0.28336722978357526\n",
            "Iteration 9493, Cost (Validation): 0.3486942635820757\n",
            "Iteration 9494, Norm of Gradient: 0.004675527974895807, Cost (Train): 0.28336504382512717\n",
            "Iteration 9494, Cost (Validation): 0.3486952845421077\n",
            "Iteration 9495, Norm of Gradient: 0.00467510990516989, Cost (Train): 0.28336285825757584\n",
            "Iteration 9495, Cost (Validation): 0.34869630562472287\n",
            "Iteration 9496, Norm of Gradient: 0.004674691905473517, Cost (Train): 0.2833606730808209\n",
            "Iteration 9496, Cost (Validation): 0.34869732682986176\n",
            "Iteration 9497, Norm of Gradient: 0.004674273975788017, Cost (Train): 0.2833584882947619\n",
            "Iteration 9497, Cost (Validation): 0.34869834815746503\n",
            "Iteration 9498, Norm of Gradient: 0.004673856116094723, Cost (Train): 0.28335630389929845\n",
            "Iteration 9498, Cost (Validation): 0.3486993696074732\n",
            "Iteration 9499, Norm of Gradient: 0.0046734383263749785, Cost (Train): 0.28335411989433024\n",
            "Iteration 9499, Cost (Validation): 0.34870039117982693\n",
            "Iteration 9500, Norm of Gradient: 0.004673020606610132, Cost (Train): 0.2833519362797571\n",
            "Iteration 9500, Cost (Validation): 0.34870141287446677\n",
            "Iteration 9501, Norm of Gradient: 0.004672602956781537, Cost (Train): 0.28334975305547866\n",
            "Iteration 9501, Cost (Validation): 0.3487024346913334\n",
            "Iteration 9502, Norm of Gradient: 0.004672185376870558, Cost (Train): 0.2833475702213948\n",
            "Iteration 9502, Cost (Validation): 0.34870345663036756\n",
            "Iteration 9503, Norm of Gradient: 0.004671767866858564, Cost (Train): 0.28334538777740537\n",
            "Iteration 9503, Cost (Validation): 0.34870447869150994\n",
            "Iteration 9504, Norm of Gradient: 0.004671350426726932, Cost (Train): 0.28334320572341004\n",
            "Iteration 9504, Cost (Validation): 0.3487055008747012\n",
            "Iteration 9505, Norm of Gradient: 0.004670933056457044, Cost (Train): 0.28334102405930883\n",
            "Iteration 9505, Cost (Validation): 0.3487065231798822\n",
            "Iteration 9506, Norm of Gradient: 0.004670515756030291, Cost (Train): 0.2833388427850017\n",
            "Iteration 9506, Cost (Validation): 0.3487075456069937\n",
            "Iteration 9507, Norm of Gradient: 0.004670098525428072, Cost (Train): 0.2833366619003886\n",
            "Iteration 9507, Cost (Validation): 0.3487085681559764\n",
            "Iteration 9508, Norm of Gradient: 0.004669681364631788, Cost (Train): 0.2833344814053695\n",
            "Iteration 9508, Cost (Validation): 0.3487095908267713\n",
            "Iteration 9509, Norm of Gradient: 0.004669264273622849, Cost (Train): 0.2833323012998444\n",
            "Iteration 9509, Cost (Validation): 0.3487106136193191\n",
            "Iteration 9510, Norm of Gradient: 0.0046688472523826775, Cost (Train): 0.28333012158371346\n",
            "Iteration 9510, Cost (Validation): 0.3487116365335608\n",
            "Iteration 9511, Norm of Gradient: 0.004668430300892694, Cost (Train): 0.2833279422568767\n",
            "Iteration 9511, Cost (Validation): 0.34871265956943726\n",
            "Iteration 9512, Norm of Gradient: 0.004668013419134332, Cost (Train): 0.28332576331923426\n",
            "Iteration 9512, Cost (Validation): 0.3487136827268894\n",
            "Iteration 9513, Norm of Gradient: 0.00466759660708903, Cost (Train): 0.2833235847706864\n",
            "Iteration 9513, Cost (Validation): 0.34871470600585824\n",
            "Iteration 9514, Norm of Gradient: 0.004667179864738233, Cost (Train): 0.28332140661113325\n",
            "Iteration 9514, Cost (Validation): 0.3487157294062847\n",
            "Iteration 9515, Norm of Gradient: 0.004666763192063393, Cost (Train): 0.28331922884047506\n",
            "Iteration 9515, Cost (Validation): 0.3487167529281098\n",
            "Iteration 9516, Norm of Gradient: 0.00466634658904597, Cost (Train): 0.283317051458612\n",
            "Iteration 9516, Cost (Validation): 0.34871777657127456\n",
            "Iteration 9517, Norm of Gradient: 0.004665930055667429, Cost (Train): 0.2833148744654447\n",
            "Iteration 9517, Cost (Validation): 0.34871880033572\n",
            "Iteration 9518, Norm of Gradient: 0.004665513591909245, Cost (Train): 0.28331269786087315\n",
            "Iteration 9518, Cost (Validation): 0.34871982422138736\n",
            "Iteration 9519, Norm of Gradient: 0.004665097197752896, Cost (Train): 0.28331052164479786\n",
            "Iteration 9519, Cost (Validation): 0.34872084822821753\n",
            "Iteration 9520, Norm of Gradient: 0.004664680873179868, Cost (Train): 0.28330834581711933\n",
            "Iteration 9520, Cost (Validation): 0.34872187235615176\n",
            "Iteration 9521, Norm of Gradient: 0.004664264618171656, Cost (Train): 0.28330617037773803\n",
            "Iteration 9521, Cost (Validation): 0.3487228966051312\n",
            "Iteration 9522, Norm of Gradient: 0.004663848432709761, Cost (Train): 0.28330399532655426\n",
            "Iteration 9522, Cost (Validation): 0.348723920975097\n",
            "Iteration 9523, Norm of Gradient: 0.0046634323167756885, Cost (Train): 0.2833018206634686\n",
            "Iteration 9523, Cost (Validation): 0.3487249454659905\n",
            "Iteration 9524, Norm of Gradient: 0.004663016270350954, Cost (Train): 0.28329964638838173\n",
            "Iteration 9524, Cost (Validation): 0.3487259700777526\n",
            "Iteration 9525, Norm of Gradient: 0.004662600293417077, Cost (Train): 0.28329747250119414\n",
            "Iteration 9525, Cost (Validation): 0.3487269948103249\n",
            "Iteration 9526, Norm of Gradient: 0.004662184385955588, Cost (Train): 0.2832952990018064\n",
            "Iteration 9526, Cost (Validation): 0.34872801966364847\n",
            "Iteration 9527, Norm of Gradient: 0.004661768547948019, Cost (Train): 0.28329312589011935\n",
            "Iteration 9527, Cost (Validation): 0.34872904463766474\n",
            "Iteration 9528, Norm of Gradient: 0.004661352779375912, Cost (Train): 0.28329095316603337\n",
            "Iteration 9528, Cost (Validation): 0.3487300697323149\n",
            "Iteration 9529, Norm of Gradient: 0.004660937080220816, Cost (Train): 0.2832887808294495\n",
            "Iteration 9529, Cost (Validation): 0.3487310949475404\n",
            "Iteration 9530, Norm of Gradient: 0.004660521450464286, Cost (Train): 0.2832866088802684\n",
            "Iteration 9530, Cost (Validation): 0.3487321202832826\n",
            "Iteration 9531, Norm of Gradient: 0.004660105890087883, Cost (Train): 0.2832844373183908\n",
            "Iteration 9531, Cost (Validation): 0.34873314573948294\n",
            "Iteration 9532, Norm of Gradient: 0.0046596903990731775, Cost (Train): 0.2832822661437176\n",
            "Iteration 9532, Cost (Validation): 0.3487341713160827\n",
            "Iteration 9533, Norm of Gradient: 0.004659274977401744, Cost (Train): 0.2832800953561497\n",
            "Iteration 9533, Cost (Validation): 0.34873519701302347\n",
            "Iteration 9534, Norm of Gradient: 0.004658859625055168, Cost (Train): 0.2832779249555878\n",
            "Iteration 9534, Cost (Validation): 0.3487362228302467\n",
            "Iteration 9535, Norm of Gradient: 0.004658444342015034, Cost (Train): 0.28327575494193297\n",
            "Iteration 9535, Cost (Validation): 0.34873724876769385\n",
            "Iteration 9536, Norm of Gradient: 0.004658029128262941, Cost (Train): 0.28327358531508623\n",
            "Iteration 9536, Cost (Validation): 0.3487382748253065\n",
            "Iteration 9537, Norm of Gradient: 0.004657613983780492, Cost (Train): 0.28327141607494855\n",
            "Iteration 9537, Cost (Validation): 0.3487393010030261\n",
            "Iteration 9538, Norm of Gradient: 0.004657198908549295, Cost (Train): 0.28326924722142094\n",
            "Iteration 9538, Cost (Validation): 0.34874032730079435\n",
            "Iteration 9539, Norm of Gradient: 0.004656783902550968, Cost (Train): 0.28326707875440443\n",
            "Iteration 9539, Cost (Validation): 0.34874135371855275\n",
            "Iteration 9540, Norm of Gradient: 0.004656368965767135, Cost (Train): 0.2832649106738002\n",
            "Iteration 9540, Cost (Validation): 0.3487423802562429\n",
            "Iteration 9541, Norm of Gradient: 0.004655954098179425, Cost (Train): 0.28326274297950943\n",
            "Iteration 9541, Cost (Validation): 0.3487434069138065\n",
            "Iteration 9542, Norm of Gradient: 0.004655539299769476, Cost (Train): 0.28326057567143315\n",
            "Iteration 9542, Cost (Validation): 0.3487444336911852\n",
            "Iteration 9543, Norm of Gradient: 0.004655124570518929, Cost (Train): 0.2832584087494726\n",
            "Iteration 9543, Cost (Validation): 0.34874546058832073\n",
            "Iteration 9544, Norm of Gradient: 0.004654709910409438, Cost (Train): 0.28325624221352913\n",
            "Iteration 9544, Cost (Validation): 0.3487464876051548\n",
            "Iteration 9545, Norm of Gradient: 0.004654295319422659, Cost (Train): 0.28325407606350395\n",
            "Iteration 9545, Cost (Validation): 0.3487475147416291\n",
            "Iteration 9546, Norm of Gradient: 0.004653880797540255, Cost (Train): 0.2832519102992983\n",
            "Iteration 9546, Cost (Validation): 0.3487485419976855\n",
            "Iteration 9547, Norm of Gradient: 0.0046534663447438986, Cost (Train): 0.2832497449208137\n",
            "Iteration 9547, Cost (Validation): 0.3487495693732657\n",
            "Iteration 9548, Norm of Gradient: 0.004653051961015267, Cost (Train): 0.28324757992795135\n",
            "Iteration 9548, Cost (Validation): 0.3487505968683115\n",
            "Iteration 9549, Norm of Gradient: 0.004652637646336045, Cost (Train): 0.28324541532061276\n",
            "Iteration 9549, Cost (Validation): 0.34875162448276475\n",
            "Iteration 9550, Norm of Gradient: 0.004652223400687923, Cost (Train): 0.28324325109869936\n",
            "Iteration 9550, Cost (Validation): 0.3487526522165675\n",
            "Iteration 9551, Norm of Gradient: 0.004651809224052599, Cost (Train): 0.2832410872621126\n",
            "Iteration 9551, Cost (Validation): 0.34875368006966134\n",
            "Iteration 9552, Norm of Gradient: 0.004651395116411778, Cost (Train): 0.28323892381075405\n",
            "Iteration 9552, Cost (Validation): 0.3487547080419883\n",
            "Iteration 9553, Norm of Gradient: 0.004650981077747173, Cost (Train): 0.28323676074452525\n",
            "Iteration 9553, Cost (Validation): 0.3487557361334905\n",
            "Iteration 9554, Norm of Gradient: 0.004650567108040498, Cost (Train): 0.28323459806332774\n",
            "Iteration 9554, Cost (Validation): 0.3487567643441096\n",
            "Iteration 9555, Norm of Gradient: 0.004650153207273484, Cost (Train): 0.28323243576706325\n",
            "Iteration 9555, Cost (Validation): 0.34875779267378787\n",
            "Iteration 9556, Norm of Gradient: 0.004649739375427857, Cost (Train): 0.2832302738556332\n",
            "Iteration 9556, Cost (Validation): 0.3487588211224671\n",
            "Iteration 9557, Norm of Gradient: 0.00464932561248536, Cost (Train): 0.2832281123289396\n",
            "Iteration 9557, Cost (Validation): 0.34875984969008944\n",
            "Iteration 9558, Norm of Gradient: 0.004648911918427737, Cost (Train): 0.2832259511868838\n",
            "Iteration 9558, Cost (Validation): 0.34876087837659686\n",
            "Iteration 9559, Norm of Gradient: 0.004648498293236739, Cost (Train): 0.2832237904293679\n",
            "Iteration 9559, Cost (Validation): 0.3487619071819315\n",
            "Iteration 9560, Norm of Gradient: 0.004648084736894124, Cost (Train): 0.28322163005629347\n",
            "Iteration 9560, Cost (Validation): 0.34876293610603554\n",
            "Iteration 9561, Norm of Gradient: 0.004647671249381661, Cost (Train): 0.2832194700675624\n",
            "Iteration 9561, Cost (Validation): 0.34876396514885094\n",
            "Iteration 9562, Norm of Gradient: 0.004647257830681119, Cost (Train): 0.2832173104630765\n",
            "Iteration 9562, Cost (Validation): 0.3487649943103201\n",
            "Iteration 9563, Norm of Gradient: 0.004646844480774278, Cost (Train): 0.2832151512427378\n",
            "Iteration 9563, Cost (Validation): 0.3487660235903849\n",
            "Iteration 9564, Norm of Gradient: 0.004646431199642924, Cost (Train): 0.2832129924064481\n",
            "Iteration 9564, Cost (Validation): 0.3487670529889878\n",
            "Iteration 9565, Norm of Gradient: 0.004646017987268849, Cost (Train): 0.28321083395410945\n",
            "Iteration 9565, Cost (Validation): 0.34876808250607083\n",
            "Iteration 9566, Norm of Gradient: 0.004645604843633852, Cost (Train): 0.2832086758856237\n",
            "Iteration 9566, Cost (Validation): 0.3487691121415764\n",
            "Iteration 9567, Norm of Gradient: 0.0046451917687197405, Cost (Train): 0.28320651820089304\n",
            "Iteration 9567, Cost (Validation): 0.3487701418954467\n",
            "Iteration 9568, Norm of Gradient: 0.004644778762508324, Cost (Train): 0.28320436089981943\n",
            "Iteration 9568, Cost (Validation): 0.348771171767624\n",
            "Iteration 9569, Norm of Gradient: 0.0046443658249814244, Cost (Train): 0.28320220398230495\n",
            "Iteration 9569, Cost (Validation): 0.3487722017580508\n",
            "Iteration 9570, Norm of Gradient: 0.004643952956120867, Cost (Train): 0.2832000474482517\n",
            "Iteration 9570, Cost (Validation): 0.3487732318666692\n",
            "Iteration 9571, Norm of Gradient: 0.004643540155908483, Cost (Train): 0.28319789129756207\n",
            "Iteration 9571, Cost (Validation): 0.34877426209342177\n",
            "Iteration 9572, Norm of Gradient: 0.0046431274243261145, Cost (Train): 0.283195735530138\n",
            "Iteration 9572, Cost (Validation): 0.34877529243825084\n",
            "Iteration 9573, Norm of Gradient: 0.0046427147613556045, Cost (Train): 0.28319358014588186\n",
            "Iteration 9573, Cost (Validation): 0.3487763229010988\n",
            "Iteration 9574, Norm of Gradient: 0.004642302166978808, Cost (Train): 0.2831914251446958\n",
            "Iteration 9574, Cost (Validation): 0.3487773534819082\n",
            "Iteration 9575, Norm of Gradient: 0.004641889641177585, Cost (Train): 0.28318927052648224\n",
            "Iteration 9575, Cost (Validation): 0.3487783841806213\n",
            "Iteration 9576, Norm of Gradient: 0.004641477183933799, Cost (Train): 0.28318711629114346\n",
            "Iteration 9576, Cost (Validation): 0.34877941499718074\n",
            "Iteration 9577, Norm of Gradient: 0.0046410647952293245, Cost (Train): 0.2831849624385818\n",
            "Iteration 9577, Cost (Validation): 0.34878044593152907\n",
            "Iteration 9578, Norm of Gradient: 0.004640652475046042, Cost (Train): 0.2831828089686997\n",
            "Iteration 9578, Cost (Validation): 0.34878147698360873\n",
            "Iteration 9579, Norm of Gradient: 0.004640240223365836, Cost (Train): 0.2831806558813995\n",
            "Iteration 9579, Cost (Validation): 0.34878250815336237\n",
            "Iteration 9580, Norm of Gradient: 0.004639828040170599, Cost (Train): 0.28317850317658383\n",
            "Iteration 9580, Cost (Validation): 0.34878353944073254\n",
            "Iteration 9581, Norm of Gradient: 0.004639415925442232, Cost (Train): 0.2831763508541551\n",
            "Iteration 9581, Cost (Validation): 0.34878457084566183\n",
            "Iteration 9582, Norm of Gradient: 0.004639003879162641, Cost (Train): 0.2831741989140158\n",
            "Iteration 9582, Cost (Validation): 0.3487856023680929\n",
            "Iteration 9583, Norm of Gradient: 0.004638591901313739, Cost (Train): 0.2831720473560685\n",
            "Iteration 9583, Cost (Validation): 0.34878663400796844\n",
            "Iteration 9584, Norm of Gradient: 0.004638179991877444, Cost (Train): 0.28316989618021593\n",
            "Iteration 9584, Cost (Validation): 0.3487876657652311\n",
            "Iteration 9585, Norm of Gradient: 0.004637768150835685, Cost (Train): 0.2831677453863606\n",
            "Iteration 9585, Cost (Validation): 0.3487886976398236\n",
            "Iteration 9586, Norm of Gradient: 0.004637356378170392, Cost (Train): 0.2831655949744052\n",
            "Iteration 9586, Cost (Validation): 0.3487897296316887\n",
            "Iteration 9587, Norm of Gradient: 0.0046369446738635045, Cost (Train): 0.28316344494425255\n",
            "Iteration 9587, Cost (Validation): 0.34879076174076906\n",
            "Iteration 9588, Norm of Gradient: 0.004636533037896971, Cost (Train): 0.28316129529580514\n",
            "Iteration 9588, Cost (Validation): 0.34879179396700755\n",
            "Iteration 9589, Norm of Gradient: 0.004636121470252743, Cost (Train): 0.2831591460289659\n",
            "Iteration 9589, Cost (Validation): 0.3487928263103469\n",
            "Iteration 9590, Norm of Gradient: 0.004635709970912781, Cost (Train): 0.28315699714363773\n",
            "Iteration 9590, Cost (Validation): 0.34879385877073\n",
            "Iteration 9591, Norm of Gradient: 0.004635298539859049, Cost (Train): 0.28315484863972323\n",
            "Iteration 9591, Cost (Validation): 0.34879489134809977\n",
            "Iteration 9592, Norm of Gradient: 0.0046348871770735215, Cost (Train): 0.2831527005171255\n",
            "Iteration 9592, Cost (Validation): 0.34879592404239895\n",
            "Iteration 9593, Norm of Gradient: 0.004634475882538178, Cost (Train): 0.2831505527757473\n",
            "Iteration 9593, Cost (Validation): 0.3487969568535705\n",
            "Iteration 9594, Norm of Gradient: 0.004634064656235002, Cost (Train): 0.2831484054154916\n",
            "Iteration 9594, Cost (Validation): 0.3487979897815573\n",
            "Iteration 9595, Norm of Gradient: 0.0046336534981459895, Cost (Train): 0.2831462584362613\n",
            "Iteration 9595, Cost (Validation): 0.3487990228263023\n",
            "Iteration 9596, Norm of Gradient: 0.004633242408253136, Cost (Train): 0.2831441118379596\n",
            "Iteration 9596, Cost (Validation): 0.3488000559877486\n",
            "Iteration 9597, Norm of Gradient: 0.004632831386538452, Cost (Train): 0.2831419656204894\n",
            "Iteration 9597, Cost (Validation): 0.34880108926583897\n",
            "Iteration 9598, Norm of Gradient: 0.0046324204329839455, Cost (Train): 0.2831398197837537\n",
            "Iteration 9598, Cost (Validation): 0.3488021226605167\n",
            "Iteration 9599, Norm of Gradient: 0.004632009547571639, Cost (Train): 0.28313767432765574\n",
            "Iteration 9599, Cost (Validation): 0.34880315617172464\n",
            "Iteration 9600, Norm of Gradient: 0.004631598730283556, Cost (Train): 0.28313552925209867\n",
            "Iteration 9600, Cost (Validation): 0.34880418979940586\n",
            "Iteration 9601, Norm of Gradient: 0.0046311879811017295, Cost (Train): 0.28313338455698556\n",
            "Iteration 9601, Cost (Validation): 0.3488052235435035\n",
            "Iteration 9602, Norm of Gradient: 0.004630777300008199, Cost (Train): 0.2831312402422196\n",
            "Iteration 9602, Cost (Validation): 0.3488062574039607\n",
            "Iteration 9603, Norm of Gradient: 0.00463036668698501, Cost (Train): 0.28312909630770416\n",
            "Iteration 9603, Cost (Validation): 0.34880729138072053\n",
            "Iteration 9604, Norm of Gradient: 0.004629956142014213, Cost (Train): 0.2831269527533424\n",
            "Iteration 9604, Cost (Validation): 0.3488083254737262\n",
            "Iteration 9605, Norm of Gradient: 0.004629545665077868, Cost (Train): 0.28312480957903774\n",
            "Iteration 9605, Cost (Validation): 0.3488093596829208\n",
            "Iteration 9606, Norm of Gradient: 0.004629135256158041, Cost (Train): 0.2831226667846934\n",
            "Iteration 9606, Cost (Validation): 0.34881039400824776\n",
            "Iteration 9607, Norm of Gradient: 0.004628724915236803, Cost (Train): 0.28312052437021273\n",
            "Iteration 9607, Cost (Validation): 0.3488114284496501\n",
            "Iteration 9608, Norm of Gradient: 0.0046283146422962315, Cost (Train): 0.28311838233549924\n",
            "Iteration 9608, Cost (Validation): 0.3488124630070711\n",
            "Iteration 9609, Norm of Gradient: 0.004627904437318414, Cost (Train): 0.2831162406804563\n",
            "Iteration 9609, Cost (Validation): 0.34881349768045405\n",
            "Iteration 9610, Norm of Gradient: 0.00462749430028544, Cost (Train): 0.2831140994049875\n",
            "Iteration 9610, Cost (Validation): 0.3488145324697423\n",
            "Iteration 9611, Norm of Gradient: 0.004627084231179409, Cost (Train): 0.28311195850899623\n",
            "Iteration 9611, Cost (Validation): 0.3488155673748793\n",
            "Iteration 9612, Norm of Gradient: 0.004626674229982425, Cost (Train): 0.283109817992386\n",
            "Iteration 9612, Cost (Validation): 0.3488166023958081\n",
            "Iteration 9613, Norm of Gradient: 0.0046262642966766005, Cost (Train): 0.28310767785506047\n",
            "Iteration 9613, Cost (Validation): 0.3488176375324723\n",
            "Iteration 9614, Norm of Gradient: 0.004625854431244053, Cost (Train): 0.2831055380969232\n",
            "Iteration 9614, Cost (Validation): 0.3488186727848152\n",
            "Iteration 9615, Norm of Gradient: 0.0046254446336669065, Cost (Train): 0.28310339871787776\n",
            "Iteration 9615, Cost (Validation): 0.3488197081527803\n",
            "Iteration 9616, Norm of Gradient: 0.004625034903927291, Cost (Train): 0.283101259717828\n",
            "Iteration 9616, Cost (Validation): 0.3488207436363109\n",
            "Iteration 9617, Norm of Gradient: 0.004624625242007346, Cost (Train): 0.2830991210966775\n",
            "Iteration 9617, Cost (Validation): 0.3488217792353506\n",
            "Iteration 9618, Norm of Gradient: 0.0046242156478892155, Cost (Train): 0.2830969828543301\n",
            "Iteration 9618, Cost (Validation): 0.3488228149498429\n",
            "Iteration 9619, Norm of Gradient: 0.00462380612155505, Cost (Train): 0.2830948449906894\n",
            "Iteration 9619, Cost (Validation): 0.3488238507797312\n",
            "Iteration 9620, Norm of Gradient: 0.004623396662987006, Cost (Train): 0.2830927075056593\n",
            "Iteration 9620, Cost (Validation): 0.3488248867249592\n",
            "Iteration 9621, Norm of Gradient: 0.004622987272167248, Cost (Train): 0.28309057039914365\n",
            "Iteration 9621, Cost (Validation): 0.34882592278547025\n",
            "Iteration 9622, Norm of Gradient: 0.0046225779490779466, Cost (Train): 0.2830884336710463\n",
            "Iteration 9622, Cost (Validation): 0.3488269589612081\n",
            "Iteration 9623, Norm of Gradient: 0.00462216869370128, Cost (Train): 0.2830862973212712\n",
            "Iteration 9623, Cost (Validation): 0.34882799525211633\n",
            "Iteration 9624, Norm of Gradient: 0.004621759506019428, Cost (Train): 0.2830841613497222\n",
            "Iteration 9624, Cost (Validation): 0.3488290316581385\n",
            "Iteration 9625, Norm of Gradient: 0.004621350386014584, Cost (Train): 0.28308202575630326\n",
            "Iteration 9625, Cost (Validation): 0.3488300681792183\n",
            "Iteration 9626, Norm of Gradient: 0.004620941333668942, Cost (Train): 0.2830798905409185\n",
            "Iteration 9626, Cost (Validation): 0.3488311048152995\n",
            "Iteration 9627, Norm of Gradient: 0.004620532348964709, Cost (Train): 0.2830777557034718\n",
            "Iteration 9627, Cost (Validation): 0.3488321415663257\n",
            "Iteration 9628, Norm of Gradient: 0.0046201234318840895, Cost (Train): 0.2830756212438674\n",
            "Iteration 9628, Cost (Validation): 0.34883317843224065\n",
            "Iteration 9629, Norm of Gradient: 0.004619714582409302, Cost (Train): 0.2830734871620094\n",
            "Iteration 9629, Cost (Validation): 0.3488342154129881\n",
            "Iteration 9630, Norm of Gradient: 0.004619305800522569, Cost (Train): 0.28307135345780177\n",
            "Iteration 9630, Cost (Validation): 0.34883525250851183\n",
            "Iteration 9631, Norm of Gradient: 0.004618897086206121, Cost (Train): 0.28306922013114877\n",
            "Iteration 9631, Cost (Validation): 0.34883628971875563\n",
            "Iteration 9632, Norm of Gradient: 0.004618488439442191, Cost (Train): 0.2830670871819546\n",
            "Iteration 9632, Cost (Validation): 0.3488373270436633\n",
            "Iteration 9633, Norm of Gradient: 0.004618079860213024, Cost (Train): 0.28306495461012343\n",
            "Iteration 9633, Cost (Validation): 0.3488383644831787\n",
            "Iteration 9634, Norm of Gradient: 0.0046176713485008665, Cost (Train): 0.28306282241555974\n",
            "Iteration 9634, Cost (Validation): 0.34883940203724567\n",
            "Iteration 9635, Norm of Gradient: 0.004617262904287974, Cost (Train): 0.2830606905981675\n",
            "Iteration 9635, Cost (Validation): 0.34884043970580825\n",
            "Iteration 9636, Norm of Gradient: 0.004616854527556608, Cost (Train): 0.28305855915785133\n",
            "Iteration 9636, Cost (Validation): 0.3488414774888101\n",
            "Iteration 9637, Norm of Gradient: 0.004616446218289038, Cost (Train): 0.2830564280945154\n",
            "Iteration 9637, Cost (Validation): 0.3488425153861954\n",
            "Iteration 9638, Norm of Gradient: 0.004616037976467538, Cost (Train): 0.28305429740806426\n",
            "Iteration 9638, Cost (Validation): 0.3488435533979079\n",
            "Iteration 9639, Norm of Gradient: 0.004615629802074388, Cost (Train): 0.28305216709840225\n",
            "Iteration 9639, Cost (Validation): 0.34884459152389174\n",
            "Iteration 9640, Norm of Gradient: 0.004615221695091878, Cost (Train): 0.28305003716543387\n",
            "Iteration 9640, Cost (Validation): 0.3488456297640908\n",
            "Iteration 9641, Norm of Gradient: 0.004614813655502299, Cost (Train): 0.2830479076090635\n",
            "Iteration 9641, Cost (Validation): 0.3488466681184492\n",
            "Iteration 9642, Norm of Gradient: 0.004614405683287954, Cost (Train): 0.28304577842919587\n",
            "Iteration 9642, Cost (Validation): 0.34884770658691105\n",
            "Iteration 9643, Norm of Gradient: 0.004613997778431151, Cost (Train): 0.28304364962573547\n",
            "Iteration 9643, Cost (Validation): 0.34884874516942016\n",
            "Iteration 9644, Norm of Gradient: 0.0046135899409142, Cost (Train): 0.28304152119858683\n",
            "Iteration 9644, Cost (Validation): 0.34884978386592097\n",
            "Iteration 9645, Norm of Gradient: 0.004613182170719423, Cost (Train): 0.2830393931476546\n",
            "Iteration 9645, Cost (Validation): 0.3488508226763574\n",
            "Iteration 9646, Norm of Gradient: 0.004612774467829146, Cost (Train): 0.28303726547284347\n",
            "Iteration 9646, Cost (Validation): 0.3488518616006737\n",
            "Iteration 9647, Norm of Gradient: 0.004612366832225704, Cost (Train): 0.2830351381740581\n",
            "Iteration 9647, Cost (Validation): 0.34885290063881386\n",
            "Iteration 9648, Norm of Gradient: 0.004611959263891434, Cost (Train): 0.28303301125120334\n",
            "Iteration 9648, Cost (Validation): 0.3488539397907222\n",
            "Iteration 9649, Norm of Gradient: 0.004611551762808684, Cost (Train): 0.2830308847041838\n",
            "Iteration 9649, Cost (Validation): 0.348854979056343\n",
            "Iteration 9650, Norm of Gradient: 0.004611144328959803, Cost (Train): 0.28302875853290427\n",
            "Iteration 9650, Cost (Validation): 0.3488560184356204\n",
            "Iteration 9651, Norm of Gradient: 0.0046107369623271536, Cost (Train): 0.28302663273726963\n",
            "Iteration 9651, Cost (Validation): 0.34885705792849864\n",
            "Iteration 9652, Norm of Gradient: 0.004610329662893097, Cost (Train): 0.2830245073171847\n",
            "Iteration 9652, Cost (Validation): 0.34885809753492203\n",
            "Iteration 9653, Norm of Gradient: 0.004609922430640009, Cost (Train): 0.28302238227255444\n",
            "Iteration 9653, Cost (Validation): 0.348859137254835\n",
            "Iteration 9654, Norm of Gradient: 0.004609515265550265, Cost (Train): 0.28302025760328375\n",
            "Iteration 9654, Cost (Validation): 0.34886017708818173\n",
            "Iteration 9655, Norm of Gradient: 0.004609108167606252, Cost (Train): 0.28301813330927755\n",
            "Iteration 9655, Cost (Validation): 0.3488612170349065\n",
            "Iteration 9656, Norm of Gradient: 0.0046087011367903575, Cost (Train): 0.2830160093904408\n",
            "Iteration 9656, Cost (Validation): 0.34886225709495394\n",
            "Iteration 9657, Norm of Gradient: 0.004608294173084981, Cost (Train): 0.28301388584667864\n",
            "Iteration 9657, Cost (Validation): 0.3488632972682683\n",
            "Iteration 9658, Norm of Gradient: 0.004607887276472527, Cost (Train): 0.283011762677896\n",
            "Iteration 9658, Cost (Validation): 0.34886433755479396\n",
            "Iteration 9659, Norm of Gradient: 0.004607480446935406, Cost (Train): 0.283009639883998\n",
            "Iteration 9659, Cost (Validation): 0.3488653779544755\n",
            "Iteration 9660, Norm of Gradient: 0.004607073684456032, Cost (Train): 0.28300751746488984\n",
            "Iteration 9660, Cost (Validation): 0.34886641846725724\n",
            "Iteration 9661, Norm of Gradient: 0.004606666989016832, Cost (Train): 0.2830053954204766\n",
            "Iteration 9661, Cost (Validation): 0.3488674590930838\n",
            "Iteration 9662, Norm of Gradient: 0.004606260360600233, Cost (Train): 0.2830032737506634\n",
            "Iteration 9662, Cost (Validation): 0.34886849983189966\n",
            "Iteration 9663, Norm of Gradient: 0.004605853799188671, Cost (Train): 0.2830011524553556\n",
            "Iteration 9663, Cost (Validation): 0.3488695406836493\n",
            "Iteration 9664, Norm of Gradient: 0.00460544730476459, Cost (Train): 0.2829990315344583\n",
            "Iteration 9664, Cost (Validation): 0.34887058164827733\n",
            "Iteration 9665, Norm of Gradient: 0.004605040877310437, Cost (Train): 0.2829969109878769\n",
            "Iteration 9665, Cost (Validation): 0.3488716227257283\n",
            "Iteration 9666, Norm of Gradient: 0.004604634516808669, Cost (Train): 0.28299479081551665\n",
            "Iteration 9666, Cost (Validation): 0.34887266391594685\n",
            "Iteration 9667, Norm of Gradient: 0.0046042282232417455, Cost (Train): 0.28299267101728287\n",
            "Iteration 9667, Cost (Validation): 0.34887370521887767\n",
            "Iteration 9668, Norm of Gradient: 0.004603821996592137, Cost (Train): 0.282990551593081\n",
            "Iteration 9668, Cost (Validation): 0.34887474663446527\n",
            "Iteration 9669, Norm of Gradient: 0.004603415836842316, Cost (Train): 0.28298843254281647\n",
            "Iteration 9669, Cost (Validation): 0.34887578816265447\n",
            "Iteration 9670, Norm of Gradient: 0.004603009743974764, Cost (Train): 0.28298631386639467\n",
            "Iteration 9670, Cost (Validation): 0.3488768298033898\n",
            "Iteration 9671, Norm of Gradient: 0.0046026037179719695, Cost (Train): 0.28298419556372106\n",
            "Iteration 9671, Cost (Validation): 0.34887787155661615\n",
            "Iteration 9672, Norm of Gradient: 0.004602197758816424, Cost (Train): 0.2829820776347011\n",
            "Iteration 9672, Cost (Validation): 0.3488789134222782\n",
            "Iteration 9673, Norm of Gradient: 0.004601791866490628, Cost (Train): 0.2829799600792406\n",
            "Iteration 9673, Cost (Validation): 0.34887995540032074\n",
            "Iteration 9674, Norm of Gradient: 0.00460138604097709, Cost (Train): 0.28297784289724476\n",
            "Iteration 9674, Cost (Validation): 0.3488809974906885\n",
            "Iteration 9675, Norm of Gradient: 0.00460098028225832, Cost (Train): 0.2829757260886195\n",
            "Iteration 9675, Cost (Validation): 0.3488820396933264\n",
            "Iteration 9676, Norm of Gradient: 0.0046005745903168384, Cost (Train): 0.28297360965327023\n",
            "Iteration 9676, Cost (Validation): 0.3488830820081791\n",
            "Iteration 9677, Norm of Gradient: 0.0046001689651351705, Cost (Train): 0.2829714935911027\n",
            "Iteration 9677, Cost (Validation): 0.3488841244351916\n",
            "Iteration 9678, Norm of Gradient: 0.0045997634066958495, Cost (Train): 0.2829693779020226\n",
            "Iteration 9678, Cost (Validation): 0.34888516697430877\n",
            "Iteration 9679, Norm of Gradient: 0.004599357914981411, Cost (Train): 0.28296726258593574\n",
            "Iteration 9679, Cost (Validation): 0.3488862096254755\n",
            "Iteration 9680, Norm of Gradient: 0.004598952489974402, Cost (Train): 0.2829651476427478\n",
            "Iteration 9680, Cost (Validation): 0.3488872523886367\n",
            "Iteration 9681, Norm of Gradient: 0.004598547131657373, Cost (Train): 0.28296303307236464\n",
            "Iteration 9681, Cost (Validation): 0.3488882952637374\n",
            "Iteration 9682, Norm of Gradient: 0.00459814184001288, Cost (Train): 0.2829609188746921\n",
            "Iteration 9682, Cost (Validation): 0.3488893382507224\n",
            "Iteration 9683, Norm of Gradient: 0.004597736615023489, Cost (Train): 0.282958805049636\n",
            "Iteration 9683, Cost (Validation): 0.34889038134953687\n",
            "Iteration 9684, Norm of Gradient: 0.004597331456671767, Cost (Train): 0.2829566915971022\n",
            "Iteration 9684, Cost (Validation): 0.3488914245601258\n",
            "Iteration 9685, Norm of Gradient: 0.004596926364940294, Cost (Train): 0.2829545785169966\n",
            "Iteration 9685, Cost (Validation): 0.3488924678824341\n",
            "Iteration 9686, Norm of Gradient: 0.00459652133981165, Cost (Train): 0.2829524658092253\n",
            "Iteration 9686, Cost (Validation): 0.348893511316407\n",
            "Iteration 9687, Norm of Gradient: 0.004596116381268426, Cost (Train): 0.28295035347369424\n",
            "Iteration 9687, Cost (Validation): 0.3488945548619895\n",
            "Iteration 9688, Norm of Gradient: 0.004595711489293216, Cost (Train): 0.2829482415103094\n",
            "Iteration 9688, Cost (Validation): 0.3488955985191267\n",
            "Iteration 9689, Norm of Gradient: 0.004595306663868623, Cost (Train): 0.2829461299189769\n",
            "Iteration 9689, Cost (Validation): 0.3488966422877638\n",
            "Iteration 9690, Norm of Gradient: 0.004594901904977253, Cost (Train): 0.2829440186996027\n",
            "Iteration 9690, Cost (Validation): 0.34889768616784583\n",
            "Iteration 9691, Norm of Gradient: 0.004594497212601724, Cost (Train): 0.282941907852093\n",
            "Iteration 9691, Cost (Validation): 0.3488987301593182\n",
            "Iteration 9692, Norm of Gradient: 0.004594092586724654, Cost (Train): 0.2829397973763541\n",
            "Iteration 9692, Cost (Validation): 0.34889977426212593\n",
            "Iteration 9693, Norm of Gradient: 0.004593688027328671, Cost (Train): 0.28293768727229196\n",
            "Iteration 9693, Cost (Validation): 0.34890081847621424\n",
            "Iteration 9694, Norm of Gradient: 0.004593283534396408, Cost (Train): 0.2829355775398129\n",
            "Iteration 9694, Cost (Validation): 0.3489018628015284\n",
            "Iteration 9695, Norm of Gradient: 0.004592879107910506, Cost (Train): 0.28293346817882314\n",
            "Iteration 9695, Cost (Validation): 0.34890290723801376\n",
            "Iteration 9696, Norm of Gradient: 0.004592474747853611, Cost (Train): 0.282931359189229\n",
            "Iteration 9696, Cost (Validation): 0.3489039517856155\n",
            "Iteration 9697, Norm of Gradient: 0.004592070454208373, Cost (Train): 0.28292925057093676\n",
            "Iteration 9697, Cost (Validation): 0.348904996444279\n",
            "Iteration 9698, Norm of Gradient: 0.004591666226957453, Cost (Train): 0.2829271423238528\n",
            "Iteration 9698, Cost (Validation): 0.34890604121394964\n",
            "Iteration 9699, Norm of Gradient: 0.004591262066083516, Cost (Train): 0.2829250344478834\n",
            "Iteration 9699, Cost (Validation): 0.3489070860945726\n",
            "Iteration 9700, Norm of Gradient: 0.004590857971569232, Cost (Train): 0.2829229269429351\n",
            "Iteration 9700, Cost (Validation): 0.3489081310860935\n",
            "Iteration 9701, Norm of Gradient: 0.00459045394339728, Cost (Train): 0.28292081980891426\n",
            "Iteration 9701, Cost (Validation): 0.34890917618845757\n",
            "Iteration 9702, Norm of Gradient: 0.004590049981550344, Cost (Train): 0.28291871304572735\n",
            "Iteration 9702, Cost (Validation): 0.34891022140161027\n",
            "Iteration 9703, Norm of Gradient: 0.0045896460860111125, Cost (Train): 0.282916606653281\n",
            "Iteration 9703, Cost (Validation): 0.3489112667254971\n",
            "Iteration 9704, Norm of Gradient: 0.004589242256762284, Cost (Train): 0.2829145006314815\n",
            "Iteration 9704, Cost (Validation): 0.3489123121600635\n",
            "Iteration 9705, Norm of Gradient: 0.00458883849378656, Cost (Train): 0.2829123949802356\n",
            "Iteration 9705, Cost (Validation): 0.348913357705255\n",
            "Iteration 9706, Norm of Gradient: 0.00458843479706665, Cost (Train): 0.28291028969945\n",
            "Iteration 9706, Cost (Validation): 0.3489144033610171\n",
            "Iteration 9707, Norm of Gradient: 0.00458803116658527, Cost (Train): 0.2829081847890311\n",
            "Iteration 9707, Cost (Validation): 0.34891544912729533\n",
            "Iteration 9708, Norm of Gradient: 0.0045876276023251415, Cost (Train): 0.2829060802488856\n",
            "Iteration 9708, Cost (Validation): 0.34891649500403527\n",
            "Iteration 9709, Norm of Gradient: 0.004587224104268991, Cost (Train): 0.28290397607892037\n",
            "Iteration 9709, Cost (Validation): 0.34891754099118244\n",
            "Iteration 9710, Norm of Gradient: 0.0045868206723995554, Cost (Train): 0.28290187227904195\n",
            "Iteration 9710, Cost (Validation): 0.34891858708868256\n",
            "Iteration 9711, Norm of Gradient: 0.0045864173066995735, Cost (Train): 0.28289976884915724\n",
            "Iteration 9711, Cost (Validation): 0.3489196332964811\n",
            "Iteration 9712, Norm of Gradient: 0.0045860140071517915, Cost (Train): 0.2828976657891729\n",
            "Iteration 9712, Cost (Validation): 0.34892067961452394\n",
            "Iteration 9713, Norm of Gradient: 0.004585610773738963, Cost (Train): 0.28289556309899594\n",
            "Iteration 9713, Cost (Validation): 0.3489217260427566\n",
            "Iteration 9714, Norm of Gradient: 0.004585207606443849, Cost (Train): 0.28289346077853306\n",
            "Iteration 9714, Cost (Validation): 0.3489227725811248\n",
            "Iteration 9715, Norm of Gradient: 0.004584804505249213, Cost (Train): 0.2828913588276911\n",
            "Iteration 9715, Cost (Validation): 0.3489238192295743\n",
            "Iteration 9716, Norm of Gradient: 0.004584401470137828, Cost (Train): 0.2828892572463772\n",
            "Iteration 9716, Cost (Validation): 0.3489248659880507\n",
            "Iteration 9717, Norm of Gradient: 0.004583998501092471, Cost (Train): 0.2828871560344981\n",
            "Iteration 9717, Cost (Validation): 0.3489259128565\n",
            "Iteration 9718, Norm of Gradient: 0.004583595598095927, Cost (Train): 0.28288505519196094\n",
            "Iteration 9718, Cost (Validation): 0.34892695983486777\n",
            "Iteration 9719, Norm of Gradient: 0.004583192761130988, Cost (Train): 0.2828829547186726\n",
            "Iteration 9719, Cost (Validation): 0.3489280069231\n",
            "Iteration 9720, Norm of Gradient: 0.0045827899901804475, Cost (Train): 0.2828808546145401\n",
            "Iteration 9720, Cost (Validation): 0.34892905412114233\n",
            "Iteration 9721, Norm of Gradient: 0.004582387285227112, Cost (Train): 0.28287875487947073\n",
            "Iteration 9721, Cost (Validation): 0.3489301014289408\n",
            "Iteration 9722, Norm of Gradient: 0.00458198464625379, Cost (Train): 0.28287665551337143\n",
            "Iteration 9722, Cost (Validation): 0.3489311488464412\n",
            "Iteration 9723, Norm of Gradient: 0.004581582073243295, Cost (Train): 0.28287455651614934\n",
            "Iteration 9723, Cost (Validation): 0.3489321963735894\n",
            "Iteration 9724, Norm of Gradient: 0.004581179566178451, Cost (Train): 0.28287245788771176\n",
            "Iteration 9724, Cost (Validation): 0.34893324401033143\n",
            "Iteration 9725, Norm of Gradient: 0.0045807771250420855, Cost (Train): 0.28287035962796575\n",
            "Iteration 9725, Cost (Validation): 0.3489342917566131\n",
            "Iteration 9726, Norm of Gradient: 0.004580374749817032, Cost (Train): 0.2828682617368186\n",
            "Iteration 9726, Cost (Validation): 0.34893533961238055\n",
            "Iteration 9727, Norm of Gradient: 0.004579972440486133, Cost (Train): 0.28286616421417765\n",
            "Iteration 9727, Cost (Validation): 0.34893638757757955\n",
            "Iteration 9728, Norm of Gradient: 0.004579570197032232, Cost (Train): 0.28286406705995015\n",
            "Iteration 9728, Cost (Validation): 0.3489374356521563\n",
            "Iteration 9729, Norm of Gradient: 0.004579168019438187, Cost (Train): 0.2828619702740433\n",
            "Iteration 9729, Cost (Validation): 0.34893848383605663\n",
            "Iteration 9730, Norm of Gradient: 0.00457876590768685, Cost (Train): 0.2828598738563647\n",
            "Iteration 9730, Cost (Validation): 0.3489395321292269\n",
            "Iteration 9731, Norm of Gradient: 0.004578363861761093, Cost (Train): 0.2828577778068216\n",
            "Iteration 9731, Cost (Validation): 0.34894058053161287\n",
            "Iteration 9732, Norm of Gradient: 0.004577961881643785, Cost (Train): 0.2828556821253214\n",
            "Iteration 9732, Cost (Validation): 0.3489416290431609\n",
            "Iteration 9733, Norm of Gradient: 0.004577559967317804, Cost (Train): 0.2828535868117716\n",
            "Iteration 9733, Cost (Validation): 0.3489426776638169\n",
            "Iteration 9734, Norm of Gradient: 0.004577158118766031, Cost (Train): 0.2828514918660797\n",
            "Iteration 9734, Cost (Validation): 0.34894372639352716\n",
            "Iteration 9735, Norm of Gradient: 0.004576756335971362, Cost (Train): 0.28284939728815334\n",
            "Iteration 9735, Cost (Validation): 0.34894477523223777\n",
            "Iteration 9736, Norm of Gradient: 0.00457635461891669, Cost (Train): 0.2828473030778998\n",
            "Iteration 9736, Cost (Validation): 0.3489458241798949\n",
            "Iteration 9737, Norm of Gradient: 0.004575952967584918, Cost (Train): 0.2828452092352269\n",
            "Iteration 9737, Cost (Validation): 0.3489468732364449\n",
            "Iteration 9738, Norm of Gradient: 0.0045755513819589555, Cost (Train): 0.2828431157600421\n",
            "Iteration 9738, Cost (Validation): 0.34894792240183387\n",
            "Iteration 9739, Norm of Gradient: 0.004575149862021717, Cost (Train): 0.28284102265225314\n",
            "Iteration 9739, Cost (Validation): 0.34894897167600814\n",
            "Iteration 9740, Norm of Gradient: 0.004574748407756122, Cost (Train): 0.28283892991176773\n",
            "Iteration 9740, Cost (Validation): 0.3489500210589139\n",
            "Iteration 9741, Norm of Gradient: 0.004574347019145101, Cost (Train): 0.2828368375384934\n",
            "Iteration 9741, Cost (Validation): 0.34895107055049757\n",
            "Iteration 9742, Norm of Gradient: 0.004573945696171586, Cost (Train): 0.2828347455323381\n",
            "Iteration 9742, Cost (Validation): 0.34895212015070537\n",
            "Iteration 9743, Norm of Gradient: 0.004573544438818516, Cost (Train): 0.2828326538932095\n",
            "Iteration 9743, Cost (Validation): 0.34895316985948377\n",
            "Iteration 9744, Norm of Gradient: 0.0045731432470688385, Cost (Train): 0.28283056262101547\n",
            "Iteration 9744, Cost (Validation): 0.3489542196767789\n",
            "Iteration 9745, Norm of Gradient: 0.004572742120905505, Cost (Train): 0.2828284717156637\n",
            "Iteration 9745, Cost (Validation): 0.3489552696025374\n",
            "Iteration 9746, Norm of Gradient: 0.004572341060311473, Cost (Train): 0.2828263811770622\n",
            "Iteration 9746, Cost (Validation): 0.3489563196367056\n",
            "Iteration 9747, Norm of Gradient: 0.004571940065269708, Cost (Train): 0.28282429100511886\n",
            "Iteration 9747, Cost (Validation): 0.34895736977922986\n",
            "Iteration 9748, Norm of Gradient: 0.0045715391357631805, Cost (Train): 0.28282220119974144\n",
            "Iteration 9748, Cost (Validation): 0.3489584200300567\n",
            "Iteration 9749, Norm of Gradient: 0.004571138271774867, Cost (Train): 0.28282011176083816\n",
            "Iteration 9749, Cost (Validation): 0.34895947038913266\n",
            "Iteration 9750, Norm of Gradient: 0.00457073747328775, Cost (Train): 0.2828180226883168\n",
            "Iteration 9750, Cost (Validation): 0.3489605208564041\n",
            "Iteration 9751, Norm of Gradient: 0.004570336740284819, Cost (Train): 0.2828159339820855\n",
            "Iteration 9751, Cost (Validation): 0.3489615714318176\n",
            "Iteration 9752, Norm of Gradient: 0.004569936072749069, Cost (Train): 0.28281384564205225\n",
            "Iteration 9752, Cost (Validation): 0.3489626221153198\n",
            "Iteration 9753, Norm of Gradient: 0.0045695354706635025, Cost (Train): 0.2828117576681252\n",
            "Iteration 9753, Cost (Validation): 0.3489636729068571\n",
            "Iteration 9754, Norm of Gradient: 0.004569134934011126, Cost (Train): 0.28280967006021235\n",
            "Iteration 9754, Cost (Validation): 0.3489647238063762\n",
            "Iteration 9755, Norm of Gradient: 0.004568734462774953, Cost (Train): 0.28280758281822205\n",
            "Iteration 9755, Cost (Validation): 0.34896577481382374\n",
            "Iteration 9756, Norm of Gradient: 0.004568334056938004, Cost (Train): 0.2828054959420623\n",
            "Iteration 9756, Cost (Validation): 0.34896682592914624\n",
            "Iteration 9757, Norm of Gradient: 0.004567933716483305, Cost (Train): 0.2828034094316413\n",
            "Iteration 9757, Cost (Validation): 0.3489678771522904\n",
            "Iteration 9758, Norm of Gradient: 0.004567533441393887, Cost (Train): 0.28280132328686747\n",
            "Iteration 9758, Cost (Validation): 0.3489689284832029\n",
            "Iteration 9759, Norm of Gradient: 0.00456713323165279, Cost (Train): 0.28279923750764885\n",
            "Iteration 9759, Cost (Validation): 0.34896997992183043\n",
            "Iteration 9760, Norm of Gradient: 0.004566733087243057, Cost (Train): 0.2827971520938939\n",
            "Iteration 9760, Cost (Validation): 0.3489710314681197\n",
            "Iteration 9761, Norm of Gradient: 0.004566333008147739, Cost (Train): 0.28279506704551094\n",
            "Iteration 9761, Cost (Validation): 0.34897208312201755\n",
            "Iteration 9762, Norm of Gradient: 0.004565932994349894, Cost (Train): 0.2827929823624083\n",
            "Iteration 9762, Cost (Validation): 0.3489731348834706\n",
            "Iteration 9763, Norm of Gradient: 0.004565533045832582, Cost (Train): 0.2827908980444945\n",
            "Iteration 9763, Cost (Validation): 0.3489741867524257\n",
            "Iteration 9764, Norm of Gradient: 0.004565133162578873, Cost (Train): 0.2827888140916777\n",
            "Iteration 9764, Cost (Validation): 0.3489752387288297\n",
            "Iteration 9765, Norm of Gradient: 0.004564733344571845, Cost (Train): 0.28278673050386666\n",
            "Iteration 9765, Cost (Validation): 0.3489762908126294\n",
            "Iteration 9766, Norm of Gradient: 0.004564333591794573, Cost (Train): 0.2827846472809696\n",
            "Iteration 9766, Cost (Validation): 0.34897734300377153\n",
            "Iteration 9767, Norm of Gradient: 0.004563933904230149, Cost (Train): 0.2827825644228953\n",
            "Iteration 9767, Cost (Validation): 0.34897839530220315\n",
            "Iteration 9768, Norm of Gradient: 0.004563534281861664, Cost (Train): 0.28278048192955213\n",
            "Iteration 9768, Cost (Validation): 0.3489794477078711\n",
            "Iteration 9769, Norm of Gradient: 0.004563134724672219, Cost (Train): 0.28277839980084873\n",
            "Iteration 9769, Cost (Validation): 0.34898050022072225\n",
            "Iteration 9770, Norm of Gradient: 0.004562735232644922, Cost (Train): 0.28277631803669373\n",
            "Iteration 9770, Cost (Validation): 0.34898155284070353\n",
            "Iteration 9771, Norm of Gradient: 0.004562335805762878, Cost (Train): 0.28277423663699575\n",
            "Iteration 9771, Cost (Validation): 0.34898260556776206\n",
            "Iteration 9772, Norm of Gradient: 0.004561936444009209, Cost (Train): 0.2827721556016635\n",
            "Iteration 9772, Cost (Validation): 0.34898365840184453\n",
            "Iteration 9773, Norm of Gradient: 0.004561537147367038, Cost (Train): 0.28277007493060563\n",
            "Iteration 9773, Cost (Validation): 0.34898471134289827\n",
            "Iteration 9774, Norm of Gradient: 0.004561137915819496, Cost (Train): 0.2827679946237309\n",
            "Iteration 9774, Cost (Validation): 0.3489857643908701\n",
            "Iteration 9775, Norm of Gradient: 0.004560738749349717, Cost (Train): 0.28276591468094814\n",
            "Iteration 9775, Cost (Validation): 0.3489868175457071\n",
            "Iteration 9776, Norm of Gradient: 0.004560339647940845, Cost (Train): 0.28276383510216613\n",
            "Iteration 9776, Cost (Validation): 0.34898787080735627\n",
            "Iteration 9777, Norm of Gradient: 0.004559940611576028, Cost (Train): 0.28276175588729363\n",
            "Iteration 9777, Cost (Validation): 0.34898892417576494\n",
            "Iteration 9778, Norm of Gradient: 0.004559541640238419, Cost (Train): 0.2827596770362396\n",
            "Iteration 9778, Cost (Validation): 0.34898997765087997\n",
            "Iteration 9779, Norm of Gradient: 0.004559142733911179, Cost (Train): 0.28275759854891286\n",
            "Iteration 9779, Cost (Validation): 0.3489910312326487\n",
            "Iteration 9780, Norm of Gradient: 0.004558743892577475, Cost (Train): 0.28275552042522234\n",
            "Iteration 9780, Cost (Validation): 0.3489920849210181\n",
            "Iteration 9781, Norm of Gradient: 0.004558345116220479, Cost (Train): 0.28275344266507707\n",
            "Iteration 9781, Cost (Validation): 0.3489931387159355\n",
            "Iteration 9782, Norm of Gradient: 0.0045579464048233695, Cost (Train): 0.28275136526838596\n",
            "Iteration 9782, Cost (Validation): 0.34899419261734804\n",
            "Iteration 9783, Norm of Gradient: 0.004557547758369331, Cost (Train): 0.28274928823505807\n",
            "Iteration 9783, Cost (Validation): 0.34899524662520287\n",
            "Iteration 9784, Norm of Gradient: 0.0045571491768415535, Cost (Train): 0.2827472115650024\n",
            "Iteration 9784, Cost (Validation): 0.3489963007394474\n",
            "Iteration 9785, Norm of Gradient: 0.0045567506602232355, Cost (Train): 0.2827451352581281\n",
            "Iteration 9785, Cost (Validation): 0.3489973549600287\n",
            "Iteration 9786, Norm of Gradient: 0.004556352208497579, Cost (Train): 0.2827430593143442\n",
            "Iteration 9786, Cost (Validation): 0.3489984092868942\n",
            "Iteration 9787, Norm of Gradient: 0.004555953821647793, Cost (Train): 0.2827409837335599\n",
            "Iteration 9787, Cost (Validation): 0.34899946371999113\n",
            "Iteration 9788, Norm of Gradient: 0.004555555499657092, Cost (Train): 0.2827389085156843\n",
            "Iteration 9788, Cost (Validation): 0.3490005182592669\n",
            "Iteration 9789, Norm of Gradient: 0.004555157242508698, Cost (Train): 0.2827368336606267\n",
            "Iteration 9789, Cost (Validation): 0.3490015729046689\n",
            "Iteration 9790, Norm of Gradient: 0.004554759050185836, Cost (Train): 0.28273475916829627\n",
            "Iteration 9790, Cost (Validation): 0.3490026276561443\n",
            "Iteration 9791, Norm of Gradient: 0.004554360922671741, Cost (Train): 0.2827326850386022\n",
            "Iteration 9791, Cost (Validation): 0.3490036825136407\n",
            "Iteration 9792, Norm of Gradient: 0.004553962859949651, Cost (Train): 0.282730611271454\n",
            "Iteration 9792, Cost (Validation): 0.34900473747710536\n",
            "Iteration 9793, Norm of Gradient: 0.0045535648620028115, Cost (Train): 0.28272853786676083\n",
            "Iteration 9793, Cost (Validation): 0.3490057925464859\n",
            "Iteration 9794, Norm of Gradient: 0.004553166928814474, Cost (Train): 0.2827264648244321\n",
            "Iteration 9794, Cost (Validation): 0.34900684772172963\n",
            "Iteration 9795, Norm of Gradient: 0.0045527690603678955, Cost (Train): 0.2827243921443771\n",
            "Iteration 9795, Cost (Validation): 0.34900790300278406\n",
            "Iteration 9796, Norm of Gradient: 0.00455237125664634, Cost (Train): 0.2827223198265055\n",
            "Iteration 9796, Cost (Validation): 0.3490089583895967\n",
            "Iteration 9797, Norm of Gradient: 0.0045519735176330735, Cost (Train): 0.28272024787072636\n",
            "Iteration 9797, Cost (Validation): 0.3490100138821151\n",
            "Iteration 9798, Norm of Gradient: 0.004551575843311376, Cost (Train): 0.28271817627694956\n",
            "Iteration 9798, Cost (Validation): 0.3490110694802869\n",
            "Iteration 9799, Norm of Gradient: 0.004551178233664526, Cost (Train): 0.2827161050450844\n",
            "Iteration 9799, Cost (Validation): 0.34901212518405944\n",
            "Iteration 9800, Norm of Gradient: 0.00455078068867581, Cost (Train): 0.2827140341750403\n",
            "Iteration 9800, Cost (Validation): 0.3490131809933805\n",
            "Iteration 9801, Norm of Gradient: 0.004550383208328525, Cost (Train): 0.2827119636667271\n",
            "Iteration 9801, Cost (Validation): 0.34901423690819766\n",
            "Iteration 9802, Norm of Gradient: 0.004549985792605967, Cost (Train): 0.2827098935200542\n",
            "Iteration 9802, Cost (Validation): 0.3490152929284585\n",
            "Iteration 9803, Norm of Gradient: 0.0045495884414914435, Cost (Train): 0.2827078237349314\n",
            "Iteration 9803, Cost (Validation): 0.34901634905411066\n",
            "Iteration 9804, Norm of Gradient: 0.004549191154968263, Cost (Train): 0.2827057543112682\n",
            "Iteration 9804, Cost (Validation): 0.3490174052851019\n",
            "Iteration 9805, Norm of Gradient: 0.004548793933019745, Cost (Train): 0.2827036852489744\n",
            "Iteration 9805, Cost (Validation): 0.34901846162137984\n",
            "Iteration 9806, Norm of Gradient: 0.004548396775629214, Cost (Train): 0.28270161654795967\n",
            "Iteration 9806, Cost (Validation): 0.3490195180628923\n",
            "Iteration 9807, Norm of Gradient: 0.004547999682779996, Cost (Train): 0.2826995482081338\n",
            "Iteration 9807, Cost (Validation): 0.34902057460958685\n",
            "Iteration 9808, Norm of Gradient: 0.00454760265445543, Cost (Train): 0.28269748022940655\n",
            "Iteration 9808, Cost (Validation): 0.3490216312614115\n",
            "Iteration 9809, Norm of Gradient: 0.0045472056906388555, Cost (Train): 0.2826954126116878\n",
            "Iteration 9809, Cost (Validation): 0.34902268801831376\n",
            "Iteration 9810, Norm of Gradient: 0.004546808791313621, Cost (Train): 0.28269334535488716\n",
            "Iteration 9810, Cost (Validation): 0.3490237448802416\n",
            "Iteration 9811, Norm of Gradient: 0.004546411956463077, Cost (Train): 0.28269127845891484\n",
            "Iteration 9811, Cost (Validation): 0.34902480184714285\n",
            "Iteration 9812, Norm of Gradient: 0.004546015186070587, Cost (Train): 0.2826892119236805\n",
            "Iteration 9812, Cost (Validation): 0.3490258589189653\n",
            "Iteration 9813, Norm of Gradient: 0.004545618480119512, Cost (Train): 0.28268714574909426\n",
            "Iteration 9813, Cost (Validation): 0.3490269160956569\n",
            "Iteration 9814, Norm of Gradient: 0.0045452218385932264, Cost (Train): 0.2826850799350659\n",
            "Iteration 9814, Cost (Validation): 0.3490279733771654\n",
            "Iteration 9815, Norm of Gradient: 0.004544825261475107, Cost (Train): 0.2826830144815056\n",
            "Iteration 9815, Cost (Validation): 0.34902903076343883\n",
            "Iteration 9816, Norm of Gradient: 0.004544428748748536, Cost (Train): 0.2826809493883232\n",
            "Iteration 9816, Cost (Validation): 0.3490300882544251\n",
            "Iteration 9817, Norm of Gradient: 0.0045440323003969035, Cost (Train): 0.28267888465542895\n",
            "Iteration 9817, Cost (Validation): 0.34903114585007217\n",
            "Iteration 9818, Norm of Gradient: 0.004543635916403603, Cost (Train): 0.28267682028273283\n",
            "Iteration 9818, Cost (Validation): 0.349032203550328\n",
            "Iteration 9819, Norm of Gradient: 0.004543239596752039, Cost (Train): 0.282674756270145\n",
            "Iteration 9819, Cost (Validation): 0.3490332613551405\n",
            "Iteration 9820, Norm of Gradient: 0.0045428433414256165, Cost (Train): 0.2826726926175755\n",
            "Iteration 9820, Cost (Validation): 0.34903431926445794\n",
            "Iteration 9821, Norm of Gradient: 0.0045424471504077486, Cost (Train): 0.2826706293249346\n",
            "Iteration 9821, Cost (Validation): 0.3490353772782281\n",
            "Iteration 9822, Norm of Gradient: 0.004542051023681853, Cost (Train): 0.28266856639213256\n",
            "Iteration 9822, Cost (Validation): 0.34903643539639917\n",
            "Iteration 9823, Norm of Gradient: 0.0045416549612313575, Cost (Train): 0.2826665038190796\n",
            "Iteration 9823, Cost (Validation): 0.34903749361891917\n",
            "Iteration 9824, Norm of Gradient: 0.004541258963039692, Cost (Train): 0.2826644416056859\n",
            "Iteration 9824, Cost (Validation): 0.34903855194573624\n",
            "Iteration 9825, Norm of Gradient: 0.004540863029090293, Cost (Train): 0.2826623797518618\n",
            "Iteration 9825, Cost (Validation): 0.34903961037679854\n",
            "Iteration 9826, Norm of Gradient: 0.004540467159366603, Cost (Train): 0.28266031825751775\n",
            "Iteration 9826, Cost (Validation): 0.3490406689120542\n",
            "Iteration 9827, Norm of Gradient: 0.004540071353852071, Cost (Train): 0.282658257122564\n",
            "Iteration 9827, Cost (Validation): 0.34904172755145135\n",
            "Iteration 9828, Norm of Gradient: 0.004539675612530152, Cost (Train): 0.28265619634691086\n",
            "Iteration 9828, Cost (Validation): 0.34904278629493823\n",
            "Iteration 9829, Norm of Gradient: 0.004539279935384308, Cost (Train): 0.28265413593046895\n",
            "Iteration 9829, Cost (Validation): 0.349043845142463\n",
            "Iteration 9830, Norm of Gradient: 0.004538884322398003, Cost (Train): 0.28265207587314856\n",
            "Iteration 9830, Cost (Validation): 0.34904490409397393\n",
            "Iteration 9831, Norm of Gradient: 0.004538488773554712, Cost (Train): 0.2826500161748603\n",
            "Iteration 9831, Cost (Validation): 0.34904596314941927\n",
            "Iteration 9832, Norm of Gradient: 0.00453809328883791, Cost (Train): 0.2826479568355146\n",
            "Iteration 9832, Cost (Validation): 0.3490470223087473\n",
            "Iteration 9833, Norm of Gradient: 0.004537697868231085, Cost (Train): 0.282645897855022\n",
            "Iteration 9833, Cost (Validation): 0.34904808157190625\n",
            "Iteration 9834, Norm of Gradient: 0.004537302511717726, Cost (Train): 0.28264383923329317\n",
            "Iteration 9834, Cost (Validation): 0.34904914093884454\n",
            "Iteration 9835, Norm of Gradient: 0.00453690721928133, Cost (Train): 0.2826417809702385\n",
            "Iteration 9835, Cost (Validation): 0.3490502004095104\n",
            "Iteration 9836, Norm of Gradient: 0.004536511990905398, Cost (Train): 0.2826397230657689\n",
            "Iteration 9836, Cost (Validation): 0.3490512599838523\n",
            "Iteration 9837, Norm of Gradient: 0.004536116826573437, Cost (Train): 0.2826376655197948\n",
            "Iteration 9837, Cost (Validation): 0.3490523196618186\n",
            "Iteration 9838, Norm of Gradient: 0.004535721726268965, Cost (Train): 0.2826356083322271\n",
            "Iteration 9838, Cost (Validation): 0.3490533794433576\n",
            "Iteration 9839, Norm of Gradient: 0.004535326689975497, Cost (Train): 0.2826335515029763\n",
            "Iteration 9839, Cost (Validation): 0.3490544393284178\n",
            "Iteration 9840, Norm of Gradient: 0.004534931717676562, Cost (Train): 0.28263149503195334\n",
            "Iteration 9840, Cost (Validation): 0.34905549931694774\n",
            "Iteration 9841, Norm of Gradient: 0.004534536809355691, Cost (Train): 0.282629438919069\n",
            "Iteration 9841, Cost (Validation): 0.3490565594088957\n",
            "Iteration 9842, Norm of Gradient: 0.004534141964996421, Cost (Train): 0.2826273831642339\n",
            "Iteration 9842, Cost (Validation): 0.3490576196042103\n",
            "Iteration 9843, Norm of Gradient: 0.004533747184582296, Cost (Train): 0.2826253277673591\n",
            "Iteration 9843, Cost (Validation): 0.34905867990283995\n",
            "Iteration 9844, Norm of Gradient: 0.004533352468096866, Cost (Train): 0.2826232727283554\n",
            "Iteration 9844, Cost (Validation): 0.34905974030473325\n",
            "Iteration 9845, Norm of Gradient: 0.004532957815523685, Cost (Train): 0.2826212180471337\n",
            "Iteration 9845, Cost (Validation): 0.34906080080983876\n",
            "Iteration 9846, Norm of Gradient: 0.004532563226846314, Cost (Train): 0.2826191637236049\n",
            "Iteration 9846, Cost (Validation): 0.34906186141810497\n",
            "Iteration 9847, Norm of Gradient: 0.004532168702048323, Cost (Train): 0.28261710975768\n",
            "Iteration 9847, Cost (Validation): 0.34906292212948054\n",
            "Iteration 9848, Norm of Gradient: 0.004531774241113281, Cost (Train): 0.28261505614927\n",
            "Iteration 9848, Cost (Validation): 0.3490639829439141\n",
            "Iteration 9849, Norm of Gradient: 0.0045313798440247705, Cost (Train): 0.2826130028982859\n",
            "Iteration 9849, Cost (Validation): 0.34906504386135423\n",
            "Iteration 9850, Norm of Gradient: 0.004530985510766374, Cost (Train): 0.2826109500046388\n",
            "Iteration 9850, Cost (Validation): 0.34906610488174966\n",
            "Iteration 9851, Norm of Gradient: 0.004530591241321682, Cost (Train): 0.28260889746823975\n",
            "Iteration 9851, Cost (Validation): 0.34906716600504895\n",
            "Iteration 9852, Norm of Gradient: 0.004530197035674291, Cost (Train): 0.28260684528899993\n",
            "Iteration 9852, Cost (Validation): 0.34906822723120084\n",
            "Iteration 9853, Norm of Gradient: 0.004529802893807806, Cost (Train): 0.28260479346683043\n",
            "Iteration 9853, Cost (Validation): 0.3490692885601542\n",
            "Iteration 9854, Norm of Gradient: 0.004529408815705831, Cost (Train): 0.2826027420016423\n",
            "Iteration 9854, Cost (Validation): 0.3490703499918575\n",
            "Iteration 9855, Norm of Gradient: 0.0045290148013519835, Cost (Train): 0.28260069089334694\n",
            "Iteration 9855, Cost (Validation): 0.34907141152625965\n",
            "Iteration 9856, Norm of Gradient: 0.004528620850729883, Cost (Train): 0.2825986401418555\n",
            "Iteration 9856, Cost (Validation): 0.34907247316330947\n",
            "Iteration 9857, Norm of Gradient: 0.004528226963823153, Cost (Train): 0.2825965897470793\n",
            "Iteration 9857, Cost (Validation): 0.34907353490295573\n",
            "Iteration 9858, Norm of Gradient: 0.004527833140615428, Cost (Train): 0.2825945397089295\n",
            "Iteration 9858, Cost (Validation): 0.34907459674514724\n",
            "Iteration 9859, Norm of Gradient: 0.004527439381090345, Cost (Train): 0.2825924900273176\n",
            "Iteration 9859, Cost (Validation): 0.3490756586898328\n",
            "Iteration 9860, Norm of Gradient: 0.004527045685231545, Cost (Train): 0.2825904407021549\n",
            "Iteration 9860, Cost (Validation): 0.34907672073696133\n",
            "Iteration 9861, Norm of Gradient: 0.00452665205302268, Cost (Train): 0.2825883917333527\n",
            "Iteration 9861, Cost (Validation): 0.34907778288648167\n",
            "Iteration 9862, Norm of Gradient: 0.004526258484447403, Cost (Train): 0.2825863431208225\n",
            "Iteration 9862, Cost (Validation): 0.34907884513834286\n",
            "Iteration 9863, Norm of Gradient: 0.004525864979489377, Cost (Train): 0.28258429486447567\n",
            "Iteration 9863, Cost (Validation): 0.3490799074924937\n",
            "Iteration 9864, Norm of Gradient: 0.004525471538132268, Cost (Train): 0.2825822469642237\n",
            "Iteration 9864, Cost (Validation): 0.3490809699488831\n",
            "Iteration 9865, Norm of Gradient: 0.004525078160359746, Cost (Train): 0.28258019941997814\n",
            "Iteration 9865, Cost (Validation): 0.34908203250746017\n",
            "Iteration 9866, Norm of Gradient: 0.004524684846155495, Cost (Train): 0.2825781522316505\n",
            "Iteration 9866, Cost (Validation): 0.3490830951681738\n",
            "Iteration 9867, Norm of Gradient: 0.004524291595503194, Cost (Train): 0.28257610539915234\n",
            "Iteration 9867, Cost (Validation): 0.3490841579309731\n",
            "Iteration 9868, Norm of Gradient: 0.004523898408386537, Cost (Train): 0.2825740589223953\n",
            "Iteration 9868, Cost (Validation): 0.3490852207958069\n",
            "Iteration 9869, Norm of Gradient: 0.004523505284789217, Cost (Train): 0.2825720128012909\n",
            "Iteration 9869, Cost (Validation): 0.3490862837626245\n",
            "Iteration 9870, Norm of Gradient: 0.004523112224694938, Cost (Train): 0.28256996703575094\n",
            "Iteration 9870, Cost (Validation): 0.3490873468313748\n",
            "Iteration 9871, Norm of Gradient: 0.004522719228087405, Cost (Train): 0.282567921625687\n",
            "Iteration 9871, Cost (Validation): 0.349088410002007\n",
            "Iteration 9872, Norm of Gradient: 0.004522326294950334, Cost (Train): 0.2825658765710106\n",
            "Iteration 9872, Cost (Validation): 0.34908947327447015\n",
            "Iteration 9873, Norm of Gradient: 0.004521933425267443, Cost (Train): 0.28256383187163386\n",
            "Iteration 9873, Cost (Validation): 0.3490905366487134\n",
            "Iteration 9874, Norm of Gradient: 0.004521540619022459, Cost (Train): 0.28256178752746836\n",
            "Iteration 9874, Cost (Validation): 0.349091600124686\n",
            "Iteration 9875, Norm of Gradient: 0.0045211478761991095, Cost (Train): 0.2825597435384259\n",
            "Iteration 9875, Cost (Validation): 0.34909266370233705\n",
            "Iteration 9876, Norm of Gradient: 0.0045207551967811335, Cost (Train): 0.28255769990441837\n",
            "Iteration 9876, Cost (Validation): 0.3490937273816157\n",
            "Iteration 9877, Norm of Gradient: 0.004520362580752273, Cost (Train): 0.28255565662535753\n",
            "Iteration 9877, Cost (Validation): 0.34909479116247116\n",
            "Iteration 9878, Norm of Gradient: 0.004519970028096275, Cost (Train): 0.28255361370115534\n",
            "Iteration 9878, Cost (Validation): 0.34909585504485285\n",
            "Iteration 9879, Norm of Gradient: 0.004519577538796897, Cost (Train): 0.28255157113172374\n",
            "Iteration 9879, Cost (Validation): 0.3490969190287098\n",
            "Iteration 9880, Norm of Gradient: 0.0045191851128378955, Cost (Train): 0.2825495289169746\n",
            "Iteration 9880, Cost (Validation): 0.3490979831139915\n",
            "Iteration 9881, Norm of Gradient: 0.0045187927502030375, Cost (Train): 0.2825474870568199\n",
            "Iteration 9881, Cost (Validation): 0.3490990473006471\n",
            "Iteration 9882, Norm of Gradient: 0.004518400450876095, Cost (Train): 0.2825454455511718\n",
            "Iteration 9882, Cost (Validation): 0.34910011158862597\n",
            "Iteration 9883, Norm of Gradient: 0.004518008214840844, Cost (Train): 0.28254340439994213\n",
            "Iteration 9883, Cost (Validation): 0.34910117597787754\n",
            "Iteration 9884, Norm of Gradient: 0.004517616042081068, Cost (Train): 0.2825413636030431\n",
            "Iteration 9884, Cost (Validation): 0.34910224046835103\n",
            "Iteration 9885, Norm of Gradient: 0.004517223932580557, Cost (Train): 0.28253932316038666\n",
            "Iteration 9885, Cost (Validation): 0.3491033050599959\n",
            "Iteration 9886, Norm of Gradient: 0.004516831886323105, Cost (Train): 0.2825372830718852\n",
            "Iteration 9886, Cost (Validation): 0.3491043697527616\n",
            "Iteration 9887, Norm of Gradient: 0.004516439903292511, Cost (Train): 0.28253524333745056\n",
            "Iteration 9887, Cost (Validation): 0.3491054345465975\n",
            "Iteration 9888, Norm of Gradient: 0.0045160479834725825, Cost (Train): 0.2825332039569951\n",
            "Iteration 9888, Cost (Validation): 0.34910649944145306\n",
            "Iteration 9889, Norm of Gradient: 0.004515656126847132, Cost (Train): 0.282531164930431\n",
            "Iteration 9889, Cost (Validation): 0.34910756443727775\n",
            "Iteration 9890, Norm of Gradient: 0.004515264333399977, Cost (Train): 0.2825291262576705\n",
            "Iteration 9890, Cost (Validation): 0.349108629534021\n",
            "Iteration 9891, Norm of Gradient: 0.00451487260311494, Cost (Train): 0.28252708793862596\n",
            "Iteration 9891, Cost (Validation): 0.3491096947316325\n",
            "Iteration 9892, Norm of Gradient: 0.004514480935975852, Cost (Train): 0.2825250499732096\n",
            "Iteration 9892, Cost (Validation): 0.34911076003006153\n",
            "Iteration 9893, Norm of Gradient: 0.004514089331966546, Cost (Train): 0.2825230123613337\n",
            "Iteration 9893, Cost (Validation): 0.3491118254292579\n",
            "Iteration 9894, Norm of Gradient: 0.004513697791070864, Cost (Train): 0.2825209751029107\n",
            "Iteration 9894, Cost (Validation): 0.34911289092917097\n",
            "Iteration 9895, Norm of Gradient: 0.0045133063132726525, Cost (Train): 0.282518938197853\n",
            "Iteration 9895, Cost (Validation): 0.3491139565297504\n",
            "Iteration 9896, Norm of Gradient: 0.004512914898555764, Cost (Train): 0.28251690164607296\n",
            "Iteration 9896, Cost (Validation): 0.34911502223094587\n",
            "Iteration 9897, Norm of Gradient: 0.004512523546904055, Cost (Train): 0.2825148654474831\n",
            "Iteration 9897, Cost (Validation): 0.34911608803270694\n",
            "Iteration 9898, Norm of Gradient: 0.004512132258301394, Cost (Train): 0.2825128296019958\n",
            "Iteration 9898, Cost (Validation): 0.3491171539349833\n",
            "Iteration 9899, Norm of Gradient: 0.004511741032731645, Cost (Train): 0.2825107941095237\n",
            "Iteration 9899, Cost (Validation): 0.34911821993772457\n",
            "Iteration 9900, Norm of Gradient: 0.004511349870178685, Cost (Train): 0.28250875896997923\n",
            "Iteration 9900, Cost (Validation): 0.34911928604088055\n",
            "Iteration 9901, Norm of Gradient: 0.004510958770626398, Cost (Train): 0.28250672418327494\n",
            "Iteration 9901, Cost (Validation): 0.34912035224440086\n",
            "Iteration 9902, Norm of Gradient: 0.004510567734058666, Cost (Train): 0.2825046897493236\n",
            "Iteration 9902, Cost (Validation): 0.3491214185482353\n",
            "Iteration 9903, Norm of Gradient: 0.004510176760459386, Cost (Train): 0.28250265566803756\n",
            "Iteration 9903, Cost (Validation): 0.3491224849523336\n",
            "Iteration 9904, Norm of Gradient: 0.004509785849812454, Cost (Train): 0.28250062193932973\n",
            "Iteration 9904, Cost (Validation): 0.34912355145664553\n",
            "Iteration 9905, Norm of Gradient: 0.004509395002101774, Cost (Train): 0.28249858856311266\n",
            "Iteration 9905, Cost (Validation): 0.3491246180611209\n",
            "Iteration 9906, Norm of Gradient: 0.004509004217311257, Cost (Train): 0.2824965555392991\n",
            "Iteration 9906, Cost (Validation): 0.3491256847657095\n",
            "Iteration 9907, Norm of Gradient: 0.0045086134954248165, Cost (Train): 0.28249452286780186\n",
            "Iteration 9907, Cost (Validation): 0.3491267515703612\n",
            "Iteration 9908, Norm of Gradient: 0.004508222836426375, Cost (Train): 0.28249249054853354\n",
            "Iteration 9908, Cost (Validation): 0.34912781847502583\n",
            "Iteration 9909, Norm of Gradient: 0.0045078322402998606, Cost (Train): 0.28249045858140714\n",
            "Iteration 9909, Cost (Validation): 0.3491288854796534\n",
            "Iteration 9910, Norm of Gradient: 0.004507441707029204, Cost (Train): 0.2824884269663353\n",
            "Iteration 9910, Cost (Validation): 0.3491299525841935\n",
            "Iteration 9911, Norm of Gradient: 0.004507051236598345, Cost (Train): 0.282486395703231\n",
            "Iteration 9911, Cost (Validation): 0.34913101978859645\n",
            "Iteration 9912, Norm of Gradient: 0.004506660828991227, Cost (Train): 0.2824843647920071\n",
            "Iteration 9912, Cost (Validation): 0.34913208709281196\n",
            "Iteration 9913, Norm of Gradient: 0.0045062704841918, Cost (Train): 0.28248233423257657\n",
            "Iteration 9913, Cost (Validation): 0.34913315449678994\n",
            "Iteration 9914, Norm of Gradient: 0.004505880202184018, Cost (Train): 0.28248030402485225\n",
            "Iteration 9914, Cost (Validation): 0.34913422200048055\n",
            "Iteration 9915, Norm of Gradient: 0.004505489982951844, Cost (Train): 0.28247827416874727\n",
            "Iteration 9915, Cost (Validation): 0.3491352896038336\n",
            "Iteration 9916, Norm of Gradient: 0.004505099826479245, Cost (Train): 0.2824762446641744\n",
            "Iteration 9916, Cost (Validation): 0.3491363573067993\n",
            "Iteration 9917, Norm of Gradient: 0.004504709732750194, Cost (Train): 0.2824742155110469\n",
            "Iteration 9917, Cost (Validation): 0.3491374251093276\n",
            "Iteration 9918, Norm of Gradient: 0.004504319701748667, Cost (Train): 0.28247218670927765\n",
            "Iteration 9918, Cost (Validation): 0.3491384930113685\n",
            "Iteration 9919, Norm of Gradient: 0.00450392973345865, Cost (Train): 0.2824701582587799\n",
            "Iteration 9919, Cost (Validation): 0.3491395610128723\n",
            "Iteration 9920, Norm of Gradient: 0.004503539827864132, Cost (Train): 0.28246813015946676\n",
            "Iteration 9920, Cost (Validation): 0.34914062911378885\n",
            "Iteration 9921, Norm of Gradient: 0.00450314998494911, Cost (Train): 0.2824661024112513\n",
            "Iteration 9921, Cost (Validation): 0.34914169731406847\n",
            "Iteration 9922, Norm of Gradient: 0.004502760204697582, Cost (Train): 0.28246407501404674\n",
            "Iteration 9922, Cost (Validation): 0.3491427656136612\n",
            "Iteration 9923, Norm of Gradient: 0.004502370487093558, Cost (Train): 0.2824620479677663\n",
            "Iteration 9923, Cost (Validation): 0.34914383401251714\n",
            "Iteration 9924, Norm of Gradient: 0.0045019808321210475, Cost (Train): 0.2824600212723231\n",
            "Iteration 9924, Cost (Validation): 0.34914490251058666\n",
            "Iteration 9925, Norm of Gradient: 0.0045015912397640715, Cost (Train): 0.2824579949276306\n",
            "Iteration 9925, Cost (Validation): 0.34914597110781986\n",
            "Iteration 9926, Norm of Gradient: 0.004501201710006651, Cost (Train): 0.282455968933602\n",
            "Iteration 9926, Cost (Validation): 0.349147039804167\n",
            "Iteration 9927, Norm of Gradient: 0.004500812242832819, Cost (Train): 0.28245394329015056\n",
            "Iteration 9927, Cost (Validation): 0.3491481085995783\n",
            "Iteration 9928, Norm of Gradient: 0.004500422838226607, Cost (Train): 0.28245191799718977\n",
            "Iteration 9928, Cost (Validation): 0.349149177494004\n",
            "Iteration 9929, Norm of Gradient: 0.004500033496172059, Cost (Train): 0.2824498930546329\n",
            "Iteration 9929, Cost (Validation): 0.34915024648739446\n",
            "Iteration 9930, Norm of Gradient: 0.0044996442166532185, Cost (Train): 0.2824478684623935\n",
            "Iteration 9930, Cost (Validation): 0.34915131557969986\n",
            "Iteration 9931, Norm of Gradient: 0.00449925499965414, Cost (Train): 0.2824458442203848\n",
            "Iteration 9931, Cost (Validation): 0.3491523847708707\n",
            "Iteration 9932, Norm of Gradient: 0.00449886584515888, Cost (Train): 0.2824438203285205\n",
            "Iteration 9932, Cost (Validation): 0.34915345406085724\n",
            "Iteration 9933, Norm of Gradient: 0.004498476753151504, Cost (Train): 0.2824417967867139\n",
            "Iteration 9933, Cost (Validation): 0.3491545234496098\n",
            "Iteration 9934, Norm of Gradient: 0.004498087723616079, Cost (Train): 0.28243977359487865\n",
            "Iteration 9934, Cost (Validation): 0.34915559293707893\n",
            "Iteration 9935, Norm of Gradient: 0.004497698756536681, Cost (Train): 0.28243775075292826\n",
            "Iteration 9935, Cost (Validation): 0.3491566625232148\n",
            "Iteration 9936, Norm of Gradient: 0.004497309851897389, Cost (Train): 0.28243572826077634\n",
            "Iteration 9936, Cost (Validation): 0.3491577322079681\n",
            "Iteration 9937, Norm of Gradient: 0.004496921009682291, Cost (Train): 0.2824337061183365\n",
            "Iteration 9937, Cost (Validation): 0.34915880199128907\n",
            "Iteration 9938, Norm of Gradient: 0.004496532229875477, Cost (Train): 0.2824316843255224\n",
            "Iteration 9938, Cost (Validation): 0.3491598718731283\n",
            "Iteration 9939, Norm of Gradient: 0.004496143512461047, Cost (Train): 0.28242966288224763\n",
            "Iteration 9939, Cost (Validation): 0.3491609418534362\n",
            "Iteration 9940, Norm of Gradient: 0.004495754857423103, Cost (Train): 0.282427641788426\n",
            "Iteration 9940, Cost (Validation): 0.3491620119321634\n",
            "Iteration 9941, Norm of Gradient: 0.004495366264745751, Cost (Train): 0.2824256210439711\n",
            "Iteration 9941, Cost (Validation): 0.3491630821092603\n",
            "Iteration 9942, Norm of Gradient: 0.004494977734413108, Cost (Train): 0.28242360064879685\n",
            "Iteration 9942, Cost (Validation): 0.34916415238467746\n",
            "Iteration 9943, Norm of Gradient: 0.004494589266409294, Cost (Train): 0.28242158060281697\n",
            "Iteration 9943, Cost (Validation): 0.34916522275836553\n",
            "Iteration 9944, Norm of Gradient: 0.004494200860718435, Cost (Train): 0.2824195609059453\n",
            "Iteration 9944, Cost (Validation): 0.3491662932302751\n",
            "Iteration 9945, Norm of Gradient: 0.004493812517324659, Cost (Train): 0.2824175415580955\n",
            "Iteration 9945, Cost (Validation): 0.34916736380035673\n",
            "Iteration 9946, Norm of Gradient: 0.0044934242362121065, Cost (Train): 0.2824155225591818\n",
            "Iteration 9946, Cost (Validation): 0.349168434468561\n",
            "Iteration 9947, Norm of Gradient: 0.004493036017364918, Cost (Train): 0.2824135039091178\n",
            "Iteration 9947, Cost (Validation): 0.3491695052348387\n",
            "Iteration 9948, Norm of Gradient: 0.004492647860767244, Cost (Train): 0.2824114856078176\n",
            "Iteration 9948, Cost (Validation): 0.34917057609914043\n",
            "Iteration 9949, Norm of Gradient: 0.0044922597664032345, Cost (Train): 0.28240946765519503\n",
            "Iteration 9949, Cost (Validation): 0.34917164706141685\n",
            "Iteration 9950, Norm of Gradient: 0.004491871734257052, Cost (Train): 0.2824074500511641\n",
            "Iteration 9950, Cost (Validation): 0.3491727181216187\n",
            "Iteration 9951, Norm of Gradient: 0.004491483764312859, Cost (Train): 0.282405432795639\n",
            "Iteration 9951, Cost (Validation): 0.3491737892796967\n",
            "Iteration 9952, Norm of Gradient: 0.004491095856554828, Cost (Train): 0.2824034158885335\n",
            "Iteration 9952, Cost (Validation): 0.34917486053560165\n",
            "Iteration 9953, Norm of Gradient: 0.004490708010967135, Cost (Train): 0.282401399329762\n",
            "Iteration 9953, Cost (Validation): 0.3491759318892843\n",
            "Iteration 9954, Norm of Gradient: 0.00449032022753396, Cost (Train): 0.2823993831192383\n",
            "Iteration 9954, Cost (Validation): 0.34917700334069535\n",
            "Iteration 9955, Norm of Gradient: 0.0044899325062394925, Cost (Train): 0.28239736725687664\n",
            "Iteration 9955, Cost (Validation): 0.34917807488978575\n",
            "Iteration 9956, Norm of Gradient: 0.004489544847067925, Cost (Train): 0.2823953517425913\n",
            "Iteration 9956, Cost (Validation): 0.3491791465365063\n",
            "Iteration 9957, Norm of Gradient: 0.004489157250003455, Cost (Train): 0.2823933365762963\n",
            "Iteration 9957, Cost (Validation): 0.34918021828080775\n",
            "Iteration 9958, Norm of Gradient: 0.0044887697150302885, Cost (Train): 0.28239132175790593\n",
            "Iteration 9958, Cost (Validation): 0.3491812901226411\n",
            "Iteration 9959, Norm of Gradient: 0.004488382242132633, Cost (Train): 0.2823893072873344\n",
            "Iteration 9959, Cost (Validation): 0.3491823620619571\n",
            "Iteration 9960, Norm of Gradient: 0.004487994831294705, Cost (Train): 0.28238729316449607\n",
            "Iteration 9960, Cost (Validation): 0.3491834340987068\n",
            "Iteration 9961, Norm of Gradient: 0.004487607482500726, Cost (Train): 0.2823852793893052\n",
            "Iteration 9961, Cost (Validation): 0.34918450623284114\n",
            "Iteration 9962, Norm of Gradient: 0.004487220195734922, Cost (Train): 0.2823832659616761\n",
            "Iteration 9962, Cost (Validation): 0.349185578464311\n",
            "Iteration 9963, Norm of Gradient: 0.004486832970981527, Cost (Train): 0.2823812528815232\n",
            "Iteration 9963, Cost (Validation): 0.3491866507930672\n",
            "Iteration 9964, Norm of Gradient: 0.004486445808224773, Cost (Train): 0.2823792401487607\n",
            "Iteration 9964, Cost (Validation): 0.349187723219061\n",
            "Iteration 9965, Norm of Gradient: 0.004486058707448909, Cost (Train): 0.28237722776330326\n",
            "Iteration 9965, Cost (Validation): 0.3491887957422433\n",
            "Iteration 9966, Norm of Gradient: 0.004485671668638183, Cost (Train): 0.2823752157250652\n",
            "Iteration 9966, Cost (Validation): 0.34918986836256516\n",
            "Iteration 9967, Norm of Gradient: 0.004485284691776848, Cost (Train): 0.282373204033961\n",
            "Iteration 9967, Cost (Validation): 0.3491909410799776\n",
            "Iteration 9968, Norm of Gradient: 0.004484897776849163, Cost (Train): 0.2823711926899053\n",
            "Iteration 9968, Cost (Validation): 0.3491920138944317\n",
            "Iteration 9969, Norm of Gradient: 0.004484510923839395, Cost (Train): 0.2823691816928123\n",
            "Iteration 9969, Cost (Validation): 0.34919308680587857\n",
            "Iteration 9970, Norm of Gradient: 0.004484124132731817, Cost (Train): 0.2823671710425969\n",
            "Iteration 9970, Cost (Validation): 0.3491941598142692\n",
            "Iteration 9971, Norm of Gradient: 0.004483737403510702, Cost (Train): 0.28236516073917356\n",
            "Iteration 9971, Cost (Validation): 0.34919523291955495\n",
            "Iteration 9972, Norm of Gradient: 0.004483350736160335, Cost (Train): 0.2823631507824569\n",
            "Iteration 9972, Cost (Validation): 0.3491963061216868\n",
            "Iteration 9973, Norm of Gradient: 0.004482964130665002, Cost (Train): 0.2823611411723615\n",
            "Iteration 9973, Cost (Validation): 0.3491973794206159\n",
            "Iteration 9974, Norm of Gradient: 0.0044825775870089984, Cost (Train): 0.28235913190880213\n",
            "Iteration 9974, Cost (Validation): 0.3491984528162935\n",
            "Iteration 9975, Norm of Gradient: 0.004482191105176622, Cost (Train): 0.2823571229916935\n",
            "Iteration 9975, Cost (Validation): 0.3491995263086709\n",
            "Iteration 9976, Norm of Gradient: 0.004481804685152175, Cost (Train): 0.2823551144209503\n",
            "Iteration 9976, Cost (Validation): 0.3492005998976992\n",
            "Iteration 9977, Norm of Gradient: 0.004481418326919972, Cost (Train): 0.28235310619648724\n",
            "Iteration 9977, Cost (Validation): 0.34920167358332976\n",
            "Iteration 9978, Norm of Gradient: 0.004481032030464325, Cost (Train): 0.28235109831821914\n",
            "Iteration 9978, Cost (Validation): 0.3492027473655137\n",
            "Iteration 9979, Norm of Gradient: 0.004480645795769556, Cost (Train): 0.2823490907860609\n",
            "Iteration 9979, Cost (Validation): 0.3492038212442024\n",
            "Iteration 9980, Norm of Gradient: 0.004480259622819993, Cost (Train): 0.2823470835999274\n",
            "Iteration 9980, Cost (Validation): 0.34920489521934717\n",
            "Iteration 9981, Norm of Gradient: 0.004479873511599968, Cost (Train): 0.28234507675973325\n",
            "Iteration 9981, Cost (Validation): 0.34920596929089937\n",
            "Iteration 9982, Norm of Gradient: 0.004479487462093815, Cost (Train): 0.28234307026539357\n",
            "Iteration 9982, Cost (Validation): 0.34920704345881026\n",
            "Iteration 9983, Norm of Gradient: 0.0044791014742858815, Cost (Train): 0.28234106411682325\n",
            "Iteration 9983, Cost (Validation): 0.34920811772303134\n",
            "Iteration 9984, Norm of Gradient: 0.0044787155481605155, Cost (Train): 0.28233905831393724\n",
            "Iteration 9984, Cost (Validation): 0.3492091920835138\n",
            "Iteration 9985, Norm of Gradient: 0.00447832968370207, Cost (Train): 0.28233705285665056\n",
            "Iteration 9985, Cost (Validation): 0.34921026654020926\n",
            "Iteration 9986, Norm of Gradient: 0.0044779438808949055, Cost (Train): 0.2823350477448781\n",
            "Iteration 9986, Cost (Validation): 0.34921134109306906\n",
            "Iteration 9987, Norm of Gradient: 0.004477558139723388, Cost (Train): 0.282333042978535\n",
            "Iteration 9987, Cost (Validation): 0.3492124157420446\n",
            "Iteration 9988, Norm of Gradient: 0.0044771724601718865, Cost (Train): 0.28233103855753633\n",
            "Iteration 9988, Cost (Validation): 0.34921349048708733\n",
            "Iteration 9989, Norm of Gradient: 0.004476786842224779, Cost (Train): 0.28232903448179725\n",
            "Iteration 9989, Cost (Validation): 0.3492145653281489\n",
            "Iteration 9990, Norm of Gradient: 0.004476401285866447, Cost (Train): 0.2823270307512327\n",
            "Iteration 9990, Cost (Validation): 0.34921564026518065\n",
            "Iteration 9991, Norm of Gradient: 0.0044760157910812785, Cost (Train): 0.28232502736575804\n",
            "Iteration 9991, Cost (Validation): 0.3492167152981342\n",
            "Iteration 9992, Norm of Gradient: 0.0044756303578536655, Cost (Train): 0.28232302432528833\n",
            "Iteration 9992, Cost (Validation): 0.34921779042696105\n",
            "Iteration 9993, Norm of Gradient: 0.004475244986168007, Cost (Train): 0.2823210216297388\n",
            "Iteration 9993, Cost (Validation): 0.3492188656516128\n",
            "Iteration 9994, Norm of Gradient: 0.004474859676008707, Cost (Train): 0.2823190192790248\n",
            "Iteration 9994, Cost (Validation): 0.34921994097204107\n",
            "Iteration 9995, Norm of Gradient: 0.004474474427360172, Cost (Train): 0.28231701727306147\n",
            "Iteration 9995, Cost (Validation): 0.3492210163881973\n",
            "Iteration 9996, Norm of Gradient: 0.004474089240206823, Cost (Train): 0.2823150156117641\n",
            "Iteration 9996, Cost (Validation): 0.34922209190003334\n",
            "Iteration 9997, Norm of Gradient: 0.004473704114533075, Cost (Train): 0.28231301429504807\n",
            "Iteration 9997, Cost (Validation): 0.3492231675075007\n",
            "Iteration 9998, Norm of Gradient: 0.004473319050323358, Cost (Train): 0.2823110133228287\n",
            "Iteration 9998, Cost (Validation): 0.349224243210551\n",
            "Iteration 9999, Norm of Gradient: 0.004472934047562099, Cost (Train): 0.2823090126950214\n",
            "Iteration 9999, Cost (Validation): 0.34922531900913606\n",
            "Terminated after 10000 iterations, with norm of the gradient equal to 0.004472934047562099\n",
            "The weight found: [-0.01216366 -0.07832575  0.06205988 ... -0.5996115  -0.09386303\n",
            " -0.0296011 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convergence of Logistic Regression\n",
        "\n",
        "Here we see a plot showing the cost minimization by gradient descent for our training and validation sets.  After roughly 6000 iterations it is clear that some slight overfitting of the model ocurs as the error for the vaildation set begins to increase."
      ],
      "metadata": {
        "id": "9Kn1PK0qloWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot convergence of training and validation data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(logistic_regression.cost_history_train)), logistic_regression.cost_history_train, label='Training', color='blue')\n",
        "if X_val_split is not None and y_val_split is not None:\n",
        "    plt.plot(range(len(logistic_regression.cost_history_val)), logistic_regression.cost_history_val, label='Validation', color='red')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Convergence of Logistic Regression')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "S6ERY5RAnm0y",
        "outputId": "1049ae06-127f-4e61-89ca-928297d5f811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 10 +/- Features from Logistic Regression\n",
        "\n",
        "Here we see the top 10 features contributing to both positive and negative sentiment from our Logistic Regression.  We see a much more logical correlation between words associated with a positive/negative distinction than was seen with the simple linear regression implementation."
      ],
      "metadata": {
        "id": "srDTqBjKormK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to sort top features by coefficients for logistic regression\n",
        "def sort_top_features_by_coefficients_logistic(feature_names, coefficients, D):\n",
        "    # Get indices of top D features based on coefficients\n",
        "    sorted_indices = np.argsort(coefficients)\n",
        "\n",
        "    # Extract top D features and their corresponding coefficients\n",
        "    top_features_with_coefficients = [(feature_names[i], coefficients[i]) for i in sorted_indices]\n",
        "\n",
        "    # Separate positive and negative coefficients\n",
        "    positive_features = [(feature, coef) for feature, coef in top_features_with_coefficients[-D:] if coef > 0]\n",
        "    negative_features = [(feature, coef) for feature, coef in top_features_with_coefficients[:D] if coef < 0]\n",
        "\n",
        "    # Sort the top positive and negative features by coefficient values\n",
        "    sorted_top_positive_features = sorted(positive_features, key=lambda x: x[1], reverse=True)\n",
        "    sorted_top_negative_features = sorted(negative_features, key=lambda x: x[1])\n",
        "\n",
        "    # Convert to lists for plotting\n",
        "    top_positive_features, top_positive_coefficients = zip(*sorted_top_positive_features)\n",
        "    top_negative_features, top_negative_coefficients = zip(*sorted_top_negative_features)\n",
        "\n",
        "    return top_positive_features, top_positive_coefficients, top_negative_features, top_negative_coefficients\n",
        "\n",
        "# Choose D = 10 for top positive and negative features\n",
        "D = 10\n",
        "\n",
        "# Get coefficients from logistic regression model\n",
        "coefficients_logistic = logistic_regression.w[:-1]  # Exclude bias term\n",
        "\n",
        "# Call the function to sort the top D features by coefficients for logistic regression\n",
        "top_positive_features, top_positive_coefficients, top_negative_features, top_negative_coefficients = sort_top_features_by_coefficients_logistic(filtered_words, coefficients_logistic, D)\n",
        "\n",
        "# Plot the top positive and negative features on the same plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot positive features\n",
        "plt.barh(top_positive_features[-10:], top_positive_coefficients[-10:], color='skyblue', label='Positive')\n",
        "\n",
        "# Plot negative features\n",
        "plt.barh(top_negative_features[:10], top_negative_coefficients[:10], color='salmon', label='Negative')\n",
        "\n",
        "plt.xlabel('Coefficient')\n",
        "plt.ylabel('Feature')\n",
        "plt.title(\"Top 10 Features Contributing to Sentiment (Logistic Regression)\")\n",
        "plt.legend()\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the highest coefficient on top\n",
        "plt.show()\n",
        "\n",
        "print(top_positive_features)\n",
        "print(top_positive_coefficients)\n",
        "\n",
        "print(top_negative_features)\n",
        "print(top_negative_coefficients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "gjSnqdZfmNwA",
        "outputId": "ce19f877-3c51-4942-aed3-e12450a7a21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('excellent', 'favorite', 'wonderfully', '7', 'superb', 'perfect', 'amazing', 'highly', 'rare', 'loved')\n",
            "(1.0271664477901652, 0.9616220483467965, 0.9125637572881781, 0.8733444879391511, 0.8682849728703693, 0.845304863947304, 0.7902154380366126, 0.7760788093888727, 0.7455531769730436, 0.7392080478277737)\n",
            "('worst', 'waste', 'poorly', 'awful', 'dull', 'awful.', 'fails', 'boring', 'lame', 'badly')\n",
            "(-1.921883624575095, -1.740160633924712, -1.3568880474743863, -1.1789621577242535, -1.109172507766721, -1.0934833178474992, -1.0664330824307509, -1.0190557483285276, -0.9763050921542816, -0.9652634891564819)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1: Small Pertubation Test\n",
        "\n",
        "Here we check our gradient descent with a small pertubation test.  We can see the results of the relative error are in line with what we would expect (a very small number, in this case 4.752217356242678e-14) where the norm of gradienct descent at the final iteration was 4.159e-03."
      ],
      "metadata": {
        "id": "1fpJl8NJVOq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize small perturbation test\n",
        "w = np.random.randn(1)\n",
        "w0 = w\n",
        "epsilon = np.random.randn(1)[0] * 1e-5\n",
        "w1 = w0 + epsilon\n",
        "w2 = w0 - epsilon\n",
        "\n",
        "# Compute activations\n",
        "a1 = w1 * X_train\n",
        "a2 = w2 * X_train\n",
        "\n",
        "# Reshape y_train to match the shape of a1\n",
        "y_train_reshaped = np.tile(y_train[:, np.newaxis], (1, X_train.shape[1]))\n",
        "\n",
        "# Compute cross-entropy loss\n",
        "ce1 = np.sum(y_train_reshaped * np.log1p(np.exp(-a1)) + (1 - y_train_reshaped) * np.log1p(np.exp(a1)))\n",
        "ce2 = np.sum(y_train_reshaped * np.log1p(np.exp(-a2)) + (1 - y_train_reshaped) * np.log1p(np.exp(a2)))\n",
        "\n",
        "# Approximated gradient\n",
        "dw_num = (ce1 - ce2) / (2 * epsilon)\n",
        "\n",
        "# Compute predictions\n",
        "yh = 1 / (1 + np.exp(-X_train * w))\n",
        "dw_cal = np.sum((yh - y_train_reshaped) * X_train)  # Analytical gradient\n",
        "\n",
        "# Print results\n",
        "print(\"Analytical Gradient:\", dw_cal)\n",
        "print(\"Approximated Gradient:\", dw_num)\n",
        "print(\"Relative Error:\", ((dw_cal - dw_num) ** 2) / ((dw_cal + dw_num) ** 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epFUqrAaGVEM",
        "outputId": "27d95e7f-cfc5-46ea-da81-86bd74e5c00e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analytical Gradient: 635274.3971889779\n",
            "Approximated Gradient: 635274.2807459347\n",
            "Relative Error: 8.39932676101525e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7HRHR-3ZZ_O"
      },
      "source": [
        "# Task 3\n",
        "\n",
        "## Method comparison using ROC curve\n",
        "\n",
        "Below we plot the area under receiver operator characteristics curve (AUROC) for our Logistic Regression model and compare it to three models from the sklearn toolkit: KNN, Decision Tree, and Logistic Regression.  With no adjustment to the sklearn hyperparameters, our model is notably the highest achiever, significantly outperforming the sklearn KNN and Decision Tree models, and marginally outperforming the sklearn Logistic Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WSM2KDdZZ_O",
        "outputId": "a1e3a1f7-5978-4097-9151-0a957cfab794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7e91a8438850>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression as sk_LogisticRegression\n",
        "\n",
        "models = [KNeighborsClassifier(),\n",
        "          DecisionTreeClassifier(),\n",
        "          sk_LogisticRegression()]\n",
        "\n",
        "perf = {}\n",
        "\n",
        "# Our Logistic Regression Implementation\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "auroc = roc_auc_score(y_test, y_pred)\n",
        "perf[\"Our LogisticRegression\"] = {'fpr': fpr, 'tpr': tpr, 'auroc': auroc}\n",
        "\n",
        "# SKLearn Models\n",
        "for model in models:\n",
        "    fit = model.fit(X_train, y_train)\n",
        "    y_test_prob = fit.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
        "    auroc = roc_auc_score(y_test, y_test_prob)\n",
        "    perf[type(model).__name__] = {'fpr': fpr, 'tpr': tpr, 'auroc': auroc}\n",
        "\n",
        "plt.clf()\n",
        "i = 0\n",
        "for model_name, model_perf in perf.items():\n",
        "    plt.plot(model_perf['fpr'], model_perf['tpr'], label=model_name)\n",
        "    plt.text(0.4, i + 0.1, model_name + ': AUC = ' + str(round(model_perf['auroc'], 2)))\n",
        "    i += 0.1\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title('ROC in predicting IMDB Sentiment')\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"upper left\")\n",
        "# plt.show()\n",
        "# plt.savefig(\"roc_curve.png\", bbox_inches='tight', dpi=300)\n",
        "# plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.5: Comparing accuracy of model as a function of dataset training size.  \n",
        "\n",
        "Here we compare area under the receiver operator characteristic curve (AUROC) between our Logistic Regression model and the sklearn Decision Tree.  We test with random selections of 20%, 40%, 60%, 80%, and 100% of the original training set size to see how this affects results.  The maximum iterations for Logistic Regression is set to 1000 for practical computation purposes.  We can see a slight but notable increase in accuracy as we increase or training data size, which is congruent with intuition that more training data allows a model to make more accurate predictions.\n"
      ],
      "metadata": {
        "id": "MbtT9opKCjdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Define different training set sizes\n",
        "training_sizes = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "\n",
        "# Initialize logistic regression model\n",
        "logistic_regression = LogisticRegression(add_bias=True, learning_rate=0.1, epsilon=1e-4, max_iters=1000, verbose=True)\n",
        "\n",
        "# Initialize decision tree classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Lists to store AUROC values for each training size subset\n",
        "auroc_logistic_subset = []\n",
        "auroc_dt_subset = []\n",
        "\n",
        "# Lists to store ROC curve data for plotting\n",
        "roc_data_logistic = []\n",
        "roc_data_dt = []\n",
        "\n",
        "# Loop through different training set sizes\n",
        "for size in training_sizes:\n",
        "    # Calculate the number of samples for the current size\n",
        "    num_samples = int(len(X_train) * size)\n",
        "\n",
        "    # Randomly select a portion of X_train with the current size\n",
        "    random_indices = np.random.choice(len(X_train), num_samples, replace=False)\n",
        "    X_train_subset = X_train[random_indices]\n",
        "    y_train_subset = y_train[random_indices]\n",
        "\n",
        "    # Fit the logistic regression model on the subset of training data\n",
        "    logistic_regression.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Fit the decision tree model on the same subset of training data\n",
        "    dt_classifier.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Make predictions on the fixed test data for logistic regression\n",
        "    y_pred_logistic = logistic_regression.predict(X_test)\n",
        "\n",
        "    # Make predictions on the fixed test data for decision tree\n",
        "    y_pred_dt = dt_classifier.predict_proba(X_test)\n",
        "\n",
        "    # Compute ROC curve for logistic regression\n",
        "    fpr_logistic, tpr_logistic, _ = roc_curve(y_test, y_pred_logistic)\n",
        "\n",
        "    # Compute ROC curve for decision tree\n",
        "    fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_dt[:, 1])\n",
        "\n",
        "    # Compute AUROC for logistic regression and decision tree using ROC curves\n",
        "    auroc_logistic_subset.append(roc_auc_score(y_test, y_pred_logistic))\n",
        "    auroc_dt_subset.append(roc_auc_score(y_test, y_pred_dt[:, 1]))\n",
        "\n",
        "    # Store ROC curve data for plotting\n",
        "    roc_data_logistic.append((fpr_logistic, tpr_logistic))\n",
        "    roc_data_dt.append((fpr_dt, tpr_dt))\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
        "for i, size in enumerate(training_sizes):\n",
        "    plt.plot(roc_data_logistic[i][0], roc_data_logistic[i][1], label=f'Logistic Regression ({size*100:.0f}% of data)')\n",
        "    plt.plot(roc_data_dt[i][0], roc_data_dt[i][1], label=f'Decision Tree ({size*100:.0f}% of data)')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print AUROC values\n",
        "print(\"AUROC for Logistic Regression:\")\n",
        "for i, size in enumerate(training_sizes):\n",
        "    print(f\"{size*100:.0f}% of data: {auroc_logistic_subset[i]}\")\n",
        "\n",
        "print(\"\\nAUROC for Decision Tree:\")\n",
        "for i, size in enumerate(training_sizes):\n",
        "    print(f\"{size*100:.0f}% of data: {auroc_dt_subset[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R8EHZ5lWyP2p",
        "outputId": "47076384-10a4-4035-ec5c-d3f4a408ecdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 23, Norm of Gradient: 0.15043710722700357, Cost (Train): 0.6276166973492121\n",
            "Iteration 24, Norm of Gradient: 0.1492928598637097, Cost (Train): 0.6253962980254026\n",
            "Iteration 25, Norm of Gradient: 0.14816407489481762, Cost (Train): 0.6232092981868472\n",
            "Iteration 26, Norm of Gradient: 0.14705056543231812, Cost (Train): 0.6210549977232844\n",
            "Iteration 27, Norm of Gradient: 0.14595214204818874, Cost (Train): 0.6189327121993405\n",
            "Iteration 28, Norm of Gradient: 0.144868613166594, Cost (Train): 0.6168417726106492\n",
            "Iteration 29, Norm of Gradient: 0.14379978544670818, Cost (Train): 0.6147815251325824\n",
            "Iteration 30, Norm of Gradient: 0.1427454641493967, Cost (Train): 0.612751330862573\n",
            "Iteration 31, Norm of Gradient: 0.14170545348440927, Cost (Train): 0.6107505655570429\n",
            "Iteration 32, Norm of Gradient: 0.14067955693665612, Cost (Train): 0.6087786193639264\n",
            "Iteration 33, Norm of Gradient: 0.13966757757120898, Cost (Train): 0.606834896551736\n",
            "Iteration 34, Norm of Gradient: 0.13866931831726798, Cost (Train): 0.6049188152360528\n",
            "Iteration 35, Norm of Gradient: 0.13768458223166913, Cost (Train): 0.603029807104269\n",
            "Iteration 36, Norm of Gradient: 0.1367131727426844, Cost (Train): 0.6011673171393364\n",
            "Iteration 37, Norm of Gradient: 0.1357548938749636, Cost (Train): 0.5993308033432168\n",
            "Iteration 38, Norm of Gradient: 0.1348095504565086, Cost (Train): 0.5975197364606666\n",
            "Iteration 39, Norm of Gradient: 0.13387694830858682, Cost (Train): 0.5957335997039296\n",
            "Iteration 40, Norm of Gradient: 0.132956894419487, Cost (Train): 0.5939718884788561\n",
            "Iteration 41, Norm of Gradient: 0.13204919710300633, Cost (Train): 0.5922341101129176\n",
            "Iteration 42, Norm of Gradient: 0.1311536661425379, Cost (Train): 0.5905197835855325\n",
            "Iteration 43, Norm of Gradient: 0.1302701129216011, Cost (Train): 0.5888284392610781\n",
            "Iteration 44, Norm of Gradient: 0.1293983505416308, Cost (Train): 0.5871596186249178\n",
            "Iteration 45, Norm of Gradient: 0.12853819392781024, Cost (Train): 0.5855128740227369\n",
            "Iteration 46, Norm of Gradient: 0.1276894599237015, Cost (Train): 0.5838877684034395\n",
            "Iteration 47, Norm of Gradient: 0.1268519673753959, Cost (Train): 0.5822838750658313\n",
            "Iteration 48, Norm of Gradient: 0.1260255372058737, Cost (Train): 0.5807007774092775\n",
            "Iteration 49, Norm of Gradient: 0.1252099924802315, Cost (Train): 0.5791380686884957\n",
            "Iteration 50, Norm of Gradient: 0.1244051584624023, Cost (Train): 0.577595351772624\n",
            "Iteration 51, Norm of Gradient: 0.12361086266396294, Cost (Train): 0.5760722389086755\n",
            "Iteration 52, Norm of Gradient: 0.12282693488559181, Cost (Train): 0.5745683514894661\n",
            "Iteration 53, Norm of Gradient: 0.12205320725171026, Cost (Train): 0.57308331982609\n",
            "Iteration 54, Norm of Gradient: 0.12128951423881135, Cost (Train): 0.5716167829249937\n",
            "Iteration 55, Norm of Gradient: 0.12053569269795132, Cost (Train): 0.5701683882696837\n",
            "Iteration 56, Norm of Gradient: 0.11979158187185186, Cost (Train): 0.5687377916070887\n",
            "Iteration 57, Norm of Gradient: 0.11905702340703464, Cost (Train): 0.5673246567385849\n",
            "Iteration 58, Norm of Gradient: 0.11833186136138463, Cost (Train): 0.5659286553156788\n",
            "Iteration 59, Norm of Gradient: 0.11761594220751398, Cost (Train): 0.5645494666403323\n",
            "Iteration 60, Norm of Gradient: 0.11690911483227565, Cost (Train): 0.5631867774699036\n",
            "Iteration 61, Norm of Gradient: 0.11621123053275312, Cost (Train): 0.5618402818266721\n",
            "Iteration 62, Norm of Gradient: 0.11552214300903291, Cost (Train): 0.5605096808119046\n",
            "Iteration 63, Norm of Gradient: 0.11484170835404442, Cost (Train): 0.5591946824244136\n",
            "Iteration 64, Norm of Gradient: 0.11416978504073548, Cost (Train): 0.5578950013835562\n",
            "Iteration 65, Norm of Gradient: 0.11350623390683158, Cost (Train): 0.5566103589566098\n",
            "Iteration 66, Norm of Gradient: 0.11285091813741137, Cost (Train): 0.5553404827904661\n",
            "Iteration 67, Norm of Gradient: 0.11220370324551428, Cost (Train): 0.5540851067475707\n",
            "Iteration 68, Norm of Gradient: 0.11156445705098192, Cost (Train): 0.552843970746041\n",
            "Iteration 69, Norm of Gradient: 0.11093304965771884, Cost (Train): 0.5516168206038854\n",
            "Iteration 70, Norm of Gradient: 0.11030935342954751, Cost (Train): 0.5504034078872523\n",
            "Iteration 71, Norm of Gradient: 0.10969324296481726, Cost (Train): 0.5492034897626249\n",
            "Iteration 72, Norm of Gradient: 0.1090845950699169, Cost (Train): 0.548016828852889\n",
            "Iteration 73, Norm of Gradient: 0.10848328873182904, Cost (Train): 0.5468431930971894\n",
            "Iteration 74, Norm of Gradient: 0.10788920508985288, Cost (Train): 0.5456823556144944\n",
            "Iteration 75, Norm of Gradient: 0.10730222740661467, Cost (Train): 0.5445340945707887\n",
            "Iteration 76, Norm of Gradient: 0.10672224103847355, Cost (Train): 0.5433981930498112\n",
            "Iteration 77, Norm of Gradient: 0.10614913340542338, Cost (Train): 0.5422744389272572\n",
            "Iteration 78, Norm of Gradient: 0.10558279396058312, Cost (Train): 0.5411626247483621\n",
            "Iteration 79, Norm of Gradient: 0.10502311415936036, Cost (Train): 0.5400625476087878\n",
            "Iteration 80, Norm of Gradient: 0.1044699874283658, Cost (Train): 0.5389740090387297\n",
            "Iteration 81, Norm of Gradient: 0.10392330913415077, Cost (Train): 0.5378968148901656\n",
            "Iteration 82, Norm of Gradient: 0.10338297655183255, Cost (Train): 0.5368307752271662\n",
            "Iteration 83, Norm of Gradient: 0.10284888883366763, Cost (Train): 0.5357757042191921\n",
            "Iteration 84, Norm of Gradient: 0.10232094697762784, Cost (Train): 0.5347314200372981\n",
            "Iteration 85, Norm of Gradient: 0.10179905379602841, Cost (Train): 0.5336977447531704\n",
            "Iteration 86, Norm of Gradient: 0.10128311388425391, Cost (Train): 0.5326745042409214\n",
            "Iteration 87, Norm of Gradient: 0.1007730335896227, Cost (Train): 0.5316615280815709\n",
            "Iteration 88, Norm of Gradient: 0.10026872098042701, Cost (Train): 0.5306586494701394\n",
            "Iteration 89, Norm of Gradient: 0.0997700858151827, Cost (Train): 0.5296657051252861\n",
            "Iteration 90, Norm of Gradient: 0.09927703951211811, Cost (Train): 0.5286825352014188\n",
            "Iteration 91, Norm of Gradient: 0.0987894951189299, Cost (Train): 0.5277089832032124\n",
            "Iteration 92, Norm of Gradient: 0.0983073672828293, Cost (Train): 0.5267448959024671\n",
            "Iteration 93, Norm of Gradient: 0.09783057222090108, Cost (Train): 0.5257901232572424\n",
            "Iteration 94, Norm of Gradient: 0.09735902769079387, Cost (Train): 0.5248445183332032\n",
            "Iteration 95, Norm of Gradient: 0.09689265296175859, Cost (Train): 0.5239079372271193\n",
            "Iteration 96, Norm of Gradient: 0.09643136878605026, Cost (Train): 0.5229802389924518\n",
            "Iteration 97, Norm of Gradient: 0.09597509737070542, Cost (Train): 0.5220612855669762\n",
            "Iteration 98, Norm of Gradient: 0.09552376234970632, Cost (Train): 0.5211509417023761\n",
            "Iteration 99, Norm of Gradient: 0.09507728875654158, Cost (Train): 0.5202490748957586\n",
            "Iteration 100, Norm of Gradient: 0.09463560299717087, Cost (Train): 0.519355555323033\n",
            "Iteration 101, Norm of Gradient: 0.09419863282340016, Cost (Train): 0.5184702557741018\n",
            "Iteration 102, Norm of Gradient: 0.09376630730667262, Cost (Train): 0.5175930515898105\n",
            "Iteration 103, Norm of Gradient: 0.09333855681227984, Cost (Train): 0.5167238206006082\n",
            "Iteration 104, Norm of Gradient: 0.09291531297399541, Cost (Train): 0.5158624430668681\n",
            "Iteration 105, Norm of Gradient: 0.09249650866913363, Cost (Train): 0.5150088016208223\n",
            "Iteration 106, Norm of Gradient: 0.09208207799403421, Cost (Train): 0.5141627812100622\n",
            "Iteration 107, Norm of Gradient: 0.09167195623997317, Cost (Train): 0.5133242690425621\n",
            "Iteration 108, Norm of Gradient: 0.0912660798694995, Cost (Train): 0.5124931545331803\n",
            "Iteration 109, Norm of Gradient: 0.09086438649319702, Cost (Train): 0.5116693292515966\n",
            "Iteration 110, Norm of Gradient: 0.09046681484686861, Cost (Train): 0.5108526868716438\n",
            "Iteration 111, Norm of Gradient: 0.09007330476914192, Cost (Train): 0.510043123121994\n",
            "Iteration 112, Norm of Gradient: 0.0896837971794926, Cost (Train): 0.5092405357381597\n",
            "Iteration 113, Norm of Gradient: 0.08929823405668273, Cost (Train): 0.5084448244157738\n",
            "Iteration 114, Norm of Gradient: 0.08891655841761026, Cost (Train): 0.5076558907651081\n",
            "Iteration 115, Norm of Gradient: 0.0885387142965658, Cost (Train): 0.5068736382667998\n",
            "Iteration 116, Norm of Gradient: 0.0881646467248924, Cost (Train): 0.5060979722287464\n",
            "Iteration 117, Norm of Gradient: 0.08779430171104399, Cost (Train): 0.5053287997441379\n",
            "Iteration 118, Norm of Gradient: 0.08742762622103707, Cost (Train): 0.504566029650594\n",
            "Iteration 119, Norm of Gradient: 0.08706456815929135, Cost (Train): 0.5038095724903731\n",
            "Iteration 120, Norm of Gradient: 0.08670507634985374, Cost (Train): 0.5030593404716226\n",
            "Iteration 121, Norm of Gradient: 0.08634910051800025, Cost (Train): 0.5023152474306432\n",
            "Iteration 122, Norm of Gradient: 0.0859965912722107, Cost (Train): 0.5015772087951332\n",
            "Iteration 123, Norm of Gradient: 0.08564750008650993, Cost (Train): 0.5008451415483902\n",
            "Iteration 124, Norm of Gradient: 0.08530177928317059, Cost (Train): 0.5001189641944392\n",
            "Iteration 125, Norm of Gradient: 0.0849593820157712, Cost (Train): 0.49939859672406256\n",
            "Iteration 126, Norm of Gradient: 0.08462026225260359, Cost (Train): 0.4986839605817044\n",
            "Iteration 127, Norm of Gradient: 0.08428437476042401, Cost (Train): 0.49797497863322676\n",
            "Iteration 128, Norm of Gradient: 0.08395167508854216, Cost (Train): 0.49727157513449183\n",
            "Iteration 129, Norm of Gradient: 0.08362211955324168, Cost (Train): 0.49657367570074673\n",
            "Iteration 130, Norm of Gradient: 0.08329566522252647, Cost (Train): 0.4958812072767898\n",
            "Iteration 131, Norm of Gradient: 0.082972269901187, Cost (Train): 0.49519409810789394\n",
            "Iteration 132, Norm of Gradient: 0.08265189211618028, Cost (Train): 0.49451227771146733\n",
            "Iteration 133, Norm of Gradient: 0.08233449110231793, Cost (Train): 0.4938356768494305\n",
            "Iteration 134, Norm of Gradient: 0.08202002678825636, Cost (Train): 0.4931642275012897\n",
            "Iteration 135, Norm of Gradient: 0.08170845978278311, Cost (Train): 0.49249786283788516\n",
            "Iteration 136, Norm of Gradient: 0.08139975136139366, Cost (Train): 0.4918365171957997\n",
            "Iteration 137, Norm of Gradient: 0.08109386345315302, Cost (Train): 0.4911801260524036\n",
            "Iteration 138, Norm of Gradient: 0.0807907586278363, Cost (Train): 0.49052862600152275\n",
            "Iteration 139, Norm of Gradient: 0.08049040008334252, Cost (Train): 0.48988195472971113\n",
            "Iteration 140, Norm of Gradient: 0.08019275163337651, Cost (Train): 0.48924005099310824\n",
            "Iteration 141, Norm of Gradient: 0.07989777769539287, Cost (Train): 0.4886028545948713\n",
            "Iteration 142, Norm of Gradient: 0.07960544327879698, Cost (Train): 0.48797030636315897\n",
            "Iteration 143, Norm of Gradient: 0.07931571397339768, Cost (Train): 0.48734234812965777\n",
            "Iteration 144, Norm of Gradient: 0.07902855593810579, Cost (Train): 0.4867189227086322\n",
            "Iteration 145, Norm of Gradient: 0.07874393588987426, Cost (Train): 0.4860999738764866\n",
            "Iteration 146, Norm of Gradient: 0.07846182109287396, Cost (Train): 0.48548544635182234\n",
            "Iteration 147, Norm of Gradient: 0.07818217934790052, Cost (Train): 0.4848752857759799\n",
            "Iteration 148, Norm of Gradient: 0.07790497898200707, Cost (Train): 0.48426943869404915\n",
            "Iteration 149, Norm of Gradient: 0.07763018883835839, Cost (Train): 0.48366785253633826\n",
            "Iteration 150, Norm of Gradient: 0.077357778266301, Cost (Train): 0.4830704756002873\n",
            "Iteration 151, Norm of Gradient: 0.07708771711164528, Cost (Train): 0.482477257032813\n",
            "Iteration 152, Norm of Gradient: 0.07681997570715444, Cost (Train): 0.4818881468130774\n",
            "Iteration 153, Norm of Gradient: 0.07655452486323609, Cost (Train): 0.48130309573566327\n",
            "Iteration 154, Norm of Gradient: 0.07629133585883188, Cost (Train): 0.4807220553941509\n",
            "Iteration 155, Norm of Gradient: 0.07603038043250089, Cost (Train): 0.48014497816508117\n",
            "Iteration 156, Norm of Gradient: 0.0757716307736924, Cost (Train): 0.479571817192297\n",
            "Iteration 157, Norm of Gradient: 0.07551505951420413, Cost (Train): 0.47900252637165314\n",
            "Iteration 158, Norm of Gradient: 0.07526063971982147, Cost (Train): 0.4784370603360827\n",
            "Iteration 159, Norm of Gradient: 0.07500834488213395, Cost (Train): 0.4778753744410121\n",
            "Iteration 160, Norm of Gradient: 0.07475814891052494, Cost (Train): 0.47731742475011707\n",
            "Iteration 161, Norm of Gradient: 0.0745100261243307, Cost (Train): 0.4767631680214063\n",
            "Iteration 162, Norm of Gradient: 0.07426395124516501, Cost (Train): 0.4762125616936283\n",
            "Iteration 163, Norm of Gradient: 0.07401989938940569, Cost (Train): 0.4756655638729917\n",
            "Iteration 164, Norm of Gradient: 0.07377784606083945, Cost (Train): 0.475122133320189\n",
            "Iteration 165, Norm of Gradient: 0.0735377671434614, Cost (Train): 0.47458222943771833\n",
            "Iteration 166, Norm of Gradient: 0.07329963889442591, Cost (Train): 0.474045812257495\n",
            "Iteration 167, Norm of Gradient: 0.07306343793714518, Cost (Train): 0.4735128424287434\n",
            "Iteration 168, Norm of Gradient: 0.0728291412545327, Cost (Train): 0.47298328120616506\n",
            "Iteration 169, Norm of Gradient: 0.07259672618238762, Cost (Train): 0.47245709043837225\n",
            "Iteration 170, Norm of Gradient: 0.07236617040291769, Cost (Train): 0.471934232556584\n",
            "Iteration 171, Norm of Gradient: 0.07213745193839682, Cost (Train): 0.4714146705635749\n",
            "Iteration 172, Norm of Gradient: 0.07191054914495507, Cost (Train): 0.47089836802287127\n",
            "Iteration 173, Norm of Gradient: 0.0716854407064973, Cost (Train): 0.47038528904818905\n",
            "Iteration 174, Norm of Gradient: 0.07146210562874834, Cost (Train): 0.4698753982931057\n",
            "Iteration 175, Norm of Gradient: 0.07124052323342128, Cost (Train): 0.4693686609409622\n",
            "Iteration 176, Norm of Gradient: 0.07102067315250628, Cost (Train): 0.4688650426949871\n",
            "Iteration 177, Norm of Gradient: 0.07080253532267726, Cost (Train): 0.4683645097686387\n",
            "Iteration 178, Norm of Gradient: 0.07058608997981393, Cost (Train): 0.4678670288761598\n",
            "Iteration 179, Norm of Gradient: 0.07037131765363629, Cost (Train): 0.4673725672233377\n",
            "Iteration 180, Norm of Gradient: 0.07015819916244923, Cost (Train): 0.4668810924984678\n",
            "Iteration 181, Norm of Gradient: 0.0699467156079949, Cost (Train): 0.4663925728635121\n",
            "Iteration 182, Norm of Gradient: 0.06973684837041, Cost (Train): 0.4659069769454506\n",
            "Iteration 183, Norm of Gradient: 0.06952857910328634, Cost (Train): 0.4654242738278184\n",
            "Iteration 184, Norm of Gradient: 0.06932188972883176, Cost (Train): 0.4649444330424262\n",
            "Iteration 185, Norm of Gradient: 0.06911676243312911, Cost (Train): 0.4644674245612584\n",
            "Iteration 186, Norm of Gradient: 0.06891317966149196, Cost (Train): 0.46399321878854405\n",
            "Iteration 187, Norm of Gradient: 0.06871112411391372, Cost (Train): 0.46352178655299797\n",
            "Iteration 188, Norm of Gradient: 0.06851057874060894, Cost (Train): 0.4630530991002257\n",
            "Iteration 189, Norm of Gradient: 0.06831152673764443, Cost (Train): 0.4625871280852909\n",
            "Iteration 190, Norm of Gradient: 0.0681139515426582, Cost (Train): 0.4621238455654387\n",
            "Iteration 191, Norm of Gradient: 0.06791783683066416, Cost (Train): 0.4616632239929733\n",
            "Iteration 192, Norm of Gradient: 0.06772316650994115, Cost (Train): 0.46120523620828424\n",
            "Iteration 193, Norm of Gradient: 0.06752992471800377, Cost (Train): 0.46074985543301983\n",
            "Iteration 194, Norm of Gradient: 0.06733809581765364, Cost (Train): 0.46029705526340187\n",
            "Iteration 195, Norm of Gradient: 0.06714766439310928, Cost (Train): 0.4598468096636805\n",
            "Iteration 196, Norm of Gradient: 0.0669586152462127, Cost (Train): 0.45939909295972436\n",
            "Iteration 197, Norm of Gradient: 0.06677093339271117, Cost (Train): 0.4589538798327432\n",
            "Iteration 198, Norm of Gradient: 0.06658460405861244, Cost (Train): 0.45851114531314036\n",
            "Iteration 199, Norm of Gradient: 0.06639961267661167, Cost (Train): 0.45807086477449066\n",
            "Iteration 200, Norm of Gradient: 0.06621594488258897, Cost (Train): 0.45763301392764316\n",
            "Iteration 201, Norm of Gradient: 0.06603358651217532, Cost (Train): 0.45719756881494283\n",
            "Iteration 202, Norm of Gradient: 0.06585252359738593, Cost (Train): 0.45676450580457095\n",
            "Iteration 203, Norm of Gradient: 0.06567274236331924, Cost (Train): 0.4563338015849992\n",
            "Iteration 204, Norm of Gradient: 0.06549422922492047, Cost (Train): 0.4559054331595577\n",
            "Iteration 205, Norm of Gradient: 0.0653169707838077, Cost (Train): 0.455479377841111\n",
            "Iteration 206, Norm of Gradient: 0.06514095382516008, Cost (Train): 0.45505561324684196\n",
            "Iteration 207, Norm of Gradient: 0.06496616531466563, Cost (Train): 0.4546341172931396\n",
            "Iteration 208, Norm of Gradient: 0.06479259239552851, Cost (Train): 0.45421486819059026\n",
            "Iteration 209, Norm of Gradient: 0.06462022238553364, Cost (Train): 0.4537978444390672\n",
            "Iteration 210, Norm of Gradient: 0.06444904277416774, Cost (Train): 0.45338302482291853\n",
            "Iteration 211, Norm of Gradient: 0.06427904121979579, Cost (Train): 0.4529703884062502\n",
            "Iteration 212, Norm of Gradient: 0.06411020554689109, Cost (Train): 0.452559914528302\n",
            "Iteration 213, Norm of Gradient: 0.06394252374331864, Cost (Train): 0.4521515827989151\n",
            "Iteration 214, Norm of Gradient: 0.06377598395766981, Cost (Train): 0.4517453730940877\n",
            "Iteration 215, Norm of Gradient: 0.06361057449664793, Cost (Train): 0.45134126555161835\n",
            "Iteration 216, Norm of Gradient: 0.0634462838225032, Cost (Train): 0.45093924056683365\n",
            "Iteration 217, Norm of Gradient: 0.06328310055051622, Cost (Train): 0.4505392787883994\n",
            "Iteration 218, Norm of Gradient: 0.06312101344652886, Cost (Train): 0.4501413611142131\n",
            "Iteration 219, Norm of Gradient: 0.06296001142452154, Cost (Train): 0.4497454686873751\n",
            "Iteration 220, Norm of Gradient: 0.0628000835442362, Cost (Train): 0.4493515828922382\n",
            "Iteration 221, Norm of Gradient: 0.06264121900884337, Cost (Train): 0.44895968535053254\n",
            "Iteration 222, Norm of Gradient: 0.062483407162653146, Cost (Train): 0.44856975791756504\n",
            "Iteration 223, Norm of Gradient: 0.0623266374888686, Cost (Train): 0.4481817826784916\n",
            "Iteration 224, Norm of Gradient: 0.06217089960738106, Cost (Train): 0.4477957419446594\n",
            "Iteration 225, Norm of Gradient: 0.062016183272606044, Cost (Train): 0.4474116182500196\n",
            "Iteration 226, Norm of Gradient: 0.06186247837135951, Cost (Train): 0.44702939434760747\n",
            "Iteration 227, Norm of Gradient: 0.06170977492077295, Cost (Train): 0.4466490532060889\n",
            "Iteration 228, Norm of Gradient: 0.06155806306624717, Cost (Train): 0.44627057800637177\n",
            "Iteration 229, Norm of Gradient: 0.06140733307944325, Cost (Train): 0.44589395213828165\n",
            "Iteration 230, Norm of Gradient: 0.061257575356310684, Cost (Train): 0.4455191591972986\n",
            "Iteration 231, Norm of Gradient: 0.06110878041515125, Cost (Train): 0.4451461829813554\n",
            "Iteration 232, Norm of Gradient: 0.06096093889471826, Cost (Train): 0.44477500748769505\n",
            "Iteration 233, Norm of Gradient: 0.06081404155235035, Cost (Train): 0.4444056169097872\n",
            "Iteration 234, Norm of Gradient: 0.060668079262139046, Cost (Train): 0.4440379956343002\n",
            "Iteration 235, Norm of Gradient: 0.06052304301312961, Cost (Train): 0.4436721282381305\n",
            "Iteration 236, Norm of Gradient: 0.06037892390755415, Cost (Train): 0.44330799948548566\n",
            "Iteration 237, Norm of Gradient: 0.060235713159096725, Cost (Train): 0.44294559432502084\n",
            "Iteration 238, Norm of Gradient: 0.060093402091189414, Cost (Train): 0.4425848978870281\n",
            "Iteration 239, Norm of Gradient: 0.05995198213533902, Cost (Train): 0.44222589548067626\n",
            "Iteration 240, Norm of Gradient: 0.05981144482948361, Cost (Train): 0.44186857259130174\n",
            "Iteration 241, Norm of Gradient: 0.05967178181637841, Cost (Train): 0.4415129148777471\n",
            "Iteration 242, Norm of Gradient: 0.05953298484201021, Cost (Train): 0.4411589081697495\n",
            "Iteration 243, Norm of Gradient: 0.059395045754040206, Cost (Train): 0.44080653846537415\n",
            "Iteration 244, Norm of Gradient: 0.059257956500273996, Cost (Train): 0.44045579192849554\n",
            "Iteration 245, Norm of Gradient: 0.05912170912715893, Cost (Train): 0.44010665488632256\n",
            "Iteration 246, Norm of Gradient: 0.058986295778307586, Cost (Train): 0.4397591138269686\n",
            "Iteration 247, Norm of Gradient: 0.05885170869304752, Cost (Train): 0.4394131553970644\n",
            "Iteration 248, Norm of Gradient: 0.05871794020499606, Cost (Train): 0.43906876639941345\n",
            "Iteration 249, Norm of Gradient: 0.058584982740660284, Cost (Train): 0.4387259337906889\n",
            "Iteration 250, Norm of Gradient: 0.0584528288180612, Cost (Train): 0.43838464467917126\n",
            "Iteration 251, Norm of Gradient: 0.058321471045382024, Cost (Train): 0.43804488632252503\n",
            "Iteration 252, Norm of Gradient: 0.058190902119639724, Cost (Train): 0.43770664612561594\n",
            "Iteration 253, Norm of Gradient: 0.058061114825379685, Cost (Train): 0.4373699116383651\n",
            "Iteration 254, Norm of Gradient: 0.057932102033392815, Cost (Train): 0.4370346705536407\n",
            "Iteration 255, Norm of Gradient: 0.05780385669945481, Cost (Train): 0.4367009107051869\n",
            "Iteration 256, Norm of Gradient: 0.05767637186308697, Cost (Train): 0.4363686200655887\n",
            "Iteration 257, Norm of Gradient: 0.05754964064633837, Cost (Train): 0.43603778674427096\n",
            "Iteration 258, Norm of Gradient: 0.05742365625258877, Cost (Train): 0.43570839898553365\n",
            "Iteration 259, Norm of Gradient: 0.057298411965371915, Cost (Train): 0.4353804451666193\n",
            "Iteration 260, Norm of Gradient: 0.057173901147218875, Cost (Train): 0.43505391379581454\n",
            "Iteration 261, Norm of Gradient: 0.05705011723852104, Cost (Train): 0.43472879351058363\n",
            "Iteration 262, Norm of Gradient: 0.05692705375641232, Cost (Train): 0.43440507307573445\n",
            "Iteration 263, Norm of Gradient: 0.056804704293670176, Cost (Train): 0.4340827413816151\n",
            "Iteration 264, Norm of Gradient: 0.05668306251763527, Cost (Train): 0.4337617874423413\n",
            "Iteration 265, Norm of Gradient: 0.05656212216914913, Cost (Train): 0.43344220039405346\n",
            "Iteration 266, Norm of Gradient: 0.056441877061509764, Cost (Train): 0.4331239694932041\n",
            "Iteration 267, Norm of Gradient: 0.0563223210794446, Cost (Train): 0.4328070841148734\n",
            "Iteration 268, Norm of Gradient: 0.056203448178100655, Cost (Train): 0.43249153375111266\n",
            "Iteration 269, Norm of Gradient: 0.05608525238205141, Cost (Train): 0.43217730800931614\n",
            "Iteration 270, Norm of Gradient: 0.055967727784320305, Cost (Train): 0.4318643966106199\n",
            "Iteration 271, Norm of Gradient: 0.05585086854542027, Cost (Train): 0.4315527893883277\n",
            "Iteration 272, Norm of Gradient: 0.055734668892409134, Cost (Train): 0.4312424762863616\n",
            "Iteration 273, Norm of Gradient: 0.05561912311796078, Cost (Train): 0.4309334473577399\n",
            "Iteration 274, Norm of Gradient: 0.055504225579451234, Cost (Train): 0.43062569276307966\n",
            "Iteration 275, Norm of Gradient: 0.05538997069806007, Cost (Train): 0.43031920276912317\n",
            "Iteration 276, Norm of Gradient: 0.05527635295788634, Cost (Train): 0.4300139677472893\n",
            "Iteration 277, Norm of Gradient: 0.05516336690507897, Cost (Train): 0.4297099781722489\n",
            "Iteration 278, Norm of Gradient: 0.05505100714698126, Cost (Train): 0.4294072246205227\n",
            "Iteration 279, Norm of Gradient: 0.05493926835128943, Cost (Train): 0.42910569776910235\n",
            "Iteration 280, Norm of Gradient: 0.05482814524522461, Cost (Train): 0.42880538839409366\n",
            "Iteration 281, Norm of Gradient: 0.054717632614718376, Cost (Train): 0.4285062873693827\n",
            "Iteration 282, Norm of Gradient: 0.05460772530361129, Cost (Train): 0.4282083856653225\n",
            "Iteration 283, Norm of Gradient: 0.05449841821286444, Cost (Train): 0.42791167434744076\n",
            "Iteration 284, Norm of Gradient: 0.05438970629978351, Cost (Train): 0.42761614457516955\n",
            "Iteration 285, Norm of Gradient: 0.05428158457725547, Cost (Train): 0.4273217876005944\n",
            "Iteration 286, Norm of Gradient: 0.05417404811299722, Cost (Train): 0.4270285947672233\n",
            "Iteration 287, Norm of Gradient: 0.054067092028816384, Cost (Train): 0.42673655750877604\n",
            "Iteration 288, Norm of Gradient: 0.05396071149988378, Cost (Train): 0.4264456673479921\n",
            "Iteration 289, Norm of Gradient: 0.053854901754017435, Cost (Train): 0.42615591589545837\n",
            "Iteration 290, Norm of Gradient: 0.05374965807097796, Cost (Train): 0.4258672948484541\n",
            "Iteration 291, Norm of Gradient: 0.05364497578177498, Cost (Train): 0.4255797959898151\n",
            "Iteration 292, Norm of Gradient: 0.053540850267984566, Cost (Train): 0.4252934111868161\n",
            "Iteration 293, Norm of Gradient: 0.053437276961077355, Cost (Train): 0.4250081323900691\n",
            "Iteration 294, Norm of Gradient: 0.05333425134175718, Cost (Train): 0.42472395163244053\n",
            "Iteration 295, Norm of Gradient: 0.053231768939310045, Cost (Train): 0.424440861027985\n",
            "Iteration 296, Norm of Gradient: 0.05312982533096333, Cost (Train): 0.4241588527708947\n",
            "Iteration 297, Norm of Gradient: 0.05302841614125486, Cost (Train): 0.42387791913446665\n",
            "Iteration 298, Norm of Gradient: 0.05292753704141185, Cost (Train): 0.4235980524700844\n",
            "Iteration 299, Norm of Gradient: 0.05282718374873941, Cost (Train): 0.42331924520621694\n",
            "Iteration 300, Norm of Gradient: 0.0527273520260186, Cost (Train): 0.42304148984743195\n",
            "Iteration 301, Norm of Gradient: 0.0526280376809136, Cost (Train): 0.42276477897342446\n",
            "Iteration 302, Norm of Gradient: 0.052529236565388236, Cost (Train): 0.42248910523806116\n",
            "Iteration 303, Norm of Gradient: 0.05243094457513119, Cost (Train): 0.42221446136843804\n",
            "Iteration 304, Norm of Gradient: 0.052333157648990256, Cost (Train): 0.42194084016395356\n",
            "Iteration 305, Norm of Gradient: 0.05223587176841507, Cost (Train): 0.4216682344953956\n",
            "Iteration 306, Norm of Gradient: 0.05213908295690841, Cost (Train): 0.4213966373040419\n",
            "Iteration 307, Norm of Gradient: 0.05204278727948583, Cost (Train): 0.42112604160077427\n",
            "Iteration 308, Norm of Gradient: 0.05194698084214351, Cost (Train): 0.42085644046520676\n",
            "Iteration 309, Norm of Gradient: 0.051851659791334055, Cost (Train): 0.42058782704482617\n",
            "Iteration 310, Norm of Gradient: 0.05175682031345047, Cost (Train): 0.42032019455414626\n",
            "Iteration 311, Norm of Gradient: 0.051662458634317523, Cost (Train): 0.42005353627387365\n",
            "Iteration 312, Norm of Gradient: 0.051568571018691126, Cost (Train): 0.41978784555008714\n",
            "Iteration 313, Norm of Gradient: 0.05147515376976501, Cost (Train): 0.4195231157934289\n",
            "Iteration 314, Norm of Gradient: 0.05138220322868483, Cost (Train): 0.4192593404783076\n",
            "Iteration 315, Norm of Gradient: 0.05128971577406957, Cost (Train): 0.4189965131421139\n",
            "Iteration 316, Norm of Gradient: 0.051197687821540036, Cost (Train): 0.41873462738444633\n",
            "Iteration 317, Norm of Gradient: 0.051106115823254326, Cost (Train): 0.41847367686635023\n",
            "Iteration 318, Norm of Gradient: 0.051014996267450284, Cost (Train): 0.418213655309567\n",
            "Iteration 319, Norm of Gradient: 0.050924325677994646, Cost (Train): 0.4179545564957938\n",
            "Iteration 320, Norm of Gradient: 0.05083410061393886, Cost (Train): 0.4176963742659559\n",
            "Iteration 321, Norm of Gradient: 0.05074431766908147, Cost (Train): 0.4174391025194876\n",
            "Iteration 322, Norm of Gradient: 0.05065497347153691, Cost (Train): 0.41718273521362537\n",
            "Iteration 323, Norm of Gradient: 0.05056606468331063, Cost (Train): 0.41692726636271\n",
            "Iteration 324, Norm of Gradient: 0.05047758799988045, Cost (Train): 0.4166726900374997\n",
            "Iteration 325, Norm of Gradient: 0.05038954014978397, Cost (Train): 0.4164190003644933\n",
            "Iteration 326, Norm of Gradient: 0.05030191789421211, Cost (Train): 0.4161661915252618\n",
            "Iteration 327, Norm of Gradient: 0.05021471802660841, Cost (Train): 0.41591425775579166\n",
            "Iteration 328, Norm of Gradient: 0.05012793737227426, Cost (Train): 0.4156631933458357\n",
            "Iteration 329, Norm of Gradient: 0.05004157278797976, Cost (Train): 0.4154129926382745\n",
            "Iteration 330, Norm of Gradient: 0.049955621161580255, Cost (Train): 0.41516365002848643\n",
            "Iteration 331, Norm of Gradient: 0.049870079411638395, Cost (Train): 0.41491515996372674\n",
            "Iteration 332, Norm of Gradient: 0.04978494448705156, Cost (Train): 0.4146675169425154\n",
            "Iteration 333, Norm of Gradient: 0.049700213366684724, Cost (Train): 0.4144207155140343\n",
            "Iteration 334, Norm of Gradient: 0.049615883059008443, Cost (Train): 0.41417475027753153\n",
            "Iteration 335, Norm of Gradient: 0.04953195060174222, Cost (Train): 0.41392961588173594\n",
            "Iteration 336, Norm of Gradient: 0.0494484130615027, Cost (Train): 0.4136853070242787\n",
            "Iteration 337, Norm of Gradient: 0.049365267533457075, Cost (Train): 0.41344181845112304\n",
            "Iteration 338, Norm of Gradient: 0.049282511140981304, Cost (Train): 0.41319914495600296\n",
            "Iteration 339, Norm of Gradient: 0.04920014103532316, Cost (Train): 0.4129572813798688\n",
            "Iteration 340, Norm of Gradient: 0.049118154395270106, Cost (Train): 0.4127162226103408\n",
            "Iteration 341, Norm of Gradient: 0.04903654842682179, Cost (Train): 0.4124759635811706\n",
            "Iteration 342, Norm of Gradient: 0.04895532036286721, Cost (Train): 0.4122364992717102\n",
            "Iteration 343, Norm of Gradient: 0.0488744674628663, Cost (Train): 0.4119978247063876\n",
            "Iteration 344, Norm of Gradient: 0.0487939870125361, Cost (Train): 0.41175993495419116\n",
            "Iteration 345, Norm of Gradient: 0.048713876323541225, Cost (Train): 0.4115228251281589\n",
            "Iteration 346, Norm of Gradient: 0.048634132733188735, Cost (Train): 0.41128649038487736\n",
            "Iteration 347, Norm of Gradient: 0.048554753604127115, Cost (Train): 0.41105092592398523\n",
            "Iteration 348, Norm of Gradient: 0.04847573632404958, Cost (Train): 0.4108161269876851\n",
            "Iteration 349, Norm of Gradient: 0.04839707830540135, Cost (Train): 0.4105820888602613\n",
            "Iteration 350, Norm of Gradient: 0.04831877698509116, Cost (Train): 0.4103488068676046\n",
            "Iteration 351, Norm of Gradient: 0.048240829824206556, Cost (Train): 0.4101162763767429\n",
            "Iteration 352, Norm of Gradient: 0.048163234307733185, Cost (Train): 0.4098844927953795\n",
            "Iteration 353, Norm of Gradient: 0.04808598794427801, Cost (Train): 0.40965345157143573\n",
            "Iteration 354, Norm of Gradient: 0.04800908826579617, Cost (Train): 0.4094231481926017\n",
            "Iteration 355, Norm of Gradient: 0.04793253282732177, Cost (Train): 0.40919357818589175\n",
            "Iteration 356, Norm of Gradient: 0.047856319206702094, Cost (Train): 0.40896473711720693\n",
            "Iteration 357, Norm of Gradient: 0.047780445004335745, Cost (Train): 0.4087366205909021\n",
            "Iteration 358, Norm of Gradient: 0.047704907842914056, Cost (Train): 0.4085092242493603\n",
            "Iteration 359, Norm of Gradient: 0.04762970536716619, Cost (Train): 0.4082825437725718\n",
            "Iteration 360, Norm of Gradient: 0.0475548352436076, Cost (Train): 0.40805657487771874\n",
            "Iteration 361, Norm of Gradient: 0.04748029516029199, Cost (Train): 0.4078313133187662\n",
            "Iteration 362, Norm of Gradient: 0.047406082826566436, Cost (Train): 0.40760675488605774\n",
            "Iteration 363, Norm of Gradient: 0.047332195972830025, Cost (Train): 0.4073828954059169\n",
            "Iteration 364, Norm of Gradient: 0.04725863235029553, Cost (Train): 0.4071597307402535\n",
            "Iteration 365, Norm of Gradient: 0.04718538973075447, Cost (Train): 0.40693725678617587\n",
            "Iteration 366, Norm of Gradient: 0.0471124659063451, Cost (Train): 0.40671546947560727\n",
            "Iteration 367, Norm of Gradient: 0.047039858689323706, Cost (Train): 0.4064943647749079\n",
            "Iteration 368, Norm of Gradient: 0.04696756591183877, Cost (Train): 0.40627393868450185\n",
            "Iteration 369, Norm of Gradient: 0.04689558542570823, Cost (Train): 0.40605418723850906\n",
            "Iteration 370, Norm of Gradient: 0.0468239151021997, Cost (Train): 0.40583510650438154\n",
            "Iteration 371, Norm of Gradient: 0.04675255283181356, Cost (Train): 0.4056166925825449\n",
            "Iteration 372, Norm of Gradient: 0.04668149652406886, Cost (Train): 0.4053989416060439\n",
            "Iteration 373, Norm of Gradient: 0.046610744107292226, Cost (Train): 0.4051818497401938\n",
            "Iteration 374, Norm of Gradient: 0.04654029352840926, Cost (Train): 0.4049654131822346\n",
            "Iteration 375, Norm of Gradient: 0.04647014275273902, Cost (Train): 0.4047496281609911\n",
            "Iteration 376, Norm of Gradient: 0.04640028976379082, Cost (Train): 0.40453449093653665\n",
            "Iteration 377, Norm of Gradient: 0.04633073256306407, Cost (Train): 0.4043199977998613\n",
            "Iteration 378, Norm of Gradient: 0.046261469169850314, Cost (Train): 0.40410614507254444\n",
            "Iteration 379, Norm of Gradient: 0.04619249762103826, Cost (Train): 0.40389292910643154\n",
            "Iteration 380, Norm of Gradient: 0.046123815970921044, Cost (Train): 0.4036803462833146\n",
            "Iteration 381, Norm of Gradient: 0.046055422291006086, Cost (Train): 0.4034683930146177\n",
            "Iteration 382, Norm of Gradient: 0.04598731466982741, Cost (Train): 0.4032570657410851\n",
            "Iteration 383, Norm of Gradient: 0.04591949121276049, Cost (Train): 0.4030463609324747\n",
            "Iteration 384, Norm of Gradient: 0.04585195004183922, Cost (Train): 0.40283627508725467\n",
            "Iteration 385, Norm of Gradient: 0.04578468929557554, Cost (Train): 0.40262680473230367\n",
            "Iteration 386, Norm of Gradient: 0.045717707128781156, Cost (Train): 0.40241794642261547\n",
            "Iteration 387, Norm of Gradient: 0.04565100171239162, Cost (Train): 0.40220969674100715\n",
            "Iteration 388, Norm of Gradient: 0.04558457123329274, Cost (Train): 0.40200205229783054\n",
            "Iteration 389, Norm of Gradient: 0.045518413894149, Cost (Train): 0.4017950097306876\n",
            "Iteration 390, Norm of Gradient: 0.04545252791323443, Cost (Train): 0.40158856570414947\n",
            "Iteration 391, Norm of Gradient: 0.04538691152426536, Cost (Train): 0.40138271690947896\n",
            "Iteration 392, Norm of Gradient: 0.04532156297623553, Cost (Train): 0.40117746006435584\n",
            "Iteration 393, Norm of Gradient: 0.04525648053325309, Cost (Train): 0.40097279191260693\n",
            "Iteration 394, Norm of Gradient: 0.04519166247437978, Cost (Train): 0.4007687092239381\n",
            "Iteration 395, Norm of Gradient: 0.045127107093472126, Cost (Train): 0.4005652087936704\n",
            "Iteration 396, Norm of Gradient: 0.04506281269902457, Cost (Train): 0.40036228744247937\n",
            "Iteration 397, Norm of Gradient: 0.04499877761401464, Cost (Train): 0.4001599420161373\n",
            "Iteration 398, Norm of Gradient: 0.04493500017575002, Cost (Train): 0.39995816938525885\n",
            "Iteration 399, Norm of Gradient: 0.04487147873571759, Cost (Train): 0.3997569664450496\n",
            "Iteration 400, Norm of Gradient: 0.044808211659434195, Cost (Train): 0.39955633011505814\n",
            "Iteration 401, Norm of Gradient: 0.044745197326299474, Cost (Train): 0.3993562573389306\n",
            "Iteration 402, Norm of Gradient: 0.04468243412945046, Cost (Train): 0.3991567450841683\n",
            "Iteration 403, Norm of Gradient: 0.044619920475617844, Cost (Train): 0.3989577903418893\n",
            "Iteration 404, Norm of Gradient: 0.0445576547849842, Cost (Train): 0.39875939012659073\n",
            "Iteration 405, Norm of Gradient: 0.0444956354910439, Cost (Train): 0.3985615414759165\n",
            "Iteration 406, Norm of Gradient: 0.044433861040464706, Cost (Train): 0.39836424145042587\n",
            "Iteration 407, Norm of Gradient: 0.04437232989295115, Cost (Train): 0.398167487133366\n",
            "Iteration 408, Norm of Gradient: 0.044311040521109506, Cost (Train): 0.39797127563044654\n",
            "Iteration 409, Norm of Gradient: 0.04424999141031453, Cost (Train): 0.3977756040696173\n",
            "Iteration 410, Norm of Gradient: 0.04418918105857772, Cost (Train): 0.3975804696008484\n",
            "Iteration 411, Norm of Gradient: 0.04412860797641724, Cost (Train): 0.39738586939591325\n",
            "Iteration 412, Norm of Gradient: 0.04406827068672944, Cost (Train): 0.3971918006481739\n",
            "Iteration 413, Norm of Gradient: 0.04400816772466191, Cost (Train): 0.39699826057236903\n",
            "Iteration 414, Norm of Gradient: 0.0439482976374881, Cost (Train): 0.3968052464044046\n",
            "Iteration 415, Norm of Gradient: 0.04388865898448347, Cost (Train): 0.3966127554011468\n",
            "Iteration 416, Norm of Gradient: 0.043829250336803065, Cost (Train): 0.3964207848402174\n",
            "Iteration 417, Norm of Gradient: 0.043770070277360684, Cost (Train): 0.39622933201979177\n",
            "Iteration 418, Norm of Gradient: 0.04371111740070943, Cost (Train): 0.39603839425839904\n",
            "Iteration 419, Norm of Gradient: 0.0436523903129237, Cost (Train): 0.3958479688947249\n",
            "Iteration 420, Norm of Gradient: 0.043593887631482614, Cost (Train): 0.3956580532874164\n",
            "Iteration 421, Norm of Gradient: 0.043535607985154864, Cost (Train): 0.3954686448148891\n",
            "Iteration 422, Norm of Gradient: 0.04347755001388488, Cost (Train): 0.3952797408751367\n",
            "Iteration 423, Norm of Gradient: 0.04341971236868042, Cost (Train): 0.39509133888554276\n",
            "Iteration 424, Norm of Gradient: 0.04336209371150142, Cost (Train): 0.39490343628269436\n",
            "Iteration 425, Norm of Gradient: 0.04330469271515027, Cost (Train): 0.39471603052219845\n",
            "Iteration 426, Norm of Gradient: 0.04324750806316335, Cost (Train): 0.3945291190784999\n",
            "Iteration 427, Norm of Gradient: 0.043190538449703754, Cost (Train): 0.39434269944470196\n",
            "Iteration 428, Norm of Gradient: 0.0431337825794555, Cost (Train): 0.39415676913238823\n",
            "Iteration 429, Norm of Gradient: 0.04307723916751873, Cost (Train): 0.3939713256714476\n",
            "Iteration 430, Norm of Gradient: 0.04302090693930641, Cost (Train): 0.3937863666099004\n",
            "Iteration 431, Norm of Gradient: 0.04296478463044204, Cost (Train): 0.39360188951372704\n",
            "Iteration 432, Norm of Gradient: 0.04290887098665859, Cost (Train): 0.39341789196669813\n",
            "Iteration 433, Norm of Gradient: 0.04285316476369882, Cost (Train): 0.39323437157020724\n",
            "Iteration 434, Norm of Gradient: 0.042797664727216474, Cost (Train): 0.39305132594310477\n",
            "Iteration 435, Norm of Gradient: 0.04274236965267885, Cost (Train): 0.39286875272153443\n",
            "Iteration 436, Norm of Gradient: 0.04268727832527043, Cost (Train): 0.39268664955877114\n",
            "Iteration 437, Norm of Gradient: 0.04263238953979759, Cost (Train): 0.3925050141250608\n",
            "Iteration 438, Norm of Gradient: 0.04257770210059448, Cost (Train): 0.3923238441074622\n",
            "Iteration 439, Norm of Gradient: 0.042523214821430004, Cost (Train): 0.3921431372096902\n",
            "Iteration 440, Norm of Gradient: 0.04246892652541579, Cost (Train): 0.3919628911519615\n",
            "Iteration 441, Norm of Gradient: 0.04241483604491535, Cost (Train): 0.3917831036708408\n",
            "Iteration 442, Norm of Gradient: 0.04236094222145414, Cost (Train): 0.3916037725190902\n",
            "Iteration 443, Norm of Gradient: 0.0423072439056308, Cost (Train): 0.3914248954655194\n",
            "Iteration 444, Norm of Gradient: 0.04225373995702935, Cost (Train): 0.39124647029483783\n",
            "Iteration 445, Norm of Gradient: 0.04220042924413234, Cost (Train): 0.39106849480750816\n",
            "Iteration 446, Norm of Gradient: 0.04214731064423507, Cost (Train): 0.3908909668196019\n",
            "Iteration 447, Norm of Gradient: 0.04209438304336082, Cost (Train): 0.39071388416265634\n",
            "Iteration 448, Norm of Gradient: 0.042041645336176915, Cost (Train): 0.39053724468353307\n",
            "Iteration 449, Norm of Gradient: 0.041989096425911886, Cost (Train): 0.3903610462442781\n",
            "Iteration 450, Norm of Gradient: 0.04193673522427349, Cost (Train): 0.3901852867219836\n",
            "Iteration 451, Norm of Gradient: 0.04188456065136769, Cost (Train): 0.3900099640086512\n",
            "Iteration 452, Norm of Gradient: 0.04183257163561854, Cost (Train): 0.3898350760110565\n",
            "Iteration 453, Norm of Gradient: 0.04178076711368902, Cost (Train): 0.3896606206506155\n",
            "Iteration 454, Norm of Gradient: 0.04172914603040266, Cost (Train): 0.3894865958632522\n",
            "Iteration 455, Norm of Gradient: 0.04167770733866619, Cost (Train): 0.3893129995992677\n",
            "Iteration 456, Norm of Gradient: 0.04162644999939294, Cost (Train): 0.38913982982321077\n",
            "Iteration 457, Norm of Gradient: 0.041575372981427175, Cost (Train): 0.38896708451375006\n",
            "Iteration 458, Norm of Gradient: 0.041524475261469226, Cost (Train): 0.38879476166354715\n",
            "Iteration 459, Norm of Gradient: 0.041473755824001514, Cost (Train): 0.38862285927913154\n",
            "Iteration 460, Norm of Gradient: 0.04142321366121533, Cost (Train): 0.3884513753807766\n",
            "Iteration 461, Norm of Gradient: 0.0413728477729385, Cost (Train): 0.3882803080023773\n",
            "Iteration 462, Norm of Gradient: 0.041322657166563845, Cost (Train): 0.38810965519132873\n",
            "Iteration 463, Norm of Gradient: 0.04127264085697836, Cost (Train): 0.3879394150084064\n",
            "Iteration 464, Norm of Gradient: 0.041222797866493285, Cost (Train): 0.38776958552764756\n",
            "Iteration 465, Norm of Gradient: 0.0411731272247749, Cost (Train): 0.3876001648362338\n",
            "Iteration 466, Norm of Gradient: 0.041123627968776055, Cost (Train): 0.3874311510343749\n",
            "Iteration 467, Norm of Gradient: 0.04107429914266853, Cost (Train): 0.3872625422351947\n",
            "Iteration 468, Norm of Gradient: 0.04102513979777606, Cost (Train): 0.38709433656461645\n",
            "Iteration 469, Norm of Gradient: 0.04097614899250818, Cost (Train): 0.38692653216125117\n",
            "Iteration 470, Norm of Gradient: 0.04092732579229468, Cost (Train): 0.3867591271762863\n",
            "Iteration 471, Norm of Gradient: 0.04087866926952095, Cost (Train): 0.38659211977337565\n",
            "Iteration 472, Norm of Gradient: 0.04083017850346382, Cost (Train): 0.3864255081285305\n",
            "Iteration 473, Norm of Gradient: 0.040781852580228324, Cost (Train): 0.38625929043001217\n",
            "Iteration 474, Norm of Gradient: 0.040733690592684946, Cost (Train): 0.3860934648782252\n",
            "Iteration 475, Norm of Gradient: 0.040685691640407716, Cost (Train): 0.38592802968561235\n",
            "Iteration 476, Norm of Gradient: 0.04063785482961286, Cost (Train): 0.38576298307654977\n",
            "Iteration 477, Norm of Gradient: 0.040590179273098215, Cost (Train): 0.3855983232872443\n",
            "Iteration 478, Norm of Gradient: 0.04054266409018318, Cost (Train): 0.38543404856563096\n",
            "Iteration 479, Norm of Gradient: 0.04049530840664945, Cost (Train): 0.38527015717127233\n",
            "Iteration 480, Norm of Gradient: 0.04044811135468231, Cost (Train): 0.3851066473752581\n",
            "Iteration 481, Norm of Gradient: 0.04040107207281256, Cost (Train): 0.3849435174601067\n",
            "Iteration 482, Norm of Gradient: 0.04035418970585906, Cost (Train): 0.3847807657196667\n",
            "Iteration 483, Norm of Gradient: 0.04030746340487197, Cost (Train): 0.38461839045902063\n",
            "Iteration 484, Norm of Gradient: 0.040260892327076525, Cost (Train): 0.3844563899943885\n",
            "Iteration 485, Norm of Gradient: 0.040214475635817384, Cost (Train): 0.38429476265303364\n",
            "Iteration 486, Norm of Gradient: 0.04016821250050367, Cost (Train): 0.3841335067731678\n",
            "Iteration 487, Norm of Gradient: 0.040122102096554535, Cost (Train): 0.38397262070385935\n",
            "Iteration 488, Norm of Gradient: 0.04007614360534528, Cost (Train): 0.38381210280494027\n",
            "Iteration 489, Norm of Gradient: 0.04003033621415415, Cost (Train): 0.38365195144691605\n",
            "Iteration 490, Norm of Gradient: 0.039984679116109534, Cost (Train): 0.38349216501087485\n",
            "Iteration 491, Norm of Gradient: 0.03993917151013788, Cost (Train): 0.3833327418883989\n",
            "Iteration 492, Norm of Gradient: 0.03989381260091211, Cost (Train): 0.3831736804814758\n",
            "Iteration 493, Norm of Gradient: 0.0398486015988005, Cost (Train): 0.38301497920241173\n",
            "Iteration 494, Norm of Gradient: 0.03980353771981624, Cost (Train): 0.3828566364737443\n",
            "Iteration 495, Norm of Gradient: 0.039758620185567375, Cost (Train): 0.38269865072815773\n",
            "Iteration 496, Norm of Gradient: 0.039713848223207414, Cost (Train): 0.3825410204083975\n",
            "Iteration 497, Norm of Gradient: 0.03966922106538637, Cost (Train): 0.38238374396718694\n",
            "Iteration 498, Norm of Gradient: 0.03962473795020231, Cost (Train): 0.38222681986714413\n",
            "Iteration 499, Norm of Gradient: 0.03958039812115345, Cost (Train): 0.3820702465806996\n",
            "Iteration 500, Norm of Gradient: 0.03953620082709075, Cost (Train): 0.3819140225900152\n",
            "Iteration 501, Norm of Gradient: 0.03949214532217097, Cost (Train): 0.38175814638690375\n",
            "Iteration 502, Norm of Gradient: 0.03944823086581023, Cost (Train): 0.38160261647274885\n",
            "Iteration 503, Norm of Gradient: 0.039404456722638086, Cost (Train): 0.38144743135842635\n",
            "Iteration 504, Norm of Gradient: 0.039360822162452, Cost (Train): 0.3812925895642264\n",
            "Iteration 505, Norm of Gradient: 0.03931732646017238, Cost (Train): 0.38113808961977585\n",
            "Iteration 506, Norm of Gradient: 0.039273968895798, Cost (Train): 0.3809839300639617\n",
            "Iteration 507, Norm of Gradient: 0.03923074875436196, Cost (Train): 0.3808301094448555\n",
            "Iteration 508, Norm of Gradient: 0.03918766532588799, Cost (Train): 0.38067662631963844\n",
            "Iteration 509, Norm of Gradient: 0.03914471790534737, Cost (Train): 0.3805234792545267\n",
            "Iteration 510, Norm of Gradient: 0.039101905792616076, Cost (Train): 0.38037066682469806\n",
            "Iteration 511, Norm of Gradient: 0.039059228292432586, Cost (Train): 0.38021818761421944\n",
            "Iteration 512, Norm of Gradient: 0.03901668471435594, Cost (Train): 0.38006604021597434\n",
            "Iteration 513, Norm of Gradient: 0.03897427437272435, Cost (Train): 0.3799142232315917\n",
            "Iteration 514, Norm of Gradient: 0.03893199658661417, Cost (Train): 0.37976273527137505\n",
            "Iteration 515, Norm of Gradient: 0.038889850679799276, Cost (Train): 0.3796115749542329\n",
            "Iteration 516, Norm of Gradient: 0.03884783598071092, Cost (Train): 0.3794607409076093\n",
            "Iteration 517, Norm of Gradient: 0.03880595182239794, Cost (Train): 0.37931023176741463\n",
            "Iteration 518, Norm of Gradient: 0.03876419754248736, Cost (Train): 0.3791600461779589\n",
            "Iteration 519, Norm of Gradient: 0.03872257248314548, Cost (Train): 0.3790101827918833\n",
            "Iteration 520, Norm of Gradient: 0.03868107599103922, Cost (Train): 0.37886064027009414\n",
            "Iteration 521, Norm of Gradient: 0.038639707417297976, Cost (Train): 0.37871141728169705\n",
            "Iteration 522, Norm of Gradient: 0.0385984661174758, Cost (Train): 0.3785625125039311\n",
            "Iteration 523, Norm of Gradient: 0.038557351451514, Cost (Train): 0.3784139246221047\n",
            "Iteration 524, Norm of Gradient: 0.03851636278370403, Cost (Train): 0.3782656523295311\n",
            "Iteration 525, Norm of Gradient: 0.03847549948265088, Cost (Train): 0.37811769432746506\n",
            "Iteration 526, Norm of Gradient: 0.03843476092123671, Cost (Train): 0.3779700493250403\n",
            "Iteration 527, Norm of Gradient: 0.03839414647658494, Cost (Train): 0.3778227160392068\n",
            "Iteration 528, Norm of Gradient: 0.03835365553002464, Cost (Train): 0.37767569319466976\n",
            "Iteration 529, Norm of Gradient: 0.038313287467055285, Cost (Train): 0.3775289795238279\n",
            "Iteration 530, Norm of Gradient: 0.038273041677311895, Cost (Train): 0.37738257376671347\n",
            "Iteration 531, Norm of Gradient: 0.038232917554530496, Cost (Train): 0.37723647467093235\n",
            "Iteration 532, Norm of Gradient: 0.0381929144965139, Cost (Train): 0.37709068099160453\n",
            "Iteration 533, Norm of Gradient: 0.03815303190509786, Cost (Train): 0.37694519149130573\n",
            "Iteration 534, Norm of Gradient: 0.03811326918611758, Cost (Train): 0.37680000494000876\n",
            "Iteration 535, Norm of Gradient: 0.038073625749374417, Cost (Train): 0.37665512011502644\n",
            "Iteration 536, Norm of Gradient: 0.03803410100860319, Cost (Train): 0.37651053580095406\n",
            "Iteration 537, Norm of Gradient: 0.03799469438143947, Cost (Train): 0.37636625078961305\n",
            "Iteration 538, Norm of Gradient: 0.03795540528938746, Cost (Train): 0.3762222638799952\n",
            "Iteration 539, Norm of Gradient: 0.03791623315778806, Cost (Train): 0.37607857387820676\n",
            "Iteration 540, Norm of Gradient: 0.03787717741578724, Cost (Train): 0.37593517959741396\n",
            "Iteration 541, Norm of Gradient: 0.037838237496304834, Cost (Train): 0.37579207985778806\n",
            "Iteration 542, Norm of Gradient: 0.037799412836003486, Cost (Train): 0.3756492734864521\n",
            "Iteration 543, Norm of Gradient: 0.03776070287525801, Cost (Train): 0.3755067593174269\n",
            "Iteration 544, Norm of Gradient: 0.037722107058125016, Cost (Train): 0.3753645361915785\n",
            "Iteration 545, Norm of Gradient: 0.0376836248323128, Cost (Train): 0.37522260295656557\n",
            "Iteration 546, Norm of Gradient: 0.03764525564915159, Cost (Train): 0.3750809584667877\n",
            "Iteration 547, Norm of Gradient: 0.037606998963564, Cost (Train): 0.37493960158333367\n",
            "Iteration 548, Norm of Gradient: 0.03756885423403586, Cost (Train): 0.37479853117393086\n",
            "Iteration 549, Norm of Gradient: 0.037530820922587235, Cost (Train): 0.37465774611289443\n",
            "Iteration 550, Norm of Gradient: 0.037492898494743826, Cost (Train): 0.37451724528107755\n",
            "Iteration 551, Norm of Gradient: 0.03745508641950852, Cost (Train): 0.37437702756582164\n",
            "Iteration 552, Norm of Gradient: 0.03741738416933334, Cost (Train): 0.37423709186090753\n",
            "Iteration 553, Norm of Gradient: 0.037379791220091586, Cost (Train): 0.3740974370665066\n",
            "Iteration 554, Norm of Gradient: 0.03734230705105027, Cost (Train): 0.37395806208913274\n",
            "Iteration 555, Norm of Gradient: 0.03730493114484282, Cost (Train): 0.3738189658415946\n",
            "Iteration 556, Norm of Gradient: 0.03726766298744198, Cost (Train): 0.373680147242948\n",
            "Iteration 557, Norm of Gradient: 0.037230502068133116, Cost (Train): 0.3735416052184495\n",
            "Iteration 558, Norm of Gradient: 0.037193447879487605, Cost (Train): 0.37340333869950965\n",
            "Iteration 559, Norm of Gradient: 0.03715649991733659, Cost (Train): 0.37326534662364724\n",
            "Iteration 560, Norm of Gradient: 0.03711965768074495, Cost (Train): 0.3731276279344433\n",
            "Iteration 561, Norm of Gradient: 0.03708292067198551, Cost (Train): 0.37299018158149627\n",
            "Iteration 562, Norm of Gradient: 0.03704628839651349, Cost (Train): 0.3728530065203772\n",
            "Iteration 563, Norm of Gradient: 0.03700976036294125, Cost (Train): 0.37271610171258507\n",
            "Iteration 564, Norm of Gradient: 0.03697333608301318, Cost (Train): 0.3725794661255033\n",
            "Iteration 565, Norm of Gradient: 0.03693701507158093, Cost (Train): 0.37244309873235576\n",
            "Iteration 566, Norm of Gradient: 0.03690079684657876, Cost (Train): 0.372306998512164\n",
            "Iteration 567, Norm of Gradient: 0.03686468092899923, Cost (Train): 0.372171164449704\n",
            "Iteration 568, Norm of Gradient: 0.03682866684286909, Cost (Train): 0.3720355955354642\n",
            "Iteration 569, Norm of Gradient: 0.03679275411522529, Cost (Train): 0.3719002907656034\n",
            "Iteration 570, Norm of Gradient: 0.03675694227609141, Cost (Train): 0.3717652491419089\n",
            "Iteration 571, Norm of Gradient: 0.036721230858454114, Cost (Train): 0.3716304696717555\n",
            "Iteration 572, Norm of Gradient: 0.036685619398239985, Cost (Train): 0.3714959513680644\n",
            "Iteration 573, Norm of Gradient: 0.036650107434292443, Cost (Train): 0.3713616932492629\n",
            "Iteration 574, Norm of Gradient: 0.036614694508348956, Cost (Train): 0.371227694339244\n",
            "Iteration 575, Norm of Gradient: 0.03657938016501848, Cost (Train): 0.3710939536673269\n",
            "Iteration 576, Norm of Gradient: 0.036544163951759, Cost (Train): 0.37096047026821716\n",
            "Iteration 577, Norm of Gradient: 0.036509045418855454, Cost (Train): 0.37082724318196786\n",
            "Iteration 578, Norm of Gradient: 0.03647402411939764, Cost (Train): 0.3706942714539409\n",
            "Iteration 579, Norm of Gradient: 0.03643909960925854, Cost (Train): 0.3705615541347684\n",
            "Iteration 580, Norm of Gradient: 0.03640427144707271, Cost (Train): 0.3704290902803148\n",
            "Iteration 581, Norm of Gradient: 0.03636953919421492, Cost (Train): 0.3702968789516387\n",
            "Iteration 582, Norm of Gradient: 0.03633490241477896, Cost (Train): 0.37016491921495637\n",
            "Iteration 583, Norm of Gradient: 0.0363003606755567, Cost (Train): 0.3700332101416037\n",
            "Iteration 584, Norm of Gradient: 0.03626591354601724, Cost (Train): 0.369901750808\n",
            "Iteration 585, Norm of Gradient: 0.036231560598286396, Cost (Train): 0.3697705402956113\n",
            "Iteration 586, Norm of Gradient: 0.03619730140712621, Cost (Train): 0.36963957769091477\n",
            "Iteration 587, Norm of Gradient: 0.03616313554991478, Cost (Train): 0.3695088620853621\n",
            "Iteration 588, Norm of Gradient: 0.03612906260662618, Cost (Train): 0.36937839257534477\n",
            "Iteration 589, Norm of Gradient: 0.036095082159810686, Cost (Train): 0.36924816826215856\n",
            "Iteration 590, Norm of Gradient: 0.03606119379457495, Cost (Train): 0.3691181882519686\n",
            "Iteration 591, Norm of Gradient: 0.03602739709856271, Cost (Train): 0.36898845165577504\n",
            "Iteration 592, Norm of Gradient: 0.03599369166193526, Cost (Train): 0.3688589575893787\n",
            "Iteration 593, Norm of Gradient: 0.03596007707735244, Cost (Train): 0.36872970517334713\n",
            "Iteration 594, Norm of Gradient: 0.03592655293995364, Cost (Train): 0.36860069353298075\n",
            "Iteration 595, Norm of Gradient: 0.03589311884733897, Cost (Train): 0.36847192179827987\n",
            "Iteration 596, Norm of Gradient: 0.03585977439955063, Cost (Train): 0.3683433891039113\n",
            "Iteration 597, Norm of Gradient: 0.03582651919905448, Cost (Train): 0.3682150945891756\n",
            "Iteration 598, Norm of Gradient: 0.03579335285072172, Cost (Train): 0.36808703739797466\n",
            "Iteration 599, Norm of Gradient: 0.035760274961810765, Cost (Train): 0.3679592166787794\n",
            "Iteration 600, Norm of Gradient: 0.03572728514194922, Cost (Train): 0.3678316315845976\n",
            "Iteration 601, Norm of Gradient: 0.035694383003116184, Cost (Train): 0.3677042812729427\n",
            "Iteration 602, Norm of Gradient: 0.0356615681596245, Cost (Train): 0.36757716490580206\n",
            "Iteration 603, Norm of Gradient: 0.035628840228103295, Cost (Train): 0.3674502816496056\n",
            "Iteration 604, Norm of Gradient: 0.03559619882748064, Cost (Train): 0.3673236306751955\n",
            "Iteration 605, Norm of Gradient: 0.03556364357896639, Cost (Train): 0.3671972111577949\n",
            "Iteration 606, Norm of Gradient: 0.03553117410603509, Cost (Train): 0.3670710222769783\n",
            "Iteration 607, Norm of Gradient: 0.03549879003440918, Cost (Train): 0.36694506321664055\n",
            "Iteration 608, Norm of Gradient: 0.03546649099204221, Cost (Train): 0.3668193331649679\n",
            "Iteration 609, Norm of Gradient: 0.03543427660910225, Cost (Train): 0.3666938313144078\n",
            "Iteration 610, Norm of Gradient: 0.03540214651795551, Cost (Train): 0.3665685568616398\n",
            "Iteration 611, Norm of Gradient: 0.03537010035314998, Cost (Train): 0.3664435090075464\n",
            "Iteration 612, Norm of Gradient: 0.03533813775139936, Cost (Train): 0.36631868695718434\n",
            "Iteration 613, Norm of Gradient: 0.03530625835156697, Cost (Train): 0.3661940899197554\n",
            "Iteration 614, Norm of Gradient: 0.03527446179464997, Cost (Train): 0.36606971710857905\n",
            "Iteration 615, Norm of Gradient: 0.03524274772376356, Cost (Train): 0.3659455677410634\n",
            "Iteration 616, Norm of Gradient: 0.03521111578412546, Cost (Train): 0.3658216410386777\n",
            "Iteration 617, Norm of Gradient: 0.03517956562304042, Cost (Train): 0.36569793622692487\n",
            "Iteration 618, Norm of Gradient: 0.0351480968898849, Cost (Train): 0.3655744525353134\n",
            "Iteration 619, Norm of Gradient: 0.0351167092360919, Cost (Train): 0.365451189197331\n",
            "Iteration 620, Norm of Gradient: 0.03508540231513594, Cost (Train): 0.36532814545041686\n",
            "Iteration 621, Norm of Gradient: 0.035054175782518085, Cost (Train): 0.3652053205359355\n",
            "Iteration 622, Norm of Gradient: 0.0350230292957512, Cost (Train): 0.36508271369914974\n",
            "Iteration 623, Norm of Gradient: 0.034991962514345255, Cost (Train): 0.3649603241891948\n",
            "Iteration 624, Norm of Gradient: 0.034960975099792846, Cost (Train): 0.36483815125905195\n",
            "Iteration 625, Norm of Gradient: 0.03493006671555471, Cost (Train): 0.36471619416552264\n",
            "Iteration 626, Norm of Gradient: 0.03489923702704551, Cost (Train): 0.3645944521692032\n",
            "Iteration 627, Norm of Gradient: 0.03486848570161965, Cost (Train): 0.36447292453445873\n",
            "Iteration 628, Norm of Gradient: 0.03483781240855722, Cost (Train): 0.3643516105293983\n",
            "Iteration 629, Norm of Gradient: 0.034807216819050094, Cost (Train): 0.36423050942585006\n",
            "Iteration 630, Norm of Gradient: 0.03477669860618817, Cost (Train): 0.364109620499336\n",
            "Iteration 631, Norm of Gradient: 0.03474625744494561, Cost (Train): 0.3639889430290474\n",
            "Iteration 632, Norm of Gradient: 0.03471589301216736, Cost (Train): 0.3638684762978207\n",
            "Iteration 633, Norm of Gradient: 0.03468560498655564, Cost (Train): 0.36374821959211306\n",
            "Iteration 634, Norm of Gradient: 0.03465539304865666, Cost (Train): 0.3636281722019783\n",
            "Iteration 635, Norm of Gradient: 0.03462525688084738, Cost (Train): 0.3635083334210431\n",
            "Iteration 636, Norm of Gradient: 0.034595196167322415, Cost (Train): 0.3633887025464835\n",
            "Iteration 637, Norm of Gradient: 0.03456521059408104, Cost (Train): 0.36326927887900096\n",
            "Iteration 638, Norm of Gradient: 0.034535299848914285, Cost (Train): 0.3631500617228\n",
            "Iteration 639, Norm of Gradient: 0.03450546362139223, Cost (Train): 0.3630310503855642\n",
            "Iteration 640, Norm of Gradient: 0.03447570160285125, Cost (Train): 0.36291224417843393\n",
            "Iteration 641, Norm of Gradient: 0.034446013486381544, Cost (Train): 0.3627936424159834\n",
            "Iteration 642, Norm of Gradient: 0.0344163989668146, Cost (Train): 0.36267524441619803\n",
            "Iteration 643, Norm of Gradient: 0.03438685774071094, Cost (Train): 0.3625570495004525\n",
            "Iteration 644, Norm of Gradient: 0.0343573895063478, Cost (Train): 0.3624390569934881\n",
            "Iteration 645, Norm of Gradient: 0.034327993963707046, Cost (Train): 0.3623212662233914\n",
            "Iteration 646, Norm of Gradient: 0.03429867081446309, Cost (Train): 0.36220367652157154\n",
            "Iteration 647, Norm of Gradient: 0.03426941976197103, Cost (Train): 0.3620862872227397\n",
            "Iteration 648, Norm of Gradient: 0.034240240511254696, Cost (Train): 0.36196909766488666\n",
            "Iteration 649, Norm of Gradient: 0.03421113276899506, Cost (Train): 0.36185210718926203\n",
            "Iteration 650, Norm of Gradient: 0.034182096243518485, Cost (Train): 0.3617353151403531\n",
            "Iteration 651, Norm of Gradient: 0.03415313064478521, Cost (Train): 0.36161872086586383\n",
            "Iteration 652, Norm of Gradient: 0.03412423568437795, Cost (Train): 0.361502323716694\n",
            "Iteration 653, Norm of Gradient: 0.0340954110754905, Cost (Train): 0.3613861230469187\n",
            "Iteration 654, Norm of Gradient: 0.03406665653291653, Cost (Train): 0.3612701182137679\n",
            "Iteration 655, Norm of Gradient: 0.03403797177303834, Cost (Train): 0.361154308577606\n",
            "Iteration 656, Norm of Gradient: 0.034009356513815904, Cost (Train): 0.36103869350191187\n",
            "Iteration 657, Norm of Gradient: 0.03398081047477579, Cost (Train): 0.360923272353259\n",
            "Iteration 658, Norm of Gradient: 0.033952333377000335, Cost (Train): 0.36080804450129506\n",
            "Iteration 659, Norm of Gradient: 0.03392392494311686, Cost (Train): 0.3606930093187231\n",
            "Iteration 660, Norm of Gradient: 0.03389558489728691, Cost (Train): 0.36057816618128147\n",
            "Iteration 661, Norm of Gradient: 0.03386731296519567, Cost (Train): 0.3604635144677246\n",
            "Iteration 662, Norm of Gradient: 0.033839108874041464, Cost (Train): 0.360349053559804\n",
            "Iteration 663, Norm of Gradient: 0.033810972352525254, Cost (Train): 0.36023478284224886\n",
            "Iteration 664, Norm of Gradient: 0.033782903130840286, Cost (Train): 0.3601207017027474\n",
            "Iteration 665, Norm of Gradient: 0.03375490094066189, Cost (Train): 0.3600068095319282\n",
            "Iteration 666, Norm of Gradient: 0.033726965515137215, Cost (Train): 0.35989310572334116\n",
            "Iteration 667, Norm of Gradient: 0.03369909658887513, Cost (Train): 0.35977958967343965\n",
            "Iteration 668, Norm of Gradient: 0.03367129389793627, Cost (Train): 0.35966626078156166\n",
            "Iteration 669, Norm of Gradient: 0.033643557179823, Cost (Train): 0.35955311844991195\n",
            "Iteration 670, Norm of Gradient: 0.03361588617346964, Cost (Train): 0.35944016208354373\n",
            "Iteration 671, Norm of Gradient: 0.03358828061923262, Cost (Train): 0.35932739109034123\n",
            "Iteration 672, Norm of Gradient: 0.033560740258880846, Cost (Train): 0.3592148048810014\n",
            "Iteration 673, Norm of Gradient: 0.033533264835585976, Cost (Train): 0.3591024028690165\n",
            "Iteration 674, Norm of Gradient: 0.033505854093913025, Cost (Train): 0.358990184470657\n",
            "Iteration 675, Norm of Gradient: 0.033478507779810755, Cost (Train): 0.3588781491049535\n",
            "Iteration 676, Norm of Gradient: 0.033451225640602385, Cost (Train): 0.3587662961936801\n",
            "Iteration 677, Norm of Gradient: 0.03342400742497622, Cost (Train): 0.3586546251613372\n",
            "Iteration 678, Norm of Gradient: 0.03339685288297644, Cost (Train): 0.3585431354351341\n",
            "Iteration 679, Norm of Gradient: 0.03336976176599394, Cost (Train): 0.35843182644497285\n",
            "Iteration 680, Norm of Gradient: 0.03334273382675724, Cost (Train): 0.3583206976234311\n",
            "Iteration 681, Norm of Gradient: 0.03331576881932341, Cost (Train): 0.3582097484057458\n",
            "Iteration 682, Norm of Gradient: 0.03328886649906924, Cost (Train): 0.35809897822979647\n",
            "Iteration 683, Norm of Gradient: 0.03326202662268227, Cost (Train): 0.35798838653608916\n",
            "Iteration 684, Norm of Gradient: 0.03323524894815201, Cost (Train): 0.3578779727677401\n",
            "Iteration 685, Norm of Gradient: 0.03320853323476125, Cost (Train): 0.35776773637045944\n",
            "Iteration 686, Norm of Gradient: 0.03318187924307735, Cost (Train): 0.35765767679253596\n",
            "Iteration 687, Norm of Gradient: 0.03315528673494367, Cost (Train): 0.3575477934848205\n",
            "Iteration 688, Norm of Gradient: 0.033128755473471036, Cost (Train): 0.35743808590071063\n",
            "Iteration 689, Norm of Gradient: 0.03310228522302932, Cost (Train): 0.35732855349613496\n",
            "Iteration 690, Norm of Gradient: 0.03307587574923898, Cost (Train): 0.3572191957295378\n",
            "Iteration 691, Norm of Gradient: 0.03304952681896281, Cost (Train): 0.35711001206186355\n",
            "Iteration 692, Norm of Gradient: 0.03302323820029766, Cost (Train): 0.35700100195654166\n",
            "Iteration 693, Norm of Gradient: 0.03299700966256618, Cost (Train): 0.3568921648794717\n",
            "Iteration 694, Norm of Gradient: 0.03297084097630885, Cost (Train): 0.3567835002990075\n",
            "Iteration 695, Norm of Gradient: 0.03294473191327574, Cost (Train): 0.3566750076859435\n",
            "Iteration 696, Norm of Gradient: 0.03291868224641862, Cost (Train): 0.356566686513499\n",
            "Iteration 697, Norm of Gradient: 0.032892691749883005, Cost (Train): 0.35645853625730367\n",
            "Iteration 698, Norm of Gradient: 0.03286676019900026, Cost (Train): 0.3563505563953833\n",
            "Iteration 699, Norm of Gradient: 0.03284088737027984, Cost (Train): 0.35624274640814513\n",
            "Iteration 700, Norm of Gradient: 0.03281507304140145, Cost (Train): 0.35613510577836344\n",
            "Iteration 701, Norm of Gradient: 0.032789316991207455, Cost (Train): 0.35602763399116527\n",
            "Iteration 702, Norm of Gradient: 0.03276361899969517, Cost (Train): 0.35592033053401656\n",
            "Iteration 703, Norm of Gradient: 0.03273797884800937, Cost (Train): 0.3558131948967078\n",
            "Iteration 704, Norm of Gradient: 0.03271239631843471, Cost (Train): 0.3557062265713404\n",
            "Iteration 705, Norm of Gradient: 0.03268687119438829, Cost (Train): 0.3555994250523122\n",
            "Iteration 706, Norm of Gradient: 0.03266140326041231, Cost (Train): 0.35549278983630495\n",
            "Iteration 707, Norm of Gradient: 0.03263599230216667, Cost (Train): 0.35538632042226925\n",
            "Iteration 708, Norm of Gradient: 0.03261063810642173, Cost (Train): 0.35528001631141226\n",
            "Iteration 709, Norm of Gradient: 0.032585340461051084, Cost (Train): 0.3551738770071834\n",
            "Iteration 710, Norm of Gradient: 0.03256009915502435, Cost (Train): 0.35506790201526156\n",
            "Iteration 711, Norm of Gradient: 0.03253491397840015, Cost (Train): 0.3549620908435415\n",
            "Iteration 712, Norm of Gradient: 0.032509784722318975, Cost (Train): 0.354856443002121\n",
            "Iteration 713, Norm of Gradient: 0.03248471117899618, Cost (Train): 0.35475095800328765\n",
            "Iteration 714, Norm of Gradient: 0.03245969314171511, Cost (Train): 0.3546456353615061\n",
            "Iteration 715, Norm of Gradient: 0.03243473040482015, Cost (Train): 0.3545404745934048\n",
            "Iteration 716, Norm of Gradient: 0.03240982276370988, Cost (Train): 0.35443547521776375\n",
            "Iteration 717, Norm of Gradient: 0.03238497001483033, Cost (Train): 0.3543306367555014\n",
            "Iteration 718, Norm of Gradient: 0.032360171955668184, Cost (Train): 0.3542259587296625\n",
            "Iteration 719, Norm of Gradient: 0.03233542838474415, Cost (Train): 0.35412144066540524\n",
            "Iteration 720, Norm of Gradient: 0.03231073910160631, Cost (Train): 0.35401708208998894\n",
            "Iteration 721, Norm of Gradient: 0.03228610390682353, Cost (Train): 0.3539128825327621\n",
            "Iteration 722, Norm of Gradient: 0.03226152260197892, Cost (Train): 0.35380884152514963\n",
            "Iteration 723, Norm of Gradient: 0.03223699498966337, Cost (Train): 0.3537049586006411\n",
            "Iteration 724, Norm of Gradient: 0.03221252087346913, Cost (Train): 0.3536012332947788\n",
            "Iteration 725, Norm of Gradient: 0.03218810005798337, Cost (Train): 0.35349766514514525\n",
            "Iteration 726, Norm of Gradient: 0.03216373234878192, Cost (Train): 0.35339425369135186\n",
            "Iteration 727, Norm of Gradient: 0.03213941755242291, Cost (Train): 0.35329099847502715\n",
            "Iteration 728, Norm of Gradient: 0.03211515547644058, Cost (Train): 0.3531878990398046\n",
            "Iteration 729, Norm of Gradient: 0.03209094592933908, Cost (Train): 0.3530849549313114\n",
            "Iteration 730, Norm of Gradient: 0.032066788720586305, Cost (Train): 0.35298216569715685\n",
            "Iteration 731, Norm of Gradient: 0.03204268366060782, Cost (Train): 0.35287953088692103\n",
            "Iteration 732, Norm of Gradient: 0.03201863056078078, Cost (Train): 0.3527770500521431\n",
            "Iteration 733, Norm of Gradient: 0.03199462923342796, Cost (Train): 0.35267472274631034\n",
            "Iteration 734, Norm of Gradient: 0.031970679491811746, Cost (Train): 0.3525725485248468\n",
            "Iteration 735, Norm of Gradient: 0.03194678115012825, Cost (Train): 0.3524705269451023\n",
            "Iteration 736, Norm of Gradient: 0.03192293402350145, Cost (Train): 0.35236865756634106\n",
            "Iteration 737, Norm of Gradient: 0.03189913792797732, Cost (Train): 0.35226693994973124\n",
            "Iteration 738, Norm of Gradient: 0.03187539268051806, Cost (Train): 0.3521653736583336\n",
            "Iteration 739, Norm of Gradient: 0.031851698098996406, Cost (Train): 0.3520639582570908\n",
            "Iteration 740, Norm of Gradient: 0.031828054002189826, Cost (Train): 0.351962693312817\n",
            "Iteration 741, Norm of Gradient: 0.031804460209774954, Cost (Train): 0.3518615783941866\n",
            "Iteration 742, Norm of Gradient: 0.031780916542321964, Cost (Train): 0.3517606130717241\n",
            "Iteration 743, Norm of Gradient: 0.03175742282128894, Cost (Train): 0.35165979691779364\n",
            "Iteration 744, Norm of Gradient: 0.03173397886901641, Cost (Train): 0.35155912950658813\n",
            "Iteration 745, Norm of Gradient: 0.03171058450872184, Cost (Train): 0.35145861041411924\n",
            "Iteration 746, Norm of Gradient: 0.031687239564494145, Cost (Train): 0.35135823921820697\n",
            "Iteration 747, Norm of Gradient: 0.031663943861288346, Cost (Train): 0.3512580154984696\n",
            "Iteration 748, Norm of Gradient: 0.03164069722492015, Cost (Train): 0.351157938836313\n",
            "Iteration 749, Norm of Gradient: 0.03161749948206064, Cost (Train): 0.35105800881492166\n",
            "Iteration 750, Norm of Gradient: 0.031594350460230985, Cost (Train): 0.3509582250192475\n",
            "Iteration 751, Norm of Gradient: 0.031571249987797194, Cost (Train): 0.3508585870360003\n",
            "Iteration 752, Norm of Gradient: 0.03154819789396491, Cost (Train): 0.3507590944536383\n",
            "Iteration 753, Norm of Gradient: 0.03152519400877419, Cost (Train): 0.3506597468623578\n",
            "Iteration 754, Norm of Gradient: 0.03150223816309446, Cost (Train): 0.35056054385408375\n",
            "Iteration 755, Norm of Gradient: 0.03147933018861933, Cost (Train): 0.35046148502246\n",
            "Iteration 756, Norm of Gradient: 0.03145646991786157, Cost (Train): 0.3503625699628397\n",
            "Iteration 757, Norm of Gradient: 0.0314336571841481, Cost (Train): 0.3502637982722757\n",
            "Iteration 758, Norm of Gradient: 0.03141089182161499, Cost (Train): 0.3501651695495114\n",
            "Iteration 759, Norm of Gradient: 0.031388173665202496, Cost (Train): 0.35006668339497066\n",
            "Iteration 760, Norm of Gradient: 0.03136550255065015, Cost (Train): 0.3499683394107491\n",
            "Iteration 761, Norm of Gradient: 0.031342878314491916, Cost (Train): 0.3498701372006049\n",
            "Iteration 762, Norm of Gradient: 0.031320300794051334, Cost (Train): 0.34977207636994895\n",
            "Iteration 763, Norm of Gradient: 0.0312977698274367, Cost (Train): 0.3496741565258361\n",
            "Iteration 764, Norm of Gradient: 0.03127528525353625, Cost (Train): 0.3495763772769564\n",
            "Iteration 765, Norm of Gradient: 0.03125284691201352, Cost (Train): 0.34947873823362513\n",
            "Iteration 766, Norm of Gradient: 0.03123045464330259, Cost (Train): 0.3493812390077749\n",
            "Iteration 767, Norm of Gradient: 0.031208108288603407, Cost (Train): 0.3492838792129462\n",
            "Iteration 768, Norm of Gradient: 0.03118580768987716, Cost (Train): 0.3491866584642784\n",
            "Iteration 769, Norm of Gradient: 0.031163552689841684, Cost (Train): 0.3490895763785015\n",
            "Iteration 770, Norm of Gradient: 0.03114134313196689, Cost (Train): 0.348992632573927\n",
            "Iteration 771, Norm of Gradient: 0.031119178860470234, Cost (Train): 0.3488958266704392\n",
            "Iteration 772, Norm of Gradient: 0.031097059720312188, Cost (Train): 0.34879915828948704\n",
            "Iteration 773, Norm of Gradient: 0.031074985557191813, Cost (Train): 0.34870262705407506\n",
            "Iteration 774, Norm of Gradient: 0.031052956217542275, Cost (Train): 0.3486062325887553\n",
            "Iteration 775, Norm of Gradient: 0.031030971548526475, Cost (Train): 0.34850997451961846\n",
            "Iteration 776, Norm of Gradient: 0.031009031398032655, Cost (Train): 0.348413852474286\n",
            "Iteration 777, Norm of Gradient: 0.030987135614670064, Cost (Train): 0.3483178660819015\n",
            "Iteration 778, Norm of Gradient: 0.03096528404776464, Cost (Train): 0.34822201497312233\n",
            "Iteration 779, Norm of Gradient: 0.030943476547354718, Cost (Train): 0.3481262987801119\n",
            "Iteration 780, Norm of Gradient: 0.030921712964186818, Cost (Train): 0.34803071713653105\n",
            "Iteration 781, Norm of Gradient: 0.03089999314971136, Cost (Train): 0.34793526967753\n",
            "Iteration 782, Norm of Gradient: 0.030878316956078553, Cost (Train): 0.34783995603974066\n",
            "Iteration 783, Norm of Gradient: 0.030856684236134156, Cost (Train): 0.3477447758612683\n",
            "Iteration 784, Norm of Gradient: 0.030835094843415397, Cost (Train): 0.34764972878168365\n",
            "Iteration 785, Norm of Gradient: 0.030813548632146855, Cost (Train): 0.34755481444201525\n",
            "Iteration 786, Norm of Gradient: 0.0307920454572364, Cost (Train): 0.34746003248474133\n",
            "Iteration 787, Norm of Gradient: 0.03077058517427111, Cost (Train): 0.3473653825537822\n",
            "Iteration 788, Norm of Gradient: 0.030749167639513318, Cost (Train): 0.3472708642944926\n",
            "Iteration 789, Norm of Gradient: 0.030727792709896572, Cost (Train): 0.3471764773536538\n",
            "Iteration 790, Norm of Gradient: 0.030706460243021688, Cost (Train): 0.34708222137946604\n",
            "Iteration 791, Norm of Gradient: 0.03068517009715284, Cost (Train): 0.3469880960215411\n",
            "Iteration 792, Norm of Gradient: 0.03066392213121364, Cost (Train): 0.3468941009308949\n",
            "Iteration 793, Norm of Gradient: 0.030642716204783243, Cost (Train): 0.34680023575993935\n",
            "Iteration 794, Norm of Gradient: 0.030621552178092565, Cost (Train): 0.34670650016247584\n",
            "Iteration 795, Norm of Gradient: 0.030600429912020367, Cost (Train): 0.346612893793687\n",
            "Iteration 796, Norm of Gradient: 0.030579349268089542, Cost (Train): 0.3465194163101302\n",
            "Iteration 797, Norm of Gradient: 0.030558310108463306, Cost (Train): 0.34642606736972964\n",
            "Iteration 798, Norm of Gradient: 0.030537312295941463, Cost (Train): 0.3463328466317693\n",
            "Iteration 799, Norm of Gradient: 0.030516355693956682, Cost (Train): 0.34623975375688587\n",
            "Iteration 800, Norm of Gradient: 0.030495440166570832, Cost (Train): 0.34614678840706153\n",
            "Iteration 801, Norm of Gradient: 0.0304745655784713, Cost (Train): 0.3460539502456171\n",
            "Iteration 802, Norm of Gradient: 0.030453731794967343, Cost (Train): 0.3459612389372042\n",
            "Iteration 803, Norm of Gradient: 0.030432938681986495, Cost (Train): 0.34586865414779944\n",
            "Iteration 804, Norm of Gradient: 0.030412186106070965, Cost (Train): 0.34577619554469646\n",
            "Iteration 805, Norm of Gradient: 0.03039147393437408, Cost (Train): 0.3456838627964995\n",
            "Iteration 806, Norm of Gradient: 0.030370802034656755, Cost (Train): 0.34559165557311655\n",
            "Iteration 807, Norm of Gradient: 0.030350170275283955, Cost (Train): 0.34549957354575217\n",
            "Iteration 808, Norm of Gradient: 0.03032957852522123, Cost (Train): 0.3454076163869014\n",
            "Iteration 809, Norm of Gradient: 0.030309026654031238, Cost (Train): 0.3453157837703422\n",
            "Iteration 810, Norm of Gradient: 0.030288514531870297, Cost (Train): 0.3452240753711293\n",
            "Iteration 811, Norm of Gradient: 0.030268042029484982, Cost (Train): 0.34513249086558767\n",
            "Iteration 812, Norm of Gradient: 0.030247609018208717, Cost (Train): 0.34504102993130525\n",
            "Iteration 813, Norm of Gradient: 0.03022721536995842, Cost (Train): 0.34494969224712707\n",
            "Iteration 814, Norm of Gradient: 0.030206860957231126, Cost (Train): 0.3448584774931485\n",
            "Iteration 815, Norm of Gradient: 0.030186545653100708, Cost (Train): 0.3447673853507087\n",
            "Iteration 816, Norm of Gradient: 0.03016626933121453, Cost (Train): 0.3446764155023842\n",
            "Iteration 817, Norm of Gradient: 0.030146031865790183, Cost (Train): 0.34458556763198256\n",
            "Iteration 818, Norm of Gradient: 0.03012583313161225, Cost (Train): 0.3444948414245362\n",
            "Iteration 819, Norm of Gradient: 0.03010567300402903, Cost (Train): 0.3444042365662957\n",
            "Iteration 820, Norm of Gradient: 0.030085551358949365, Cost (Train): 0.34431375274472376\n",
            "Iteration 821, Norm of Gradient: 0.03006546807283941, Cost (Train): 0.34422338964848903\n",
            "Iteration 822, Norm of Gradient: 0.030045423022719512, Cost (Train): 0.34413314696745984\n",
            "Iteration 823, Norm of Gradient: 0.03002541608616102, Cost (Train): 0.34404302439269785\n",
            "Iteration 824, Norm of Gradient: 0.030005447141283167, Cost (Train): 0.3439530216164525\n",
            "Iteration 825, Norm of Gradient: 0.029985516066749995, Cost (Train): 0.34386313833215415\n",
            "Iteration 826, Norm of Gradient: 0.029965622741767235, Cost (Train): 0.34377337423440896\n",
            "Iteration 827, Norm of Gradient: 0.02994576704607926, Cost (Train): 0.34368372901899186\n",
            "Iteration 828, Norm of Gradient: 0.029925948859966033, Cost (Train): 0.34359420238284144\n",
            "Iteration 829, Norm of Gradient: 0.029906168064240124, Cost (Train): 0.34350479402405387\n",
            "Iteration 830, Norm of Gradient: 0.02988642454024364, Cost (Train): 0.3434155036418765\n",
            "Iteration 831, Norm of Gradient: 0.029866718169845298, Cost (Train): 0.3433263309367027\n",
            "Iteration 832, Norm of Gradient: 0.02984704883543746, Cost (Train): 0.3432372756100655\n",
            "Iteration 833, Norm of Gradient: 0.029827416419933164, Cost (Train): 0.34314833736463224\n",
            "Iteration 834, Norm of Gradient: 0.029807820806763224, Cost (Train): 0.3430595159041986\n",
            "Iteration 835, Norm of Gradient: 0.029788261879873334, Cost (Train): 0.342970810933683\n",
            "Iteration 836, Norm of Gradient: 0.029768739523721163, Cost (Train): 0.3428822221591206\n",
            "Iteration 837, Norm of Gradient: 0.029749253623273506, Cost (Train): 0.3427937492876584\n",
            "Iteration 838, Norm of Gradient: 0.029729804064003447, Cost (Train): 0.34270539202754885\n",
            "Iteration 839, Norm of Gradient: 0.029710390731887523, Cost (Train): 0.3426171500881448\n",
            "Iteration 840, Norm of Gradient: 0.029691013513402923, Cost (Train): 0.3425290231798938\n",
            "Iteration 841, Norm of Gradient: 0.029671672295524695, Cost (Train): 0.3424410110143326\n",
            "Iteration 842, Norm of Gradient: 0.029652366965722998, Cost (Train): 0.34235311330408175\n",
            "Iteration 843, Norm of Gradient: 0.02963309741196032, Cost (Train): 0.34226532976284013\n",
            "Iteration 844, Norm of Gradient: 0.02961386352268877, Cost (Train): 0.3421776601053796\n",
            "Iteration 845, Norm of Gradient: 0.02959466518684736, Cost (Train): 0.34209010404753964\n",
            "Iteration 846, Norm of Gradient: 0.029575502293859315, Cost (Train): 0.3420026613062219\n",
            "Iteration 847, Norm of Gradient: 0.02955637473362936, Cost (Train): 0.34191533159938525\n",
            "Iteration 848, Norm of Gradient: 0.029537282396541124, Cost (Train): 0.3418281146460401\n",
            "Iteration 849, Norm of Gradient: 0.02951822517345444, Cost (Train): 0.34174101016624353\n",
            "Iteration 850, Norm of Gradient: 0.029499202955702738, Cost (Train): 0.34165401788109384\n",
            "Iteration 851, Norm of Gradient: 0.029480215635090467, Cost (Train): 0.3415671375127255\n",
            "Iteration 852, Norm of Gradient: 0.029461263103890446, Cost (Train): 0.3414803687843041\n",
            "Iteration 853, Norm of Gradient: 0.029442345254841358, Cost (Train): 0.341393711420021\n",
            "Iteration 854, Norm of Gradient: 0.02942346198114513, Cost (Train): 0.34130716514508874\n",
            "Iteration 855, Norm of Gradient: 0.02940461317646444, Cost (Train): 0.34122072968573547\n",
            "Iteration 856, Norm of Gradient: 0.029385798734920174, Cost (Train): 0.3411344047692002\n",
            "Iteration 857, Norm of Gradient: 0.029367018551088937, Cost (Train): 0.34104819012372783\n",
            "Iteration 858, Norm of Gradient: 0.029348272520000513, Cost (Train): 0.34096208547856427\n",
            "Iteration 859, Norm of Gradient: 0.02932956053713547, Cost (Train): 0.34087609056395135\n",
            "Iteration 860, Norm of Gradient: 0.02931088249842264, Cost (Train): 0.340790205111122\n",
            "Iteration 861, Norm of Gradient: 0.0292922383002367, Cost (Train): 0.3407044288522957\n",
            "Iteration 862, Norm of Gradient: 0.029273627839395744, Cost (Train): 0.34061876152067294\n",
            "Iteration 863, Norm of Gradient: 0.029255051013158884, Cost (Train): 0.3405332028504312\n",
            "Iteration 864, Norm of Gradient: 0.029236507719223852, Cost (Train): 0.34044775257671983\n",
            "Iteration 865, Norm of Gradient: 0.029217997855724598, Cost (Train): 0.3403624104356551\n",
            "Iteration 866, Norm of Gradient: 0.02919952132122895, Cost (Train): 0.34027717616431624\n",
            "Iteration 867, Norm of Gradient: 0.02918107801473628, Cost (Train): 0.3401920495007397\n",
            "Iteration 868, Norm of Gradient: 0.02916266783567514, Cost (Train): 0.34010703018391536\n",
            "Iteration 869, Norm of Gradient: 0.029144290683900967, Cost (Train): 0.3400221179537816\n",
            "Iteration 870, Norm of Gradient: 0.02912594645969377, Cost (Train): 0.33993731255122056\n",
            "Iteration 871, Norm of Gradient: 0.029107635063755848, Cost (Train): 0.3398526137180539\n",
            "Iteration 872, Norm of Gradient: 0.029089356397209516, Cost (Train): 0.339768021197038\n",
            "Iteration 873, Norm of Gradient: 0.02907111036159484, Cost (Train): 0.3396835347318595\n",
            "Iteration 874, Norm of Gradient: 0.029052896858867434, Cost (Train): 0.3395991540671309\n",
            "Iteration 875, Norm of Gradient: 0.029034715791396146, Cost (Train): 0.3395148789483861\n",
            "Iteration 876, Norm of Gradient: 0.029016567061960936, Cost (Train): 0.3394307091220758\n",
            "Iteration 877, Norm of Gradient: 0.02899845057375063, Cost (Train): 0.3393466443355632\n",
            "Iteration 878, Norm of Gradient: 0.02898036623036073, Cost (Train): 0.33926268433711987\n",
            "Iteration 879, Norm of Gradient: 0.02896231393579126, Cost (Train): 0.3391788288759209\n",
            "Iteration 880, Norm of Gradient: 0.028944293594444597, Cost (Train): 0.3390950777020409\n",
            "Iteration 881, Norm of Gradient: 0.028926305111123325, Cost (Train): 0.3390114305664498\n",
            "Iteration 882, Norm of Gradient: 0.028908348391028127, Cost (Train): 0.3389278872210081\n",
            "Iteration 883, Norm of Gradient: 0.028890423339755625, Cost (Train): 0.33884444741846326\n",
            "Iteration 884, Norm of Gradient: 0.028872529863296324, Cost (Train): 0.33876111091244493\n",
            "Iteration 885, Norm of Gradient: 0.028854667868032487, Cost (Train): 0.338677877457461\n",
            "Iteration 886, Norm of Gradient: 0.02883683726073608, Cost (Train): 0.33859474680889345\n",
            "Iteration 887, Norm of Gradient: 0.028819037948566693, Cost (Train): 0.3385117187229941\n",
            "Iteration 888, Norm of Gradient: 0.028801269839069495, Cost (Train): 0.33842879295688055\n",
            "Iteration 889, Norm of Gradient: 0.02878353284017321, Cost (Train): 0.33834596926853205\n",
            "Iteration 890, Norm of Gradient: 0.02876582686018807, Cost (Train): 0.3382632474167853\n",
            "Iteration 891, Norm of Gradient: 0.028748151807803806, Cost (Train): 0.3381806271613308\n",
            "Iteration 892, Norm of Gradient: 0.028730507592087683, Cost (Train): 0.33809810826270825\n",
            "Iteration 893, Norm of Gradient: 0.028712894122482468, Cost (Train): 0.33801569048230296\n",
            "Iteration 894, Norm of Gradient: 0.028695311308804473, Cost (Train): 0.3379333735823418\n",
            "Iteration 895, Norm of Gradient: 0.028677759061241606, Cost (Train): 0.3378511573258891\n",
            "Iteration 896, Norm of Gradient: 0.028660237290351415, Cost (Train): 0.33776904147684284\n",
            "Iteration 897, Norm of Gradient: 0.02864274590705914, Cost (Train): 0.3376870257999307\n",
            "Iteration 898, Norm of Gradient: 0.02862528482265581, Cost (Train): 0.3376051100607062\n",
            "Iteration 899, Norm of Gradient: 0.028607853948796308, Cost (Train): 0.33752329402554465\n",
            "Iteration 900, Norm of Gradient: 0.02859045319749748, Cost (Train): 0.3374415774616396\n",
            "Iteration 901, Norm of Gradient: 0.028573082481136262, Cost (Train): 0.33735996013699887\n",
            "Iteration 902, Norm of Gradient: 0.02855574171244777, Cost (Train): 0.33727844182044064\n",
            "Iteration 903, Norm of Gradient: 0.028538430804523472, Cost (Train): 0.33719702228159\n",
            "Iteration 904, Norm of Gradient: 0.028521149670809296, Cost (Train): 0.3371157012908747\n",
            "Iteration 905, Norm of Gradient: 0.028503898225103838, Cost (Train): 0.3370344786195218\n",
            "Iteration 906, Norm of Gradient: 0.028486676381556476, Cost (Train): 0.3369533540395541\n",
            "Iteration 907, Norm of Gradient: 0.028469484054665605, Cost (Train): 0.33687232732378575\n",
            "Iteration 908, Norm of Gradient: 0.02845232115927677, Cost (Train): 0.33679139824581933\n",
            "Iteration 909, Norm of Gradient: 0.028435187610580934, Cost (Train): 0.33671056658004195\n",
            "Iteration 910, Norm of Gradient: 0.028418083324112647, Cost (Train): 0.33662983210162134\n",
            "Iteration 911, Norm of Gradient: 0.028401008215748293, Cost (Train): 0.3365491945865026\n",
            "Iteration 912, Norm of Gradient: 0.02838396220170432, Cost (Train): 0.3364686538114045\n",
            "Iteration 913, Norm of Gradient: 0.02836694519853548, Cost (Train): 0.3363882095538158\n",
            "Iteration 914, Norm of Gradient: 0.028349957123133117, Cost (Train): 0.33630786159199205\n",
            "Iteration 915, Norm of Gradient: 0.028332997892723394, Cost (Train): 0.3362276097049516\n",
            "Iteration 916, Norm of Gradient: 0.028316067424865615, Cost (Train): 0.3361474536724724\n",
            "Iteration 917, Norm of Gradient: 0.028299165637450493, Cost (Train): 0.33606739327508833\n",
            "Iteration 918, Norm of Gradient: 0.028282292448698444, Cost (Train): 0.335987428294086\n",
            "Iteration 919, Norm of Gradient: 0.028265447777157927, Cost (Train): 0.33590755851150106\n",
            "Iteration 920, Norm of Gradient: 0.02824863154170374, Cost (Train): 0.3358277837101149\n",
            "Iteration 921, Norm of Gradient: 0.028231843661535364, Cost (Train): 0.3357481036734512\n",
            "Iteration 922, Norm of Gradient: 0.028215084056175316, Cost (Train): 0.3356685181857724\n",
            "Iteration 923, Norm of Gradient: 0.028198352645467467, Cost (Train): 0.33558902703207677\n",
            "Iteration 924, Norm of Gradient: 0.028181649349575455, Cost (Train): 0.3355096299980945\n",
            "Iteration 925, Norm of Gradient: 0.02816497408898101, Cost (Train): 0.3354303268702848\n",
            "Iteration 926, Norm of Gradient: 0.02814832678448237, Cost (Train): 0.3353511174358323\n",
            "Iteration 927, Norm of Gradient: 0.028131707357192667, Cost (Train): 0.3352720014826441\n",
            "Iteration 928, Norm of Gradient: 0.028115115728538316, Cost (Train): 0.33519297879934595\n",
            "Iteration 929, Norm of Gradient: 0.028098551820257445, Cost (Train): 0.3351140491752797\n",
            "Iteration 930, Norm of Gradient: 0.028082015554398307, Cost (Train): 0.3350352124004995\n",
            "Iteration 931, Norm of Gradient: 0.028065506853317712, Cost (Train): 0.33495646826576886\n",
            "Iteration 932, Norm of Gradient: 0.02804902563967948, Cost (Train): 0.3348778165625572\n",
            "Iteration 933, Norm of Gradient: 0.02803257183645287, Cost (Train): 0.3347992570830371\n",
            "Iteration 934, Norm of Gradient: 0.028016145366911063, Cost (Train): 0.3347207896200806\n",
            "Iteration 935, Norm of Gradient: 0.027999746154629614, Cost (Train): 0.3346424139672566\n",
            "Iteration 936, Norm of Gradient: 0.027983374123484943, Cost (Train): 0.33456412991882717\n",
            "Iteration 937, Norm of Gradient: 0.027967029197652822, Cost (Train): 0.33448593726974496\n",
            "Iteration 938, Norm of Gradient: 0.027950711301606868, Cost (Train): 0.33440783581564976\n",
            "Iteration 939, Norm of Gradient: 0.02793442036011703, Cost (Train): 0.33432982535286554\n",
            "Iteration 940, Norm of Gradient: 0.02791815629824816, Cost (Train): 0.33425190567839747\n",
            "Iteration 941, Norm of Gradient: 0.027901919041358473, Cost (Train): 0.3341740765899287\n",
            "Iteration 942, Norm of Gradient: 0.027885708515098116, Cost (Train): 0.3340963378858174\n",
            "Iteration 943, Norm of Gradient: 0.02786952464540771, Cost (Train): 0.33401868936509393\n",
            "Iteration 944, Norm of Gradient: 0.027853367358516887, Cost (Train): 0.3339411308274576\n",
            "Iteration 945, Norm of Gradient: 0.027837236580942847, Cost (Train): 0.33386366207327367\n",
            "Iteration 946, Norm of Gradient: 0.02782113223948896, Cost (Train): 0.3337862829035708\n",
            "Iteration 947, Norm of Gradient: 0.02780505426124329, Cost (Train): 0.3337089931200377\n",
            "Iteration 948, Norm of Gradient: 0.027789002573577217, Cost (Train): 0.3336317925250202\n",
            "Iteration 949, Norm of Gradient: 0.027772977104144038, Cost (Train): 0.3335546809215185\n",
            "Iteration 950, Norm of Gradient: 0.027756977780877518, Cost (Train): 0.33347765811318447\n",
            "Iteration 951, Norm of Gradient: 0.027741004531990566, Cost (Train): 0.33340072390431824\n",
            "Iteration 952, Norm of Gradient: 0.027725057285973808, Cost (Train): 0.33332387809986597\n",
            "Iteration 953, Norm of Gradient: 0.027709135971594225, Cost (Train): 0.3332471205054162\n",
            "Iteration 954, Norm of Gradient: 0.027693240517893795, Cost (Train): 0.33317045092719805\n",
            "Iteration 955, Norm of Gradient: 0.02767737085418814, Cost (Train): 0.3330938691720775\n",
            "Iteration 956, Norm of Gradient: 0.027661526910065154, Cost (Train): 0.33301737504755485\n",
            "Iteration 957, Norm of Gradient: 0.027645708615383697, Cost (Train): 0.3329409683617625\n",
            "Iteration 958, Norm of Gradient: 0.02762991590027222, Cost (Train): 0.3328646489234612\n",
            "Iteration 959, Norm of Gradient: 0.027614148695127485, Cost (Train): 0.33278841654203806\n",
            "Iteration 960, Norm of Gradient: 0.027598406930613203, Cost (Train): 0.33271227102750356\n",
            "Iteration 961, Norm of Gradient: 0.02758269053765877, Cost (Train): 0.33263621219048867\n",
            "Iteration 962, Norm of Gradient: 0.02756699944745793, Cost (Train): 0.3325602398422425\n",
            "Iteration 963, Norm of Gradient: 0.027551333591467497, Cost (Train): 0.3324843537946291\n",
            "Iteration 964, Norm of Gradient: 0.02753569290140606, Cost (Train): 0.33240855386012547\n",
            "Iteration 965, Norm of Gradient: 0.027520077309252728, Cost (Train): 0.3323328398518181\n",
            "Iteration 966, Norm of Gradient: 0.027504486747245822, Cost (Train): 0.33225721158340105\n",
            "Iteration 967, Norm of Gradient: 0.02748892114788164, Cost (Train): 0.3321816688691728\n",
            "Iteration 968, Norm of Gradient: 0.027473380443913186, Cost (Train): 0.3321062115240338\n",
            "Iteration 969, Norm of Gradient: 0.02745786456834893, Cost (Train): 0.33203083936348393\n",
            "Iteration 970, Norm of Gradient: 0.02744237345445156, Cost (Train): 0.33195555220362\n",
            "Iteration 971, Norm of Gradient: 0.02742690703573675, Cost (Train): 0.3318803498611327\n",
            "Iteration 972, Norm of Gradient: 0.027411465245971937, Cost (Train): 0.3318052321533046\n",
            "Iteration 973, Norm of Gradient: 0.027396048019175095, Cost (Train): 0.3317301988980072\n",
            "Iteration 974, Norm of Gradient: 0.027380655289613524, Cost (Train): 0.33165524991369877\n",
            "Iteration 975, Norm of Gradient: 0.02736528699180265, Cost (Train): 0.3315803850194212\n",
            "Iteration 976, Norm of Gradient: 0.027349943060504808, Cost (Train): 0.3315056040347984\n",
            "Iteration 977, Norm of Gradient: 0.027334623430728074, Cost (Train): 0.3314309067800329\n",
            "Iteration 978, Norm of Gradient: 0.02731932803772507, Cost (Train): 0.33135629307590386\n",
            "Iteration 979, Norm of Gradient: 0.02730405681699178, Cost (Train): 0.3312817627437646\n",
            "Iteration 980, Norm of Gradient: 0.027288809704266385, Cost (Train): 0.33120731560553984\n",
            "Iteration 981, Norm of Gradient: 0.027273586635528112, Cost (Train): 0.33113295148372374\n",
            "Iteration 982, Norm of Gradient: 0.027258387546996048, Cost (Train): 0.3310586702013768\n",
            "Iteration 983, Norm of Gradient: 0.027243212375127997, Cost (Train): 0.33098447158212413\n",
            "Iteration 984, Norm of Gradient: 0.027228061056619367, Cost (Train): 0.3309103554501527\n",
            "Iteration 985, Norm of Gradient: 0.027212933528401997, Cost (Train): 0.33083632163020876\n",
            "Iteration 986, Norm of Gradient: 0.027197829727643025, Cost (Train): 0.33076236994759595\n",
            "Iteration 987, Norm of Gradient: 0.027182749591743795, Cost (Train): 0.33068850022817253\n",
            "Iteration 988, Norm of Gradient: 0.027167693058338705, Cost (Train): 0.33061471229834927\n",
            "Iteration 989, Norm of Gradient: 0.027152660065294126, Cost (Train): 0.33054100598508673\n",
            "Iteration 990, Norm of Gradient: 0.02713765055070727, Cost (Train): 0.3304673811158936\n",
            "Iteration 991, Norm of Gradient: 0.027122664452905107, Cost (Train): 0.3303938375188235\n",
            "Iteration 992, Norm of Gradient: 0.02710770171044325, Cost (Train): 0.3303203750224737\n",
            "Iteration 993, Norm of Gradient: 0.027092762262104934, Cost (Train): 0.3302469934559818\n",
            "Iteration 994, Norm of Gradient: 0.02707784604689983, Cost (Train): 0.3301736926490242\n",
            "Iteration 995, Norm of Gradient: 0.027062953004063077, Cost (Train): 0.33010047243181345\n",
            "Iteration 996, Norm of Gradient: 0.027048083073054143, Cost (Train): 0.33002733263509615\n",
            "Iteration 997, Norm of Gradient: 0.027033236193555788, Cost (Train): 0.3299542730901507\n",
            "Iteration 998, Norm of Gradient: 0.027018412305473024, Cost (Train): 0.3298812936287849\n",
            "Iteration 999, Norm of Gradient: 0.02700361134893202, Cost (Train): 0.32980839408333396\n",
            "Terminated after 1000 iterations, with norm of the gradient equal to 0.02700361134893202\n",
            "The weight found: [ 0.01943444  0.00976937 -0.10746618 ... -0.17803684 -0.04345314\n",
            "  0.10404273]\n",
            "x_train shape after bias term: (10000, 1877)\n",
            "self.w shape: (1877,)\n",
            "Iteration 0, Norm of Gradient: 0.18548066610835753, Cost (Train): 0.6897397526691151\n",
            "Iteration 1, Norm of Gradient: 0.1821688485949601, Cost (Train): 0.6864455174423687\n",
            "Iteration 2, Norm of Gradient: 0.1796347730066188, Cost (Train): 0.6832380601882981\n",
            "Iteration 3, Norm of Gradient: 0.17755171105241496, Cost (Train): 0.68010214913496\n",
            "Iteration 4, Norm of Gradient: 0.1757321229721201, Cost (Train): 0.6770288088544099\n",
            "Iteration 5, Norm of Gradient: 0.17406925139000481, Cost (Train): 0.6740125651839256\n",
            "Iteration 6, Norm of Gradient: 0.17250260633882591, Cost (Train): 0.6710499060927746\n",
            "Iteration 7, Norm of Gradient: 0.17099799579188552, Cost (Train): 0.6681384223259458\n",
            "Iteration 8, Norm of Gradient: 0.16953610869978353, Cost (Train): 0.6652763277592352\n",
            "Iteration 9, Norm of Gradient: 0.16810603152383768, Cost (Train): 0.6624621917562944\n",
            "Iteration 10, Norm of Gradient: 0.16670158055815967, Cost (Train): 0.6596947898562233\n",
            "Iteration 11, Norm of Gradient: 0.1653192318072988, Cost (Train): 0.65697302049156\n",
            "Iteration 12, Norm of Gradient: 0.16395695419258666, Cost (Train): 0.6542958585416827\n",
            "Iteration 13, Norm of Gradient: 0.16261355255113735, Cost (Train): 0.651662329426871\n",
            "Iteration 14, Norm of Gradient: 0.1612882980072907, Cost (Train): 0.6490714946496638\n",
            "Iteration 15, Norm of Gradient: 0.15998072022265694, Cost (Train): 0.6465224437097764\n",
            "Iteration 16, Norm of Gradient: 0.15869049078935782, Cost (Train): 0.6440142895622907\n",
            "Iteration 17, Norm of Gradient: 0.15741735791995212, Cost (Train): 0.6415461660408589\n",
            "Iteration 18, Norm of Gradient: 0.1561611099962546, Cost (Train): 0.6391172263663062\n",
            "Iteration 19, Norm of Gradient: 0.15492155534536667, Cost (Train): 0.636726642250845\n",
            "Iteration 20, Norm of Gradient: 0.15369851113304914, Cost (Train): 0.6343736033255886\n",
            "Iteration 21, Norm of Gradient: 0.1524917973732043, Cost (Train): 0.6320573167403379\n",
            "Iteration 22, Norm of Gradient: 0.15130123380207436, Cost (Train): 0.6297770068522446\n",
            "Iteration 23, Norm of Gradient: 0.15012663835063395, Cost (Train): 0.627531914957632\n",
            "Iteration 24, Norm of Gradient: 0.14896782650294035, Cost (Train): 0.6253212990422236\n",
            "Iteration 25, Norm of Gradient: 0.14782461114013343, Cost (Train): 0.6231444335366816\n",
            "Iteration 26, Norm of Gradient: 0.14669680264531174, Cost (Train): 0.6210006090708076\n",
            "Iteration 27, Norm of Gradient: 0.1455842091432826, Cost (Train): 0.6188891322233094\n",
            "Iteration 28, Norm of Gradient: 0.1444866368047586, Cost (Train): 0.6168093252659789\n",
            "Iteration 29, Norm of Gradient: 0.14340389017583563, Cost (Train): 0.6147605259021628\n",
            "Iteration 30, Norm of Gradient: 0.1423357725111755, Cost (Train): 0.6127420869999434\n",
            "Iteration 31, Norm of Gradient: 0.1412820860992012, Cost (Train): 0.610753376320722\n",
            "Iteration 32, Norm of Gradient: 0.14024263257317207, Cost (Train): 0.6087937762439909\n",
            "Iteration 33, Norm of Gradient: 0.13921721320512143, Cost (Train): 0.606862683489124\n",
            "Iteration 34, Norm of Gradient: 0.13820562918139165, Cost (Train): 0.6049595088349895\n",
            "Iteration 35, Norm of Gradient: 0.13720768185947246, Cost (Train): 0.6030836768381538\n",
            "Iteration 36, Norm of Gradient: 0.13622317300639045, Cost (Train): 0.6012346255503914\n",
            "Iteration 37, Norm of Gradient: 0.1352519050191939, Cost (Train): 0.5994118062361659\n",
            "Iteration 38, Norm of Gradient: 0.1342936811282298, Cost (Train): 0.5976146830906871\n",
            "Iteration 39, Norm of Gradient: 0.13334830558399183, Cost (Train): 0.5958427329590994\n",
            "Iteration 40, Norm of Gradient: 0.13241558382834656, Cost (Train): 0.5940954450572961\n",
            "Iteration 41, Norm of Gradient: 0.1314953226509561, Cost (Train): 0.5923723206948185\n",
            "Iteration 42, Norm of Gradient: 0.1305873303317031, Cost (Train): 0.5906728730002377\n",
            "Iteration 43, Norm of Gradient: 0.12969141676991142, Cost (Train): 0.5889966266493822\n",
            "Iteration 44, Norm of Gradient: 0.12880739360112972, Cost (Train): 0.5873431175967307\n",
            "Iteration 45, Norm of Gradient: 0.12793507430222265, Cost (Train): 0.5857118928102534\n",
            "Iteration 46, Norm of Gradient: 0.1270742742854862, Cost (Train): 0.5841025100099453\n",
            "Iteration 47, Norm of Gradient: 0.126224810982474, Cost (Train): 0.5825145374102704\n",
            "Iteration 48, Norm of Gradient: 0.12538650391819578, Cost (Train): 0.5809475534666966\n",
            "Iteration 49, Norm of Gradient: 0.1245591747763154, Cost (Train): 0.5794011466264821\n",
            "Iteration 50, Norm of Gradient: 0.12374264745595227, Cost (Train): 0.5778749150838397\n",
            "Iteration 51, Norm of Gradient: 0.12293674812065573, Cost (Train): 0.5763684665395916\n",
            "Iteration 52, Norm of Gradient: 0.1221413052400977, Cost (Train): 0.5748814179653998\n",
            "Iteration 53, Norm of Gradient: 0.12135614962499902, Cost (Train): 0.5734133953726354\n",
            "Iteration 54, Norm of Gradient: 0.1205811144557784, Cost (Train): 0.5719640335859418\n",
            "Iteration 55, Norm of Gradient: 0.11981603530538594, Cost (Train): 0.5705329760215222\n",
            "Iteration 56, Norm of Gradient: 0.11906075015675868, Cost (Train): 0.569119874470168\n",
            "Iteration 57, Norm of Gradient: 0.11831509941530968, Cost (Train): 0.5677243888850372\n",
            "Iteration 58, Norm of Gradient: 0.11757892591683887, Cost (Train): 0.5663461871741728\n",
            "Iteration 59, Norm of Gradient: 0.11685207493123158, Cost (Train): 0.5649849449977471\n",
            "Iteration 60, Norm of Gradient: 0.11613439416228734, Cost (Train): 0.5636403455700001\n",
            "Iteration 61, Norm of Gradient: 0.11542573374400145, Cost (Train): 0.5623120794658435\n",
            "Iteration 62, Norm of Gradient: 0.11472594623360234, Cost (Train): 0.5609998444320791\n",
            "Iteration 63, Norm of Gradient: 0.11403488660162667, Cost (Train): 0.5597033452031894\n",
            "Iteration 64, Norm of Gradient: 0.11335241221929841, Cost (Train): 0.5584222933216403\n",
            "Iteration 65, Norm of Gradient: 0.11267838284345846, Cost (Train): 0.5571564069626346\n",
            "Iteration 66, Norm of Gradient: 0.11201266059927689, Cost (Train): 0.5559054107632552\n",
            "Iteration 67, Norm of Gradient: 0.11135510996096354, Cost (Train): 0.5546690356559251\n",
            "Iteration 68, Norm of Gradient: 0.11070559773067724, Cost (Train): 0.553447018706113\n",
            "Iteration 69, Norm of Gradient: 0.11006399301582233, Cost (Train): 0.5522391029542115\n",
            "Iteration 70, Norm of Gradient: 0.10943016720490512, Cost (Train): 0.5510450372615056\n",
            "Iteration 71, Norm of Gradient: 0.10880399394211346, Cost (Train): 0.5498645761601592\n",
            "Iteration 72, Norm of Gradient: 0.10818534910076827, Cost (Train): 0.5486974797071325\n",
            "Iteration 73, Norm of Gradient: 0.10757411075578767, Cost (Train): 0.5475435133419547\n",
            "Iteration 74, Norm of Gradient: 0.10697015915529158, Cost (Train): 0.5464024477482636\n",
            "Iteration 75, Norm of Gradient: 0.10637337669146701, Cost (Train): 0.545274058719035\n",
            "Iteration 76, Norm of Gradient: 0.10578364787080356, Cost (Train): 0.5441581270254139\n",
            "Iteration 77, Norm of Gradient: 0.10520085928380202, Cost (Train): 0.5430544382890695\n",
            "Iteration 78, Norm of Gradient: 0.1046248995742488, Cost (Train): 0.5419627828579867\n",
            "Iteration 79, Norm of Gradient: 0.10405565940814372, Cost (Train): 0.5408829556856146\n",
            "Iteration 80, Norm of Gradient: 0.10349303144236009, Cost (Train): 0.5398147562132898\n",
            "Iteration 81, Norm of Gradient: 0.10293691029311042, Cost (Train): 0.5387579882558527\n",
            "Iteration 82, Norm of Gradient: 0.10238719250428463, Cost (Train): 0.5377124598903779\n",
            "Iteration 83, Norm of Gradient: 0.10184377651572207, Cost (Train): 0.5366779833479386\n",
            "Iteration 84, Norm of Gradient: 0.1013065626314738, Cost (Train): 0.5356543749083282\n",
            "Iteration 85, Norm of Gradient: 0.10077545298810593, Cost (Train): 0.5346414547976599\n",
            "Iteration 86, Norm of Gradient: 0.10025035152309034, Cost (Train): 0.5336390470887719\n",
            "Iteration 87, Norm of Gradient: 0.09973116394332637, Cost (Train): 0.532646979604362\n",
            "Iteration 88, Norm of Gradient: 0.09921779769383002, Cost (Train): 0.531665083822778\n",
            "Iteration 89, Norm of Gradient: 0.09871016192662714, Cost (Train): 0.5306931947863933\n",
            "Iteration 90, Norm of Gradient: 0.0982081674698808, Cost (Train): 0.5297311510124989\n",
            "Iteration 91, Norm of Gradient: 0.09771172679728138, Cost (Train): 0.5287787944066398\n",
            "Iteration 92, Norm of Gradient: 0.09722075399772462, Cost (Train): 0.5278359701783318\n",
            "Iteration 93, Norm of Gradient: 0.09673516474529996, Cost (Train): 0.5269025267590901\n",
            "Iteration 94, Norm of Gradient: 0.09625487626960917, Cost (Train): 0.5259783157227089\n",
            "Iteration 95, Norm of Gradient: 0.09577980732643299, Cost (Train): 0.5250631917077251\n",
            "Iteration 96, Norm of Gradient: 0.09530987816876145, Cost (Train): 0.524157012342008\n",
            "Iteration 97, Norm of Gradient: 0.0948450105182005, Cost (Train): 0.5232596381694142\n",
            "Iteration 98, Norm of Gradient: 0.09438512753676809, Cost (Train): 0.5223709325784494\n",
            "Iteration 99, Norm of Gradient: 0.09393015379908823, Cost (Train): 0.5214907617328792\n",
            "Iteration 100, Norm of Gradient: 0.09348001526499225, Cost (Train): 0.5206189945042358\n",
            "Iteration 101, Norm of Gradient: 0.09303463925253411, Cost (Train): 0.5197555024061641\n",
            "Iteration 102, Norm of Gradient: 0.09259395441142525, Cost (Train): 0.5189001595305562\n",
            "Iteration 103, Norm of Gradient: 0.09215789069689365, Cost (Train): 0.5180528424854245\n",
            "Iteration 104, Norm of Gradient: 0.09172637934397039, Cost (Train): 0.5172134303344579\n",
            "Iteration 105, Norm of Gradient: 0.09129935284220587, Cost (Train): 0.5163818045382219\n",
            "Iteration 106, Norm of Gradient: 0.09087674491081746, Cost (Train): 0.5155578488969457\n",
            "Iteration 107, Norm of Gradient: 0.09045849047426884, Cost (Train): 0.5147414494948581\n",
            "Iteration 108, Norm of Gradient: 0.0900445256382808, Cost (Train): 0.5139324946460234\n",
            "Iteration 109, Norm of Gradient: 0.08963478766627316, Cost (Train): 0.5131308748416359\n",
            "Iteration 110, Norm of Gradient: 0.08922921495623531, Cost (Train): 0.5123364826987298\n",
            "Iteration 111, Norm of Gradient: 0.08882774701802425, Cost (Train): 0.5115492129102646\n",
            "Iteration 112, Norm of Gradient: 0.08843032445108674, Cost (Train): 0.510768962196547\n",
            "Iteration 113, Norm of Gradient: 0.08803688892260293, Cost (Train): 0.5099956292579474\n",
            "Iteration 114, Norm of Gradient: 0.08764738314604764, Cost (Train): 0.5092291147288786\n",
            "Iteration 115, Norm of Gradient: 0.08726175086016569, Cost (Train): 0.5084693211329951\n",
            "Iteration 116, Norm of Gradient: 0.08687993680835665, Cost (Train): 0.5077161528395838\n",
            "Iteration 117, Norm of Gradient: 0.08650188671846491, Cost (Train): 0.5069695160211041\n",
            "Iteration 118, Norm of Gradient: 0.08612754728297012, Cost (Train): 0.5062293186118518\n",
            "Iteration 119, Norm of Gradient: 0.08575686613957242, Cost (Train): 0.5054954702677107\n",
            "Iteration 120, Norm of Gradient: 0.0853897918521685, Cost (Train): 0.5047678823269589\n",
            "Iteration 121, Norm of Gradient: 0.0850262738922116, Cost (Train): 0.5040464677721053\n",
            "Iteration 122, Norm of Gradient: 0.08466626262045103, Cost (Train): 0.5033311411927196\n",
            "Iteration 123, Norm of Gradient: 0.08430970926904498, Cost (Train): 0.5026218187492334\n",
            "Iteration 124, Norm of Gradient: 0.08395656592404084, Cost (Train): 0.5019184181376801\n",
            "Iteration 125, Norm of Gradient: 0.08360678550821765, Cost (Train): 0.5012208585553511\n",
            "Iteration 126, Norm of Gradient: 0.0832603217642837, Cost (Train): 0.5005290606673368\n",
            "Iteration 127, Norm of Gradient: 0.08291712923842486, Cost (Train): 0.4998429465739319\n",
            "Iteration 128, Norm of Gradient: 0.08257716326419588, Cost (Train): 0.49916243977887864\n",
            "Iteration 129, Norm of Gradient: 0.08224037994675, Cost (Train): 0.49848746515842257\n",
            "Iteration 130, Norm of Gradient: 0.08190673614739993, Cost (Train): 0.4978179489311611\n",
            "Iteration 131, Norm of Gradient: 0.08157618946850477, Cost (Train): 0.49715381862865904\n",
            "Iteration 132, Norm of Gradient: 0.08124869823867632, Cost (Train): 0.49649500306681155\n",
            "Iteration 133, Norm of Gradient: 0.08092422149829911, Cost (Train): 0.4958414323179326\n",
            "Iteration 134, Norm of Gradient: 0.08060271898535773, Cost (Train): 0.49519303768354883\n",
            "Iteration 135, Norm of Gradient: 0.08028415112156623, Cost (Train): 0.4945497516678783\n",
            "Iteration 136, Norm of Gradient: 0.07996847899879303, Cost (Train): 0.4939115079519755\n",
            "Iteration 137, Norm of Gradient: 0.0796556643657755, Cost (Train): 0.4932782413685245\n",
            "Iteration 138, Norm of Gradient: 0.07934566961511884, Cost (Train): 0.4926498878772599\n",
            "Iteration 139, Norm of Gradient: 0.07903845777057329, Cost (Train): 0.4920263845410009\n",
            "Iteration 140, Norm of Gradient: 0.07873399247458372, Cost (Train): 0.49140766950227915\n",
            "Iteration 141, Norm of Gradient: 0.07843223797610645, Cost (Train): 0.4907936819605464\n",
            "Iteration 142, Norm of Gradient: 0.07813315911868744, Cost (Train): 0.4901843621499409\n",
            "Iteration 143, Norm of Gradient: 0.07783672132879653, Cost (Train): 0.48957965131760484\n",
            "Iteration 144, Norm of Gradient: 0.07754289060441219, Cost (Train): 0.4889794917025294\n",
            "Iteration 145, Norm of Gradient: 0.07725163350385178, Cost (Train): 0.4883838265149192\n",
            "Iteration 146, Norm of Gradient: 0.07696291713484175, Cost (Train): 0.48779259991605783\n",
            "Iteration 147, Norm of Gradient: 0.07667670914382275, Cost (Train): 0.4872057569986635\n",
            "Iteration 148, Norm of Gradient: 0.07639297770548473, Cost (Train): 0.48662324376771887\n",
            "Iteration 149, Norm of Gradient: 0.07611169151252686, Cost (Train): 0.48604500712176496\n",
            "Iteration 150, Norm of Gradient: 0.07583281976563759, Cost (Train): 0.4854709948346436\n",
            "Iteration 151, Norm of Gradient: 0.07555633216368976, Cost (Train): 0.48490115553767854\n",
            "Iteration 152, Norm of Gradient: 0.07528219889414635, Cost (Train): 0.48433543870228185\n",
            "Iteration 153, Norm of Gradient: 0.07501039062367208, Cost (Train): 0.48377379462297504\n",
            "Iteration 154, Norm of Gradient: 0.07474087848894628, Cost (Train): 0.48321617440081366\n",
            "Iteration 155, Norm of Gradient: 0.07447363408767274, Cost (Train): 0.4826625299272035\n",
            "Iteration 156, Norm of Gradient: 0.07420862946978204, Cost (Train): 0.4821128138680995\n",
            "Iteration 157, Norm of Gradient: 0.07394583712882205, Cost (Train): 0.4815669796485756\n",
            "Iteration 158, Norm of Gradient: 0.07368522999353264, Cost (Train): 0.481024981437757\n",
            "Iteration 159, Norm of Gradient: 0.07342678141960017, Cost (Train): 0.48048677413410423\n",
            "Iteration 160, Norm of Gradient: 0.07317046518158787, Cost (Train): 0.4799523133510397\n",
            "Iteration 161, Norm of Gradient: 0.07291625546503828, Cost (Train): 0.47942155540290887\n",
            "Iteration 162, Norm of Gradient: 0.07266412685874386, Cost (Train): 0.4788944572912656\n",
            "Iteration 163, Norm of Gradient: 0.07241405434718155, Cost (Train): 0.47837097669147405\n",
            "Iteration 164, Norm of Gradient: 0.07216601330310851, Cost (Train): 0.4778510719396187\n",
            "Iteration 165, Norm of Gradient: 0.07191997948031435, Cost (Train): 0.47733470201971484\n",
            "Iteration 166, Norm of Gradient: 0.07167592900652708, Cost (Train): 0.4768218265512106\n",
            "Iteration 167, Norm of Gradient: 0.07143383837646901, Cost (Train): 0.4763124057767738\n",
            "Iteration 168, Norm of Gradient: 0.07119368444505923, Cost (Train): 0.47580640055035645\n",
            "Iteration 169, Norm of Gradient: 0.07095544442075938, Cost (Train): 0.47530377232552873\n",
            "Iteration 170, Norm of Gradient: 0.07071909585905946, Cost (Train): 0.4748044831440763\n",
            "Iteration 171, Norm of Gradient: 0.07048461665610056, Cost (Train): 0.4743084956248543\n",
            "Iteration 172, Norm of Gradient: 0.07025198504243133, Cost (Train): 0.47381577295289007\n",
            "Iteration 173, Norm of Gradient: 0.0700211795768951, Cost (Train): 0.4733262788687305\n",
            "Iteration 174, Norm of Gradient: 0.0697921791406451, Cost (Train): 0.47283997765802604\n",
            "Iteration 175, Norm of Gradient: 0.06956496293128422, Cost (Train): 0.4723568341413452\n",
            "Iteration 176, Norm of Gradient: 0.06933951045712702, Cost (Train): 0.4718768136642152\n",
            "Iteration 177, Norm of Gradient: 0.06911580153158105, Cost (Train): 0.47139988208738215\n",
            "Iteration 178, Norm of Gradient: 0.06889381626764474, Cost (Train): 0.4709260057772843\n",
            "Iteration 179, Norm of Gradient: 0.06867353507251919, Cost (Train): 0.47045515159673523\n",
            "Iteration 180, Norm of Gradient: 0.06845493864233165, Cost (Train): 0.46998728689580976\n",
            "Iteration 181, Norm of Gradient: 0.06823800795696747, Cost (Train): 0.46952237950292836\n",
            "Iteration 182, Norm of Gradient: 0.06802272427500863, Cost (Train): 0.46906039771613434\n",
            "Iteration 183, Norm of Gradient: 0.06780906912877657, Cost (Train): 0.46860131029456126\n",
            "Iteration 184, Norm of Gradient: 0.06759702431947619, Cost (Train): 0.4681450864500822\n",
            "Iteration 185, Norm of Gradient: 0.06738657191243955, Cost (Train): 0.46769169583914016\n",
            "Iteration 186, Norm of Gradient: 0.06717769423246686, Cost (Train): 0.4672411085547525\n",
            "Iteration 187, Norm of Gradient: 0.0669703738592619, Cost (Train): 0.46679329511868584\n",
            "Iteration 188, Norm of Gradient: 0.0667645936229608, Cost (Train): 0.46634822647379853\n",
            "Iteration 189, Norm of Gradient: 0.06656033659975133, Cost (Train): 0.46590587397654437\n",
            "Iteration 190, Norm of Gradient: 0.06635758610758091, Cost (Train): 0.46546620938963534\n",
            "Iteration 191, Norm of Gradient: 0.06615632570195132, Cost (Train): 0.46502920487485916\n",
            "Iteration 192, Norm of Gradient: 0.06595653917179826, Cost (Train): 0.4645948329860471\n",
            "Iteration 193, Norm of Gradient: 0.06575821053545383, Cost (Train): 0.4641630666621897\n",
            "Iteration 194, Norm of Gradient: 0.06556132403668985, Cost (Train): 0.46373387922069575\n",
            "Iteration 195, Norm of Gradient: 0.06536586414084068, Cost (Train): 0.4633072443507916\n",
            "Iteration 196, Norm of Gradient: 0.06517181553100326, Cost (Train): 0.46288313610705734\n",
            "Iteration 197, Norm of Gradient: 0.0649791631043132, Cost (Train): 0.4624615289030971\n",
            "Iteration 198, Norm of Gradient: 0.06478789196829446, Cost (Train): 0.4620423975053387\n",
            "Iteration 199, Norm of Gradient: 0.06459798743728208, Cost (Train): 0.46162571702696176\n",
            "Iteration 200, Norm of Gradient: 0.06440943502891508, Cost (Train): 0.4612114629219499\n",
            "Iteration 201, Norm of Gradient: 0.06422222046069921, Cost (Train): 0.46079961097926364\n",
            "Iteration 202, Norm of Gradient: 0.06403632964663694, Cost (Train): 0.46039013731713274\n",
            "Iteration 203, Norm of Gradient: 0.06385174869392403, Cost (Train): 0.4599830183774642\n",
            "Iteration 204, Norm of Gradient: 0.06366846389971068, Cost (Train): 0.4595782309203625\n",
            "Iteration 205, Norm of Gradient: 0.06348646174792615, Cost (Train): 0.4591757520187616\n",
            "Iteration 206, Norm of Gradient: 0.06330572890616526, Cost (Train): 0.4587755590531636\n",
            "Iteration 207, Norm of Gradient: 0.06312625222263565, Cost (Train): 0.45837762970648344\n",
            "Iteration 208, Norm of Gradient: 0.0629480187231641, Cost (Train): 0.45798194195899694\n",
            "Iteration 209, Norm of Gradient: 0.06277101560826105, Cost (Train): 0.4575884740833879\n",
            "Iteration 210, Norm of Gradient: 0.06259523025024176, Cost (Train): 0.45719720463989527\n",
            "Iteration 211, Norm of Gradient: 0.06242065019040301, Cost (Train): 0.4568081124715552\n",
            "Iteration 212, Norm of Gradient: 0.0622472631362541, Cost (Train): 0.4564211766995375\n",
            "Iteration 213, Norm of Gradient: 0.062075056958800875, Cost (Train): 0.4560363767185732\n",
            "Iteration 214, Norm of Gradient: 0.06190401968988186, Cost (Train): 0.45565369219247204\n",
            "Iteration 215, Norm of Gradient: 0.06173413951955519, Cost (Train): 0.45527310304972807\n",
            "Iteration 216, Norm of Gradient: 0.061565404793535304, Cost (Train): 0.45489458947920974\n",
            "Iteration 217, Norm of Gradient: 0.061397804010678236, Cost (Train): 0.4545181319259347\n",
            "Iteration 218, Norm of Gradient: 0.061231325820514756, Cost (Train): 0.45414371108692575\n",
            "Iteration 219, Norm of Gradient: 0.06106595902082998, Cost (Train): 0.4537713079071475\n",
            "Iteration 220, Norm of Gradient: 0.060901692555288565, Cost (Train): 0.45340090357551943\n",
            "Iteration 221, Norm of Gradient: 0.06073851551110477, Cost (Train): 0.45303247952100856\n",
            "Iteration 222, Norm of Gradient: 0.06057641711675602, Cost (Train): 0.4526660174087936\n",
            "Iteration 223, Norm of Gradient: 0.06041538673973932, Cost (Train): 0.4523014991365042\n",
            "Iteration 224, Norm of Gradient: 0.06025541388436961, Cost (Train): 0.4519389068305311\n",
            "Iteration 225, Norm of Gradient: 0.06009648818961897, Cost (Train): 0.451578222842406\n",
            "Iteration 226, Norm of Gradient: 0.05993859942699608, Cost (Train): 0.45121942974525037\n",
            "Iteration 227, Norm of Gradient: 0.05978173749846483, Cost (Train): 0.45086251033029057\n",
            "Iteration 228, Norm of Gradient: 0.05962589243440153, Cost (Train): 0.45050744760343897\n",
            "Iteration 229, Norm of Gradient: 0.05947105439158961, Cost (Train): 0.4501542247819385\n",
            "Iteration 230, Norm of Gradient: 0.059317213651251345, Cost (Train): 0.44980282529107035\n",
            "Iteration 231, Norm of Gradient: 0.05916436061711544, Cost (Train): 0.4494532327609227\n",
            "Iteration 232, Norm of Gradient: 0.059012485813520185, Cost (Train): 0.44910543102321937\n",
            "Iteration 233, Norm of Gradient: 0.058861579883551125, Cost (Train): 0.44875940410820714\n",
            "Iteration 234, Norm of Gradient: 0.05871163358721251, Cost (Train): 0.4484151362416009\n",
            "Iteration 235, Norm of Gradient: 0.0585626377996321, Cost (Train): 0.4480726118415843\n",
            "Iteration 236, Norm of Gradient: 0.058414583509298304, Cost (Train): 0.4477318155158659\n",
            "Iteration 237, Norm of Gradient: 0.058267461816329295, Cost (Train): 0.4473927320587887\n",
            "Iteration 238, Norm of Gradient: 0.058121263930773215, Cost (Train): 0.4470553464484934\n",
            "Iteration 239, Norm of Gradient: 0.05797598117093884, Cost (Train): 0.4467196438441316\n",
            "Iteration 240, Norm of Gradient: 0.057831604961756364, Cost (Train): 0.44638560958313084\n",
            "Iteration 241, Norm of Gradient: 0.05768812683316722, Cost (Train): 0.44605322917850826\n",
            "Iteration 242, Norm of Gradient: 0.057545538418542805, Cost (Train): 0.4457224883162336\n",
            "Iteration 243, Norm of Gradient: 0.05740383145313122, Cost (Train): 0.4453933728526383\n",
            "Iteration 244, Norm of Gradient: 0.057262997772531556, Cost (Train): 0.44506586881187177\n",
            "Iteration 245, Norm of Gradient: 0.057123029311195214, Cost (Train): 0.4447399623834034\n",
            "Iteration 246, Norm of Gradient: 0.056983918100953566, Cost (Train): 0.4444156399195683\n",
            "Iteration 247, Norm of Gradient: 0.05684565626957176, Cost (Train): 0.4440928879331568\n",
            "Iteration 248, Norm of Gradient: 0.056708236039327536, Cost (Train): 0.44377169309504705\n",
            "Iteration 249, Norm of Gradient: 0.056571649725615215, Cost (Train): 0.4434520422318794\n",
            "Iteration 250, Norm of Gradient: 0.05643588973557406, Cost (Train): 0.44313392232377063\n",
            "Iteration 251, Norm of Gradient: 0.05630094856674029, Cost (Train): 0.4428173205020701\n",
            "Iteration 252, Norm of Gradient: 0.056166818805722815, Cost (Train): 0.4425022240471535\n",
            "Iteration 253, Norm of Gradient: 0.05603349312690166, Cost (Train): 0.4421886203862562\n",
            "Iteration 254, Norm of Gradient: 0.05590096429114904, Cost (Train): 0.4418764970913437\n",
            "Iteration 255, Norm of Gradient: 0.055769225144572464, Cost (Train): 0.44156584187701947\n",
            "Iteration 256, Norm of Gradient: 0.05563826861727939, Cost (Train): 0.44125664259846864\n",
            "Iteration 257, Norm of Gradient: 0.05550808772216313, Cost (Train): 0.4409488872494376\n",
            "Iteration 258, Norm of Gradient: 0.05537867555370946, Cost (Train): 0.4406425639602476\n",
            "Iteration 259, Norm of Gradient: 0.055250025286823684, Cost (Train): 0.4403376609958431\n",
            "Iteration 260, Norm of Gradient: 0.055122130175677596, Cost (Train): 0.4400341667538738\n",
            "Iteration 261, Norm of Gradient: 0.054994983552575996, Cost (Train): 0.43973206976280843\n",
            "Iteration 262, Norm of Gradient: 0.054868578826842505, Cost (Train): 0.4394313586800811\n",
            "Iteration 263, Norm of Gradient: 0.05474290948372407, Cost (Train): 0.43913202229026965\n",
            "Iteration 264, Norm of Gradient: 0.054617969083314045, Cost (Train): 0.4388340495033037\n",
            "Iteration 265, Norm of Gradient: 0.05449375125949327, Cost (Train): 0.438537429352704\n",
            "Iteration 266, Norm of Gradient: 0.05437024971888895, Cost (Train): 0.438242150993851\n",
            "Iteration 267, Norm of Gradient: 0.05424745823985094, Cost (Train): 0.4379482037022819\n",
            "Iteration 268, Norm of Gradient: 0.05412537067144512, Cost (Train): 0.43765557687201745\n",
            "Iteration 269, Norm of Gradient: 0.054003980932463475, Cost (Train): 0.43736426001391526\n",
            "Iteration 270, Norm of Gradient: 0.05388328301045056, Cost (Train): 0.4370742427540522\n",
            "Iteration 271, Norm of Gradient: 0.053763270960746255, Cost (Train): 0.4367855148321314\n",
            "Iteration 272, Norm of Gradient: 0.05364393890554408, Cost (Train): 0.4364980660999178\n",
            "Iteration 273, Norm of Gradient: 0.05352528103296527, Cost (Train): 0.4362118865196985\n",
            "Iteration 274, Norm of Gradient: 0.05340729159614783, Cost (Train): 0.43592696616276827\n",
            "Iteration 275, Norm of Gradient: 0.05328996491235064, Cost (Train): 0.4356432952079404\n",
            "Iteration 276, Norm of Gradient: 0.05317329536207222, Cost (Train): 0.4353608639400816\n",
            "Iteration 277, Norm of Gradient: 0.053057277388183784, Cost (Train): 0.4350796627486713\n",
            "Iteration 278, Norm of Gradient: 0.052941905495076415, Cost (Train): 0.43479968212638387\n",
            "Iteration 279, Norm of Gradient: 0.052827174247822184, Cost (Train): 0.4345209126676942\n",
            "Iteration 280, Norm of Gradient: 0.0527130782713486, Cost (Train): 0.4342433450675054\n",
            "Iteration 281, Norm of Gradient: 0.05259961224962665, Cost (Train): 0.43396697011979957\n",
            "Iteration 282, Norm of Gradient: 0.05248677092487176, Cost (Train): 0.43369177871630926\n",
            "Iteration 283, Norm of Gradient: 0.05237454909675756, Cost (Train): 0.4334177618452116\n",
            "Iteration 284, Norm of Gradient: 0.052262941621642425, Cost (Train): 0.4331449105898411\n",
            "Iteration 285, Norm of Gradient: 0.052151943411808135, Cost (Train): 0.43287321612742596\n",
            "Iteration 286, Norm of Gradient: 0.05204154943471091, Cost (Train): 0.4326026697278422\n",
            "Iteration 287, Norm of Gradient: 0.0519317547122441, Cost (Train): 0.4323332627523887\n",
            "Iteration 288, Norm of Gradient: 0.05182255432001283, Cost (Train): 0.4320649866525813\n",
            "Iteration 289, Norm of Gradient: 0.05171394338661992, Cost (Train): 0.431797832968966\n",
            "Iteration 290, Norm of Gradient: 0.0516059170929632, Cost (Train): 0.43153179332995123\n",
            "Iteration 291, Norm of Gradient: 0.05149847067154383, Cost (Train): 0.43126685945065785\n",
            "Iteration 292, Norm of Gradient: 0.05139159940578548, Cost (Train): 0.4310030231317879\n",
            "Iteration 293, Norm of Gradient: 0.051285298629364144, Cost (Train): 0.43074027625851047\n",
            "Iteration 294, Norm of Gradient: 0.05117956372554864, Cost (Train): 0.43047861079936556\n",
            "Iteration 295, Norm of Gradient: 0.05107439012655103, Cost (Train): 0.4302180188051838\n",
            "Iteration 296, Norm of Gradient: 0.05096977331288744, Cost (Train): 0.4299584924080249\n",
            "Iteration 297, Norm of Gradient: 0.05086570881274873, Cost (Train): 0.4297000238201309\n",
            "Iteration 298, Norm of Gradient: 0.05076219220138074, Cost (Train): 0.42944260533289597\n",
            "Iteration 299, Norm of Gradient: 0.0506592191004744, Cost (Train): 0.4291862293158529\n",
            "Iteration 300, Norm of Gradient: 0.050556785177564935, Cost (Train): 0.42893088821567343\n",
            "Iteration 301, Norm of Gradient: 0.05045488614544045, Cost (Train): 0.4286765745551865\n",
            "Iteration 302, Norm of Gradient: 0.05035351776155965, Cost (Train): 0.42842328093240856\n",
            "Iteration 303, Norm of Gradient: 0.05025267582747828, Cost (Train): 0.42817100001959124\n",
            "Iteration 304, Norm of Gradient: 0.05015235618828443, Cost (Train): 0.4279197245622816\n",
            "Iteration 305, Norm of Gradient: 0.05005255473204243, Cost (Train): 0.4276694473783981\n",
            "Iteration 306, Norm of Gradient: 0.04995326738924516, Cost (Train): 0.42742016135731936\n",
            "Iteration 307, Norm of Gradient: 0.04985449013227466, Cost (Train): 0.4271718594589877\n",
            "Iteration 308, Norm of Gradient: 0.049756218974870835, Cost (Train): 0.42692453471302544\n",
            "Iteration 309, Norm of Gradient: 0.049658449971608294, Cost (Train): 0.42667818021786463\n",
            "Iteration 310, Norm of Gradient: 0.04956117921738097, Cost (Train): 0.42643278913989036\n",
            "Iteration 311, Norm of Gradient: 0.04946440284689439, Cost (Train): 0.426188354712596\n",
            "Iteration 312, Norm of Gradient: 0.04936811703416578, Cost (Train): 0.425944870235752\n",
            "Iteration 313, Norm of Gradient: 0.04927231799203131, Cost (Train): 0.4257023290745861\n",
            "Iteration 314, Norm of Gradient: 0.049177001971661034, Cost (Train): 0.4254607246589765\n",
            "Iteration 315, Norm of Gradient: 0.04908216526208064, Cost (Train): 0.42522005048265626\n",
            "Iteration 316, Norm of Gradient: 0.04898780418970075, Cost (Train): 0.4249803001024302\n",
            "Iteration 317, Norm of Gradient: 0.0488939151178527, Cost (Train): 0.42474146713740285\n",
            "Iteration 318, Norm of Gradient: 0.048800494446331485, Cost (Train): 0.42450354526821776\n",
            "Iteration 319, Norm of Gradient: 0.04870753861094543, Cost (Train): 0.424266528236308\n",
            "Iteration 320, Norm of Gradient: 0.04861504408307224, Cost (Train): 0.4240304098431582\n",
            "Iteration 321, Norm of Gradient: 0.048523007369221764, Cost (Train): 0.42379518394957605\n",
            "Iteration 322, Norm of Gradient: 0.04843142501060509, Cost (Train): 0.4235608444749756\n",
            "Iteration 323, Norm of Gradient: 0.04834029358270995, Cost (Train): 0.4233273853966707\n",
            "Iteration 324, Norm of Gradient: 0.04824960969488215, Cost (Train): 0.4230948007491781\n",
            "Iteration 325, Norm of Gradient: 0.0481593699899133, Cost (Train): 0.42286308462353145\n",
            "Iteration 326, Norm of Gradient: 0.04806957114363436, Cost (Train): 0.42263223116660437\n",
            "Iteration 327, Norm of Gradient: 0.04798020986451509, Cost (Train): 0.4224022345804441\n",
            "Iteration 328, Norm of Gradient: 0.0478912828932693, Cost (Train): 0.4221730891216138\n",
            "Iteration 329, Norm of Gradient: 0.04780278700246576, Cost (Train): 0.42194478910054495\n",
            "Iteration 330, Norm of Gradient: 0.047714718996144656, Cost (Train): 0.4217173288808989\n",
            "Iteration 331, Norm of Gradient: 0.047627075709439595, Cost (Train): 0.4214907028789373\n",
            "Iteration 332, Norm of Gradient: 0.04753985400820499, Cost (Train): 0.421264905562901\n",
            "Iteration 333, Norm of Gradient: 0.04745305078864869, Cost (Train): 0.42103993145239926\n",
            "Iteration 334, Norm of Gradient: 0.04736666297696997, Cost (Train): 0.42081577511780605\n",
            "Iteration 335, Norm of Gradient: 0.047280687529002476, Cost (Train): 0.42059243117966544\n",
            "Iteration 336, Norm of Gradient: 0.04719512142986234, Cost (Train): 0.42036989430810534\n",
            "Iteration 337, Norm of Gradient: 0.0471099616936013, Cost (Train): 0.42014815922226\n",
            "Iteration 338, Norm of Gradient: 0.04702520536286455, Cost (Train): 0.4199272206896994\n",
            "Iteration 339, Norm of Gradient: 0.046940849508553574, Cost (Train): 0.4197070735258678\n",
            "Iteration 340, Norm of Gradient: 0.04685689122949354, Cost (Train): 0.4194877125935294\n",
            "Iteration 341, Norm of Gradient: 0.0467733276521055, Cost (Train): 0.4192691328022222\n",
            "Iteration 342, Norm of Gradient: 0.04669015593008303, Cost (Train): 0.4190513291077188\n",
            "Iteration 343, Norm of Gradient: 0.046607373244073526, Cost (Train): 0.418834296511495\n",
            "Iteration 344, Norm of Gradient: 0.04652497680136375, Cost (Train): 0.41861803006020637\n",
            "Iteration 345, Norm of Gradient: 0.046442963835569874, Cost (Train): 0.41840252484517104\n",
            "Iteration 346, Norm of Gradient: 0.04636133160633175, Cost (Train): 0.4181877760018597\n",
            "Iteration 347, Norm of Gradient: 0.046280077399011445, Cost (Train): 0.4179737787093936\n",
            "Iteration 348, Norm of Gradient: 0.046199198524395836, Cost (Train): 0.41776052819004816\n",
            "Iteration 349, Norm of Gradient: 0.04611869231840337, Cost (Train): 0.417548019708764\n",
            "Iteration 350, Norm of Gradient: 0.04603855614179489, Cost (Train): 0.41733624857266477\n",
            "Iteration 351, Norm of Gradient: 0.04595878737988827, Cost (Train): 0.4171252101305811\n",
            "Iteration 352, Norm of Gradient: 0.045879383442277065, Cost (Train): 0.4169148997725815\n",
            "Iteration 353, Norm of Gradient: 0.04580034176255293, Cost (Train): 0.4167053129295088\n",
            "Iteration 354, Norm of Gradient: 0.045721659798031875, Cost (Train): 0.41649644507252387\n",
            "Iteration 355, Norm of Gradient: 0.045643335029484165, Cost (Train): 0.4162882917126545\n",
            "Iteration 356, Norm of Gradient: 0.045565364960867966, Cost (Train): 0.4160808484003517\n",
            "Iteration 357, Norm of Gradient: 0.04548774711906635, Cost (Train): 0.41587411072504976\n",
            "Iteration 358, Norm of Gradient: 0.04541047905362825, Cost (Train): 0.4156680743147346\n",
            "Iteration 359, Norm of Gradient: 0.04533355833651253, Cost (Train): 0.41546273483551666\n",
            "Iteration 360, Norm of Gradient: 0.04525698256183558, Cost (Train): 0.4152580879912088\n",
            "Iteration 361, Norm of Gradient: 0.045180749345622474, Cost (Train): 0.41505412952291154\n",
            "Iteration 362, Norm of Gradient: 0.04510485632556119, Cost (Train): 0.4148508552086025\n",
            "Iteration 363, Norm of Gradient: 0.04502930116076026, Cost (Train): 0.4146482608627313\n",
            "Iteration 364, Norm of Gradient: 0.04495408153150958, Cost (Train): 0.4144463423358213\n",
            "Iteration 365, Norm of Gradient: 0.044879195139044516, Cost (Train): 0.4142450955140738\n",
            "Iteration 366, Norm of Gradient: 0.04480463970531292, Cost (Train): 0.4140445163189805\n",
            "Iteration 367, Norm of Gradient: 0.0447304129727454, Cost (Train): 0.41384460070693857\n",
            "Iteration 368, Norm of Gradient: 0.044656512704028546, Cost (Train): 0.4136453446688728\n",
            "Iteration 369, Norm of Gradient: 0.04458293668188114, Cost (Train): 0.4134467442298609\n",
            "Iteration 370, Norm of Gradient: 0.04450968270883334, Cost (Train): 0.41324879544876475\n",
            "Iteration 371, Norm of Gradient: 0.04443674860700876, Cost (Train): 0.41305149441786615\n",
            "Iteration 372, Norm of Gradient: 0.0443641322179093, Cost (Train): 0.4128548372625075\n",
            "Iteration 373, Norm of Gradient: 0.0442918314022029, Cost (Train): 0.4126588201407367\n",
            "Iteration 374, Norm of Gradient: 0.04421984403951398, Cost (Train): 0.4124634392429567\n",
            "Iteration 375, Norm of Gradient: 0.044148168028216694, Cost (Train): 0.41226869079158024\n",
            "Iteration 376, Norm of Gradient: 0.04407680128523076, Cost (Train): 0.4120745710406882\n",
            "Iteration 377, Norm of Gradient: 0.044005741745819976, Cost (Train): 0.4118810762756927\n",
            "Iteration 378, Norm of Gradient: 0.04393498736339332, Cost (Train): 0.41168820281300433\n",
            "Iteration 379, Norm of Gradient: 0.04386453610930867, Cost (Train): 0.4114959469997044\n",
            "Iteration 380, Norm of Gradient: 0.04379438597267898, Cost (Train): 0.41130430521321976\n",
            "Iteration 381, Norm of Gradient: 0.043724534960180944, Cost (Train): 0.41111327386100355\n",
            "Iteration 382, Norm of Gradient: 0.0436549810958662, Cost (Train): 0.41092284938021884\n",
            "Iteration 383, Norm of Gradient: 0.043585722420974776, Cost (Train): 0.4107330282374268\n",
            "Iteration 384, Norm of Gradient: 0.04351675699375112, Cost (Train): 0.41054380692827824\n",
            "Iteration 385, Norm of Gradient: 0.0434480828892623, Cost (Train): 0.4103551819772101\n",
            "Iteration 386, Norm of Gradient: 0.043379698199218567, Cost (Train): 0.4101671499371445\n",
            "Iteration 387, Norm of Gradient: 0.043311601031796286, Cost (Train): 0.40997970738919237\n",
            "Iteration 388, Norm of Gradient: 0.04324378951146293, Cost (Train): 0.40979285094236073\n",
            "Iteration 389, Norm of Gradient: 0.0431762617788044, Cost (Train): 0.4096065772332635\n",
            "Iteration 390, Norm of Gradient: 0.04310901599035456, Cost (Train): 0.40942088292583534\n",
            "Iteration 391, Norm of Gradient: 0.04304205031842673, Cost (Train): 0.40923576471105055\n",
            "Iteration 392, Norm of Gradient: 0.042975362950947504, Cost (Train): 0.40905121930664384\n",
            "Iteration 393, Norm of Gradient: 0.04290895209129251, Cost (Train): 0.40886724345683567\n",
            "Iteration 394, Norm of Gradient: 0.04284281595812429, Cost (Train): 0.4086838339320601\n",
            "Iteration 395, Norm of Gradient: 0.04277695278523215, Cost (Train): 0.40850098752869696\n",
            "Iteration 396, Norm of Gradient: 0.042711360821374035, Cost (Train): 0.4083187010688067\n",
            "Iteration 397, Norm of Gradient: 0.04264603833012039, Cost (Train): 0.4081369713998685\n",
            "Iteration 398, Norm of Gradient: 0.042580983589699875, Cost (Train): 0.40795579539452187\n",
            "Iteration 399, Norm of Gradient: 0.04251619489284708, Cost (Train): 0.4077751699503113\n",
            "Iteration 400, Norm of Gradient: 0.04245167054665206, Cost (Train): 0.40759509198943394\n",
            "Iteration 401, Norm of Gradient: 0.04238740887241172, Cost (Train): 0.40741555845849037\n",
            "Iteration 402, Norm of Gradient: 0.0423234082054831, Cost (Train): 0.4072365663282386\n",
            "Iteration 403, Norm of Gradient: 0.04225966689513836, Cost (Train): 0.40705811259335095\n",
            "Iteration 404, Norm of Gradient: 0.0421961833044216, Cost (Train): 0.4068801942721736\n",
            "Iteration 405, Norm of Gradient: 0.04213295581000742, Cost (Train): 0.4067028084064896\n",
            "Iteration 406, Norm of Gradient: 0.042069982802061215, Cost (Train): 0.40652595206128417\n",
            "Iteration 407, Norm of Gradient: 0.04200726268410113, Cost (Train): 0.40634962232451344\n",
            "Iteration 408, Norm of Gradient: 0.04194479387286173, Cost (Train): 0.40617381630687516\n",
            "Iteration 409, Norm of Gradient: 0.0418825747981593, Cost (Train): 0.405998531141583\n",
            "Iteration 410, Norm of Gradient: 0.04182060390275885, Cost (Train): 0.40582376398414327\n",
            "Iteration 411, Norm of Gradient: 0.04175887964224252, Cost (Train): 0.4056495120121335\n",
            "Iteration 412, Norm of Gradient: 0.04169740048487988, Cost (Train): 0.4054757724249855\n",
            "Iteration 413, Norm of Gradient: 0.041636164911499504, Cost (Train): 0.4053025424437689\n",
            "Iteration 414, Norm of Gradient: 0.04157517141536224, Cost (Train): 0.40512981931097863\n",
            "Iteration 415, Norm of Gradient: 0.04151441850203599, Cost (Train): 0.4049576002903247\n",
            "Iteration 416, Norm of Gradient: 0.0414539046892719, Cost (Train): 0.404785882666524\n",
            "Iteration 417, Norm of Gradient: 0.04139362850688216, Cost (Train): 0.4046146637450951\n",
            "Iteration 418, Norm of Gradient: 0.04133358849661913, Cost (Train): 0.40444394085215507\n",
            "Iteration 419, Norm of Gradient: 0.041273783212055964, Cost (Train): 0.40427371133421874\n",
            "Iteration 420, Norm of Gradient: 0.041214211218468716, Cost (Train): 0.40410397255800096\n",
            "Iteration 421, Norm of Gradient: 0.04115487109271966, Cost (Train): 0.40393472191021984\n",
            "Iteration 422, Norm of Gradient: 0.04109576142314219, Cost (Train): 0.40376595679740374\n",
            "Iteration 423, Norm of Gradient: 0.04103688080942696, Cost (Train): 0.4035976746456997\n",
            "Iteration 424, Norm of Gradient: 0.04097822786250942, Cost (Train): 0.403429872900684\n",
            "Iteration 425, Norm of Gradient: 0.040919801204458574, Cost (Train): 0.40326254902717557\n",
            "Iteration 426, Norm of Gradient: 0.04086159946836724, Cost (Train): 0.4030957005090509\n",
            "Iteration 427, Norm of Gradient: 0.04080362129824335, Cost (Train): 0.4029293248490615\n",
            "Iteration 428, Norm of Gradient: 0.04074586534890274, Cost (Train): 0.4027634195686534\n",
            "Iteration 429, Norm of Gradient: 0.040688330285863016, Cost (Train): 0.4025979822077884\n",
            "Iteration 430, Norm of Gradient: 0.040631014785238805, Cost (Train): 0.40243301032476814\n",
            "Iteration 431, Norm of Gradient: 0.04057391753363809, Cost (Train): 0.40226850149605897\n",
            "Iteration 432, Norm of Gradient: 0.040517037228059875, Cost (Train): 0.40210445331612055\n",
            "Iteration 433, Norm of Gradient: 0.04046037257579291, Cost (Train): 0.40194086339723456\n",
            "Iteration 434, Norm of Gradient: 0.04040392229431567, Cost (Train): 0.40177772936933637\n",
            "Iteration 435, Norm of Gradient: 0.040347685111197476, Cost (Train): 0.4016150488798492\n",
            "Iteration 436, Norm of Gradient: 0.04029165976400074, Cost (Train): 0.4014528195935183\n",
            "Iteration 437, Norm of Gradient: 0.040235845000184296, Cost (Train): 0.4012910391922493\n",
            "Iteration 438, Norm of Gradient: 0.04018023957700795, Cost (Train): 0.4011297053749464\n",
            "Iteration 439, Norm of Gradient: 0.040124842261438, Cost (Train): 0.4009688158573537\n",
            "Iteration 440, Norm of Gradient: 0.04006965183005387, Cost (Train): 0.4008083683718979\n",
            "Iteration 441, Norm of Gradient: 0.04001466706895592, Cost (Train): 0.40064836066753257\n",
            "Iteration 442, Norm of Gradient: 0.03995988677367413, Cost (Train): 0.4004887905095842\n",
            "Iteration 443, Norm of Gradient: 0.03990530974907793, Cost (Train): 0.4003296556796004\n",
            "Iteration 444, Norm of Gradient: 0.039850934809287024, Cost (Train): 0.40017095397519964\n",
            "Iteration 445, Norm of Gradient: 0.039796760777583234, Cost (Train): 0.40001268320992217\n",
            "Iteration 446, Norm of Gradient: 0.03974278648632336, Cost (Train): 0.39985484121308307\n",
            "Iteration 447, Norm of Gradient: 0.039689010776852954, Cost (Train): 0.3996974258296273\n",
            "Iteration 448, Norm of Gradient: 0.039635432499421114, Cost (Train): 0.39954043491998525\n",
            "Iteration 449, Norm of Gradient: 0.03958205051309628, Cost (Train): 0.39938386635993117\n",
            "Iteration 450, Norm of Gradient: 0.03952886368568289, Cost (Train): 0.39922771804044194\n",
            "Iteration 451, Norm of Gradient: 0.039475870893639, Cost (Train): 0.39907198786755876\n",
            "Iteration 452, Norm of Gradient: 0.039423071021994834, Cost (Train): 0.398916673762249\n",
            "Iteration 453, Norm of Gradient: 0.039370462964272275, Cost (Train): 0.3987617736602704\n",
            "Iteration 454, Norm of Gradient: 0.039318045622405144, Cost (Train): 0.39860728551203684\n",
            "Iteration 455, Norm of Gradient: 0.03926581790666057, Cost (Train): 0.3984532072824849\n",
            "Iteration 456, Norm of Gradient: 0.03921377873556096, Cost (Train): 0.39829953695094256\n",
            "Iteration 457, Norm of Gradient: 0.03916192703580703, Cost (Train): 0.3981462725109987\n",
            "Iteration 458, Norm of Gradient: 0.03911026174220171, Cost (Train): 0.39799341197037513\n",
            "Iteration 459, Norm of Gradient: 0.0390587817975747, Cost (Train): 0.39784095335079844\n",
            "Iteration 460, Norm of Gradient: 0.03900748615270804, Cost (Train): 0.3976888946878745\n",
            "Iteration 461, Norm of Gradient: 0.0389563737662624, Cost (Train): 0.3975372340309639\n",
            "Iteration 462, Norm of Gradient: 0.03890544360470425, Cost (Train): 0.3973859694430588\n",
            "Iteration 463, Norm of Gradient: 0.038854694642233734, Cost (Train): 0.3972350990006607\n",
            "Iteration 464, Norm of Gradient: 0.03880412586071344, Cost (Train): 0.3970846207936602\n",
            "Iteration 465, Norm of Gradient: 0.03875373624959787, Cost (Train): 0.39693453292521774\n",
            "Iteration 466, Norm of Gradient: 0.03870352480586371, Cost (Train): 0.3967848335116455\n",
            "Iteration 467, Norm of Gradient: 0.038653490533940876, Cost (Train): 0.3966355206822906\n",
            "Iteration 468, Norm of Gradient: 0.03860363244564424, Cost (Train): 0.39648659257941987\n",
            "Iteration 469, Norm of Gradient: 0.038553949560106236, Cost (Train): 0.3963380473581056\n",
            "Iteration 470, Norm of Gradient: 0.03850444090370999, Cost (Train): 0.39618988318611237\n",
            "Iteration 471, Norm of Gradient: 0.038455105510023395, Cost (Train): 0.39604209824378517\n",
            "Iteration 472, Norm of Gradient: 0.038405942419733725, Cost (Train): 0.39589469072393924\n",
            "Iteration 473, Norm of Gradient: 0.03835695068058308, Cost (Train): 0.39574765883174995\n",
            "Iteration 474, Norm of Gradient: 0.03830812934730438, Cost (Train): 0.3956010007846453\n",
            "Iteration 475, Norm of Gradient: 0.038259477481558245, Cost (Train): 0.3954547148121981\n",
            "Iteration 476, Norm of Gradient: 0.038210994151870334, Cost (Train): 0.39530879915602063\n",
            "Iteration 477, Norm of Gradient: 0.03816267843356952, Cost (Train): 0.39516325206965897\n",
            "Iteration 478, Norm of Gradient: 0.038114529408726686, Cost (Train): 0.3950180718184897\n",
            "Iteration 479, Norm of Gradient: 0.03806654616609409, Cost (Train): 0.3948732566796176\n",
            "Iteration 480, Norm of Gradient: 0.03801872780104548, Cost (Train): 0.39472880494177304\n",
            "Iteration 481, Norm of Gradient: 0.037971073415516816, Cost (Train): 0.3945847149052123\n",
            "Iteration 482, Norm of Gradient: 0.03792358211794756, Cost (Train): 0.3944409848816177\n",
            "Iteration 483, Norm of Gradient: 0.0378762530232227, Cost (Train): 0.3942976131939992\n",
            "Iteration 484, Norm of Gradient: 0.0378290852526153, Cost (Train): 0.3941545981765969\n",
            "Iteration 485, Norm of Gradient: 0.03778207793372966, Cost (Train): 0.3940119381747848\n",
            "Iteration 486, Norm of Gradient: 0.037735230200445134, Cost (Train): 0.39386963154497523\n",
            "Iteration 487, Norm of Gradient: 0.03768854119286049, Cost (Train): 0.39372767665452446\n",
            "Iteration 488, Norm of Gradient: 0.037642010057238856, Cost (Train): 0.39358607188163913\n",
            "Iteration 489, Norm of Gradient: 0.037595635945953276, Cost (Train): 0.3934448156152841\n",
            "Iteration 490, Norm of Gradient: 0.037549418017432785, Cost (Train): 0.39330390625509043\n",
            "Iteration 491, Norm of Gradient: 0.03750335543610913, Cost (Train): 0.39316334221126537\n",
            "Iteration 492, Norm of Gradient: 0.037457447372363895, Cost (Train): 0.393023121904502\n",
            "Iteration 493, Norm of Gradient: 0.03741169300247643, Cost (Train): 0.39288324376589107\n",
            "Iteration 494, Norm of Gradient: 0.03736609150857202, Cost (Train): 0.39274370623683297\n",
            "Iteration 495, Norm of Gradient: 0.037320642078570825, Cost (Train): 0.39260450776895056\n",
            "Iteration 496, Norm of Gradient: 0.03727534390613722, Cost (Train): 0.3924656468240036\n",
            "Iteration 497, Norm of Gradient: 0.037230196190629776, Cost (Train): 0.39232712187380303\n",
            "Iteration 498, Norm of Gradient: 0.03718519813705157, Cost (Train): 0.39218893140012706\n",
            "Iteration 499, Norm of Gradient: 0.03714034895600124, Cost (Train): 0.39205107389463756\n",
            "Iteration 500, Norm of Gradient: 0.03709564786362435, Cost (Train): 0.3919135478587974\n",
            "Iteration 501, Norm of Gradient: 0.03705109408156538, Cost (Train): 0.39177635180378884\n",
            "Iteration 502, Norm of Gradient: 0.03700668683692014, Cost (Train): 0.3916394842504324\n",
            "Iteration 503, Norm of Gradient: 0.036962425362188696, Cost (Train): 0.39150294372910677\n",
            "Iteration 504, Norm of Gradient: 0.03691830889522875, Cost (Train): 0.3913667287796695\n",
            "Iteration 505, Norm of Gradient: 0.03687433667920957, Cost (Train): 0.3912308379513786\n",
            "Iteration 506, Norm of Gradient: 0.036830507962566345, Cost (Train): 0.39109526980281456\n",
            "Iteration 507, Norm of Gradient: 0.03678682199895491, Cost (Train): 0.3909600229018036\n",
            "Iteration 508, Norm of Gradient: 0.03674327804720715, Cost (Train): 0.39082509582534114\n",
            "Iteration 509, Norm of Gradient: 0.036699875371286626, Cost (Train): 0.39069048715951676\n",
            "Iteration 510, Norm of Gradient: 0.03665661324024482, Cost (Train): 0.3905561954994393\n",
            "Iteration 511, Norm of Gradient: 0.03661349092817771, Cost (Train): 0.390422219449163\n",
            "Iteration 512, Norm of Gradient: 0.03657050771418286, Cost (Train): 0.39028855762161413\n",
            "Iteration 513, Norm of Gradient: 0.0365276628823169, Cost (Train): 0.3901552086385186\n",
            "Iteration 514, Norm of Gradient: 0.03648495572155347, Cost (Train): 0.3900221711303304\n",
            "Iteration 515, Norm of Gradient: 0.03644238552574152, Cost (Train): 0.38988944373616\n",
            "Iteration 516, Norm of Gradient: 0.03639995159356411, Cost (Train): 0.3897570251037047\n",
            "Iteration 517, Norm of Gradient: 0.03635765322849756, Cost (Train): 0.38962491388917836\n",
            "Iteration 518, Norm of Gradient: 0.03631548973877106, Cost (Train): 0.3894931087572425\n",
            "Iteration 519, Norm of Gradient: 0.036273460437326634, Cost (Train): 0.38936160838093864\n",
            "Iteration 520, Norm of Gradient: 0.036231564641779554, Cost (Train): 0.38923041144161963\n",
            "Iteration 521, Norm of Gradient: 0.036189801674379074, Cost (Train): 0.3890995166288834\n",
            "Iteration 522, Norm of Gradient: 0.036148170861969676, Cost (Train): 0.3889689226405065\n",
            "Iteration 523, Norm of Gradient: 0.03610667153595251, Cost (Train): 0.38883862818237824\n",
            "Iteration 524, Norm of Gradient: 0.036065303032247475, Cost (Train): 0.38870863196843586\n",
            "Iteration 525, Norm of Gradient: 0.03602406469125542, Cost (Train): 0.38857893272059985\n",
            "Iteration 526, Norm of Gradient: 0.03598295585782088, Cost (Train): 0.3884495291687105\n",
            "Iteration 527, Norm of Gradient: 0.03594197588119514, Cost (Train): 0.3883204200504646\n",
            "Iteration 528, Norm of Gradient: 0.0359011241149996, Cost (Train): 0.38819160411135256\n",
            "Iteration 529, Norm of Gradient: 0.03586039991718962, Cost (Train): 0.3880630801045972\n",
            "Iteration 530, Norm of Gradient: 0.03581980265001859, Cost (Train): 0.3879348467910916\n",
            "Iteration 531, Norm of Gradient: 0.035779331680002496, Cost (Train): 0.3878069029393388\n",
            "Iteration 532, Norm of Gradient: 0.03573898637788466, Cost (Train): 0.3876792473253914\n",
            "Iteration 533, Norm of Gradient: 0.03569876611860096, Cost (Train): 0.3875518787327921\n",
            "Iteration 534, Norm of Gradient: 0.035658670281245405, Cost (Train): 0.3874247959525148\n",
            "Iteration 535, Norm of Gradient: 0.03561869824903587, Cost (Train): 0.38729799778290547\n",
            "Iteration 536, Norm of Gradient: 0.035578849409280376, Cost (Train): 0.3871714830296251\n",
            "Iteration 537, Norm of Gradient: 0.03553912315334353, Cost (Train): 0.3870452505055919\n",
            "Iteration 538, Norm of Gradient: 0.03549951887661346, Cost (Train): 0.3869192990309242\n",
            "Iteration 539, Norm of Gradient: 0.03546003597846888, Cost (Train): 0.38679362743288487\n",
            "Iteration 540, Norm of Gradient: 0.0354206738622466, Cost (Train): 0.38666823454582483\n",
            "Iteration 541, Norm of Gradient: 0.03538143193520933, Cost (Train): 0.38654311921112844\n",
            "Iteration 542, Norm of Gradient: 0.03534230960851377, Cost (Train): 0.3864182802771584\n",
            "Iteration 543, Norm of Gradient: 0.035303306297179024, Cost (Train): 0.3862937165992018\n",
            "Iteration 544, Norm of Gradient: 0.035264421420055314, Cost (Train): 0.3861694270394163\n",
            "Iteration 545, Norm of Gradient: 0.03522565439979294, Cost (Train): 0.38604541046677704\n",
            "Iteration 546, Norm of Gradient: 0.0351870046628117, Cost (Train): 0.38592166575702397\n",
            "Iteration 547, Norm of Gradient: 0.03514847163927038, Cost (Train): 0.3857981917926095\n",
            "Iteration 548, Norm of Gradient: 0.0351100547630367, Cost (Train): 0.38567498746264717\n",
            "Iteration 549, Norm of Gradient: 0.03507175347165751, Cost (Train): 0.38555205166286005\n",
            "Iteration 550, Norm of Gradient: 0.03503356720632921, Cost (Train): 0.38542938329553045\n",
            "Iteration 551, Norm of Gradient: 0.03499549541186853, Cost (Train): 0.38530698126944907\n",
            "Iteration 552, Norm of Gradient: 0.034957537536683514, Cost (Train): 0.38518484449986584\n",
            "Iteration 553, Norm of Gradient: 0.0349196930327449, Cost (Train): 0.3850629719084402\n",
            "Iteration 554, Norm of Gradient: 0.034881961355557566, Cost (Train): 0.3849413624231922\n",
            "Iteration 555, Norm of Gradient: 0.03484434196413248, Cost (Train): 0.3848200149784544\n",
            "Iteration 556, Norm of Gradient: 0.034806834320958734, Cost (Train): 0.3846989285148236\n",
            "Iteration 557, Norm of Gradient: 0.03476943789197597, Cost (Train): 0.3845781019791136\n",
            "Iteration 558, Norm of Gradient: 0.03473215214654692, Cost (Train): 0.3844575343243075\n",
            "Iteration 559, Norm of Gradient: 0.03469497655743038, Cost (Train): 0.3843372245095119\n",
            "Iteration 560, Norm of Gradient: 0.03465791060075431, Cost (Train): 0.3842171714999102\n",
            "Iteration 561, Norm of Gradient: 0.03462095375598919, Cost (Train): 0.38409737426671653\n",
            "Iteration 562, Norm of Gradient: 0.03458410550592175, Cost (Train): 0.38397783178713096\n",
            "Iteration 563, Norm of Gradient: 0.03454736533662877, Cost (Train): 0.383858543044294\n",
            "Iteration 564, Norm of Gradient: 0.034510732737451214, Cost (Train): 0.3837395070272423\n",
            "Iteration 565, Norm of Gradient: 0.03447420720096868, Cost (Train): 0.3836207227308648\n",
            "Iteration 566, Norm of Gradient: 0.03443778822297391, Cost (Train): 0.383502189155858\n",
            "Iteration 567, Norm of Gradient: 0.03440147530244769, Cost (Train): 0.3833839053086837\n",
            "Iteration 568, Norm of Gradient: 0.034365267941533925, Cost (Train): 0.3832658702015256\n",
            "Iteration 569, Norm of Gradient: 0.03432916564551493, Cost (Train): 0.38314808285224605\n",
            "Iteration 570, Norm of Gradient: 0.034293167922786964, Cost (Train): 0.38303054228434513\n",
            "Iteration 571, Norm of Gradient: 0.03425727428483603, Cost (Train): 0.3829132475269181\n",
            "Iteration 572, Norm of Gradient: 0.034221484246213796, Cost (Train): 0.3827961976146137\n",
            "Iteration 573, Norm of Gradient: 0.034185797324513885, Cost (Train): 0.38267939158759384\n",
            "Iteration 574, Norm of Gradient: 0.034150213040348223, Cost (Train): 0.3825628284914923\n",
            "Iteration 575, Norm of Gradient: 0.03411473091732371, Cost (Train): 0.38244650737737446\n",
            "Iteration 576, Norm of Gradient: 0.034079350482019134, Cost (Train): 0.38233042730169775\n",
            "Iteration 577, Norm of Gradient: 0.034044071263962104, Cost (Train): 0.38221458732627167\n",
            "Iteration 578, Norm of Gradient: 0.03400889279560649, Cost (Train): 0.38209898651821844\n",
            "Iteration 579, Norm of Gradient: 0.03397381461230977, Cost (Train): 0.38198362394993446\n",
            "Iteration 580, Norm of Gradient: 0.03393883625231081, Cost (Train): 0.3818684986990516\n",
            "Iteration 581, Norm of Gradient: 0.03390395725670772, Cost (Train): 0.38175360984839857\n",
            "Iteration 582, Norm of Gradient: 0.033869177169435966, Cost (Train): 0.38163895648596396\n",
            "Iteration 583, Norm of Gradient: 0.033834495537246614, Cost (Train): 0.3815245377048576\n",
            "Iteration 584, Norm of Gradient: 0.03379991190968492, Cost (Train): 0.3814103526032742\n",
            "Iteration 585, Norm of Gradient: 0.0337654258390689, Cost (Train): 0.381296400284456\n",
            "Iteration 586, Norm of Gradient: 0.033731036880468294, Cost (Train): 0.3811826798566563\n",
            "Iteration 587, Norm of Gradient: 0.03369674459168358, Cost (Train): 0.3810691904331036\n",
            "Iteration 588, Norm of Gradient: 0.03366254853322527, Cost (Train): 0.3809559311319648\n",
            "Iteration 589, Norm of Gradient: 0.03362844826829334, Cost (Train): 0.3808429010763106\n",
            "Iteration 590, Norm of Gradient: 0.03359444336275682, Cost (Train): 0.3807300993940795\n",
            "Iteration 591, Norm of Gradient: 0.033560533385133685, Cost (Train): 0.3806175252180432\n",
            "Iteration 592, Norm of Gradient: 0.03352671790657077, Cost (Train): 0.38050517768577163\n",
            "Iteration 593, Norm of Gradient: 0.033492996500823984, Cost (Train): 0.380393055939599\n",
            "Iteration 594, Norm of Gradient: 0.033459368744238625, Cost (Train): 0.38028115912658905\n",
            "Iteration 595, Norm of Gradient: 0.03342583421572998, Cost (Train): 0.38016948639850195\n",
            "Iteration 596, Norm of Gradient: 0.033392392496763894, Cost (Train): 0.38005803691176043\n",
            "Iteration 597, Norm of Gradient: 0.03335904317133776, Cost (Train): 0.3799468098274167\n",
            "Iteration 598, Norm of Gradient: 0.033325785825961504, Cost (Train): 0.3798358043111194\n",
            "Iteration 599, Norm of Gradient: 0.03329262004963879, Cost (Train): 0.37972501953308135\n",
            "Iteration 600, Norm of Gradient: 0.033259545433848396, Cost (Train): 0.37961445466804655\n",
            "Iteration 601, Norm of Gradient: 0.03322656157252578, Cost (Train): 0.37950410889525893\n",
            "Iteration 602, Norm of Gradient: 0.03319366806204473, Cost (Train): 0.37939398139842984\n",
            "Iteration 603, Norm of Gradient: 0.033160864501199296, Cost (Train): 0.3792840713657069\n",
            "Iteration 604, Norm of Gradient: 0.03312815049118575, Cost (Train): 0.37917437798964276\n",
            "Iteration 605, Norm of Gradient: 0.03309552563558478, Cost (Train): 0.3790649004671639\n",
            "Iteration 606, Norm of Gradient: 0.03306298954034386, Cost (Train): 0.3789556379995402\n",
            "Iteration 607, Norm of Gradient: 0.03303054181375974, Cost (Train): 0.3788465897923543\n",
            "Iteration 608, Norm of Gradient: 0.03299818206646102, Cost (Train): 0.37873775505547136\n",
            "Iteration 609, Norm of Gradient: 0.03296590991139106, Cost (Train): 0.3786291330030093\n",
            "Iteration 610, Norm of Gradient: 0.03293372496379086, Cost (Train): 0.3785207228533091\n",
            "Iteration 611, Norm of Gradient: 0.032901626841182155, Cost (Train): 0.37841252382890517\n",
            "Iteration 612, Norm of Gradient: 0.03286961516335068, Cost (Train): 0.3783045351564964\n",
            "Iteration 613, Norm of Gradient: 0.03283768955232956, Cost (Train): 0.37819675606691705\n",
            "Iteration 614, Norm of Gradient: 0.032805849632382846, Cost (Train): 0.37808918579510825\n",
            "Iteration 615, Norm of Gradient: 0.032774095029989174, Cost (Train): 0.3779818235800894\n",
            "Iteration 616, Norm of Gradient: 0.03274242537382562, Cost (Train): 0.3778746686649301\n",
            "Iteration 617, Norm of Gradient: 0.03271084029475162, Cost (Train): 0.37776772029672195\n",
            "Iteration 618, Norm of Gradient: 0.0326793394257931, Cost (Train): 0.377660977726551\n",
            "Iteration 619, Norm of Gradient: 0.03264792240212668, Cost (Train): 0.3775544402094703\n",
            "Iteration 620, Norm of Gradient: 0.032616588861064104, Cost (Train): 0.37744810700447246\n",
            "Iteration 621, Norm of Gradient: 0.03258533844203668, Cost (Train): 0.37734197737446284\n",
            "Iteration 622, Norm of Gradient: 0.03255417078657993, Cost (Train): 0.37723605058623244\n",
            "Iteration 623, Norm of Gradient: 0.03252308553831843, Cost (Train): 0.3771303259104315\n",
            "Iteration 624, Norm of Gradient: 0.03249208234295062, Cost (Train): 0.377024802621543\n",
            "Iteration 625, Norm of Gradient: 0.03246116084823388, Cost (Train): 0.3769194799978567\n",
            "Iteration 626, Norm of Gradient: 0.0324303207039697, Cost (Train): 0.3768143573214431\n",
            "Iteration 627, Norm of Gradient: 0.03239956156198895, Cost (Train): 0.3767094338781277\n",
            "Iteration 628, Norm of Gradient: 0.03236888307613729, Cost (Train): 0.3766047089574655\n",
            "Iteration 629, Norm of Gradient: 0.03233828490226071, Cost (Train): 0.3765001818527157\n",
            "Iteration 630, Norm of Gradient: 0.032307766698191194, Cost (Train): 0.3763958518608167\n",
            "Iteration 631, Norm of Gradient: 0.032277328123732484, Cost (Train): 0.37629171828236097\n",
            "Iteration 632, Norm of Gradient: 0.03224696884064603, Cost (Train): 0.37618778042157086\n",
            "Iteration 633, Norm of Gradient: 0.03221668851263694, Cost (Train): 0.3760840375862736\n",
            "Iteration 634, Norm of Gradient: 0.03218648680534016, Cost (Train): 0.37598048908787735\n",
            "Iteration 635, Norm of Gradient: 0.03215636338630676, Cost (Train): 0.37587713424134683\n",
            "Iteration 636, Norm of Gradient: 0.032126317924990225, Cost (Train): 0.37577397236518006\n",
            "Iteration 637, Norm of Gradient: 0.03209635009273303, Cost (Train): 0.37567100278138393\n",
            "Iteration 638, Norm of Gradient: 0.0320664595627532, Cost (Train): 0.3755682248154512\n",
            "Iteration 639, Norm of Gradient: 0.032036646010130995, Cost (Train): 0.37546563779633696\n",
            "Iteration 640, Norm of Gradient: 0.03200690911179581, Cost (Train): 0.3753632410564358\n",
            "Iteration 641, Norm of Gradient: 0.03197724854651303, Cost (Train): 0.3752610339315585\n",
            "Iteration 642, Norm of Gradient: 0.03194766399487116, Cost (Train): 0.37515901576090976\n",
            "Iteration 643, Norm of Gradient: 0.031918155139268885, Cost (Train): 0.37505718588706516\n",
            "Iteration 644, Norm of Gradient: 0.031888721663902435, Cost (Train): 0.3749555436559493\n",
            "Iteration 645, Norm of Gradient: 0.03185936325475284, Cost (Train): 0.37485408841681317\n",
            "Iteration 646, Norm of Gradient: 0.031830079599573506, Cost (Train): 0.3747528195222125\n",
            "Iteration 647, Norm of Gradient: 0.03180087038787774, Cost (Train): 0.3746517363279856\n",
            "Iteration 648, Norm of Gradient: 0.03177173531092641, Cost (Train): 0.3745508381932321\n",
            "Iteration 649, Norm of Gradient: 0.0317426740617158, Cost (Train): 0.3744501244802911\n",
            "Iteration 650, Norm of Gradient: 0.031713686334965414, Cost (Train): 0.37434959455472\n",
            "Iteration 651, Norm of Gradient: 0.03168477182710602, Cost (Train): 0.37424924778527363\n",
            "Iteration 652, Norm of Gradient: 0.031655930236267685, Cost (Train): 0.37414908354388277\n",
            "Iteration 653, Norm of Gradient: 0.03162716126226801, Cost (Train): 0.3740491012056338\n",
            "Iteration 654, Norm of Gradient: 0.03159846460660037, Cost (Train): 0.3739493001487477\n",
            "Iteration 655, Norm of Gradient: 0.03156983997242229, Cost (Train): 0.3738496797545602\n",
            "Iteration 656, Norm of Gradient: 0.03154128706454395, Cost (Train): 0.3737502394075005\n",
            "Iteration 657, Norm of Gradient: 0.03151280558941671, Cost (Train): 0.3736509784950723\n",
            "Iteration 658, Norm of Gradient: 0.03148439525512181, Cost (Train): 0.37355189640783265\n",
            "Iteration 659, Norm of Gradient: 0.031456055771359084, Cost (Train): 0.37345299253937303\n",
            "Iteration 660, Norm of Gradient: 0.03142778684943585, Cost (Train): 0.3733542662862992\n",
            "Iteration 661, Norm of Gradient: 0.03139958820225579, Cost (Train): 0.3732557170482119\n",
            "Iteration 662, Norm of Gradient: 0.03137145954430804, Cost (Train): 0.3731573442276874\n",
            "Iteration 663, Norm of Gradient: 0.031343400591656245, Cost (Train): 0.37305914723025824\n",
            "Iteration 664, Norm of Gradient: 0.03131541106192782, Cost (Train): 0.3729611254643944\n",
            "Iteration 665, Norm of Gradient: 0.031287490674303184, Cost (Train): 0.3728632783414841\n",
            "Iteration 666, Norm of Gradient: 0.03125963914950522, Cost (Train): 0.37276560527581526\n",
            "Iteration 667, Norm of Gradient: 0.031231856209788633, Cost (Train): 0.3726681056845568\n",
            "Iteration 668, Norm of Gradient: 0.031204141578929607, Cost (Train): 0.37257077898774044\n",
            "Iteration 669, Norm of Gradient: 0.031176494982215374, Cost (Train): 0.3724736246082418\n",
            "Iteration 670, Norm of Gradient: 0.031148916146433954, Cost (Train): 0.3723766419717626\n",
            "Iteration 671, Norm of Gradient: 0.03112140479986396, Cost (Train): 0.372279830506813\n",
            "Iteration 672, Norm of Gradient: 0.031093960672264487, Cost (Train): 0.37218318964469277\n",
            "Iteration 673, Norm of Gradient: 0.031066583494865057, Cost (Train): 0.37208671881947447\n",
            "Iteration 674, Norm of Gradient: 0.03103927300035568, Cost (Train): 0.3719904174679853\n",
            "Iteration 675, Norm of Gradient: 0.031012028922876998, Cost (Train): 0.37189428502978966\n",
            "Iteration 676, Norm of Gradient: 0.03098485099801044, Cost (Train): 0.3717983209471722\n",
            "Iteration 677, Norm of Gradient: 0.03095773896276859, Cost (Train): 0.37170252466511994\n",
            "Iteration 678, Norm of Gradient: 0.030930692555585455, Cost (Train): 0.3716068956313059\n",
            "Iteration 679, Norm of Gradient: 0.030903711516306952, Cost (Train): 0.37151143329607167\n",
            "Iteration 680, Norm of Gradient: 0.030876795586181446, Cost (Train): 0.3714161371124107\n",
            "Iteration 681, Norm of Gradient: 0.030849944507850276, Cost (Train): 0.3713210065359519\n",
            "Iteration 682, Norm of Gradient: 0.030823158025338495, Cost (Train): 0.3712260410249425\n",
            "Iteration 683, Norm of Gradient: 0.030796435884045563, Cost (Train): 0.37113124004023235\n",
            "Iteration 684, Norm of Gradient: 0.03076977783073616, Cost (Train): 0.37103660304525704\n",
            "Iteration 685, Norm of Gradient: 0.03074318361353109, Cost (Train): 0.3709421295060218\n",
            "Iteration 686, Norm of Gradient: 0.030716652981898237, Cost (Train): 0.37084781889108576\n",
            "Iteration 687, Norm of Gradient: 0.0306901856866436, Cost (Train): 0.3707536706715459\n",
            "Iteration 688, Norm of Gradient: 0.03066378147990238, Cost (Train): 0.370659684321021\n",
            "Iteration 689, Norm of Gradient: 0.030637440115130157, Cost (Train): 0.3705658593156361\n",
            "Iteration 690, Norm of Gradient: 0.030611161347094133, Cost (Train): 0.37047219513400703\n",
            "Iteration 691, Norm of Gradient: 0.03058494493186443, Cost (Train): 0.37037869125722483\n",
            "Iteration 692, Norm of Gradient: 0.030558790626805513, Cost (Train): 0.3702853471688405\n",
            "Iteration 693, Norm of Gradient: 0.030532698190567573, Cost (Train): 0.37019216235484953\n",
            "Iteration 694, Norm of Gradient: 0.03050666738307808, Cost (Train): 0.3700991363036773\n",
            "Iteration 695, Norm of Gradient: 0.030480697965533352, Cost (Train): 0.37000626850616347\n",
            "Iteration 696, Norm of Gradient: 0.0304547897003902, Cost (Train): 0.36991355845554774\n",
            "Iteration 697, Norm of Gradient: 0.03042894235135765, Cost (Train): 0.3698210056474546\n",
            "Iteration 698, Norm of Gradient: 0.030403155683388697, Cost (Train): 0.3697286095798788\n",
            "Iteration 699, Norm of Gradient: 0.030377429462672155, Cost (Train): 0.36963636975317077\n",
            "Iteration 700, Norm of Gradient: 0.03035176345662458, Cost (Train): 0.3695442856700225\n",
            "Iteration 701, Norm of Gradient: 0.030326157433882197, Cost (Train): 0.3694523568354529\n",
            "Iteration 702, Norm of Gradient: 0.03030061116429298, Cost (Train): 0.36936058275679345\n",
            "Iteration 703, Norm of Gradient: 0.030275124418908665, Cost (Train): 0.3692689629436744\n",
            "Iteration 704, Norm of Gradient: 0.030249696969977, Cost (Train): 0.36917749690801077\n",
            "Iteration 705, Norm of Gradient: 0.030224328590933876, Cost (Train): 0.3690861841639882\n",
            "Iteration 706, Norm of Gradient: 0.030199019056395642, Cost (Train): 0.3689950242280492\n",
            "Iteration 707, Norm of Gradient: 0.030173768142151426, Cost (Train): 0.3689040166188797\n",
            "Iteration 708, Norm of Gradient: 0.030148575625155535, Cost (Train): 0.3688131608573952\n",
            "Iteration 709, Norm of Gradient: 0.030123441283519875, Cost (Train): 0.36872245646672724\n",
            "Iteration 710, Norm of Gradient: 0.030098364896506503, Cost (Train): 0.3686319029722104\n",
            "Iteration 711, Norm of Gradient: 0.030073346244520176, Cost (Train): 0.36854149990136825\n",
            "Iteration 712, Norm of Gradient: 0.03004838510910094, Cost (Train): 0.3684512467839011\n",
            "Iteration 713, Norm of Gradient: 0.030023481272916874, Cost (Train): 0.3683611431516722\n",
            "Iteration 714, Norm of Gradient: 0.02999863451975677, Cost (Train): 0.3682711885386949\n",
            "Iteration 715, Norm of Gradient: 0.02997384463452296, Cost (Train): 0.3681813824811198\n",
            "Iteration 716, Norm of Gradient: 0.029949111403224128, Cost (Train): 0.36809172451722205\n",
            "Iteration 717, Norm of Gradient: 0.02992443461296826, Cost (Train): 0.36800221418738854\n",
            "Iteration 718, Norm of Gradient: 0.02989981405195555, Cost (Train): 0.367912851034105\n",
            "Iteration 719, Norm of Gradient: 0.029875249509471413, Cost (Train): 0.36782363460194395\n",
            "Iteration 720, Norm of Gradient: 0.02985074077587961, Cost (Train): 0.36773456443755187\n",
            "Iteration 721, Norm of Gradient: 0.02982628764261527, Cost (Train): 0.3676456400896369\n",
            "Iteration 722, Norm of Gradient: 0.02980188990217813, Cost (Train): 0.36755686110895697\n",
            "Iteration 723, Norm of Gradient: 0.029777547348125693, Cost (Train): 0.36746822704830717\n",
            "Iteration 724, Norm of Gradient: 0.029753259775066574, Cost (Train): 0.36737973746250774\n",
            "Iteration 725, Norm of Gradient: 0.029729026978653737, Cost (Train): 0.36729139190839216\n",
            "Iteration 726, Norm of Gradient: 0.029704848755577936, Cost (Train): 0.36720318994479534\n",
            "Iteration 727, Norm of Gradient: 0.02968072490356108, Cost (Train): 0.36711513113254163\n",
            "Iteration 728, Norm of Gradient: 0.029656655221349745, Cost (Train): 0.3670272150344332\n",
            "Iteration 729, Norm of Gradient: 0.02963263950870868, Cost (Train): 0.36693944121523814\n",
            "Iteration 730, Norm of Gradient: 0.029608677566414384, Cost (Train): 0.36685180924167937\n",
            "Iteration 731, Norm of Gradient: 0.02958476919624868, Cost (Train): 0.3667643186824226\n",
            "Iteration 732, Norm of Gradient: 0.02956091420099245, Cost (Train): 0.36667696910806524\n",
            "Iteration 733, Norm of Gradient: 0.029537112384419268, Cost (Train): 0.36658976009112537\n",
            "Iteration 734, Norm of Gradient: 0.029513363551289258, Cost (Train): 0.3665026912060295\n",
            "Iteration 735, Norm of Gradient: 0.029489667507342793, Cost (Train): 0.36641576202910264\n",
            "Iteration 736, Norm of Gradient: 0.029466024059294424, Cost (Train): 0.3663289721385564\n",
            "Iteration 737, Norm of Gradient: 0.029442433014826734, Cost (Train): 0.3662423211144783\n",
            "Iteration 738, Norm of Gradient: 0.02941889418258432, Cost (Train): 0.36615580853882074\n",
            "Iteration 739, Norm of Gradient: 0.029395407372167733, Cost (Train): 0.36606943399539027\n",
            "Iteration 740, Norm of Gradient: 0.029371972394127557, Cost (Train): 0.3659831970698367\n",
            "Iteration 741, Norm of Gradient: 0.029348589059958474, Cost (Train): 0.3658970973496424\n",
            "Iteration 742, Norm of Gradient: 0.029325257182093344, Cost (Train): 0.36581113442411206\n",
            "Iteration 743, Norm of Gradient: 0.02930197657389744, Cost (Train): 0.36572530788436125\n",
            "Iteration 744, Norm of Gradient: 0.02927874704966261, Cost (Train): 0.36563961732330724\n",
            "Iteration 745, Norm of Gradient: 0.029255568424601523, Cost (Train): 0.3655540623356573\n",
            "Iteration 746, Norm of Gradient: 0.029232440514841987, Cost (Train): 0.3654686425178996\n",
            "Iteration 747, Norm of Gradient: 0.02920936313742127, Cost (Train): 0.36538335746829187\n",
            "Iteration 748, Norm of Gradient: 0.02918633611028049, Cost (Train): 0.36529820678685193\n",
            "Iteration 749, Norm of Gradient: 0.029163359252259016, Cost (Train): 0.36521319007534736\n",
            "Iteration 750, Norm of Gradient: 0.02914043238308894, Cost (Train): 0.3651283069372855\n",
            "Iteration 751, Norm of Gradient: 0.02911755532338955, Cost (Train): 0.3650435569779034\n",
            "Iteration 752, Norm of Gradient: 0.02909472789466192, Cost (Train): 0.3649589398041584\n",
            "Iteration 753, Norm of Gradient: 0.029071949919283458, Cost (Train): 0.36487445502471727\n",
            "Iteration 754, Norm of Gradient: 0.029049221220502505, Cost (Train): 0.3647901022499478\n",
            "Iteration 755, Norm of Gradient: 0.029026541622433046, Cost (Train): 0.3647058810919078\n",
            "Iteration 756, Norm of Gradient: 0.029003910950049363, Cost (Train): 0.3646217911643368\n",
            "Iteration 757, Norm of Gradient: 0.028981329029180793, Cost (Train): 0.36453783208264523\n",
            "Iteration 758, Norm of Gradient: 0.0289587956865065, Cost (Train): 0.3644540034639058\n",
            "Iteration 759, Norm of Gradient: 0.028936310749550293, Cost (Train): 0.36437030492684386\n",
            "Iteration 760, Norm of Gradient: 0.028913874046675458, Cost (Train): 0.3642867360918278\n",
            "Iteration 761, Norm of Gradient: 0.028891485407079667, Cost (Train): 0.36420329658086037\n",
            "Iteration 762, Norm of Gradient: 0.028869144660789877, Cost (Train): 0.3641199860175685\n",
            "Iteration 763, Norm of Gradient: 0.02884685163865733, Cost (Train): 0.3640368040271951\n",
            "Iteration 764, Norm of Gradient: 0.028824606172352524, Cost (Train): 0.36395375023658966\n",
            "Iteration 765, Norm of Gradient: 0.028802408094360236, Cost (Train): 0.3638708242741986\n",
            "Iteration 766, Norm of Gradient: 0.028780257237974626, Cost (Train): 0.3637880257700575\n",
            "Iteration 767, Norm of Gradient: 0.02875815343729432, Cost (Train): 0.363705354355781\n",
            "Iteration 768, Norm of Gradient: 0.028736096527217554, Cost (Train): 0.36362280966455474\n",
            "Iteration 769, Norm of Gradient: 0.028714086343437344, Cost (Train): 0.3635403913311262\n",
            "Iteration 770, Norm of Gradient: 0.028692122722436722, Cost (Train): 0.36345809899179615\n",
            "Iteration 771, Norm of Gradient: 0.02867020550148394, Cost (Train): 0.36337593228441006\n",
            "Iteration 772, Norm of Gradient: 0.028648334518627788, Cost (Train): 0.363293890848349\n",
            "Iteration 773, Norm of Gradient: 0.028626509612692877, Cost (Train): 0.3632119743245218\n",
            "Iteration 774, Norm of Gradient: 0.028604730623275, Cost (Train): 0.363130182355356\n",
            "Iteration 775, Norm of Gradient: 0.02858299739073651, Cost (Train): 0.3630485145847898\n",
            "Iteration 776, Norm of Gradient: 0.028561309756201737, Cost (Train): 0.3629669706582634\n",
            "Iteration 777, Norm of Gradient: 0.028539667561552417, Cost (Train): 0.36288555022271074\n",
            "Iteration 778, Norm of Gradient: 0.028518070649423168, Cost (Train): 0.36280425292655155\n",
            "Iteration 779, Norm of Gradient: 0.028496518863197037, Cost (Train): 0.36272307841968265\n",
            "Iteration 780, Norm of Gradient: 0.028475012047000987, Cost (Train): 0.36264202635347015\n",
            "Iteration 781, Norm of Gradient: 0.02845355004570152, Cost (Train): 0.36256109638074135\n",
            "Iteration 782, Norm of Gradient: 0.02843213270490023, Cost (Train): 0.36248028815577654\n",
            "Iteration 783, Norm of Gradient: 0.028410759870929503, Cost (Train): 0.36239960133430094\n",
            "Iteration 784, Norm of Gradient: 0.02838943139084811, Cost (Train): 0.3623190355734772\n",
            "Iteration 785, Norm of Gradient: 0.02836814711243696, Cost (Train): 0.36223859053189705\n",
            "Iteration 786, Norm of Gradient: 0.028346906884194806, Cost (Train): 0.3621582658695738\n",
            "Iteration 787, Norm of Gradient: 0.028325710555334, Cost (Train): 0.3620780612479341\n",
            "Iteration 788, Norm of Gradient: 0.028304557975776294, Cost (Train): 0.36199797632981123\n",
            "Iteration 789, Norm of Gradient: 0.02828344899614863, Cost (Train): 0.36191801077943603\n",
            "Iteration 790, Norm of Gradient: 0.028262383467779012, Cost (Train): 0.36183816426243054\n",
            "Iteration 791, Norm of Gradient: 0.028241361242692376, Cost (Train): 0.3617584364457995\n",
            "Iteration 792, Norm of Gradient: 0.028220382173606482, Cost (Train): 0.3616788269979237\n",
            "Iteration 793, Norm of Gradient: 0.02819944611392786, Cost (Train): 0.3615993355885518\n",
            "Iteration 794, Norm of Gradient: 0.028178552917747762, Cost (Train): 0.36151996188879304\n",
            "Iteration 795, Norm of Gradient: 0.028157702439838166, Cost (Train): 0.36144070557111024\n",
            "Iteration 796, Norm of Gradient: 0.028136894535647772, Cost (Train): 0.3613615663093124\n",
            "Iteration 797, Norm of Gradient: 0.02811612906129806, Cost (Train): 0.36128254377854685\n",
            "Iteration 798, Norm of Gradient: 0.02809540587357937, Cost (Train): 0.36120363765529284\n",
            "Iteration 799, Norm of Gradient: 0.028074724829946978, Cost (Train): 0.361124847617354\n",
            "Iteration 800, Norm of Gradient: 0.02805408578851725, Cost (Train): 0.36104617334385086\n",
            "Iteration 801, Norm of Gradient: 0.02803348860806379, Cost (Train): 0.36096761451521453\n",
            "Iteration 802, Norm of Gradient: 0.028012933148013598, Cost (Train): 0.3608891708131791\n",
            "Iteration 803, Norm of Gradient: 0.027992419268443296, Cost (Train): 0.3608108419207749\n",
            "Iteration 804, Norm of Gradient: 0.027971946830075388, Cost (Train): 0.3607326275223213\n",
            "Iteration 805, Norm of Gradient: 0.027951515694274443, Cost (Train): 0.3606545273034203\n",
            "Iteration 806, Norm of Gradient: 0.02793112572304348, Cost (Train): 0.3605765409509491\n",
            "Iteration 807, Norm of Gradient: 0.02791077677902018, Cost (Train): 0.3604986681530538\n",
            "Iteration 808, Norm of Gradient: 0.027890468725473298, Cost (Train): 0.36042090859914233\n",
            "Iteration 809, Norm of Gradient: 0.027870201426298983, Cost (Train): 0.3603432619798779\n",
            "Iteration 810, Norm of Gradient: 0.027849974746017178, Cost (Train): 0.36026572798717227\n",
            "Iteration 811, Norm of Gradient: 0.027829788549768048, Cost (Train): 0.36018830631417903\n",
            "Iteration 812, Norm of Gradient: 0.027809642703308365, Cost (Train): 0.3601109966552872\n",
            "Iteration 813, Norm of Gradient: 0.027789537073008037, Cost (Train): 0.3600337987061145\n",
            "Iteration 814, Norm of Gradient: 0.027769471525846523, Cost (Train): 0.3599567121635012\n",
            "Iteration 815, Norm of Gradient: 0.027749445929409387, Cost (Train): 0.3598797367255032\n",
            "Iteration 816, Norm of Gradient: 0.027729460151884832, Cost (Train): 0.35980287209138584\n",
            "Iteration 817, Norm of Gradient: 0.027709514062060226, Cost (Train): 0.35972611796161746\n",
            "Iteration 818, Norm of Gradient: 0.0276896075293187, Cost (Train): 0.35964947403786357\n",
            "Iteration 819, Norm of Gradient: 0.02766974042363576, Cost (Train): 0.35957294002297957\n",
            "Iteration 820, Norm of Gradient: 0.02764991261557588, Cost (Train): 0.35949651562100543\n",
            "Iteration 821, Norm of Gradient: 0.027630123976289195, Cost (Train): 0.35942020053715895\n",
            "Iteration 822, Norm of Gradient: 0.02761037437750813, Cost (Train): 0.3593439944778298\n",
            "Iteration 823, Norm of Gradient: 0.027590663691544122, Cost (Train): 0.3592678971505733\n",
            "Iteration 824, Norm of Gradient: 0.027570991791284336, Cost (Train): 0.3591919082641045\n",
            "Iteration 825, Norm of Gradient: 0.027551358550188376, Cost (Train): 0.3591160275282918\n",
            "Iteration 826, Norm of Gradient: 0.02753176384228508, Cost (Train): 0.35904025465415146\n",
            "Iteration 827, Norm of Gradient: 0.02751220754216927, Cost (Train): 0.35896458935384096\n",
            "Iteration 828, Norm of Gradient: 0.027492689524998615, Cost (Train): 0.35888903134065364\n",
            "Iteration 829, Norm of Gradient: 0.027473209666490354, Cost (Train): 0.35881358032901256\n",
            "Iteration 830, Norm of Gradient: 0.02745376784291826, Cost (Train): 0.3587382360344646\n",
            "Iteration 831, Norm of Gradient: 0.027434363931109405, Cost (Train): 0.3586629981736748\n",
            "Iteration 832, Norm of Gradient: 0.02741499780844114, Cost (Train): 0.3585878664644204\n",
            "Iteration 833, Norm of Gradient: 0.02739566935283794, Cost (Train): 0.35851284062558514\n",
            "Iteration 834, Norm of Gradient: 0.027376378442768357, Cost (Train): 0.3584379203771537\n",
            "Iteration 835, Norm of Gradient: 0.027357124957241956, Cost (Train): 0.3583631054402059\n",
            "Iteration 836, Norm of Gradient: 0.02733790877580635, Cost (Train): 0.3582883955369109\n",
            "Iteration 837, Norm of Gradient: 0.027318729778544083, Cost (Train): 0.3582137903905219\n",
            "Iteration 838, Norm of Gradient: 0.027299587846069756, Cost (Train): 0.35813928972537046\n",
            "Iteration 839, Norm of Gradient: 0.027280482859526962, Cost (Train): 0.3580648932668609\n",
            "Iteration 840, Norm of Gradient: 0.027261414700585417, Cost (Train): 0.35799060074146494\n",
            "Iteration 841, Norm of Gradient: 0.027242383251437963, Cost (Train): 0.35791641187671613\n",
            "Iteration 842, Norm of Gradient: 0.02722338839479772, Cost (Train): 0.35784232640120406\n",
            "Iteration 843, Norm of Gradient: 0.027204430013895147, Cost (Train): 0.35776834404457003\n",
            "Iteration 844, Norm of Gradient: 0.027185507992475214, Cost (Train): 0.35769446453750053\n",
            "Iteration 845, Norm of Gradient: 0.027166622214794496, Cost (Train): 0.3576206876117225\n",
            "Iteration 846, Norm of Gradient: 0.0271477725656184, Cost (Train): 0.35754701299999786\n",
            "Iteration 847, Norm of Gradient: 0.027128958930218303, Cost (Train): 0.3574734404361185\n",
            "Iteration 848, Norm of Gradient: 0.027110181194368796, Cost (Train): 0.35739996965490084\n",
            "Iteration 849, Norm of Gradient: 0.027091439244344876, Cost (Train): 0.35732660039218045\n",
            "Iteration 850, Norm of Gradient: 0.02707273296691918, Cost (Train): 0.3572533323848073\n",
            "Iteration 851, Norm of Gradient: 0.02705406224935929, Cost (Train): 0.3571801653706405\n",
            "Iteration 852, Norm of Gradient: 0.02703542697942499, Cost (Train): 0.3571070990885428\n",
            "Iteration 853, Norm of Gradient: 0.027016827045365514, Cost (Train): 0.3570341332783764\n",
            "Iteration 854, Norm of Gradient: 0.026998262335916925, Cost (Train): 0.3569612676809968\n",
            "Iteration 855, Norm of Gradient: 0.026979732740299422, Cost (Train): 0.35688850203824884\n",
            "Iteration 856, Norm of Gradient: 0.026961238148214668, Cost (Train): 0.35681583609296097\n",
            "Iteration 857, Norm of Gradient: 0.026942778449843195, Cost (Train): 0.35674326958894076\n",
            "Iteration 858, Norm of Gradient: 0.02692435353584174, Cost (Train): 0.35667080227096987\n",
            "Iteration 859, Norm of Gradient: 0.02690596329734067, Cost (Train): 0.3565984338847987\n",
            "Iteration 860, Norm of Gradient: 0.026887607625941442, Cost (Train): 0.3565261641771427\n",
            "Iteration 861, Norm of Gradient: 0.02686928641371392, Cost (Train): 0.35645399289567625\n",
            "Iteration 862, Norm of Gradient: 0.02685099955319396, Cost (Train): 0.3563819197890286\n",
            "Iteration 863, Norm of Gradient: 0.026832746937380764, Cost (Train): 0.3563099446067791\n",
            "Iteration 864, Norm of Gradient: 0.026814528459734453, Cost (Train): 0.3562380670994521\n",
            "Iteration 865, Norm of Gradient: 0.026796344014173495, Cost (Train): 0.3561662870185126\n",
            "Iteration 866, Norm of Gradient: 0.02677819349507228, Cost (Train): 0.3560946041163613\n",
            "Iteration 867, Norm of Gradient: 0.026760076797258597, Cost (Train): 0.3560230181463303\n",
            "Iteration 868, Norm of Gradient: 0.026741993816011226, Cost (Train): 0.35595152886267817\n",
            "Iteration 869, Norm of Gradient: 0.026723944447057467, Cost (Train): 0.3558801360205854\n",
            "Iteration 870, Norm of Gradient: 0.02670592858657076, Cost (Train): 0.35580883937615\n",
            "Iteration 871, Norm of Gradient: 0.02668794613116824, Cost (Train): 0.35573763868638286\n",
            "Iteration 872, Norm of Gradient: 0.02666999697790835, Cost (Train): 0.3556665337092033\n",
            "Iteration 873, Norm of Gradient: 0.02665208102428852, Cost (Train): 0.3555955242034344\n",
            "Iteration 874, Norm of Gradient: 0.02663419816824272, Cost (Train): 0.355524609928799\n",
            "Iteration 875, Norm of Gradient: 0.02661634830813921, Cost (Train): 0.35545379064591454\n",
            "Iteration 876, Norm of Gradient: 0.026598531342778127, Cost (Train): 0.3553830661162895\n",
            "Iteration 877, Norm of Gradient: 0.026580747171389224, Cost (Train): 0.3553124361023185\n",
            "Iteration 878, Norm of Gradient: 0.026562995693629576, Cost (Train): 0.35524190036727804\n",
            "Iteration 879, Norm of Gradient: 0.02654527680958125, Cost (Train): 0.35517145867532224\n",
            "Iteration 880, Norm of Gradient: 0.02652759041974907, Cost (Train): 0.35510111079147855\n",
            "Iteration 881, Norm of Gradient: 0.026509936425058347, Cost (Train): 0.3550308564816435\n",
            "Iteration 882, Norm of Gradient: 0.026492314726852657, Cost (Train): 0.3549606955125784\n",
            "Iteration 883, Norm of Gradient: 0.02647472522689159, Cost (Train): 0.3548906276519051\n",
            "Iteration 884, Norm of Gradient: 0.026457167827348558, Cost (Train): 0.35482065266810214\n",
            "Iteration 885, Norm of Gradient: 0.026439642430808583, Cost (Train): 0.35475077033049995\n",
            "Iteration 886, Norm of Gradient: 0.02642214894026612, Cost (Train): 0.35468098040927715\n",
            "Iteration 887, Norm of Gradient: 0.026404687259122876, Cost (Train): 0.35461128267545666\n",
            "Iteration 888, Norm of Gradient: 0.02638725729118567, Cost (Train): 0.35454167690090077\n",
            "Iteration 889, Norm of Gradient: 0.026369858940664282, Cost (Train): 0.354472162858308\n",
            "Iteration 890, Norm of Gradient: 0.0263524921121693, Cost (Train): 0.35440274032120866\n",
            "Iteration 891, Norm of Gradient: 0.02633515671071006, Cost (Train): 0.3543334090639606\n",
            "Iteration 892, Norm of Gradient: 0.026317852641692493, Cost (Train): 0.3542641688617457\n",
            "Iteration 893, Norm of Gradient: 0.02630057981091704, Cost (Train): 0.3541950194905656\n",
            "Iteration 894, Norm of Gradient: 0.026283338124576602, Cost (Train): 0.3541259607272376\n",
            "Iteration 895, Norm of Gradient: 0.026266127489254464, Cost (Train): 0.3540569923493913\n",
            "Iteration 896, Norm of Gradient: 0.026248947811922232, Cost (Train): 0.3539881141354641\n",
            "Iteration 897, Norm of Gradient: 0.02623179899993781, Cost (Train): 0.3539193258646977\n",
            "Iteration 898, Norm of Gradient: 0.026214680961043377, Cost (Train): 0.35385062731713396\n",
            "Iteration 899, Norm of Gradient: 0.026197593603363364, Cost (Train): 0.3537820182736115\n",
            "Iteration 900, Norm of Gradient: 0.02618053683540246, Cost (Train): 0.35371349851576145\n",
            "Iteration 901, Norm of Gradient: 0.026163510566043623, Cost (Train): 0.35364506782600375\n",
            "Iteration 902, Norm of Gradient: 0.026146514704546115, Cost (Train): 0.35357672598754375\n",
            "Iteration 903, Norm of Gradient: 0.026129549160543535, Cost (Train): 0.353508472784368\n",
            "Iteration 904, Norm of Gradient: 0.026112613844041847, Cost (Train): 0.3534403080012407\n",
            "Iteration 905, Norm of Gradient: 0.026095708665417487, Cost (Train): 0.3533722314237002\n",
            "Iteration 906, Norm of Gradient: 0.026078833535415402, Cost (Train): 0.3533042428380551\n",
            "Iteration 907, Norm of Gradient: 0.026061988365147136, Cost (Train): 0.3532363420313806\n",
            "Iteration 908, Norm of Gradient: 0.026045173066088955, Cost (Train): 0.35316852879151495\n",
            "Iteration 909, Norm of Gradient: 0.026028387550079937, Cost (Train): 0.3531008029070558\n",
            "Iteration 910, Norm of Gradient: 0.026011631729320098, Cost (Train): 0.3530331641673567\n",
            "Iteration 911, Norm of Gradient: 0.02599490551636852, Cost (Train): 0.3529656123625233\n",
            "Iteration 912, Norm of Gradient: 0.02597820882414149, Cost (Train): 0.35289814728341007\n",
            "Iteration 913, Norm of Gradient: 0.025961541565910705, Cost (Train): 0.3528307687216167\n",
            "Iteration 914, Norm of Gradient: 0.02594490365530136, Cost (Train): 0.35276347646948464\n",
            "Iteration 915, Norm of Gradient: 0.025928295006290396, Cost (Train): 0.35269627032009304\n",
            "Iteration 916, Norm of Gradient: 0.025911715533204645, Cost (Train): 0.3526291500672563\n",
            "Iteration 917, Norm of Gradient: 0.025895165150719077, Cost (Train): 0.35256211550552\n",
            "Iteration 918, Norm of Gradient: 0.02587864377385495, Cost (Train): 0.3524951664301574\n",
            "Iteration 919, Norm of Gradient: 0.025862151317978085, Cost (Train): 0.35242830263716624\n",
            "Iteration 920, Norm of Gradient: 0.025845687698797103, Cost (Train): 0.3523615239232654\n",
            "Iteration 921, Norm of Gradient: 0.02582925283236162, Cost (Train): 0.35229483008589146\n",
            "Iteration 922, Norm of Gradient: 0.025812846635060557, Cost (Train): 0.3522282209231952\n",
            "Iteration 923, Norm of Gradient: 0.025796469023620365, Cost (Train): 0.3521616962340386\n",
            "Iteration 924, Norm of Gradient: 0.025780119915103338, Cost (Train): 0.35209525581799117\n",
            "Iteration 925, Norm of Gradient: 0.025763799226905883, Cost (Train): 0.35202889947532706\n",
            "Iteration 926, Norm of Gradient: 0.025747506876756806, Cost (Train): 0.3519626270070212\n",
            "Iteration 927, Norm of Gradient: 0.025731242782715647, Cost (Train): 0.35189643821474664\n",
            "Iteration 928, Norm of Gradient: 0.025715006863170988, Cost (Train): 0.35183033290087135\n",
            "Iteration 929, Norm of Gradient: 0.02569879903683876, Cost (Train): 0.3517643108684543\n",
            "Iteration 930, Norm of Gradient: 0.025682619222760623, Cost (Train): 0.3516983719212428\n",
            "Iteration 931, Norm of Gradient: 0.02566646734030229, Cost (Train): 0.3516325158636694\n",
            "Iteration 932, Norm of Gradient: 0.025650343309151877, Cost (Train): 0.3515667425008485\n",
            "Iteration 933, Norm of Gradient: 0.025634247049318323, Cost (Train): 0.35150105163857315\n",
            "Iteration 934, Norm of Gradient: 0.025618178481129694, Cost (Train): 0.35143544308331215\n",
            "Iteration 935, Norm of Gradient: 0.025602137525231612, Cost (Train): 0.35136991664220674\n",
            "Iteration 936, Norm of Gradient: 0.02558612410258569, Cost (Train): 0.3513044721230677\n",
            "Iteration 937, Norm of Gradient: 0.02557013813446787, Cost (Train): 0.35123910933437225\n",
            "Iteration 938, Norm of Gradient: 0.02555417954246688, Cost (Train): 0.35117382808526065\n",
            "Iteration 939, Norm of Gradient: 0.025538248248482665, Cost (Train): 0.3511086281855337\n",
            "Iteration 940, Norm of Gradient: 0.025522344174724806, Cost (Train): 0.3510435094456493\n",
            "Iteration 941, Norm of Gradient: 0.02550646724371098, Cost (Train): 0.35097847167671986\n",
            "Iteration 942, Norm of Gradient: 0.025490617378265412, Cost (Train): 0.3509135146905087\n",
            "Iteration 943, Norm of Gradient: 0.025474794501517337, Cost (Train): 0.35084863829942775\n",
            "Iteration 944, Norm of Gradient: 0.025458998536899473, Cost (Train): 0.3507838423165341\n",
            "Iteration 945, Norm of Gradient: 0.025443229408146525, Cost (Train): 0.35071912655552734\n",
            "Iteration 946, Norm of Gradient: 0.025427487039293633, Cost (Train): 0.3506544908307466\n",
            "Iteration 947, Norm of Gradient: 0.025411771354674925, Cost (Train): 0.3505899349571676\n",
            "Iteration 948, Norm of Gradient: 0.025396082278921994, Cost (Train): 0.3505254587503996\n",
            "Iteration 949, Norm of Gradient: 0.02538041973696243, Cost (Train): 0.3504610620266828\n",
            "Iteration 950, Norm of Gradient: 0.025364783654018327, Cost (Train): 0.3503967446028856\n",
            "Iteration 951, Norm of Gradient: 0.025349173955604876, Cost (Train): 0.3503325062965012\n",
            "Iteration 952, Norm of Gradient: 0.025333590567528843, Cost (Train): 0.35026834692564546\n",
            "Iteration 953, Norm of Gradient: 0.025318033415887165, Cost (Train): 0.3502042663090534\n",
            "Iteration 954, Norm of Gradient: 0.025302502427065495, Cost (Train): 0.350140264266077\n",
            "Iteration 955, Norm of Gradient: 0.025286997527736787, Cost (Train): 0.35007634061668214\n",
            "Iteration 956, Norm of Gradient: 0.02527151864485986, Cost (Train): 0.3500124951814459\n",
            "Iteration 957, Norm of Gradient: 0.025256065705678003, Cost (Train): 0.34994872778155384\n",
            "Iteration 958, Norm of Gradient: 0.02524063863771756, Cost (Train): 0.34988503823879724\n",
            "Iteration 959, Norm of Gradient: 0.025225237368786544, Cost (Train): 0.3498214263755703\n",
            "Iteration 960, Norm of Gradient: 0.02520986182697324, Cost (Train): 0.3497578920148675\n",
            "Iteration 961, Norm of Gradient: 0.025194511940644822, Cost (Train): 0.34969443498028124\n",
            "Iteration 962, Norm of Gradient: 0.02517918763844602, Cost (Train): 0.34963105509599846\n",
            "Iteration 963, Norm of Gradient: 0.025163888849297697, Cost (Train): 0.3495677521867986\n",
            "Iteration 964, Norm of Gradient: 0.025148615502395565, Cost (Train): 0.34950452607805094\n",
            "Iteration 965, Norm of Gradient: 0.025133367527208756, Cost (Train): 0.34944137659571134\n",
            "Iteration 966, Norm of Gradient: 0.025118144853478568, Cost (Train): 0.34937830356632055\n",
            "Iteration 967, Norm of Gradient: 0.025102947411217075, Cost (Train): 0.3493153068170009\n",
            "Iteration 968, Norm of Gradient: 0.025087775130705835, Cost (Train): 0.349252386175454\n",
            "Iteration 969, Norm of Gradient: 0.025072627942494548, Cost (Train): 0.3491895414699581\n",
            "Iteration 970, Norm of Gradient: 0.025057505777399788, Cost (Train): 0.34912677252936575\n",
            "Iteration 971, Norm of Gradient: 0.02504240856650366, Cost (Train): 0.34906407918310084\n",
            "Iteration 972, Norm of Gradient: 0.02502733624115254, Cost (Train): 0.3490014612611565\n",
            "Iteration 973, Norm of Gradient: 0.02501228873295577, Cost (Train): 0.34893891859409226\n",
            "Iteration 974, Norm of Gradient: 0.0249972659737844, Cost (Train): 0.3488764510130319\n",
            "Iteration 975, Norm of Gradient: 0.024982267895769905, Cost (Train): 0.34881405834966045\n",
            "Iteration 976, Norm of Gradient: 0.02496729443130292, Cost (Train): 0.34875174043622237\n",
            "Iteration 977, Norm of Gradient: 0.024952345513032, Cost (Train): 0.3486894971055186\n",
            "Iteration 978, Norm of Gradient: 0.024937421073862348, Cost (Train): 0.3486273281909042\n",
            "Iteration 979, Norm of Gradient: 0.024922521046954613, Cost (Train): 0.3485652335262861\n",
            "Iteration 980, Norm of Gradient: 0.024907645365723614, Cost (Train): 0.34850321294612047\n",
            "Iteration 981, Norm of Gradient: 0.024892793963837142, Cost (Train): 0.34844126628541056\n",
            "Iteration 982, Norm of Gradient: 0.02487796677521473, Cost (Train): 0.3483793933797039\n",
            "Iteration 983, Norm of Gradient: 0.024863163734026448, Cost (Train): 0.34831759406509055\n",
            "Iteration 984, Norm of Gradient: 0.024848384774691694, Cost (Train): 0.34825586817820003\n",
            "Iteration 985, Norm of Gradient: 0.02483362983187799, Cost (Train): 0.3481942155561995\n",
            "Iteration 986, Norm of Gradient: 0.02481889884049981, Cost (Train): 0.3481326360367913\n",
            "Iteration 987, Norm of Gradient: 0.024804191735717365, Cost (Train): 0.3480711294582101\n",
            "Iteration 988, Norm of Gradient: 0.024789508452935455, Cost (Train): 0.3480096956592217\n",
            "Iteration 989, Norm of Gradient: 0.02477484892780229, Cost (Train): 0.3479483344791195\n",
            "Iteration 990, Norm of Gradient: 0.02476021309620832, Cost (Train): 0.34788704575772295\n",
            "Iteration 991, Norm of Gradient: 0.024745600894285062, Cost (Train): 0.34782582933537526\n",
            "Iteration 992, Norm of Gradient: 0.024731012258404017, Cost (Train): 0.3477646850529408\n",
            "Iteration 993, Norm of Gradient: 0.02471644712517543, Cost (Train): 0.34770361275180284\n",
            "Iteration 994, Norm of Gradient: 0.024701905431447225, Cost (Train): 0.3476426122738619\n",
            "Iteration 995, Norm of Gradient: 0.024687387114303835, Cost (Train): 0.3475816834615327\n",
            "Iteration 996, Norm of Gradient: 0.024672892111065103, Cost (Train): 0.3475208261577426\n",
            "Iteration 997, Norm of Gradient: 0.024658420359285158, Cost (Train): 0.347460040205929\n",
            "Iteration 998, Norm of Gradient: 0.024643971796751277, Cost (Train): 0.3473993254500373\n",
            "Iteration 999, Norm of Gradient: 0.02462954636148282, Cost (Train): 0.3473386817345188\n",
            "Terminated after 1000 iterations, with norm of the gradient equal to 0.02462954636148282\n",
            "The weight found: [-0.0346338  -0.03968371  0.04298932 ... -0.16692012 -0.03006533\n",
            "  0.07674923]\n",
            "x_train shape after bias term: (15000, 1877)\n",
            "self.w shape: (1877,)\n",
            "Iteration 0, Norm of Gradient: 0.18262071502311986, Cost (Train): 0.6898469605876589\n",
            "Iteration 1, Norm of Gradient: 0.17907889699498053, Cost (Train): 0.6866650863243684\n",
            "Iteration 2, Norm of Gradient: 0.17643769975816512, Cost (Train): 0.6835715720211631\n",
            "Iteration 3, Norm of Gradient: 0.17431554386384637, Cost (Train): 0.6805493040638401\n",
            "Iteration 4, Norm of Gradient: 0.17249387767409616, Cost (Train): 0.6775883317585639\n",
            "Iteration 5, Norm of Gradient: 0.17084873198064204, Cost (Train): 0.6746826908166319\n",
            "Iteration 6, Norm of Gradient: 0.16931017032480747, Cost (Train): 0.6718286378959859\n",
            "Iteration 7, Norm of Gradient: 0.1678388534056525, Cost (Train): 0.6690236699283698\n",
            "Iteration 8, Norm of Gradient: 0.16641268061274084, Cost (Train): 0.666265979547951\n",
            "Iteration 9, Norm of Gradient: 0.1650192343721615, Cost (Train): 0.6635541527559532\n",
            "Iteration 10, Norm of Gradient: 0.16365152523687043, Cost (Train): 0.6608870011044441\n",
            "Iteration 11, Norm of Gradient: 0.16230560176226516, Cost (Train): 0.6582634685672997\n",
            "Iteration 12, Norm of Gradient: 0.16097920990978817, Cost (Train): 0.6556825798694185\n",
            "Iteration 13, Norm of Gradient: 0.15967104187342876, Cost (Train): 0.6531434118215933\n",
            "Iteration 14, Norm of Gradient: 0.1583803155220356, Cost (Train): 0.6506450774149428\n",
            "Iteration 15, Norm of Gradient: 0.15710653915177175, Cost (Train): 0.6481867169862766\n",
            "Iteration 16, Norm of Gradient: 0.15584938005493573, Cost (Train): 0.6457674932966364\n",
            "Iteration 17, Norm of Gradient: 0.1546085912284359, Cost (Train): 0.6433865887706476\n",
            "Iteration 18, Norm of Gradient: 0.15338397063011813, Cost (Train): 0.6410432039246742\n",
            "Iteration 19, Norm of Gradient: 0.15217533864781344, Cost (Train): 0.6387365564450492\n",
            "Iteration 20, Norm of Gradient: 0.15098252575272, Cost (Train): 0.636465880618182\n",
            "Iteration 21, Norm of Gradient: 0.14980536584145937, Cost (Train): 0.6342304269478533\n",
            "Iteration 22, Norm of Gradient: 0.14864369274973988, Cost (Train): 0.6320294618690872\n",
            "Iteration 23, Norm of Gradient: 0.14749733852862382, Cost (Train): 0.6298622675090692\n",
            "Iteration 24, Norm of Gradient: 0.14636613269489102, Cost (Train): 0.6277281414683453\n",
            "Iteration 25, Norm of Gradient: 0.1452499020144458, Cost (Train): 0.6256263966081212\n",
            "Iteration 26, Norm of Gradient: 0.14414847057226177, Cost (Train): 0.6235563608364274\n",
            "Iteration 27, Norm of Gradient: 0.14306165999128562, Cost (Train): 0.6215173768897336\n",
            "Iteration 28, Norm of Gradient: 0.14198928972370153, Cost (Train): 0.6195088021086631\n",
            "Iteration 29, Norm of Gradient: 0.14093117737209765, Cost (Train): 0.6175300082075745\n",
            "Iteration 30, Norm of Gradient: 0.13988713901718075, Cost (Train): 0.6155803810383598\n",
            "Iteration 31, Norm of Gradient: 0.13885698953937936, Cost (Train): 0.6136593203490893\n",
            "Iteration 32, Norm of Gradient: 0.1378405429276541, Cost (Train): 0.6117662395382649\n",
            "Iteration 33, Norm of Gradient: 0.13683761257218086, Cost (Train): 0.6099005654054752\n",
            "Iteration 34, Norm of Gradient: 0.13584801153943477, Cost (Train): 0.6080617378992415\n",
            "Iteration 35, Norm of Gradient: 0.13487155282924848, Cost (Train): 0.6062492098628034\n",
            "Iteration 36, Norm of Gradient: 0.13390804961398708, Cost (Train): 0.6044624467785562\n",
            "Iteration 37, Norm of Gradient: 0.13295731546030587, Cost (Train): 0.6027009265117903\n",
            "Iteration 38, Norm of Gradient: 0.13201916453412021, Cost (Train): 0.6009641390543391\n",
            "Iteration 39, Norm of Gradient: 0.13109341178950804, Cost (Train): 0.5992515862686869\n",
            "Iteration 40, Norm of Gradient: 0.130179873142302, Cost (Train): 0.5975627816330371\n",
            "Iteration 41, Norm of Gradient: 0.12927836562914474, Cost (Train): 0.5958972499877984\n",
            "Iteration 42, Norm of Gradient: 0.1283887075527731, Cost (Train): 0.5942545272838955\n",
            "Iteration 43, Norm of Gradient: 0.12751071861428906, Cost (Train): 0.5926341603332747\n",
            "Iteration 44, Norm of Gradient: 0.12664422003315365, Cost (Train): 0.591035706561931\n",
            "Iteration 45, Norm of Gradient: 0.1257890346556201, Cost (Train): 0.5894587337657509\n",
            "Iteration 46, Norm of Gradient: 0.12494498705229778, Cost (Train): 0.587902819869424\n",
            "Iteration 47, Norm of Gradient: 0.12411190360551332, Cost (Train): 0.5863675526886543\n",
            "Iteration 48, Norm of Gradient: 0.12328961258710855, Cost (Train): 0.5848525296958638\n",
            "Iteration 49, Norm of Gradient: 0.12247794422728824, Cost (Train): 0.5833573577895598\n",
            "Iteration 50, Norm of Gradient: 0.12167673077510448, Cost (Train): 0.5818816530675089\n",
            "Iteration 51, Norm of Gradient: 0.12088580655113576, Cost (Train): 0.5804250406038391\n",
            "Iteration 52, Norm of Gradient: 0.12010500799289375, Cost (Train): 0.5789871542301719\n",
            "Iteration 53, Norm of Gradient: 0.1193341736934639, Cost (Train): 0.5775676363208605\n",
            "Iteration 54, Norm of Gradient: 0.11857314443385962, Cost (Train): 0.5761661375824043\n",
            "Iteration 55, Norm of Gradient: 0.11782176320954513, Cost (Train): 0.5747823168470786\n",
            "Iteration 56, Norm of Gradient: 0.11707987525155708, Cost (Train): 0.5734158408708188\n",
            "Iteration 57, Norm of Gradient: 0.11634732804263183, Cost (Train): 0.5720663841353721\n",
            "Iteration 58, Norm of Gradient: 0.11562397132872143, Cost (Train): 0.5707336286547288\n",
            "Iteration 59, Norm of Gradient: 0.11490965712625942, Cost (Train): 0.5694172637858254\n",
            "Iteration 60, Norm of Gradient: 0.1142042397255166, Cost (Train): 0.568116986043507\n",
            "Iteration 61, Norm of Gradient: 0.11350757569036576, Cost (Train): 0.5668324989197285\n",
            "Iteration 62, Norm of Gradient: 0.11281952385475567, Cost (Train): 0.56556351270696\n",
            "Iteration 63, Norm of Gradient: 0.11213994531617466, Cost (Train): 0.5643097443257643\n",
            "Iteration 64, Norm of Gradient: 0.11146870342636751, Cost (Train): 0.5630709171564982\n",
            "Iteration 65, Norm of Gradient: 0.11080566377955206, Cost (Train): 0.5618467608750938\n",
            "Iteration 66, Norm of Gradient: 0.11015069419836503, Cost (Train): 0.5606370112928619\n",
            "Iteration 67, Norm of Gradient: 0.10950366471775239, Cost (Train): 0.5594414102002632\n",
            "Iteration 68, Norm of Gradient: 0.10886444756700485, Cost (Train): 0.5582597052145836\n",
            "Iteration 69, Norm of Gradient: 0.10823291715012472, Cost (Train): 0.5570916496314521\n",
            "Iteration 70, Norm of Gradient: 0.10760895002469827, Cost (Train): 0.555937002280133\n",
            "Iteration 71, Norm of Gradient: 0.10699242487943494, Cost (Train): 0.5547955273825232\n",
            "Iteration 72, Norm of Gradient: 0.10638322251052407, Cost (Train): 0.5536669944157867\n",
            "Iteration 73, Norm of Gradient: 0.10578122579694742, Cost (Train): 0.5525511779785508\n",
            "Iteration 74, Norm of Gradient: 0.10518631967487792, Cost (Train): 0.5514478576605948\n",
            "Iteration 75, Norm of Gradient: 0.10459839111128359, Cost (Train): 0.5503568179159554\n",
            "Iteration 76, Norm of Gradient: 0.10401732907684663, Cost (Train): 0.5492778479393753\n",
            "Iteration 77, Norm of Gradient: 0.10344302451830148, Cost (Train): 0.5482107415460207\n",
            "Iteration 78, Norm of Gradient: 0.10287537033028443, Cost (Train): 0.5471552970543941\n",
            "Iteration 79, Norm of Gradient: 0.1023142613267833, Cost (Train): 0.5461113171723678\n",
            "Iteration 80, Norm of Gradient: 0.10175959421226624, Cost (Train): 0.545078608886263\n",
            "Iteration 81, Norm of Gradient: 0.10121126755256363, Cost (Train): 0.5440569833529042\n",
            "Iteration 82, Norm of Gradient: 0.10066918174557078, Cost (Train): 0.5430462557945732\n",
            "Iteration 83, Norm of Gradient: 0.10013323899183342, Cost (Train): 0.5420462453967932\n",
            "Iteration 84, Norm of Gradient: 0.09960334326507225, Cost (Train): 0.5410567752088702\n",
            "Iteration 85, Norm of Gradient: 0.0990794002826994, Cost (Train): 0.5400776720471222\n",
            "Iteration 86, Norm of Gradient: 0.09856131747637281, Cost (Train): 0.539108766400728\n",
            "Iteration 87, Norm of Gradient: 0.09804900396263296, Cost (Train): 0.5381498923401261\n",
            "Iteration 88, Norm of Gradient: 0.09754237051365978, Cost (Train): 0.5372008874278971\n",
            "Iteration 89, Norm of Gradient: 0.09704132952818637, Cost (Train): 0.5362615926320653\n",
            "Iteration 90, Norm of Gradient: 0.0965457950026007, Cost (Train): 0.5353318522417533\n",
            "Iteration 91, Norm of Gradient: 0.09605568250226482, Cost (Train): 0.5344115137851275\n",
            "Iteration 92, Norm of Gradient: 0.095570909133077, Cost (Train): 0.5335004279495715\n",
            "Iteration 93, Norm of Gradient: 0.09509139351330076, Cost (Train): 0.5325984485040289\n",
            "Iteration 94, Norm of Gradient: 0.0946170557456807, Cost (Train): 0.5317054322234525\n",
            "Iteration 95, Norm of Gradient: 0.09414781738986436, Cost (Train): 0.5308212388153066\n",
            "Iteration 96, Norm of Gradient: 0.09368360143514584, Cost (Train): 0.5299457308480618\n",
            "Iteration 97, Norm of Gradient: 0.0932243322735456, Cost (Train): 0.5290787736816293\n",
            "Iteration 98, Norm of Gradient: 0.09276993567323916, Cost (Train): 0.5282202353996781\n",
            "Iteration 99, Norm of Gradient: 0.09232033875234488, Cost (Train): 0.5273699867437878\n",
            "Iteration 100, Norm of Gradient: 0.09187546995308063, Cost (Train): 0.5265279010493776\n",
            "Iteration 101, Norm of Gradient: 0.0914352590162968, Cost (Train): 0.5256938541833701\n",
            "Iteration 102, Norm of Gradient: 0.09099963695639181, Cost (Train): 0.524867724483533\n",
            "Iteration 103, Norm of Gradient: 0.09056853603661622, Cost (Train): 0.5240493926994582\n",
            "Iteration 104, Norm of Gradient: 0.09014188974476849, Cost (Train): 0.5232387419351265\n",
            "Iteration 105, Norm of Gradient: 0.08971963276928599, Cost (Train): 0.522435657593017\n",
            "Iteration 106, Norm of Gradient: 0.08930170097573387, Cost (Train): 0.5216400273197142\n",
            "Iteration 107, Norm of Gradient: 0.08888803138369203, Cost (Train): 0.520851740952972\n",
            "Iteration 108, Norm of Gradient: 0.08847856214404164, Cost (Train): 0.5200706904701929\n",
            "Iteration 109, Norm of Gradient: 0.0880732325166506, Cost (Train): 0.5192967699382814\n",
            "Iteration 110, Norm of Gradient: 0.08767198284845727, Cost (Train): 0.5185298754648329\n",
            "Iteration 111, Norm of Gradient: 0.08727475455195063, Cost (Train): 0.5177699051506194\n",
            "Iteration 112, Norm of Gradient: 0.0868814900840459, Cost (Train): 0.5170167590433364\n",
            "Iteration 113, Norm of Gradient: 0.08649213292535192, Cost (Train): 0.5162703390925728\n",
            "Iteration 114, Norm of Gradient: 0.08610662755982836, Cost (Train): 0.5155305491059714\n",
            "Iteration 115, Norm of Gradient: 0.08572491945482934, Cost (Train): 0.5147972947065428\n",
            "Iteration 116, Norm of Gradient: 0.08534695504152955, Cost (Train): 0.5140704832911035\n",
            "Iteration 117, Norm of Gradient: 0.08497268169572912, Cost (Train): 0.513350023989803\n",
            "Iteration 118, Norm of Gradient: 0.08460204771903351, Cost (Train): 0.5126358276267101\n",
            "Iteration 119, Norm of Gradient: 0.08423500232040321, Cost (Train): 0.5119278066814295\n",
            "Iteration 120, Norm of Gradient: 0.08387149559806911, Cost (Train): 0.5112258752517151\n",
            "Iteration 121, Norm of Gradient: 0.0835114785218088, Cost (Train): 0.510529949017058\n",
            "Iteration 122, Norm of Gradient: 0.08315490291557846, Cost (Train): 0.5098399452032152\n",
            "Iteration 123, Norm of Gradient: 0.08280172144049579, Cost (Train): 0.5091557825476557\n",
            "Iteration 124, Norm of Gradient: 0.08245188757816775, Cost (Train): 0.5084773812658989\n",
            "Iteration 125, Norm of Gradient: 0.08210535561435924, Cost (Train): 0.5078046630187156\n",
            "Iteration 126, Norm of Gradient: 0.08176208062299571, Cost (Train): 0.5071375508801723\n",
            "Iteration 127, Norm of Gradient: 0.08142201845049578, Cost (Train): 0.5064759693064911\n",
            "Iteration 128, Norm of Gradient: 0.08108512570042693, Cost (Train): 0.5058198441057042\n",
            "Iteration 129, Norm of Gradient: 0.08075135971847994, Cost (Train): 0.5051691024080808\n",
            "Iteration 130, Norm of Gradient: 0.08042067857775557, Cost (Train): 0.5045236726373019\n",
            "Iteration 131, Norm of Gradient: 0.08009304106435867, Cost (Train): 0.5038834844823661\n",
            "Iteration 132, Norm of Gradient: 0.0797684066632935, Cost (Train): 0.5032484688702024\n",
            "Iteration 133, Norm of Gradient: 0.07944673554465496, Cost (Train): 0.5026185579389716\n",
            "Iteration 134, Norm of Gradient: 0.07912798855011045, Cost (Train): 0.5019936850120384\n",
            "Iteration 135, Norm of Gradient: 0.07881212717966649, Cost (Train): 0.5013737845725917\n",
            "Iteration 136, Norm of Gradient: 0.07849911357871457, Cost (Train): 0.5007587922388995\n",
            "Iteration 137, Norm of Gradient: 0.07818891052535111, Cost (Train): 0.5001486447401775\n",
            "Iteration 138, Norm of Gradient: 0.07788148141796594, Cost (Train): 0.4995432798930556\n",
            "Iteration 139, Norm of Gradient: 0.07757679026309401, Cost (Train): 0.4989426365786264\n",
            "Iteration 140, Norm of Gradient: 0.07727480166352474, Cost (Train): 0.498346654720058\n",
            "Iteration 141, Norm of Gradient: 0.07697548080666448, Cost (Train): 0.4977552752607568\n",
            "Iteration 142, Norm of Gradient: 0.07667879345314617, Cost (Train): 0.49716844014306494\n",
            "Iteration 143, Norm of Gradient: 0.07638470592568138, Cost (Train): 0.49658609228747674\n",
            "Iteration 144, Norm of Gradient: 0.07609318509814994, Cost (Train): 0.4960081755723624\n",
            "Iteration 145, Norm of Gradient: 0.07580419838492192, Cost (Train): 0.495434634814181\n",
            "Iteration 146, Norm of Gradient: 0.07551771373040682, Cost (Train): 0.4948654157481743\n",
            "Iteration 147, Norm of Gradient: 0.07523369959882596, Cost (Train): 0.4943004650095241\n",
            "Iteration 148, Norm of Gradient: 0.07495212496420242, Cost (Train): 0.49373973011496336\n",
            "Iteration 149, Norm of Gradient: 0.07467295930056457, Cost (Train): 0.4931831594448269\n",
            "Iteration 150, Norm of Gradient: 0.07439617257235803, Cost (Train): 0.4926307022255313\n",
            "Iteration 151, Norm of Gradient: 0.07412173522506227, Cost (Train): 0.4920823085124716\n",
            "Iteration 152, Norm of Gradient: 0.07384961817600641, Cost (Train): 0.49153792917332345\n",
            "Iteration 153, Norm of Gradient: 0.07357979280538109, Cost (Train): 0.4909975158717413\n",
            "Iteration 154, Norm of Gradient: 0.07331223094744103, Cost (Train): 0.49046102105143885\n",
            "Iteration 155, Norm of Gradient: 0.073046904881895, Cost (Train): 0.48992839792064574\n",
            "Iteration 156, Norm of Gradient: 0.07278378732547809, Cost (Train): 0.4893996004369275\n",
            "Iteration 157, Norm of Gradient: 0.07252285142370361, Cost (Train): 0.48887458329235933\n",
            "Iteration 158, Norm of Gradient: 0.07226407074278923, Cost (Train): 0.48835330189904647\n",
            "Iteration 159, Norm of Gradient: 0.07200741926175455, Cost (Train): 0.48783571237497914\n",
            "Iteration 160, Norm of Gradient: 0.07175287136468571, Cost (Train): 0.48732177153021544\n",
            "Iteration 161, Norm of Gradient: 0.0715004018331637, Cost (Train): 0.4868114368533824\n",
            "Iteration 162, Norm of Gradient: 0.07124998583885242, Cost (Train): 0.48630466649848786\n",
            "Iteration 163, Norm of Gradient: 0.07100159893624314, Cost (Train): 0.4858014192720337\n",
            "Iteration 164, Norm of Gradient: 0.07075521705555178, Cost (Train): 0.48530165462042335\n",
            "Iteration 165, Norm of Gradient: 0.07051081649576575, Cost (Train): 0.4848053326176572\n",
            "Iteration 166, Norm of Gradient: 0.07026837391783651, Cost (Train): 0.4843124139533052\n",
            "Iteration 167, Norm of Gradient: 0.07002786633801542, Cost (Train): 0.4838228599207531\n",
            "Iteration 168, Norm of Gradient: 0.06978927112132878, Cost (Train): 0.48333663240571284\n",
            "Iteration 169, Norm of Gradient: 0.06955256597518966, Cost (Train): 0.48285369387499105\n",
            "Iteration 170, Norm of Gradient: 0.06931772894314298, Cost (Train): 0.4823740073655096\n",
            "Iteration 171, Norm of Gradient: 0.06908473839874098, Cost (Train): 0.48189753647357103\n",
            "Iteration 172, Norm of Gradient: 0.06885357303954637, Cost (Train): 0.4814242453443625\n",
            "Iteration 173, Norm of Gradient: 0.06862421188125992, Cost (Train): 0.48095409866169353\n",
            "Iteration 174, Norm of Gradient: 0.0683966342519701, Cost (Train): 0.4804870616379593\n",
            "Iteration 175, Norm of Gradient: 0.0681708197865214, Cost (Train): 0.48002310000432613\n",
            "Iteration 176, Norm of Gradient: 0.06794674842099967, Cost (Train): 0.47956218000113277\n",
            "Iteration 177, Norm of Gradient: 0.06772440038733077, Cost (Train): 0.4791042683685007\n",
            "Iteration 178, Norm of Gradient: 0.06750375620799064, Cost (Train): 0.4786493323371506\n",
            "Iteration 179, Norm of Gradient: 0.06728479669082418, Cost (Train): 0.47819733961941807\n",
            "Iteration 180, Norm of Gradient: 0.06706750292397044, Cost (Train): 0.4777482584004641\n",
            "Iteration 181, Norm of Gradient: 0.06685185627089146, Cost (Train): 0.4773020573296759\n",
            "Iteration 182, Norm of Gradient: 0.06663783836550338, Cost (Train): 0.47685870551225285\n",
            "Iteration 183, Norm of Gradient: 0.06642543110740601, Cost (Train): 0.47641817250097324\n",
            "Iteration 184, Norm of Gradient: 0.06621461665721011, Cost (Train): 0.475980428288137\n",
            "Iteration 185, Norm of Gradient: 0.0660053774319595, Cost (Train): 0.47554544329768134\n",
            "Iteration 186, Norm of Gradient: 0.06579769610064573, Cost (Train): 0.47511318837746247\n",
            "Iteration 187, Norm of Gradient: 0.06559155557981383, Cost (Train): 0.4746836347917028\n",
            "Iteration 188, Norm of Gradient: 0.06538693902925642, Cost (Train): 0.4742567542135965\n",
            "Iteration 189, Norm of Gradient: 0.06518382984779492, Cost (Train): 0.47383251871807164\n",
            "Iteration 190, Norm of Gradient: 0.06498221166914528, Cost (Train): 0.47341090077470416\n",
            "Iteration 191, Norm of Gradient: 0.0647820683578667, Cost (Train): 0.47299187324077974\n",
            "Iteration 192, Norm of Gradient: 0.06458338400539154, Cost (Train): 0.472575409354501\n",
            "Iteration 193, Norm of Gradient: 0.06438614292613434, Cost (Train): 0.47216148272833625\n",
            "Iteration 194, Norm of Gradient: 0.06419032965367849, Cost (Train): 0.47175006734250535\n",
            "Iteration 195, Norm of Gradient: 0.06399592893703873, Cost (Train): 0.47134113753860113\n",
            "Iteration 196, Norm of Gradient: 0.0638029257369975, Cost (Train): 0.47093466801334155\n",
            "Iteration 197, Norm of Gradient: 0.06361130522251426, Cost (Train): 0.47053063381245097\n",
            "Iteration 198, Norm of Gradient: 0.0634210527672052, Cost (Train): 0.47012901032466553\n",
            "Iteration 199, Norm of Gradient: 0.0632321539458927, Cost (Train): 0.46972977327586257\n",
            "Iteration 200, Norm of Gradient: 0.0630445945312223, Cost (Train): 0.46933289872330813\n",
            "Iteration 201, Norm of Gradient: 0.06285836049034622, Cost (Train): 0.4689383630500215\n",
            "Iteration 202, Norm of Gradient: 0.06267343798167156, Cost (Train): 0.46854614295925434\n",
            "Iteration 203, Norm of Gradient: 0.06248981335167209, Cost (Train): 0.4681562154690804\n",
            "Iteration 204, Norm of Gradient: 0.062307473131762055, Cost (Train): 0.46776855790709404\n",
            "Iteration 205, Norm of Gradient: 0.062126404035230676, Cost (Train): 0.46738314790521557\n",
            "Iteration 206, Norm of Gradient: 0.06194659295423603, Cost (Train): 0.46699996339459954\n",
            "Iteration 207, Norm of Gradient: 0.06176802695685707, Cost (Train): 0.46661898260064505\n",
            "Iteration 208, Norm of Gradient: 0.0615906932842025, Cost (Train): 0.4662401840381045\n",
            "Iteration 209, Norm of Gradient: 0.06141457934757516, Cost (Train): 0.46586354650628986\n",
            "Iteration 210, Norm of Gradient: 0.061239672725690926, Cost (Train): 0.46548904908437294\n",
            "Iteration 211, Norm of Gradient: 0.06106596116195077, Cost (Train): 0.4651166711267781\n",
            "Iteration 212, Norm of Gradient: 0.060893432561764924, Cost (Train): 0.46474639225866554\n",
            "Iteration 213, Norm of Gradient: 0.06072207498992796, Cost (Train): 0.4643781923715027\n",
            "Iteration 214, Norm of Gradient: 0.060551876668043875, Cost (Train): 0.4640120516187223\n",
            "Iteration 215, Norm of Gradient: 0.06038282597199984, Cost (Train): 0.46364795041146395\n",
            "Iteration 216, Norm of Gradient: 0.06021491142948775, Cost (Train): 0.463285869414399\n",
            "Iteration 217, Norm of Gradient: 0.06004812171757249, Cost (Train): 0.4629257895416357\n",
            "Iteration 218, Norm of Gradient: 0.05988244566030602, Cost (Train): 0.4625676919527031\n",
            "Iteration 219, Norm of Gradient: 0.059717872226386096, Cost (Train): 0.4622115580486121\n",
            "Iteration 220, Norm of Gradient: 0.05955439052685899, Cost (Train): 0.4618573694679919\n",
            "Iteration 221, Norm of Gradient: 0.05939198981286492, Cost (Train): 0.4615051080833\n",
            "Iteration 222, Norm of Gradient: 0.059230659473425575, Cost (Train): 0.46115475599710454\n",
            "Iteration 223, Norm of Gradient: 0.059070389033272656, Cost (Train): 0.46080629553843677\n",
            "Iteration 224, Norm of Gradient: 0.0589111681507168, Cost (Train): 0.4604597092592126\n",
            "Iteration 225, Norm of Gradient: 0.05875298661555569, Cost (Train): 0.46011497993072187\n",
            "Iteration 226, Norm of Gradient: 0.05859583434702088, Cost (Train): 0.45977209054018314\n",
            "Iteration 227, Norm of Gradient: 0.05843970139176232, Cost (Train): 0.4594310242873631\n",
            "Iteration 228, Norm of Gradient: 0.05828457792186983, Cost (Train): 0.4590917645812596\n",
            "Iteration 229, Norm of Gradient: 0.058130454232930825, Cost (Train): 0.4587542950368456\n",
            "Iteration 230, Norm of Gradient: 0.05797732074212339, Cost (Train): 0.45841859947187447\n",
            "Iteration 231, Norm of Gradient: 0.05782516798634424, Cost (Train): 0.4580846619037435\n",
            "Iteration 232, Norm of Gradient: 0.057673986620370384, Cost (Train): 0.4577524665464163\n",
            "Iteration 233, Norm of Gradient: 0.05752376741505438, Cost (Train): 0.45742199780740095\n",
            "Iteration 234, Norm of Gradient: 0.05737450125555205, Cost (Train): 0.45709324028478415\n",
            "Iteration 235, Norm of Gradient: 0.05722617913958203, Cost (Train): 0.4567661787643192\n",
            "Iteration 236, Norm of Gradient: 0.05707879217571681, Cost (Train): 0.45644079821656763\n",
            "Iteration 237, Norm of Gradient: 0.056932331581704215, Cost (Train): 0.4561170837940922\n",
            "Iteration 238, Norm of Gradient: 0.056786788682819035, Cost (Train): 0.4557950208287012\n",
            "Iteration 239, Norm of Gradient: 0.05664215491024394, Cost (Train): 0.45547459482874264\n",
            "Iteration 240, Norm of Gradient: 0.05649842179947927, Cost (Train): 0.45515579147644664\n",
            "Iteration 241, Norm of Gradient: 0.05635558098878102, Cost (Train): 0.4548385966253165\n",
            "Iteration 242, Norm of Gradient: 0.05621362421762639, Cost (Train): 0.4545229962975657\n",
            "Iteration 243, Norm of Gradient: 0.05607254332520662, Cost (Train): 0.4542089766816008\n",
            "Iteration 244, Norm of Gradient: 0.05593233024894606, Cost (Train): 0.45389652412955006\n",
            "Iteration 245, Norm of Gradient: 0.055792977023047564, Cost (Train): 0.4535856251548348\n",
            "Iteration 246, Norm of Gradient: 0.055654475777063075, Cost (Train): 0.4532762664297845\n",
            "Iteration 247, Norm of Gradient: 0.05551681873448944, Cost (Train): 0.45296843478329385\n",
            "Iteration 248, Norm of Gradient: 0.055379998211388434, Cost (Train): 0.452662117198521\n",
            "Iteration 249, Norm of Gradient: 0.055244006615031024, Cost (Train): 0.45235730081062636\n",
            "Iteration 250, Norm of Gradient: 0.05510883644256499, Cost (Train): 0.452053972904551\n",
            "Iteration 251, Norm of Gradient: 0.05497448027970553, Cost (Train): 0.4517521209128337\n",
            "Iteration 252, Norm of Gradient: 0.05484093079944863, Cost (Train): 0.4514517324134662\n",
            "Iteration 253, Norm of Gradient: 0.054708180760806385, Cost (Train): 0.45115279512778567\n",
            "Iteration 254, Norm of Gradient: 0.054576223007564185, Cost (Train): 0.4508552969184035\n",
            "Iteration 255, Norm of Gradient: 0.05444505046705903, Cost (Train): 0.4505592257871701\n",
            "Iteration 256, Norm of Gradient: 0.05431465614897876, Cost (Train): 0.4502645698731747\n",
            "Iteration 257, Norm of Gradient: 0.05418503314418179, Cost (Train): 0.4499713174507792\n",
            "Iteration 258, Norm of Gradient: 0.05405617462353678, Cost (Train): 0.4496794569276861\n",
            "Iteration 259, Norm of Gradient: 0.05392807383678201, Cost (Train): 0.4493889768430393\n",
            "Iteration 260, Norm of Gradient: 0.05380072411140406, Cost (Train): 0.44909986586555717\n",
            "Iteration 261, Norm of Gradient: 0.05367411885153539, Cost (Train): 0.4488121127916972\n",
            "Iteration 262, Norm of Gradient: 0.05354825153687034, Cost (Train): 0.44852570654385193\n",
            "Iteration 263, Norm of Gradient: 0.05342311572159948, Cost (Train): 0.4482406361685755\n",
            "Iteration 264, Norm of Gradient: 0.053298705033361676, Cost (Train): 0.44795689083483975\n",
            "Iteration 265, Norm of Gradient: 0.05317501317221361, Cost (Train): 0.4476744598323196\n",
            "Iteration 266, Norm of Gradient: 0.05305203390961656, Cost (Train): 0.4473933325697074\n",
            "Iteration 267, Norm of Gradient: 0.05292976108743993, Cost (Train): 0.4471134985730553\n",
            "Iteration 268, Norm of Gradient: 0.05280818861698129, Cost (Train): 0.4468349474841455\n",
            "Iteration 269, Norm of Gradient: 0.05268731047800247, Cost (Train): 0.4465576690588864\n",
            "Iteration 270, Norm of Gradient: 0.052567120717781905, Cost (Train): 0.4462816531657377\n",
            "Iteration 271, Norm of Gradient: 0.05244761345018209, Cost (Train): 0.44600688978415887\n",
            "Iteration 272, Norm of Gradient: 0.05232878285473261, Cost (Train): 0.44573336900308486\n",
            "Iteration 273, Norm of Gradient: 0.052210623175728096, Cost (Train): 0.4454610810194263\n",
            "Iteration 274, Norm of Gradient: 0.05209312872134081, Cost (Train): 0.44519001613659437\n",
            "Iteration 275, Norm of Gradient: 0.05197629386274791, Cost (Train): 0.44492016476304996\n",
            "Iteration 276, Norm of Gradient: 0.05186011303327245, Cost (Train): 0.4446515174108759\n",
            "Iteration 277, Norm of Gradient: 0.0517445807275387, Cost (Train): 0.4443840646943732\n",
            "Iteration 278, Norm of Gradient: 0.05162969150064091, Cost (Train): 0.4441177973286793\n",
            "Iteration 279, Norm of Gradient: 0.05151543996732545, Cost (Train): 0.44385270612840927\n",
            "Iteration 280, Norm of Gradient: 0.051401820801186125, Cost (Train): 0.4435887820063188\n",
            "Iteration 281, Norm of Gradient: 0.051288828733872374, Cost (Train): 0.44332601597198795\n",
            "Iteration 282, Norm of Gradient: 0.051176458554310175, Cost (Train): 0.4430643991305274\n",
            "Iteration 283, Norm of Gradient: 0.051064705107935246, Cost (Train): 0.4428039226813042\n",
            "Iteration 284, Norm of Gradient: 0.0509535632959386, Cost (Train): 0.4425445779166881\n",
            "Iteration 285, Norm of Gradient: 0.05084302807452391, Cost (Train): 0.4422863562208187\n",
            "Iteration 286, Norm of Gradient: 0.050733094454176765, Cost (Train): 0.44202924906839103\n",
            "Iteration 287, Norm of Gradient: 0.05062375749894537, Cost (Train): 0.44177324802346074\n",
            "Iteration 288, Norm of Gradient: 0.05051501232573261, Cost (Train): 0.4415183447382684\n",
            "Iteration 289, Norm of Gradient: 0.050406854103599186, Cost (Train): 0.44126453095208235\n",
            "Iteration 290, Norm of Gradient: 0.05029927805307777, Cost (Train): 0.44101179849005945\n",
            "Iteration 291, Norm of Gradient: 0.05019227944549776, Cost (Train): 0.4407601392621234\n",
            "Iteration 292, Norm of Gradient: 0.05008585360232066, Cost (Train): 0.44050954526186165\n",
            "Iteration 293, Norm of Gradient: 0.04997999589448572, Cost (Train): 0.44026000856543807\n",
            "Iteration 294, Norm of Gradient: 0.04987470174176579, Cost (Train): 0.44001152133052424\n",
            "Iteration 295, Norm of Gradient: 0.04976996661213309, Cost (Train): 0.4397640757952452\n",
            "Iteration 296, Norm of Gradient: 0.04966578602113485, Cost (Train): 0.43951766427714356\n",
            "Iteration 297, Norm of Gradient: 0.04956215553127839, Cost (Train): 0.4392722791721586\n",
            "Iteration 298, Norm of Gradient: 0.04945907075142595, Cost (Train): 0.43902791295362076\n",
            "Iteration 299, Norm of Gradient: 0.049356527336198476, Cost (Train): 0.43878455817126244\n",
            "Iteration 300, Norm of Gradient: 0.04925452098538869, Cost (Train): 0.438542207450243\n",
            "Iteration 301, Norm of Gradient: 0.0491530474433831, Cost (Train): 0.43830085349019005\n",
            "Iteration 302, Norm of Gradient: 0.04905210249859278, Cost (Train): 0.4380604890642533\n",
            "Iteration 303, Norm of Gradient: 0.0489516819828928, Cost (Train): 0.4378211070181749\n",
            "Iteration 304, Norm of Gradient: 0.0488517817710701, Cost (Train): 0.43758270026937246\n",
            "Iteration 305, Norm of Gradient: 0.04875239778027976, Cost (Train): 0.43734526180603633\n",
            "Iteration 306, Norm of Gradient: 0.04865352596950947, Cost (Train): 0.4371087846862409\n",
            "Iteration 307, Norm of Gradient: 0.04855516233905199, Cost (Train): 0.43687326203706883\n",
            "Iteration 308, Norm of Gradient: 0.04845730292998557, Cost (Train): 0.4366386870537482\n",
            "Iteration 309, Norm of Gradient: 0.048359943823662065, Cost (Train): 0.4364050529988033\n",
            "Iteration 310, Norm of Gradient: 0.048263081141202875, Cost (Train): 0.4361723532012175\n",
            "Iteration 311, Norm of Gradient: 0.04816671104300216, Cost (Train): 0.4359405810556091\n",
            "Iteration 312, Norm of Gradient: 0.04807082972823751, Cost (Train): 0.4357097300214186\n",
            "Iteration 313, Norm of Gradient: 0.04797543343438803, Cost (Train): 0.4354797936221089\n",
            "Iteration 314, Norm of Gradient: 0.047880518436759285, Cost (Train): 0.43525076544437713\n",
            "Iteration 315, Norm of Gradient: 0.04778608104801544, Cost (Train): 0.4350226391373776\n",
            "Iteration 316, Norm of Gradient: 0.047692117617718154, Cost (Train): 0.4347954084119566\n",
            "Iteration 317, Norm of Gradient: 0.0475986245318724, Cost (Train): 0.4345690670398985\n",
            "Iteration 318, Norm of Gradient: 0.04750559821247879, Cost (Train): 0.43434360885318285\n",
            "Iteration 319, Norm of Gradient: 0.04741303511709246, Cost (Train): 0.43411902774325206\n",
            "Iteration 320, Norm of Gradient: 0.0473209317383885, Cost (Train): 0.43389531766029\n",
            "Iteration 321, Norm of Gradient: 0.04722928460373352, Cost (Train): 0.4336724726125109\n",
            "Iteration 322, Norm of Gradient: 0.04713809027476358, Cost (Train): 0.4334504866654589\n",
            "Iteration 323, Norm of Gradient: 0.04704734534696822, Cost (Train): 0.43322935394131745\n",
            "Iteration 324, Norm of Gradient: 0.04695704644928037, Cost (Train): 0.43300906861822863\n",
            "Iteration 325, Norm of Gradient: 0.04686719024367228, Cost (Train): 0.4327896249296229\n",
            "Iteration 326, Norm of Gradient: 0.04677777342475724, Cost (Train): 0.432571017163557\n",
            "Iteration 327, Norm of Gradient: 0.04668879271939696, Cost (Train): 0.43235323966206335\n",
            "Iteration 328, Norm of Gradient: 0.04660024488631468, Cost (Train): 0.432136286820507\n",
            "Iteration 329, Norm of Gradient: 0.04651212671571359, Cost (Train): 0.43192015308695286\n",
            "Iteration 330, Norm of Gradient: 0.04642443502890095, Cost (Train): 0.43170483296154133\n",
            "Iteration 331, Norm of Gradient: 0.04633716667791736, Cost (Train): 0.4314903209958732\n",
            "Iteration 332, Norm of Gradient: 0.0462503185451713, Cost (Train): 0.4312766117924026\n",
            "Iteration 333, Norm of Gradient: 0.04616388754307895, Cost (Train): 0.4310637000038401\n",
            "Iteration 334, Norm of Gradient: 0.04607787061370894, Cost (Train): 0.4308515803325614\n",
            "Iteration 335, Norm of Gradient: 0.045992264728432285, Cost (Train): 0.43064024753002766\n",
            "Iteration 336, Norm of Gradient: 0.045907066887577, Cost (Train): 0.43042969639621165\n",
            "Iteration 337, Norm of Gradient: 0.045822274120087804, Cost (Train): 0.4302199217790323\n",
            "Iteration 338, Norm of Gradient: 0.04573788348319036, Cost (Train): 0.4300109185737984\n",
            "Iteration 339, Norm of Gradient: 0.04565389206206038, Cost (Train): 0.42980268172265823\n",
            "Iteration 340, Norm of Gradient: 0.04557029696949721, Cost (Train): 0.4295952062140584\n",
            "Iteration 341, Norm of Gradient: 0.04548709534560199, Cost (Train): 0.42938848708220917\n",
            "Iteration 342, Norm of Gradient: 0.04540428435746031, Cost (Train): 0.4291825194065576\n",
            "Iteration 343, Norm of Gradient: 0.045321861198829166, Cost (Train): 0.4289772983112681\n",
            "Iteration 344, Norm of Gradient: 0.04523982308982832, Cost (Train): 0.4287728189647096\n",
            "Iteration 345, Norm of Gradient: 0.04515816727663584, Cost (Train): 0.42856907657895077\n",
            "Iteration 346, Norm of Gradient: 0.04507689103118794, Cost (Train): 0.42836606640926045\n",
            "Iteration 347, Norm of Gradient: 0.0449959916508827, Cost (Train): 0.42816378375361724\n",
            "Iteration 348, Norm of Gradient: 0.04491546645828807, Cost (Train): 0.42796222395222344\n",
            "Iteration 349, Norm of Gradient: 0.044835312800853754, Cost (Train): 0.4277613823870272\n",
            "Iteration 350, Norm of Gradient: 0.04475552805062704, Cost (Train): 0.4275612544812508\n",
            "Iteration 351, Norm of Gradient: 0.04467610960397244, Cost (Train): 0.42736183569892455\n",
            "Iteration 352, Norm of Gradient: 0.04459705488129515, Cost (Train): 0.4271631215444283\n",
            "Iteration 353, Norm of Gradient: 0.044518361326768334, Cost (Train): 0.42696510756203815\n",
            "Iteration 354, Norm of Gradient: 0.044440026408063917, Cost (Train): 0.42676778933547926\n",
            "Iteration 355, Norm of Gradient: 0.044362047616087225, Cost (Train): 0.4265711624874857\n",
            "Iteration 356, Norm of Gradient: 0.044284422464714875, Cost (Train): 0.42637522267936484\n",
            "Iteration 357, Norm of Gradient: 0.04420714849053659, Cost (Train): 0.42617996561056837\n",
            "Iteration 358, Norm of Gradient: 0.044130223252600013, Cost (Train): 0.4259853870182691\n",
            "Iteration 359, Norm of Gradient: 0.04405364433215936, Cost (Train): 0.42579148267694295\n",
            "Iteration 360, Norm of Gradient: 0.04397740933242704, Cost (Train): 0.4255982483979567\n",
            "Iteration 361, Norm of Gradient: 0.04390151587832886, Cost (Train): 0.4254056800291613\n",
            "Iteration 362, Norm of Gradient: 0.043825961616262316, Cost (Train): 0.42521377345449063\n",
            "Iteration 363, Norm of Gradient: 0.0437507442138581, Cost (Train): 0.42502252459356493\n",
            "Iteration 364, Norm of Gradient: 0.04367586135974486, Cost (Train): 0.42483192940130066\n",
            "Iteration 365, Norm of Gradient: 0.04360131076331689, Cost (Train): 0.424641983867524\n",
            "Iteration 366, Norm of Gradient: 0.04352709015450503, Cost (Train): 0.42445268401659075\n",
            "Iteration 367, Norm of Gradient: 0.04345319728355044, Cost (Train): 0.42426402590701023\n",
            "Iteration 368, Norm of Gradient: 0.04337962992078146, Cost (Train): 0.42407600563107467\n",
            "Iteration 369, Norm of Gradient: 0.04330638585639327, Cost (Train): 0.42388861931449356\n",
            "Iteration 370, Norm of Gradient: 0.04323346290023051, Cost (Train): 0.4237018631160314\n",
            "Iteration 371, Norm of Gradient: 0.04316085888157263, Cost (Train): 0.4235157332271528\n",
            "Iteration 372, Norm of Gradient: 0.04308857164892216, Cost (Train): 0.423330225871669\n",
            "Iteration 373, Norm of Gradient: 0.04301659906979558, Cost (Train): 0.4231453373053913\n",
            "Iteration 374, Norm of Gradient: 0.04294493903051699, Cost (Train): 0.4229610638157882\n",
            "Iteration 375, Norm of Gradient: 0.04287358943601437, Cost (Train): 0.4227774017216467\n",
            "Iteration 376, Norm of Gradient: 0.04280254820961854, Cost (Train): 0.42259434737273815\n",
            "Iteration 377, Norm of Gradient: 0.042731813292864636, Cost (Train): 0.4224118971494887\n",
            "Iteration 378, Norm of Gradient: 0.042661382645296154, Cost (Train): 0.4222300474626533\n",
            "Iteration 379, Norm of Gradient: 0.0425912542442715, Cost (Train): 0.4220487947529943\n",
            "Iteration 380, Norm of Gradient: 0.042521426084772995, Cost (Train): 0.42186813549096425\n",
            "Iteration 381, Norm of Gradient: 0.0424518961792183, Cost (Train): 0.42168806617639226\n",
            "Iteration 382, Norm of Gradient: 0.04238266255727436, Cost (Train): 0.42150858333817487\n",
            "Iteration 383, Norm of Gradient: 0.04231372326567345, Cost (Train): 0.4213296835339702\n",
            "Iteration 384, Norm of Gradient: 0.04224507636803187, Cost (Train): 0.421151363349897\n",
            "Iteration 385, Norm of Gradient: 0.04217671994467058, Cost (Train): 0.42097361940023587\n",
            "Iteration 386, Norm of Gradient: 0.04210865209243848, Cost (Train): 0.4207964483271359\n",
            "Iteration 387, Norm of Gradient: 0.04204087092453753, Cost (Train): 0.42061984680032405\n",
            "Iteration 388, Norm of Gradient: 0.041973374570350386, Cost (Train): 0.42044381151681826\n",
            "Iteration 389, Norm of Gradient: 0.041906161175269976, Cost (Train): 0.4202683392006446\n",
            "Iteration 390, Norm of Gradient: 0.04183922890053129, Cost (Train): 0.420093426602557\n",
            "Iteration 391, Norm of Gradient: 0.04177257592304528, Cost (Train): 0.4199190704997616\n",
            "Iteration 392, Norm of Gradient: 0.041706200435234826, Cost (Train): 0.41974526769564396\n",
            "Iteration 393, Norm of Gradient: 0.04164010064487269, Cost (Train): 0.41957201501949937\n",
            "Iteration 394, Norm of Gradient: 0.041574274774921545, Cost (Train): 0.41939930932626696\n",
            "Iteration 395, Norm of Gradient: 0.0415087210633759, Cost (Train): 0.4192271474962671\n",
            "Iteration 396, Norm of Gradient: 0.04144343776310611, Cost (Train): 0.4190555264349416\n",
            "Iteration 397, Norm of Gradient: 0.04137842314170414, Cost (Train): 0.41888444307259737\n",
            "Iteration 398, Norm of Gradient: 0.041313675481331386, Cost (Train): 0.4187138943641534\n",
            "Iteration 399, Norm of Gradient: 0.04124919307856821, Cost (Train): 0.41854387728889064\n",
            "Iteration 400, Norm of Gradient: 0.04118497424426551, Cost (Train): 0.41837438885020467\n",
            "Iteration 401, Norm of Gradient: 0.04112101730339786, Cost (Train): 0.41820542607536204\n",
            "Iteration 402, Norm of Gradient: 0.041057320594918685, Cost (Train): 0.4180369860152587\n",
            "Iteration 403, Norm of Gradient: 0.04099388247161704, Cost (Train): 0.4178690657441823\n",
            "Iteration 404, Norm of Gradient: 0.040930701299976155, Cost (Train): 0.41770166235957673\n",
            "Iteration 405, Norm of Gradient: 0.04086777546003379, Cost (Train): 0.41753477298180974\n",
            "Iteration 406, Norm of Gradient: 0.040805103345244134, Cost (Train): 0.4173683947539431\n",
            "Iteration 407, Norm of Gradient: 0.040742683362341506, Cost (Train): 0.41720252484150605\n",
            "Iteration 408, Norm of Gradient: 0.04068051393120559, Cost (Train): 0.4170371604322709\n",
            "Iteration 409, Norm of Gradient: 0.0406185934847284, Cost (Train): 0.4168722987360319\n",
            "Iteration 410, Norm of Gradient: 0.04055692046868272, Cost (Train): 0.41670793698438585\n",
            "Iteration 411, Norm of Gradient: 0.04049549334159224, Cost (Train): 0.4165440724305166\n",
            "Iteration 412, Norm of Gradient: 0.04043431057460317, Cost (Train): 0.41638070234898095\n",
            "Iteration 413, Norm of Gradient: 0.04037337065135742, Cost (Train): 0.4162178240354978\n",
            "Iteration 414, Norm of Gradient: 0.04031267206786723, Cost (Train): 0.41605543480673945\n",
            "Iteration 415, Norm of Gradient: 0.04025221333239146, Cost (Train): 0.4158935320001257\n",
            "Iteration 416, Norm of Gradient: 0.04019199296531307, Cost (Train): 0.41573211297361984\n",
            "Iteration 417, Norm of Gradient: 0.04013200949901835, Cost (Train): 0.41557117510552766\n",
            "Iteration 418, Norm of Gradient: 0.04007226147777735, Cost (Train): 0.4154107157942986\n",
            "Iteration 419, Norm of Gradient: 0.04001274745762587, Cost (Train): 0.41525073245832905\n",
            "Iteration 420, Norm of Gradient: 0.039953466006248745, Cost (Train): 0.41509122253576797\n",
            "Iteration 421, Norm of Gradient: 0.03989441570286457, Cost (Train): 0.4149321834843252\n",
            "Iteration 422, Norm of Gradient: 0.03983559513811167, Cost (Train): 0.4147736127810817\n",
            "Iteration 423, Norm of Gradient: 0.03977700291393559, Cost (Train): 0.4146155079223018\n",
            "Iteration 424, Norm of Gradient: 0.03971863764347773, Cost (Train): 0.41445786642324794\n",
            "Iteration 425, Norm of Gradient: 0.03966049795096534, Cost (Train): 0.4143006858179976\n",
            "Iteration 426, Norm of Gradient: 0.03960258247160282, Cost (Train): 0.4141439636592624\n",
            "Iteration 427, Norm of Gradient: 0.03954488985146423, Cost (Train): 0.41398769751820846\n",
            "Iteration 428, Norm of Gradient: 0.039487418747387135, Cost (Train): 0.41383188498428036\n",
            "Iteration 429, Norm of Gradient: 0.03943016782686756, Cost (Train): 0.41367652366502566\n",
            "Iteration 430, Norm of Gradient: 0.039373135767956265, Cost (Train): 0.4135216111859221\n",
            "Iteration 431, Norm of Gradient: 0.03931632125915615, Cost (Train): 0.4133671451902073\n",
            "Iteration 432, Norm of Gradient: 0.039259722999320894, Cost (Train): 0.413213123338709\n",
            "Iteration 433, Norm of Gradient: 0.03920333969755468, Cost (Train): 0.41305954330967887\n",
            "Iteration 434, Norm of Gradient: 0.03914717007311319, Cost (Train): 0.41290640279862684\n",
            "Iteration 435, Norm of Gradient: 0.039091212855305625, Cost (Train): 0.4127536995181583\n",
            "Iteration 436, Norm of Gradient: 0.03903546678339785, Cost (Train): 0.4126014311978127\n",
            "Iteration 437, Norm of Gradient: 0.038979930606516784, Cost (Train): 0.41244959558390354\n",
            "Iteration 438, Norm of Gradient: 0.03892460308355565, Cost (Train): 0.4122981904393616\n",
            "Iteration 439, Norm of Gradient: 0.038869482983080544, Cost (Train): 0.4121472135435785\n",
            "Iteration 440, Norm of Gradient: 0.038814569083237827, Cost (Train): 0.4119966626922522\n",
            "Iteration 441, Norm of Gradient: 0.03875986017166282, Cost (Train): 0.41184653569723534\n",
            "Iteration 442, Norm of Gradient: 0.03870535504538926, Cost (Train): 0.41169683038638405\n",
            "Iteration 443, Norm of Gradient: 0.038651052510760044, Cost (Train): 0.4115475446034089\n",
            "Iteration 444, Norm of Gradient: 0.03859695138333874, Cost (Train): 0.41139867620772796\n",
            "Iteration 445, Norm of Gradient: 0.038543050487822264, Cost (Train): 0.4112502230743207\n",
            "Iteration 446, Norm of Gradient: 0.03848934865795449, Cost (Train): 0.41110218309358404\n",
            "Iteration 447, Norm of Gradient: 0.03843584473644075, Cost (Train): 0.4109545541711901\n",
            "Iteration 448, Norm of Gradient: 0.03838253757486339, Cost (Train): 0.4108073342279451\n",
            "Iteration 449, Norm of Gradient: 0.03832942603359826, Cost (Train): 0.4106605211996499\n",
            "Iteration 450, Norm of Gradient: 0.03827650898173208, Cost (Train): 0.4105141130369626\n",
            "Iteration 451, Norm of Gradient: 0.038223785296980714, Cost (Train): 0.41036810770526194\n",
            "Iteration 452, Norm of Gradient: 0.038171253865608476, Cost (Train): 0.4102225031845131\n",
            "Iteration 453, Norm of Gradient: 0.03811891358234819, Cost (Train): 0.41007729746913363\n",
            "Iteration 454, Norm of Gradient: 0.03806676335032216, Cost (Train): 0.4099324885678623\n",
            "Iteration 455, Norm of Gradient: 0.03801480208096412, Cost (Train): 0.4097880745036285\n",
            "Iteration 456, Norm of Gradient: 0.03796302869394191, Cost (Train): 0.40964405331342324\n",
            "Iteration 457, Norm of Gradient: 0.03791144211708107, Cost (Train): 0.40950042304817197\n",
            "Iteration 458, Norm of Gradient: 0.03786004128628929, Cost (Train): 0.4093571817726081\n",
            "Iteration 459, Norm of Gradient: 0.03780882514548162, Cost (Train): 0.4092143275651484\n",
            "Iteration 460, Norm of Gradient: 0.037757792646506555, Cost (Train): 0.4090718585177697\n",
            "Iteration 461, Norm of Gradient: 0.037706942749072964, Cost (Train): 0.4089297727358867\n",
            "Iteration 462, Norm of Gradient: 0.03765627442067766, Cost (Train): 0.4087880683382312\n",
            "Iteration 463, Norm of Gradient: 0.03760578663653396, Cost (Train): 0.4086467434567329\n",
            "Iteration 464, Norm of Gradient: 0.03755547837950088, Cost (Train): 0.40850579623640115\n",
            "Iteration 465, Norm of Gradient: 0.03750534864001317, Cost (Train): 0.40836522483520804\n",
            "Iteration 466, Norm of Gradient: 0.037455396416012025, Cost (Train): 0.40822502742397276\n",
            "Iteration 467, Norm of Gradient: 0.0374056207128767, Cost (Train): 0.4080852021862475\n",
            "Iteration 468, Norm of Gradient: 0.03735602054335666, Cost (Train): 0.407945747318204\n",
            "Iteration 469, Norm of Gradient: 0.037306594927504655, Cost (Train): 0.4078066610285219\n",
            "Iteration 470, Norm of Gradient: 0.037257342892610346, Cost (Train): 0.4076679415382779\n",
            "Iteration 471, Norm of Gradient: 0.037208263473134794, Cost (Train): 0.4075295870808359\n",
            "Iteration 472, Norm of Gradient: 0.03715935571064553, Cost (Train): 0.40739159590173935\n",
            "Iteration 473, Norm of Gradient: 0.03711061865375241, Cost (Train): 0.4072539662586034\n",
            "Iteration 474, Norm of Gradient: 0.037062051358044086, Cost (Train): 0.4071166964210088\n",
            "Iteration 475, Norm of Gradient: 0.03701365288602521, Cost (Train): 0.40697978467039736\n",
            "Iteration 476, Norm of Gradient: 0.036965422307054276, Cost (Train): 0.406843229299968\n",
            "Iteration 477, Norm of Gradient: 0.03691735869728212, Cost (Train): 0.40670702861457364\n",
            "Iteration 478, Norm of Gradient: 0.03686946113959109, Cost (Train): 0.4065711809306196\n",
            "Iteration 479, Norm of Gradient: 0.036821728723534834, Cost (Train): 0.40643568457596346\n",
            "Iteration 480, Norm of Gradient: 0.03677416054527881, Cost (Train): 0.40630053788981496\n",
            "Iteration 481, Norm of Gradient: 0.036726755707541255, Cost (Train): 0.40616573922263766\n",
            "Iteration 482, Norm of Gradient: 0.03667951331953495, Cost (Train): 0.40603128693605156\n",
            "Iteration 483, Norm of Gradient: 0.036632432496909534, Cost (Train): 0.40589717940273673\n",
            "Iteration 484, Norm of Gradient: 0.03658551236169437, Cost (Train): 0.40576341500633734\n",
            "Iteration 485, Norm of Gradient: 0.036538752042242065, Cost (Train): 0.40562999214136775\n",
            "Iteration 486, Norm of Gradient: 0.03649215067317262, Cost (Train): 0.4054969092131189\n",
            "Iteration 487, Norm of Gradient: 0.036445707395318085, Cost (Train): 0.4053641646375654\n",
            "Iteration 488, Norm of Gradient: 0.036399421355667795, Cost (Train): 0.4052317568412745\n",
            "Iteration 489, Norm of Gradient: 0.036353291707314286, Cost (Train): 0.40509968426131526\n",
            "Iteration 490, Norm of Gradient: 0.03630731760939958, Cost (Train): 0.4049679453451689\n",
            "Iteration 491, Norm of Gradient: 0.03626149822706228, Cost (Train): 0.4048365385506398\n",
            "Iteration 492, Norm of Gradient: 0.03621583273138495, Cost (Train): 0.4047054623457681\n",
            "Iteration 493, Norm of Gradient: 0.036170320299342223, Cost (Train): 0.40457471520874233\n",
            "Iteration 494, Norm of Gradient: 0.036124960113749396, Cost (Train): 0.4044442956278137\n",
            "Iteration 495, Norm of Gradient: 0.03607975136321151, Cost (Train): 0.4043142021012108\n",
            "Iteration 496, Norm of Gradient: 0.03603469324207303, Cost (Train): 0.40418443313705504\n",
            "Iteration 497, Norm of Gradient: 0.03598978495036797, Cost (Train): 0.4040549872532777\n",
            "Iteration 498, Norm of Gradient: 0.03594502569377059, Cost (Train): 0.403925862977537\n",
            "Iteration 499, Norm of Gradient: 0.03590041468354654, Cost (Train): 0.40379705884713657\n",
            "Iteration 500, Norm of Gradient: 0.035855951136504555, Cost (Train): 0.403668573408944\n",
            "Iteration 501, Norm of Gradient: 0.03581163427494865, Cost (Train): 0.4035404052193115\n",
            "Iteration 502, Norm of Gradient: 0.03576746332663074, Cost (Train): 0.4034125528439961\n",
            "Iteration 503, Norm of Gradient: 0.03572343752470378, Cost (Train): 0.40328501485808105\n",
            "Iteration 504, Norm of Gradient: 0.03567955610767537, Cost (Train): 0.4031577898458988\n",
            "Iteration 505, Norm of Gradient: 0.03563581831936189, Cost (Train): 0.40303087640095303\n",
            "Iteration 506, Norm of Gradient: 0.035592223408843035, Cost (Train): 0.4029042731258435\n",
            "Iteration 507, Norm of Gradient: 0.03554877063041679, Cost (Train): 0.40277797863219006\n",
            "Iteration 508, Norm of Gradient: 0.035505459243554915, Cost (Train): 0.40265199154055825\n",
            "Iteration 509, Norm of Gradient: 0.035462288512858896, Cost (Train): 0.40252631048038523\n",
            "Iteration 510, Norm of Gradient: 0.03541925770801625, Cost (Train): 0.40240093408990674\n",
            "Iteration 511, Norm of Gradient: 0.03537636610375736, Cost (Train): 0.4022758610160848\n",
            "Iteration 512, Norm of Gradient: 0.0353336129798127, Cost (Train): 0.4021510899145357\n",
            "Iteration 513, Norm of Gradient: 0.03529099762087048, Cost (Train): 0.40202661944945917\n",
            "Iteration 514, Norm of Gradient: 0.03524851931653473, Cost (Train): 0.4019024482935683\n",
            "Iteration 515, Norm of Gradient: 0.035206177361283834, Cost (Train): 0.4017785751280194\n",
            "Iteration 516, Norm of Gradient: 0.03516397105442943, Cost (Train): 0.4016549986423437\n",
            "Iteration 517, Norm of Gradient: 0.03512189970007571, Cost (Train): 0.40153171753437866\n",
            "Iteration 518, Norm of Gradient: 0.035079962607079185, Cost (Train): 0.4014087305102006\n",
            "Iteration 519, Norm of Gradient: 0.035038159089008813, Cost (Train): 0.40128603628405773\n",
            "Iteration 520, Norm of Gradient: 0.034996488464106495, Cost (Train): 0.40116363357830415\n",
            "Iteration 521, Norm of Gradient: 0.03495495005524802, Cost (Train): 0.40104152112333386\n",
            "Iteration 522, Norm of Gradient: 0.034913543189904375, Cost (Train): 0.400919697657516\n",
            "Iteration 523, Norm of Gradient: 0.034872267200103384, Cost (Train): 0.40079816192713075\n",
            "Iteration 524, Norm of Gradient: 0.03483112142239182, Cost (Train): 0.4006769126863052\n",
            "Iteration 525, Norm of Gradient: 0.034790105197797815, Cost (Train): 0.4005559486969504\n",
            "Iteration 526, Norm of Gradient: 0.03474921787179365, Cost (Train): 0.4004352687286994\n",
            "Iteration 527, Norm of Gradient: 0.03470845879425897, Cost (Train): 0.4003148715588447\n",
            "Iteration 528, Norm of Gradient: 0.034667827319444275, Cost (Train): 0.4001947559722774\n",
            "Iteration 529, Norm of Gradient: 0.0346273228059348, Cost (Train): 0.40007492076142614\n",
            "Iteration 530, Norm of Gradient: 0.034586944616614795, Cost (Train): 0.39995536472619775\n",
            "Iteration 531, Norm of Gradient: 0.034546692118632066, Cost (Train): 0.39983608667391707\n",
            "Iteration 532, Norm of Gradient: 0.034506564683362925, Cost (Train): 0.3997170854192681\n",
            "Iteration 533, Norm of Gradient: 0.03446656168637748, Cost (Train): 0.39959835978423586\n",
            "Iteration 534, Norm of Gradient: 0.03442668250740522, Cost (Train): 0.3994799085980487\n",
            "Iteration 535, Norm of Gradient: 0.034386926530300954, Cost (Train): 0.3993617306971208\n",
            "Iteration 536, Norm of Gradient: 0.034347293143011104, Cost (Train): 0.39924382492499516\n",
            "Iteration 537, Norm of Gradient: 0.0343077817375403, Cost (Train): 0.39912619013228834\n",
            "Iteration 538, Norm of Gradient: 0.034268391709918286, Cost (Train): 0.3990088251766338\n",
            "Iteration 539, Norm of Gradient: 0.03422912246016715, Cost (Train): 0.39889172892262753\n",
            "Iteration 540, Norm of Gradient: 0.03418997339226888, Cost (Train): 0.39877490024177326\n",
            "Iteration 541, Norm of Gradient: 0.03415094391413331, Cost (Train): 0.39865833801242817\n",
            "Iteration 542, Norm of Gradient: 0.03411203343756613, Cost (Train): 0.39854204111974983\n",
            "Iteration 543, Norm of Gradient: 0.034073241378237494, Cost (Train): 0.3984260084556425\n",
            "Iteration 544, Norm of Gradient: 0.03403456715565079, Cost (Train): 0.3983102389187054\n",
            "Iteration 545, Norm of Gradient: 0.03399601019311163, Cost (Train): 0.39819473141417955\n",
            "Iteration 546, Norm of Gradient: 0.03395756991769731, Cost (Train): 0.39807948485389755\n",
            "Iteration 547, Norm of Gradient: 0.03391924576022642, Cost (Train): 0.3979644981562315\n",
            "Iteration 548, Norm of Gradient: 0.033881037155228826, Cost (Train): 0.3978497702460427\n",
            "Iteration 549, Norm of Gradient: 0.033842943540915896, Cost (Train): 0.3977353000546317\n",
            "Iteration 550, Norm of Gradient: 0.03380496435915099, Cost (Train): 0.3976210865196883\n",
            "Iteration 551, Norm of Gradient: 0.033767099055420295, Cost (Train): 0.39750712858524273\n",
            "Iteration 552, Norm of Gradient: 0.033729347078803894, Cost (Train): 0.39739342520161686\n",
            "Iteration 553, Norm of Gradient: 0.033691707881947086, Cost (Train): 0.3972799753253755\n",
            "Iteration 554, Norm of Gradient: 0.033654180921032, Cost (Train): 0.39716677791927907\n",
            "Iteration 555, Norm of Gradient: 0.03361676565574951, Cost (Train): 0.39705383195223587\n",
            "Iteration 556, Norm of Gradient: 0.03357946154927135, Cost (Train): 0.3969411363992554\n",
            "Iteration 557, Norm of Gradient: 0.03354226806822256, Cost (Train): 0.3968286902414017\n",
            "Iteration 558, Norm of Gradient: 0.033505184682654045, Cost (Train): 0.39671649246574725\n",
            "Iteration 559, Norm of Gradient: 0.033468210866015674, Cost (Train): 0.3966045420653274\n",
            "Iteration 560, Norm of Gradient: 0.03343134609512926, Cost (Train): 0.3964928380390949\n",
            "Iteration 561, Norm of Gradient: 0.03339458985016215, Cost (Train): 0.3963813793918755\n",
            "Iteration 562, Norm of Gradient: 0.033357941614600775, Cost (Train): 0.39627016513432295\n",
            "Iteration 563, Norm of Gradient: 0.033321400875224616, Cost (Train): 0.39615919428287555\n",
            "Iteration 564, Norm of Gradient: 0.033284967122080365, Cost (Train): 0.3960484658597122\n",
            "Iteration 565, Norm of Gradient: 0.03324863984845631, Cost (Train): 0.39593797889270926\n",
            "Iteration 566, Norm of Gradient: 0.03321241855085697, Cost (Train): 0.3958277324153978\n",
            "Iteration 567, Norm of Gradient: 0.03317630272897796, Cost (Train): 0.3957177254669213\n",
            "Iteration 568, Norm of Gradient: 0.03314029188568112, Cost (Train): 0.39560795709199315\n",
            "Iteration 569, Norm of Gradient: 0.03310438552696978, Cost (Train): 0.39549842634085536\n",
            "Iteration 570, Norm of Gradient: 0.0330685831619644, Cost (Train): 0.39538913226923755\n",
            "Iteration 571, Norm of Gradient: 0.033032884302878256, Cost (Train): 0.39528007393831516\n",
            "Iteration 572, Norm of Gradient: 0.032997288464993536, Cost (Train): 0.3951712504146697\n",
            "Iteration 573, Norm of Gradient: 0.03296179516663751, Cost (Train): 0.3950626607702485\n",
            "Iteration 574, Norm of Gradient: 0.03292640392915898, Cost (Train): 0.39495430408232435\n",
            "Iteration 575, Norm of Gradient: 0.03289111427690492, Cost (Train): 0.39484617943345673\n",
            "Iteration 576, Norm of Gradient: 0.0328559257371974, Cost (Train): 0.3947382859114522\n",
            "Iteration 577, Norm of Gradient: 0.0328208378403106, Cost (Train): 0.3946306226093259\n",
            "Iteration 578, Norm of Gradient: 0.03278585011944811, Cost (Train): 0.394523188625263\n",
            "Iteration 579, Norm of Gradient: 0.03275096211072049, Cost (Train): 0.39441598306258085\n",
            "Iteration 580, Norm of Gradient: 0.03271617335312287, Cost (Train): 0.39430900502969113\n",
            "Iteration 581, Norm of Gradient: 0.032681483388512914, Cost (Train): 0.39420225364006234\n",
            "Iteration 582, Norm of Gradient: 0.03264689176158891, Cost (Train): 0.39409572801218296\n",
            "Iteration 583, Norm of Gradient: 0.032612398019868066, Cost (Train): 0.39398942726952463\n",
            "Iteration 584, Norm of Gradient: 0.03257800171366498, Cost (Train): 0.39388335054050577\n",
            "Iteration 585, Norm of Gradient: 0.03254370239607035, Cost (Train): 0.3937774969584555\n",
            "Iteration 586, Norm of Gradient: 0.03250949962292987, Cost (Train): 0.39367186566157775\n",
            "Iteration 587, Norm of Gradient: 0.032475392952823263, Cost (Train): 0.39356645579291616\n",
            "Iteration 588, Norm of Gradient: 0.03244138194704357, Cost (Train): 0.3934612665003185\n",
            "Iteration 589, Norm of Gradient: 0.03240746616957654, Cost (Train): 0.3933562969364022\n",
            "Iteration 590, Norm of Gradient: 0.03237364518708036, Cost (Train): 0.3932515462585199\n",
            "Iteration 591, Norm of Gradient: 0.03233991856886532, Cost (Train): 0.3931470136287246\n",
            "Iteration 592, Norm of Gradient: 0.03230628588687393, Cost (Train): 0.3930426982137366\n",
            "Iteration 593, Norm of Gradient: 0.03227274671566101, Cost (Train): 0.3929385991849095\n",
            "Iteration 594, Norm of Gradient: 0.03223930063237406, Cost (Train): 0.39283471571819645\n",
            "Iteration 595, Norm of Gradient: 0.03220594721673379, Cost (Train): 0.392731046994118\n",
            "Iteration 596, Norm of Gradient: 0.032172686051014796, Cost (Train): 0.39262759219772864\n",
            "Iteration 597, Norm of Gradient: 0.032139516720026386, Cost (Train): 0.3925243505185846\n",
            "Iteration 598, Norm of Gradient: 0.0321064388110937, Cost (Train): 0.3924213211507118\n",
            "Iteration 599, Norm of Gradient: 0.03207345191403883, Cost (Train): 0.39231850329257373\n",
            "Iteration 600, Norm of Gradient: 0.03204055562116219, Cost (Train): 0.39221589614703983\n",
            "Iteration 601, Norm of Gradient: 0.0320077495272241, Cost (Train): 0.3921134989213544\n",
            "Iteration 602, Norm of Gradient: 0.03197503322942644, Cost (Train): 0.3920113108271053\n",
            "Iteration 603, Norm of Gradient: 0.03194240632739447, Cost (Train): 0.39190933108019294\n",
            "Iteration 604, Norm of Gradient: 0.03190986842315893, Cost (Train): 0.3918075589008003\n",
            "Iteration 605, Norm of Gradient: 0.0318774191211381, Cost (Train): 0.39170599351336227\n",
            "Iteration 606, Norm of Gradient: 0.03184505802812022, Cost (Train): 0.3916046341465356\n",
            "Iteration 607, Norm of Gradient: 0.03181278475324591, Cost (Train): 0.3915034800331692\n",
            "Iteration 608, Norm of Gradient: 0.03178059890799084, Cost (Train): 0.3914025304102748\n",
            "Iteration 609, Norm of Gradient: 0.031748500106148485, Cost (Train): 0.3913017845189974\n",
            "Iteration 610, Norm of Gradient: 0.031716487963813066, Cost (Train): 0.3912012416045863\n",
            "Iteration 611, Norm of Gradient: 0.03168456209936264, Cost (Train): 0.39110090091636684\n",
            "Iteration 612, Norm of Gradient: 0.03165272213344236, Cost (Train): 0.391000761707711\n",
            "Iteration 613, Norm of Gradient: 0.03162096768894776, Cost (Train): 0.39090082323601\n",
            "Iteration 614, Norm of Gradient: 0.03158929839100835, Cost (Train): 0.3908010847626455\n",
            "Iteration 615, Norm of Gradient: 0.03155771386697129, Cost (Train): 0.3907015455529627\n",
            "Iteration 616, Norm of Gradient: 0.031526213746385104, Cost (Train): 0.39060220487624175\n",
            "Iteration 617, Norm of Gradient: 0.031494797660983724, Cost (Train): 0.39050306200567114\n",
            "Iteration 618, Norm of Gradient: 0.03146346524467048, Cost (Train): 0.3904041162183202\n",
            "Iteration 619, Norm of Gradient: 0.031432216133502396, Cost (Train): 0.39030536679511274\n",
            "Iteration 620, Norm of Gradient: 0.03140104996567449, Cost (Train): 0.39020681302079985\n",
            "Iteration 621, Norm of Gradient: 0.03136996638150428, Cost (Train): 0.3901084541839337\n",
            "Iteration 622, Norm of Gradient: 0.031338965023416376, Cost (Train): 0.39001028957684153\n",
            "Iteration 623, Norm of Gradient: 0.031308045535927316, Cost (Train): 0.38991231849559915\n",
            "Iteration 624, Norm of Gradient: 0.03127720756563033, Cost (Train): 0.38981454024000595\n",
            "Iteration 625, Norm of Gradient: 0.03124645076118042, Cost (Train): 0.3897169541135585\n",
            "Iteration 626, Norm of Gradient: 0.031215774773279508, Cost (Train): 0.38961955942342585\n",
            "Iteration 627, Norm of Gradient: 0.031185179254661674, Cost (Train): 0.3895223554804241\n",
            "Iteration 628, Norm of Gradient: 0.031154663860078542, Cost (Train): 0.3894253415989916\n",
            "Iteration 629, Norm of Gradient: 0.03112422824628481, Cost (Train): 0.3893285170971644\n",
            "Iteration 630, Norm of Gradient: 0.03109387207202387, Cost (Train): 0.389231881296551\n",
            "Iteration 631, Norm of Gradient: 0.031063594998013606, Cost (Train): 0.3891354335223093\n",
            "Iteration 632, Norm of Gradient: 0.031033396686932224, Cost (Train): 0.38903917310312136\n",
            "Iteration 633, Norm of Gradient: 0.031003276803404277, Cost (Train): 0.38894309937116994\n",
            "Iteration 634, Norm of Gradient: 0.030973235013986772, Cost (Train): 0.3888472116621151\n",
            "Iteration 635, Norm of Gradient: 0.03094327098715542, Cost (Train): 0.38875150931507\n",
            "Iteration 636, Norm of Gradient: 0.030913384393290957, Cost (Train): 0.38865599167257836\n",
            "Iteration 637, Norm of Gradient: 0.030883574904665638, Cost (Train): 0.3885606580805907\n",
            "Iteration 638, Norm of Gradient: 0.030853842195429804, Cost (Train): 0.38846550788844164\n",
            "Iteration 639, Norm of Gradient: 0.030824185941598546, Cost (Train): 0.3883705404488274\n",
            "Iteration 640, Norm of Gradient: 0.030794605821038542, Cost (Train): 0.3882757551177827\n",
            "Iteration 641, Norm of Gradient: 0.03076510151345497, Cost (Train): 0.38818115125465896\n",
            "Iteration 642, Norm of Gradient: 0.030735672700378497, Cost (Train): 0.38808672822210155\n",
            "Iteration 643, Norm of Gradient: 0.030706319065152435, Cost (Train): 0.3879924853860284\n",
            "Iteration 644, Norm of Gradient: 0.030677040292919962, Cost (Train): 0.38789842211560754\n",
            "Iteration 645, Norm of Gradient: 0.03064783607061147, Cost (Train): 0.387804537783236\n",
            "Iteration 646, Norm of Gradient: 0.030618706086932037, Cost (Train): 0.3877108317645176\n",
            "Iteration 647, Norm of Gradient: 0.030589650032348918, Cost (Train): 0.38761730343824263\n",
            "Iteration 648, Norm of Gradient: 0.030560667599079272, Cost (Train): 0.3875239521863658\n",
            "Iteration 649, Norm of Gradient: 0.03053175848107786, Cost (Train): 0.387430777393986\n",
            "Iteration 650, Norm of Gradient: 0.030502922374024955, Cost (Train): 0.3873377784493247\n",
            "Iteration 651, Norm of Gradient: 0.03047415897531424, Cost (Train): 0.3872449547437062\n",
            "Iteration 652, Norm of Gradient: 0.03044546798404094, Cost (Train): 0.3871523056715367\n",
            "Iteration 653, Norm of Gradient: 0.030416849100989903, Cost (Train): 0.3870598306302835\n",
            "Iteration 654, Norm of Gradient: 0.030388302028623908, Cost (Train): 0.3869675290204562\n",
            "Iteration 655, Norm of Gradient: 0.030359826471071983, Cost (Train): 0.3868754002455856\n",
            "Iteration 656, Norm of Gradient: 0.030331422134117873, Cost (Train): 0.3867834437122039\n",
            "Iteration 657, Norm of Gradient: 0.03030308872518858, Cost (Train): 0.3866916588298259\n",
            "Iteration 658, Norm of Gradient: 0.03027482595334296, Cost (Train): 0.38660004501092876\n",
            "Iteration 659, Norm of Gradient: 0.030246633529260488, Cost (Train): 0.3865086016709326\n",
            "Iteration 660, Norm of Gradient: 0.030218511165230083, Cost (Train): 0.386417328228182\n",
            "Iteration 661, Norm of Gradient: 0.030190458575138975, Cost (Train): 0.38632622410392614\n",
            "Iteration 662, Norm of Gradient: 0.03016247547446175, Cost (Train): 0.3862352887223007\n",
            "Iteration 663, Norm of Gradient: 0.03013456158024941, Cost (Train): 0.38614452151030826\n",
            "Iteration 664, Norm of Gradient: 0.030106716611118568, Cost (Train): 0.3860539218978005\n",
            "Iteration 665, Norm of Gradient: 0.030078940287240712, Cost (Train): 0.3859634893174594\n",
            "Iteration 666, Norm of Gradient: 0.030051232330331537, Cost (Train): 0.3858732232047788\n",
            "Iteration 667, Norm of Gradient: 0.03002359246364042, Cost (Train): 0.38578312299804657\n",
            "Iteration 668, Norm of Gradient: 0.029996020411939914, Cost (Train): 0.38569318813832637\n",
            "Iteration 669, Norm of Gradient: 0.029968515901515374, Cost (Train): 0.38560341806943993\n",
            "Iteration 670, Norm of Gradient: 0.02994107866015465, Cost (Train): 0.38551381223794917\n",
            "Iteration 671, Norm of Gradient: 0.029913708417137853, Cost (Train): 0.38542437009313874\n",
            "Iteration 672, Norm of Gradient: 0.0298864049032272, Cost (Train): 0.3853350910869983\n",
            "Iteration 673, Norm of Gradient: 0.029859167850657, Cost (Train): 0.38524597467420585\n",
            "Iteration 674, Norm of Gradient: 0.029831996993123635, Cost (Train): 0.38515702031210963\n",
            "Iteration 675, Norm of Gradient: 0.029804892065775645, Cost (Train): 0.3850682274607119\n",
            "Iteration 676, Norm of Gradient: 0.029777852805203984, Cost (Train): 0.3849795955826515\n",
            "Iteration 677, Norm of Gradient: 0.029750878949432184, Cost (Train): 0.3848911241431873\n",
            "Iteration 678, Norm of Gradient: 0.029723970237906745, Cost (Train): 0.3848028126101816\n",
            "Iteration 679, Norm of Gradient: 0.029697126411487546, Cost (Train): 0.3847146604540833\n",
            "Iteration 680, Norm of Gradient: 0.029670347212438306, Cost (Train): 0.3846266671479118\n",
            "Iteration 681, Norm of Gradient: 0.02964363238441718, Cost (Train): 0.38453883216724066\n",
            "Iteration 682, Norm of Gradient: 0.02961698167246737, Cost (Train): 0.3844511549901814\n",
            "Iteration 683, Norm of Gradient: 0.029590394823007873, Cost (Train): 0.3843636350973674\n",
            "Iteration 684, Norm of Gradient: 0.02956387158382424, Cost (Train): 0.38427627197193825\n",
            "Iteration 685, Norm of Gradient: 0.029537411704059442, Cost (Train): 0.3841890650995239\n",
            "Iteration 686, Norm of Gradient: 0.029511014934204823, Cost (Train): 0.38410201396822874\n",
            "Iteration 687, Norm of Gradient: 0.029484681026091055, Cost (Train): 0.3840151180686164\n",
            "Iteration 688, Norm of Gradient: 0.02945840973287928, Cost (Train): 0.383928376893694\n",
            "Iteration 689, Norm of Gradient: 0.029432200809052195, Cost (Train): 0.3838417899388975\n",
            "Iteration 690, Norm of Gradient: 0.029406054010405316, Cost (Train): 0.3837553567020757\n",
            "Iteration 691, Norm of Gradient: 0.02937996909403821, Cost (Train): 0.3836690766834757\n",
            "Iteration 692, Norm of Gradient: 0.029353945818345918, Cost (Train): 0.3835829493857277\n",
            "Iteration 693, Norm of Gradient: 0.0293279839430103, Cost (Train): 0.38349697431383034\n",
            "Iteration 694, Norm of Gradient: 0.02930208322899157, Cost (Train): 0.383411150975136\n",
            "Iteration 695, Norm of Gradient: 0.029276243438519832, Cost (Train): 0.38332547887933593\n",
            "Iteration 696, Norm of Gradient: 0.029250464335086716, Cost (Train): 0.383239957538446\n",
            "Iteration 697, Norm of Gradient: 0.02922474568343704, Cost (Train): 0.3831545864667922\n",
            "Iteration 698, Norm of Gradient: 0.029199087249560583, Cost (Train): 0.3830693651809964\n",
            "Iteration 699, Norm of Gradient: 0.02917348880068388, Cost (Train): 0.38298429319996224\n",
            "Iteration 700, Norm of Gradient: 0.029147950105262124, Cost (Train): 0.3828993700448608\n",
            "Iteration 701, Norm of Gradient: 0.02912247093297108, Cost (Train): 0.38281459523911704\n",
            "Iteration 702, Norm of Gradient: 0.029097051054699105, Cost (Train): 0.38272996830839545\n",
            "Iteration 703, Norm of Gradient: 0.029071690242539205, Cost (Train): 0.38264548878058685\n",
            "Iteration 704, Norm of Gradient: 0.029046388269781193, Cost (Train): 0.3825611561857943\n",
            "Iteration 705, Norm of Gradient: 0.02902114491090382, Cost (Train): 0.38247697005631975\n",
            "Iteration 706, Norm of Gradient: 0.028995959941567062, Cost (Train): 0.3823929299266506\n",
            "Iteration 707, Norm of Gradient: 0.02897083313860444, Cost (Train): 0.3823090353334463\n",
            "Iteration 708, Norm of Gradient: 0.02894576428001535, Cost (Train): 0.38222528581552495\n",
            "Iteration 709, Norm of Gradient: 0.028920753144957498, Cost (Train): 0.3821416809138506\n",
            "Iteration 710, Norm of Gradient: 0.02889579951373942, Cost (Train): 0.3820582201715198\n",
            "Iteration 711, Norm of Gradient: 0.028870903167812986, Cost (Train): 0.38197490313374866\n",
            "Iteration 712, Norm of Gradient: 0.028846063889765995, Cost (Train): 0.3818917293478603\n",
            "Iteration 713, Norm of Gradient: 0.028821281463314873, Cost (Train): 0.38180869836327175\n",
            "Iteration 714, Norm of Gradient: 0.028796555673297336, Cost (Train): 0.38172580973148146\n",
            "Iteration 715, Norm of Gradient: 0.02877188630566518, Cost (Train): 0.38164306300605666\n",
            "Iteration 716, Norm of Gradient: 0.02874727314747711, Cost (Train): 0.381560457742621\n",
            "Iteration 717, Norm of Gradient: 0.028722715986891634, Cost (Train): 0.381477993498842\n",
            "Iteration 718, Norm of Gradient: 0.028698214613159915, Cost (Train): 0.38139566983441875\n",
            "Iteration 719, Norm of Gradient: 0.02867376881661886, Cost (Train): 0.38131348631107004\n",
            "Iteration 720, Norm of Gradient: 0.028649378388684102, Cost (Train): 0.38123144249252183\n",
            "Iteration 721, Norm of Gradient: 0.028625043121843087, Cost (Train): 0.3811495379444951\n",
            "Iteration 722, Norm of Gradient: 0.02860076280964824, Cost (Train): 0.38106777223469473\n",
            "Iteration 723, Norm of Gradient: 0.028576537246710155, Cost (Train): 0.38098614493279664\n",
            "Iteration 724, Norm of Gradient: 0.02855236622869084, Cost (Train): 0.38090465561043674\n",
            "Iteration 725, Norm of Gradient: 0.028528249552296995, Cost (Train): 0.380823303841199\n",
            "Iteration 726, Norm of Gradient: 0.02850418701527342, Cost (Train): 0.3807420892006035\n",
            "Iteration 727, Norm of Gradient: 0.028480178416396357, Cost (Train): 0.3806610112660958\n",
            "Iteration 728, Norm of Gradient: 0.02845622355546692, Cost (Train): 0.38058006961703456\n",
            "Iteration 729, Norm of Gradient: 0.028432322233304694, Cost (Train): 0.38049926383468086\n",
            "Iteration 730, Norm of Gradient: 0.028408474251741162, Cost (Train): 0.38041859350218654\n",
            "Iteration 731, Norm of Gradient: 0.028384679413613386, Cost (Train): 0.38033805820458333\n",
            "Iteration 732, Norm of Gradient: 0.028360937522757597, Cost (Train): 0.3802576575287715\n",
            "Iteration 733, Norm of Gradient: 0.02833724838400294, Cost (Train): 0.3801773910635089\n",
            "Iteration 734, Norm of Gradient: 0.02831361180316514, Cost (Train): 0.3800972583994001\n",
            "Iteration 735, Norm of Gradient: 0.028290027587040356, Cost (Train): 0.3800172591288857\n",
            "Iteration 736, Norm of Gradient: 0.028266495543398964, Cost (Train): 0.3799373928462308\n",
            "Iteration 737, Norm of Gradient: 0.028243015480979454, Cost (Train): 0.37985765914751535\n",
            "Iteration 738, Norm of Gradient: 0.028219587209482352, Cost (Train): 0.3797780576306229\n",
            "Iteration 739, Norm of Gradient: 0.02819621053956419, Cost (Train): 0.37969858789522987\n",
            "Iteration 740, Norm of Gradient: 0.028172885282831513, Cost (Train): 0.37961924954279563\n",
            "Iteration 741, Norm of Gradient: 0.028149611251834922, Cost (Train): 0.3795400421765518\n",
            "Iteration 742, Norm of Gradient: 0.028126388260063206, Cost (Train): 0.3794609654014917\n",
            "Iteration 743, Norm of Gradient: 0.02810321612193746, Cost (Train): 0.3793820188243608\n",
            "Iteration 744, Norm of Gradient: 0.02808009465280529, Cost (Train): 0.3793032020536454\n",
            "Iteration 745, Norm of Gradient: 0.02805702366893502, Cost (Train): 0.379224514699564\n",
            "Iteration 746, Norm of Gradient: 0.028034002987510007, Cost (Train): 0.3791459563740557\n",
            "Iteration 747, Norm of Gradient: 0.0280110324266229, Cost (Train): 0.3790675266907716\n",
            "Iteration 748, Norm of Gradient: 0.027988111805270055, Cost (Train): 0.37898922526506396\n",
            "Iteration 749, Norm of Gradient: 0.02796524094334589, Cost (Train): 0.3789110517139769\n",
            "Iteration 750, Norm of Gradient: 0.02794241966163734, Cost (Train): 0.37883300565623623\n",
            "Iteration 751, Norm of Gradient: 0.027919647781818355, Cost (Train): 0.3787550867122403\n",
            "Iteration 752, Norm of Gradient: 0.027896925126444386, Cost (Train): 0.37867729450404986\n",
            "Iteration 753, Norm of Gradient: 0.027874251518946975, Cost (Train): 0.3785996286553788\n",
            "Iteration 754, Norm of Gradient: 0.027851626783628337, Cost (Train): 0.37852208879158455\n",
            "Iteration 755, Norm of Gradient: 0.02782905074565601, Cost (Train): 0.37844467453965885\n",
            "Iteration 756, Norm of Gradient: 0.027806523231057548, Cost (Train): 0.37836738552821814\n",
            "Iteration 757, Norm of Gradient: 0.027784044066715207, Cost (Train): 0.3782902213874944\n",
            "Iteration 758, Norm of Gradient: 0.027761613080360723, Cost (Train): 0.37821318174932606\n",
            "Iteration 759, Norm of Gradient: 0.0277392301005701, Cost (Train): 0.3781362662471484\n",
            "Iteration 760, Norm of Gradient: 0.027716894956758453, Cost (Train): 0.378059474515985\n",
            "Iteration 761, Norm of Gradient: 0.027694607479174863, Cost (Train): 0.3779828061924387\n",
            "Iteration 762, Norm of Gradient: 0.027672367498897295, Cost (Train): 0.377906260914682\n",
            "Iteration 763, Norm of Gradient: 0.027650174847827527, Cost (Train): 0.3778298383224487\n",
            "Iteration 764, Norm of Gradient: 0.027628029358686156, Cost (Train): 0.377753538057025\n",
            "Iteration 765, Norm of Gradient: 0.027605930865007587, Cost (Train): 0.3776773597612408\n",
            "Iteration 766, Norm of Gradient: 0.027583879201135104, Cost (Train): 0.37760130307946044\n",
            "Iteration 767, Norm of Gradient: 0.02756187420221595, Cost (Train): 0.3775253676575749\n",
            "Iteration 768, Norm of Gradient: 0.02753991570419646, Cost (Train): 0.3774495531429922\n",
            "Iteration 769, Norm of Gradient: 0.027518003543817213, Cost (Train): 0.37737385918463007\n",
            "Iteration 770, Norm of Gradient: 0.0274961375586082, Cost (Train): 0.3772982854329062\n",
            "Iteration 771, Norm of Gradient: 0.02747431758688413, Cost (Train): 0.3772228315397309\n",
            "Iteration 772, Norm of Gradient: 0.02745254346773958, Cost (Train): 0.3771474971584977\n",
            "Iteration 773, Norm of Gradient: 0.027430815041044394, Cost (Train): 0.3770722819440762\n",
            "Iteration 774, Norm of Gradient: 0.02740913214743895, Cost (Train): 0.3769971855528028\n",
            "Iteration 775, Norm of Gradient: 0.02738749462832955, Cost (Train): 0.3769222076424731\n",
            "Iteration 776, Norm of Gradient: 0.027365902325883804, Cost (Train): 0.3768473478723336\n",
            "Iteration 777, Norm of Gradient: 0.027344355083026066, Cost (Train): 0.37677260590307354\n",
            "Iteration 778, Norm of Gradient: 0.027322852743432893, Cost (Train): 0.3766979813968171\n",
            "Iteration 779, Norm of Gradient: 0.027301395151528542, Cost (Train): 0.37662347401711505\n",
            "Iteration 780, Norm of Gradient: 0.02727998215248049, Cost (Train): 0.37654908342893717\n",
            "Iteration 781, Norm of Gradient: 0.02725861359219498, Cost (Train): 0.37647480929866434\n",
            "Iteration 782, Norm of Gradient: 0.027237289317312653, Cost (Train): 0.37640065129408057\n",
            "Iteration 783, Norm of Gradient: 0.027216009175204118, Cost (Train): 0.37632660908436544\n",
            "Iteration 784, Norm of Gradient: 0.027194773013965636, Cost (Train): 0.3762526823400862\n",
            "Iteration 785, Norm of Gradient: 0.027173580682414787, Cost (Train): 0.37617887073319023\n",
            "Iteration 786, Norm of Gradient: 0.02715243203008618, Cost (Train): 0.3761051739369975\n",
            "Iteration 787, Norm of Gradient: 0.027131326907227214, Cost (Train): 0.37603159162619293\n",
            "Iteration 788, Norm of Gradient: 0.027110265164793836, Cost (Train): 0.37595812347681895\n",
            "Iteration 789, Norm of Gradient: 0.027089246654446338, Cost (Train): 0.37588476916626773\n",
            "Iteration 790, Norm of Gradient: 0.027068271228545198, Cost (Train): 0.37581152837327464\n",
            "Iteration 791, Norm of Gradient: 0.027047338740146955, Cost (Train): 0.3757384007779098\n",
            "Iteration 792, Norm of Gradient: 0.027026449043000063, Cost (Train): 0.37566538606157157\n",
            "Iteration 793, Norm of Gradient: 0.027005601991540854, Cost (Train): 0.3755924839069789\n",
            "Iteration 794, Norm of Gradient: 0.026984797440889444, Cost (Train): 0.3755196939981644\n",
            "Iteration 795, Norm of Gradient: 0.026964035246845743, Cost (Train): 0.3754470160204672\n",
            "Iteration 796, Norm of Gradient: 0.026943315265885424, Cost (Train): 0.3753744496605254\n",
            "Iteration 797, Norm of Gradient: 0.026922637355155998, Cost (Train): 0.3753019946062696\n",
            "Iteration 798, Norm of Gradient: 0.026902001372472825, Cost (Train): 0.3752296505469153\n",
            "Iteration 799, Norm of Gradient: 0.02688140717631526, Cost (Train): 0.37515741717295675\n",
            "Iteration 800, Norm of Gradient: 0.026860854625822706, Cost (Train): 0.375085294176159\n",
            "Iteration 801, Norm of Gradient: 0.026840343580790793, Cost (Train): 0.375013281249552\n",
            "Iteration 802, Norm of Gradient: 0.02681987390166753, Cost (Train): 0.374941378087423\n",
            "Iteration 803, Norm of Gradient: 0.026799445449549507, Cost (Train): 0.3748695843853104\n",
            "Iteration 804, Norm of Gradient: 0.026779058086178125, Cost (Train): 0.3747978998399966\n",
            "Iteration 805, Norm of Gradient: 0.026758711673935814, Cost (Train): 0.3747263241495014\n",
            "Iteration 806, Norm of Gradient: 0.02673840607584233, Cost (Train): 0.37465485701307555\n",
            "Iteration 807, Norm of Gradient: 0.026718141155551055, Cost (Train): 0.37458349813119374\n",
            "Iteration 808, Norm of Gradient: 0.02669791677734531, Cost (Train): 0.3745122472055485\n",
            "Iteration 809, Norm of Gradient: 0.026677732806134707, Cost (Train): 0.37444110393904356\n",
            "Iteration 810, Norm of Gradient: 0.026657589107451515, Cost (Train): 0.3743700680357871\n",
            "Iteration 811, Norm of Gradient: 0.026637485547447087, Cost (Train): 0.37429913920108543\n",
            "Iteration 812, Norm of Gradient: 0.02661742199288826, Cost (Train): 0.3742283171414371\n",
            "Iteration 813, Norm of Gradient: 0.026597398311153802, Cost (Train): 0.3741576015645258\n",
            "Iteration 814, Norm of Gradient: 0.0265774143702309, Cost (Train): 0.37408699217921443\n",
            "Iteration 815, Norm of Gradient: 0.026557470038711655, Cost (Train): 0.37401648869553894\n",
            "Iteration 816, Norm of Gradient: 0.026537565185789595, Cost (Train): 0.373946090824702\n",
            "Iteration 817, Norm of Gradient: 0.02651769968125622, Cost (Train): 0.3738757982790664\n",
            "Iteration 818, Norm of Gradient: 0.026497873395497584, Cost (Train): 0.3738056107721501\n",
            "Iteration 819, Norm of Gradient: 0.02647808619949086, Cost (Train): 0.3737355280186184\n",
            "Iteration 820, Norm of Gradient: 0.026458337964801, Cost (Train): 0.3736655497342795\n",
            "Iteration 821, Norm of Gradient: 0.02643862856357732, Cost (Train): 0.37359567563607743\n",
            "Iteration 822, Norm of Gradient: 0.02641895786855019, Cost (Train): 0.37352590544208664\n",
            "Iteration 823, Norm of Gradient: 0.026399325753027742, Cost (Train): 0.37345623887150564\n",
            "Iteration 824, Norm of Gradient: 0.026379732090892515, Cost (Train): 0.3733866756446515\n",
            "Iteration 825, Norm of Gradient: 0.026360176756598235, Cost (Train): 0.37331721548295366\n",
            "Iteration 826, Norm of Gradient: 0.026340659625166532, Cost (Train): 0.37324785810894817\n",
            "Iteration 827, Norm of Gradient: 0.026321180572183755, Cost (Train): 0.37317860324627206\n",
            "Iteration 828, Norm of Gradient: 0.02630173947379771, Cost (Train): 0.37310945061965745\n",
            "Iteration 829, Norm of Gradient: 0.026282336206714538, Cost (Train): 0.3730403999549257\n",
            "Iteration 830, Norm of Gradient: 0.026262970648195483, Cost (Train): 0.3729714509789822\n",
            "Iteration 831, Norm of Gradient: 0.026243642676053825, Cost (Train): 0.3729026034198101\n",
            "Iteration 832, Norm of Gradient: 0.026224352168651707, Cost (Train): 0.37283385700646504\n",
            "Iteration 833, Norm of Gradient: 0.02620509900489703, Cost (Train): 0.3727652114690698\n",
            "Iteration 834, Norm of Gradient: 0.026185883064240436, Cost (Train): 0.37269666653880823\n",
            "Iteration 835, Norm of Gradient: 0.026166704226672194, Cost (Train): 0.3726282219479202\n",
            "Iteration 836, Norm of Gradient: 0.026147562372719167, Cost (Train): 0.37255987742969593\n",
            "Iteration 837, Norm of Gradient: 0.026128457383441828, Cost (Train): 0.3724916327184704\n",
            "Iteration 838, Norm of Gradient: 0.026109389140431233, Cost (Train): 0.37242348754961846\n",
            "Iteration 839, Norm of Gradient: 0.02609035752580606, Cost (Train): 0.37235544165954887\n",
            "Iteration 840, Norm of Gradient: 0.026071362422209633, Cost (Train): 0.3722874947856994\n",
            "Iteration 841, Norm of Gradient: 0.026052403712807017, Cost (Train): 0.37221964666653135\n",
            "Iteration 842, Norm of Gradient: 0.02603348128128207, Cost (Train): 0.37215189704152435\n",
            "Iteration 843, Norm of Gradient: 0.026014595011834566, Cost (Train): 0.37208424565117104\n",
            "Iteration 844, Norm of Gradient: 0.025995744789177294, Cost (Train): 0.372016692236972\n",
            "Iteration 845, Norm of Gradient: 0.02597693049853322, Cost (Train): 0.3719492365414305\n",
            "Iteration 846, Norm of Gradient: 0.02595815202563264, Cost (Train): 0.3718818783080475\n",
            "Iteration 847, Norm of Gradient: 0.025939409256710354, Cost (Train): 0.37181461728131615\n",
            "Iteration 848, Norm of Gradient: 0.025920702078502843, Cost (Train): 0.3717474532067174\n",
            "Iteration 849, Norm of Gradient: 0.025902030378245542, Cost (Train): 0.37168038583071455\n",
            "Iteration 850, Norm of Gradient: 0.02588339404367001, Cost (Train): 0.37161341490074784\n",
            "Iteration 851, Norm of Gradient: 0.025864792963001218, Cost (Train): 0.37154654016523053\n",
            "Iteration 852, Norm of Gradient: 0.025846227024954805, Cost (Train): 0.3714797613735431\n",
            "Iteration 853, Norm of Gradient: 0.025827696118734374, Cost (Train): 0.3714130782760283\n",
            "Iteration 854, Norm of Gradient: 0.025809200134028792, Cost (Train): 0.3713464906239872\n",
            "Iteration 855, Norm of Gradient: 0.025790738961009516, Cost (Train): 0.3712799981696733\n",
            "Iteration 856, Norm of Gradient: 0.025772312490327944, Cost (Train): 0.37121360066628817\n",
            "Iteration 857, Norm of Gradient: 0.025753920613112734, Cost (Train): 0.3711472978679768\n",
            "Iteration 858, Norm of Gradient: 0.025735563220967237, Cost (Train): 0.3710810895298223\n",
            "Iteration 859, Norm of Gradient: 0.02571724020596686, Cost (Train): 0.371014975407842\n",
            "Iteration 860, Norm of Gradient: 0.025698951460656462, Cost (Train): 0.3709489552589821\n",
            "Iteration 861, Norm of Gradient: 0.025680696878047804, Cost (Train): 0.370883028841113\n",
            "Iteration 862, Norm of Gradient: 0.025662476351616993, Cost (Train): 0.37081719591302525\n",
            "Iteration 863, Norm of Gradient: 0.025644289775301917, Cost (Train): 0.3707514562344241\n",
            "Iteration 864, Norm of Gradient: 0.02562613704349976, Cost (Train): 0.3706858095659256\n",
            "Iteration 865, Norm of Gradient: 0.025608018051064458, Cost (Train): 0.37062025566905193\n",
            "Iteration 866, Norm of Gradient: 0.025589932693304233, Cost (Train): 0.37055479430622656\n",
            "Iteration 867, Norm of Gradient: 0.025571880865979116, Cost (Train): 0.3704894252407698\n",
            "Iteration 868, Norm of Gradient: 0.02555386246529848, Cost (Train): 0.37042414823689485\n",
            "Iteration 869, Norm of Gradient: 0.025535877387918588, Cost (Train): 0.3703589630597026\n",
            "Iteration 870, Norm of Gradient: 0.02551792553094021, Cost (Train): 0.3702938694751779\n",
            "Iteration 871, Norm of Gradient: 0.02550000679190615, Cost (Train): 0.37022886725018456\n",
            "Iteration 872, Norm of Gradient: 0.025482121068798904, Cost (Train): 0.3701639561524616\n",
            "Iteration 873, Norm of Gradient: 0.025464268260038257, Cost (Train): 0.37009913595061833\n",
            "Iteration 874, Norm of Gradient: 0.025446448264478914, Cost (Train): 0.3700344064141305\n",
            "Iteration 875, Norm of Gradient: 0.025428660981408172, Cost (Train): 0.36996976731333553\n",
            "Iteration 876, Norm of Gradient: 0.025410906310543546, Cost (Train): 0.36990521841942914\n",
            "Iteration 877, Norm of Gradient: 0.025393184152030516, Cost (Train): 0.3698407595044599\n",
            "Iteration 878, Norm of Gradient: 0.02537549440644013, Cost (Train): 0.3697763903413259\n",
            "Iteration 879, Norm of Gradient: 0.025357836974766818, Cost (Train): 0.36971211070377047\n",
            "Iteration 880, Norm of Gradient: 0.025340211758426042, Cost (Train): 0.3696479203663778\n",
            "Iteration 881, Norm of Gradient: 0.02532261865925206, Cost (Train): 0.3695838191045687\n",
            "Iteration 882, Norm of Gradient: 0.02530505757949569, Cost (Train): 0.369519806694597\n",
            "Iteration 883, Norm of Gradient: 0.025287528421822063, Cost (Train): 0.3694558829135449\n",
            "Iteration 884, Norm of Gradient: 0.02527003108930842, Cost (Train): 0.3693920475393194\n",
            "Iteration 885, Norm of Gradient: 0.025252565485441893, Cost (Train): 0.369328300350648\n",
            "Iteration 886, Norm of Gradient: 0.02523513151411732, Cost (Train): 0.36926464112707436\n",
            "Iteration 887, Norm of Gradient: 0.025217729079635105, Cost (Train): 0.3692010696489553\n",
            "Iteration 888, Norm of Gradient: 0.025200358086698973, Cost (Train): 0.36913758569745586\n",
            "Iteration 889, Norm of Gradient: 0.025183018440413943, Cost (Train): 0.36907418905454603\n",
            "Iteration 890, Norm of Gradient: 0.02516571004628408, Cost (Train): 0.3690108795029962\n",
            "Iteration 891, Norm of Gradient: 0.02514843281021044, Cost (Train): 0.3689476568263738\n",
            "Iteration 892, Norm of Gradient: 0.025131186638488925, Cost (Train): 0.3688845208090395\n",
            "Iteration 893, Norm of Gradient: 0.025113971437808243, Cost (Train): 0.36882147123614273\n",
            "Iteration 894, Norm of Gradient: 0.025096787115247775, Cost (Train): 0.3687585078936185\n",
            "Iteration 895, Norm of Gradient: 0.025079633578275532, Cost (Train): 0.36869563056818355\n",
            "Iteration 896, Norm of Gradient: 0.025062510734746093, Cost (Train): 0.36863283904733213\n",
            "Iteration 897, Norm of Gradient: 0.025045418492898572, Cost (Train): 0.3685701331193325\n",
            "Iteration 898, Norm of Gradient: 0.02502835676135459, Cost (Train): 0.3685075125732234\n",
            "Iteration 899, Norm of Gradient: 0.02501132544911625, Cost (Train): 0.3684449771988103\n",
            "Iteration 900, Norm of Gradient: 0.024994324465564124, Cost (Train): 0.36838252678666117\n",
            "Iteration 901, Norm of Gradient: 0.024977353720455307, Cost (Train): 0.3683201611281037\n",
            "Iteration 902, Norm of Gradient: 0.024960413123921395, Cost (Train): 0.36825788001522075\n",
            "Iteration 903, Norm of Gradient: 0.02494350258646653, Cost (Train): 0.36819568324084784\n",
            "Iteration 904, Norm of Gradient: 0.02492662201896546, Cost (Train): 0.368133570598568\n",
            "Iteration 905, Norm of Gradient: 0.024909771332661593, Cost (Train): 0.36807154188271\n",
            "Iteration 906, Norm of Gradient: 0.024892950439165045, Cost (Train): 0.3680095968883434\n",
            "Iteration 907, Norm of Gradient: 0.024876159250450773, Cost (Train): 0.3679477354112754\n",
            "Iteration 908, Norm of Gradient: 0.024859397678856615, Cost (Train): 0.3678859572480479\n",
            "Iteration 909, Norm of Gradient: 0.024842665637081443, Cost (Train): 0.3678242621959331\n",
            "Iteration 910, Norm of Gradient: 0.024825963038183244, Cost (Train): 0.36776265005293074\n",
            "Iteration 911, Norm of Gradient: 0.024809289795577286, Cost (Train): 0.3677011206177642\n",
            "Iteration 912, Norm of Gradient: 0.024792645823034233, Cost (Train): 0.36763967368987754\n",
            "Iteration 913, Norm of Gradient: 0.024776031034678306, Cost (Train): 0.36757830906943145\n",
            "Iteration 914, Norm of Gradient: 0.02475944534498546, Cost (Train): 0.3675170265573004\n",
            "Iteration 915, Norm of Gradient: 0.024742888668781535, Cost (Train): 0.36745582595506926\n",
            "Iteration 916, Norm of Gradient: 0.024726360921240493, Cost (Train): 0.3673947070650293\n",
            "Iteration 917, Norm of Gradient: 0.024709862017882542, Cost (Train): 0.3673336696901759\n",
            "Iteration 918, Norm of Gradient: 0.024693391874572427, Cost (Train): 0.3672727136342041\n",
            "Iteration 919, Norm of Gradient: 0.024676950407517585, Cost (Train): 0.3672118387015063\n",
            "Iteration 920, Norm of Gradient: 0.024660537533266417, Cost (Train): 0.3671510446971684\n",
            "Iteration 921, Norm of Gradient: 0.02464415316870653, Cost (Train): 0.36709033142696684\n",
            "Iteration 922, Norm of Gradient: 0.024627797231062957, Cost (Train): 0.36702969869736507\n",
            "Iteration 923, Norm of Gradient: 0.024611469637896462, Cost (Train): 0.36696914631551036\n",
            "Iteration 924, Norm of Gradient: 0.024595170307101795, Cost (Train): 0.3669086740892313\n",
            "Iteration 925, Norm of Gradient: 0.024578899156905976, Cost (Train): 0.3668482818270334\n",
            "Iteration 926, Norm of Gradient: 0.024562656105866606, Cost (Train): 0.36678796933809693\n",
            "Iteration 927, Norm of Gradient: 0.024546441072870153, Cost (Train): 0.3667277364322736\n",
            "Iteration 928, Norm of Gradient: 0.024530253977130292, Cost (Train): 0.3666675829200828\n",
            "Iteration 929, Norm of Gradient: 0.02451409473818621, Cost (Train): 0.36660750861270935\n",
            "Iteration 930, Norm of Gradient: 0.02449796327590097, Cost (Train): 0.3665475133219998\n",
            "Iteration 931, Norm of Gradient: 0.02448185951045983, Cost (Train): 0.36648759686045995\n",
            "Iteration 932, Norm of Gradient: 0.024465783362368616, Cost (Train): 0.3664277590412509\n",
            "Iteration 933, Norm of Gradient: 0.024449734752452105, Cost (Train): 0.3663679996781869\n",
            "Iteration 934, Norm of Gradient: 0.024433713601852366, Cost (Train): 0.3663083185857319\n",
            "Iteration 935, Norm of Gradient: 0.024417719832027174, Cost (Train): 0.3662487155789966\n",
            "Iteration 936, Norm of Gradient: 0.02440175336474841, Cost (Train): 0.3661891904737355\n",
            "Iteration 937, Norm of Gradient: 0.02438581412210046, Cost (Train): 0.36612974308634383\n",
            "Iteration 938, Norm of Gradient: 0.024369902026478624, Cost (Train): 0.36607037323385494\n",
            "Iteration 939, Norm of Gradient: 0.02435401700058754, Cost (Train): 0.36601108073393696\n",
            "Iteration 940, Norm of Gradient: 0.024338158967439665, Cost (Train): 0.36595186540488994\n",
            "Iteration 941, Norm of Gradient: 0.024322327850353653, Cost (Train): 0.36589272706564324\n",
            "Iteration 942, Norm of Gradient: 0.02430652357295286, Cost (Train): 0.36583366553575225\n",
            "Iteration 943, Norm of Gradient: 0.024290746059163787, Cost (Train): 0.36577468063539603\n",
            "Iteration 944, Norm of Gradient: 0.024274995233214556, Cost (Train): 0.3657157721853739\n",
            "Iteration 945, Norm of Gradient: 0.0242592710196334, Cost (Train): 0.36565694000710286\n",
            "Iteration 946, Norm of Gradient: 0.024243573343247136, Cost (Train): 0.3655981839226151\n",
            "Iteration 947, Norm of Gradient: 0.024227902129179697, Cost (Train): 0.36553950375455435\n",
            "Iteration 948, Norm of Gradient: 0.024212257302850607, Cost (Train): 0.3654808993261742\n",
            "Iteration 949, Norm of Gradient: 0.024196638789973534, Cost (Train): 0.36542237046133424\n",
            "Iteration 950, Norm of Gradient: 0.024181046516554772, Cost (Train): 0.3653639169844981\n",
            "Iteration 951, Norm of Gradient: 0.024165480408891847, Cost (Train): 0.36530553872073035\n",
            "Iteration 952, Norm of Gradient: 0.024149940393571972, Cost (Train): 0.36524723549569377\n",
            "Iteration 953, Norm of Gradient: 0.024134426397470692, Cost (Train): 0.3651890071356468\n",
            "Iteration 954, Norm of Gradient: 0.02411893834775038, Cost (Train): 0.36513085346744073\n",
            "Iteration 955, Norm of Gradient: 0.024103476171858842, Cost (Train): 0.3650727743185171\n",
            "Iteration 956, Norm of Gradient: 0.02408803979752788, Cost (Train): 0.3650147695169048\n",
            "Iteration 957, Norm of Gradient: 0.024072629152771897, Cost (Train): 0.3649568388912179\n",
            "Iteration 958, Norm of Gradient: 0.024057244165886477, Cost (Train): 0.3648989822706524\n",
            "Iteration 959, Norm of Gradient: 0.024041884765447, Cost (Train): 0.3648411994849843\n",
            "Iteration 960, Norm of Gradient: 0.024026550880307248, Cost (Train): 0.36478349036456614\n",
            "Iteration 961, Norm of Gradient: 0.024011242439598027, Cost (Train): 0.36472585474032554\n",
            "Iteration 962, Norm of Gradient: 0.023995959372725813, Cost (Train): 0.36466829244376125\n",
            "Iteration 963, Norm of Gradient: 0.023980701609371362, Cost (Train): 0.36461080330694184\n",
            "Iteration 964, Norm of Gradient: 0.023965469079488374, Cost (Train): 0.3645533871625026\n",
            "Iteration 965, Norm of Gradient: 0.02395026171330215, Cost (Train): 0.3644960438436426\n",
            "Iteration 966, Norm of Gradient: 0.02393507944130824, Cost (Train): 0.36443877318412315\n",
            "Iteration 967, Norm of Gradient: 0.02391992219427112, Cost (Train): 0.3643815750182644\n",
            "Iteration 968, Norm of Gradient: 0.023904789903222857, Cost (Train): 0.36432444918094337\n",
            "Iteration 969, Norm of Gradient: 0.023889682499461825, Cost (Train): 0.3642673955075911\n",
            "Iteration 970, Norm of Gradient: 0.023874599914551375, Cost (Train): 0.3642104138341906\n",
            "Iteration 971, Norm of Gradient: 0.02385954208031853, Cost (Train): 0.3641535039972742\n",
            "Iteration 972, Norm of Gradient: 0.02384450892885271, Cost (Train): 0.3640966658339208\n",
            "Iteration 973, Norm of Gradient: 0.023829500392504428, Cost (Train): 0.36403989918175406\n",
            "Iteration 974, Norm of Gradient: 0.023814516403884038, Cost (Train): 0.36398320387893957\n",
            "Iteration 975, Norm of Gradient: 0.023799556895860458, Cost (Train): 0.3639265797641824\n",
            "Iteration 976, Norm of Gradient: 0.02378462180155989, Cost (Train): 0.3638700266767252\n",
            "Iteration 977, Norm of Gradient: 0.02376971105436458, Cost (Train): 0.36381354445634523\n",
            "Iteration 978, Norm of Gradient: 0.023754824587911585, Cost (Train): 0.36375713294335205\n",
            "Iteration 979, Norm of Gradient: 0.0237399623360915, Cost (Train): 0.3637007919785861\n",
            "Iteration 980, Norm of Gradient: 0.023725124233047276, Cost (Train): 0.3636445214034152\n",
            "Iteration 981, Norm of Gradient: 0.023710310213172905, Cost (Train): 0.3635883210597325\n",
            "Iteration 982, Norm of Gradient: 0.02369552021111232, Cost (Train): 0.36353219078995486\n",
            "Iteration 983, Norm of Gradient: 0.02368075416175809, Cost (Train): 0.3634761304370195\n",
            "Iteration 984, Norm of Gradient: 0.023666012000250265, Cost (Train): 0.363420139844383\n",
            "Iteration 985, Norm of Gradient: 0.02365129366197515, Cost (Train): 0.3633642188560175\n",
            "Iteration 986, Norm of Gradient: 0.023636599082564127, Cost (Train): 0.3633083673164098\n",
            "Iteration 987, Norm of Gradient: 0.02362192819789249, Cost (Train): 0.3632525850705586\n",
            "Iteration 988, Norm of Gradient: 0.02360728094407824, Cost (Train): 0.3631968719639717\n",
            "Iteration 989, Norm of Gradient: 0.02359265725748093, Cost (Train): 0.363141227842665\n",
            "Iteration 990, Norm of Gradient: 0.023578057074700495, Cost (Train): 0.363085652553159\n",
            "Iteration 991, Norm of Gradient: 0.023563480332576093, Cost (Train): 0.36303014594247757\n",
            "Iteration 992, Norm of Gradient: 0.02354892696818499, Cost (Train): 0.36297470785814523\n",
            "Iteration 993, Norm of Gradient: 0.02353439691884136, Cost (Train): 0.3629193381481854\n",
            "Iteration 994, Norm of Gradient: 0.02351989012209519, Cost (Train): 0.36286403666111766\n",
            "Iteration 995, Norm of Gradient: 0.023505406515731145, Cost (Train): 0.362808803245956\n",
            "Iteration 996, Norm of Gradient: 0.02349094603776742, Cost (Train): 0.3627536377522068\n",
            "Iteration 997, Norm of Gradient: 0.02347650862645467, Cost (Train): 0.36269854002986635\n",
            "Iteration 998, Norm of Gradient: 0.023462094220274836, Cost (Train): 0.3626435099294189\n",
            "Iteration 999, Norm of Gradient: 0.023447702757940107, Cost (Train): 0.36258854730183465\n",
            "Terminated after 1000 iterations, with norm of the gradient equal to 0.023447702757940107\n",
            "The weight found: [-0.05299544 -0.03853025 -0.02095686 ... -0.20258951 -0.03122547\n",
            "  0.03179315]\n",
            "x_train shape after bias term: (20000, 1877)\n",
            "self.w shape: (1877,)\n",
            "Iteration 0, Norm of Gradient: 0.18126416496373463, Cost (Train): 0.6898904875879663\n",
            "Iteration 1, Norm of Gradient: 0.17826732304195567, Cost (Train): 0.6867343806448453\n",
            "Iteration 2, Norm of Gradient: 0.17593489433978435, Cost (Train): 0.6836567917725394\n",
            "Iteration 3, Norm of Gradient: 0.1739864062451894, Cost (Train): 0.6806449942402276\n",
            "Iteration 4, Norm of Gradient: 0.17226194227048103, Cost (Train): 0.6776914666431249\n",
            "Iteration 5, Norm of Gradient: 0.17067098064491953, Cost (Train): 0.6747915895663801\n",
            "Iteration 6, Norm of Gradient: 0.1691624872900674, Cost (Train): 0.671942364031467\n",
            "Iteration 7, Norm of Gradient: 0.16770774926287868, Cost (Train): 0.6691416988426634\n",
            "Iteration 8, Norm of Gradient: 0.16629062178696033, Cost (Train): 0.6663880144364566\n",
            "Iteration 9, Norm of Gradient: 0.16490201931775034, Cost (Train): 0.6636800227348529\n",
            "Iteration 10, Norm of Gradient: 0.1635368141915369, Cost (Train): 0.6610166048414763\n",
            "Iteration 11, Norm of Gradient: 0.16219209410665586, Cost (Train): 0.6583967431139908\n",
            "Iteration 12, Norm of Gradient: 0.1608661843021148, Cost (Train): 0.6558194834460722\n",
            "Iteration 13, Norm of Gradient: 0.1595580993559189, Cost (Train): 0.6532839143249215\n",
            "Iteration 14, Norm of Gradient: 0.15826723610914126, Cost (Train): 0.6507891551978059\n",
            "Iteration 15, Norm of Gradient: 0.15699320183704588, Cost (Train): 0.648334349998622\n",
            "Iteration 16, Norm of Gradient: 0.15573571824618918, Cost (Train): 0.6459186635295828\n",
            "Iteration 17, Norm of Gradient: 0.15449456796708017, Cost (Train): 0.6435412794180776\n",
            "Iteration 18, Norm of Gradient: 0.15326956485309295, Cost (Train): 0.6412013989383901\n",
            "Iteration 19, Norm of Gradient: 0.15206053760842428, Cost (Train): 0.6388982403044698\n",
            "Iteration 20, Norm of Gradient: 0.1508673208725969, Cost (Train): 0.6366310382158146\n",
            "Iteration 21, Norm of Gradient: 0.14968975047044766, Cost (Train): 0.6343990435361954\n",
            "Iteration 22, Norm of Gradient: 0.1485276609835253, Cost (Train): 0.6322015230391861\n",
            "Iteration 23, Norm of Gradient: 0.14738088460983167, Cost (Train): 0.630037759184549\n",
            "Iteration 24, Norm of Gradient: 0.14624925073336845, Cost (Train): 0.6279070499061994\n",
            "Iteration 25, Norm of Gradient: 0.14513258587967356, Cost (Train): 0.6258087084016961\n",
            "Iteration 26, Norm of Gradient: 0.14403071387628585, Cost (Train): 0.623742062918281\n",
            "Iteration 27, Norm of Gradient: 0.14294345611706497, Cost (Train): 0.621706456533279\n",
            "Iteration 28, Norm of Gradient: 0.14187063187412088, Cost (Train): 0.6197012469281693\n",
            "Iteration 29, Norm of Gradient: 0.1408120586262271, Cost (Train): 0.6177258061564451\n",
            "Iteration 30, Norm of Gradient: 0.13976755238666047, Cost (Train): 0.6157795204057809\n",
            "Iteration 31, Norm of Gradient: 0.13873692802129806, Cost (Train): 0.6138617897552243\n",
            "Iteration 32, Norm of Gradient: 0.13771999955221698, Cost (Train): 0.6119720279282034\n",
            "Iteration 33, Norm of Gradient: 0.13671658044451365, Cost (Train): 0.6101096620421457\n",
            "Iteration 34, Norm of Gradient: 0.1357264838754411, Cost (Train): 0.6082741323554921\n",
            "Iteration 35, Norm of Gradient: 0.134749522985734, Cost (Train): 0.6064648920128372\n",
            "Iteration 36, Norm of Gradient: 0.13378551111341813, Cost (Train): 0.6046814067888874\n",
            "Iteration 37, Norm of Gradient: 0.1328342620106318, Cost (Train): 0.6029231548318728\n",
            "Iteration 38, Norm of Gradient: 0.13189559004411572, Cost (Train): 0.6011896264069974\n",
            "Iteration 39, Norm of Gradient: 0.13096931038008466, Cost (Train): 0.5994803236404636\n",
            "Iteration 40, Norm of Gradient: 0.13005523915422537, Cost (Train): 0.5977947602645536\n",
            "Iteration 41, Norm of Gradient: 0.12915319362756789, Cost (Train): 0.5961324613642122\n",
            "Iteration 42, Norm of Gradient: 0.1282629923289733, Cost (Train): 0.5944929631255235\n",
            "Iteration 43, Norm of Gradient: 0.12738445518496538, Cost (Train): 0.5928758125864402\n",
            "Iteration 44, Norm of Gradient: 0.1265174036376169, Cost (Train): 0.5912805673900818\n",
            "Iteration 45, Norm of Gradient: 0.12566166075117874, Cost (Train): 0.5897067955408837\n",
            "Iteration 46, Norm of Gradient: 0.12481705130811792, Cost (Train): 0.5881540751638468\n",
            "Iteration 47, Norm of Gradient: 0.12398340189520524, Cost (Train): 0.5866219942671068\n",
            "Iteration 48, Norm of Gradient: 0.12316054098026939, Cost (Train): 0.5851101505080115\n",
            "Iteration 49, Norm of Gradient: 0.12234829898020771, Cost (Train): 0.5836181509628688\n",
            "Iteration 50, Norm of Gradient: 0.12154650832081884, Cost (Train): 0.5821456119005097\n",
            "Iteration 51, Norm of Gradient: 0.1207550034889965, Cost (Train): 0.5806921585597771\n",
            "Iteration 52, Norm of Gradient: 0.11997362107779921, Cost (Train): 0.579257424931041\n",
            "Iteration 53, Norm of Gradient: 0.11920219982488338, Cost (Train): 0.5778410535418165\n",
            "Iteration 54, Norm of Gradient: 0.11844058064476581, Cost (Train): 0.5764426952465445\n",
            "Iteration 55, Norm of Gradient: 0.11768860665535547, Cost (Train): 0.5750620090205817\n",
            "Iteration 56, Norm of Gradient: 0.11694612319917054, Cost (Train): 0.5736986617584294\n",
            "Iteration 57, Norm of Gradient: 0.11621297785963665, Cost (Train): 0.5723523280762182\n",
            "Iteration 58, Norm of Gradient: 0.11548902047283761, Cost (Train): 0.571022690118455\n",
            "Iteration 59, Norm of Gradient: 0.11477410313506965, Cost (Train): 0.5697094373690292\n",
            "Iteration 60, Norm of Gradient: 0.11406808020653061, Cost (Train): 0.5684122664664614\n",
            "Iteration 61, Norm of Gradient: 0.11337080831145455, Cost (Train): 0.5671308810233724\n",
            "Iteration 62, Norm of Gradient: 0.11268214633498466, Cost (Train): 0.5658649914501452\n",
            "Iteration 63, Norm of Gradient: 0.1120019554170584, Cost (Train): 0.5646143147827372\n",
            "Iteration 64, Norm of Gradient: 0.111330098943563, Cost (Train): 0.5633785745146055\n",
            "Iteration 65, Norm of Gradient: 0.1106664425350015, Cost (Train): 0.5621575004326923\n",
            "Iteration 66, Norm of Gradient: 0.11001085403289562, Cost (Train): 0.56095082845742\n",
            "Iteration 67, Norm of Gradient: 0.10936320348413606, Cost (Train): 0.5597583004866377\n",
            "Iteration 68, Norm of Gradient: 0.10872336312347664, Cost (Train): 0.5585796642434583\n",
            "Iteration 69, Norm of Gradient: 0.1080912073543574, Cost (Train): 0.5574146731279248\n",
            "Iteration 70, Norm of Gradient: 0.10746661272822569, Cost (Train): 0.5562630860724358\n",
            "Iteration 71, Norm of Gradient: 0.10684945792251682, Cost (Train): 0.555124667400867\n",
            "Iteration 72, Norm of Gradient: 0.1062396237174414, Cost (Train): 0.5539991866913132\n",
            "Iteration 73, Norm of Gradient: 0.1056369929717168, Cost (Train): 0.552886418642387\n",
            "Iteration 74, Norm of Gradient: 0.10504145059737167, Cost (Train): 0.5517861429429929\n",
            "Iteration 75, Norm of Gradient: 0.10445288353374128, Cost (Train): 0.5506981441455125\n",
            "Iteration 76, Norm of Gradient: 0.10387118072076414, Cost (Train): 0.549622211542322\n",
            "Iteration 77, Norm of Gradient: 0.1032962330716805, Cost (Train): 0.5485581390455702\n",
            "Iteration 78, Norm of Gradient: 0.10272793344522806, Cost (Train): 0.5475057250701427\n",
            "Iteration 79, Norm of Gradient: 0.10216617661742054, Cost (Train): 0.5464647724197402\n",
            "Iteration 80, Norm of Gradient: 0.10161085925298939, Cost (Train): 0.5454350881759962\n",
            "Iteration 81, Norm of Gradient: 0.10106187987656244, Cost (Train): 0.5444164835905605\n",
            "Iteration 82, Norm of Gradient: 0.10051913884364701, Cost (Train): 0.5434087739800804\n",
            "Iteration 83, Norm of Gradient: 0.09998253831147959, Cost (Train): 0.5424117786240026\n",
            "Iteration 84, Norm of Gradient: 0.09945198220979908, Cost (Train): 0.5414253206651306\n",
            "Iteration 85, Norm of Gradient: 0.09892737621159631, Cost (Train): 0.5404492270128629\n",
            "Iteration 86, Norm of Gradient: 0.09840862770388648, Cost (Train): 0.5394833282490488\n",
            "Iteration 87, Norm of Gradient: 0.09789564575854898, Cost (Train): 0.5385274585363881\n",
            "Iteration 88, Norm of Gradient: 0.09738834110327375, Cost (Train): 0.537581455529314\n",
            "Iteration 89, Norm of Gradient: 0.09688662609264985, Cost (Train): 0.5366451602872891\n",
            "Iteration 90, Norm of Gradient: 0.09639041467942933, Cost (Train): 0.5357184171904547\n",
            "Iteration 91, Norm of Gradient: 0.09589962238599593, Cost (Train): 0.5348010738575651\n",
            "Iteration 92, Norm of Gradient: 0.09541416627606392, Cost (Train): 0.5338929810661519\n",
            "Iteration 93, Norm of Gradient: 0.09493396492663238, Cost (Train): 0.532993992674852\n",
            "Iteration 94, Norm of Gradient: 0.09445893840021545, Cost (Train): 0.5321039655478442\n",
            "Iteration 95, Norm of Gradient: 0.09398900821736725, Cost (Train): 0.5312227594813332\n",
            "Iteration 96, Norm of Gradient: 0.09352409732951902, Cost (Train): 0.5303502371320298\n",
            "Iteration 97, Norm of Gradient: 0.09306413009214251, Cost (Train): 0.5294862639475648\n",
            "Iteration 98, Norm of Gradient: 0.09260903223825306, Cost (Train): 0.5286307080987909\n",
            "Iteration 99, Norm of Gradient: 0.09215873085226353, Cost (Train): 0.5277834404139123\n",
            "Iteration 100, Norm of Gradient: 0.0917131543441985, Cost (Train): 0.5269443343143986\n",
            "Iteration 101, Norm of Gradient: 0.09127223242427727, Cost (Train): 0.5261132657526247\n",
            "Iteration 102, Norm of Gradient: 0.09083589607787265, Cost (Train): 0.525290113151196\n",
            "Iteration 103, Norm of Gradient: 0.09040407754085074, Cost (Train): 0.5244747573439046\n",
            "Iteration 104, Norm of Gradient: 0.0899767102752963, Cost (Train): 0.5236670815182768\n",
            "Iteration 105, Norm of Gradient: 0.08955372894562802, Cost (Train): 0.5228669711596614\n",
            "Iteration 106, Norm of Gradient: 0.08913506939510518, Cost (Train): 0.5220743139968174\n",
            "Iteration 107, Norm of Gradient: 0.08872066862272768, Cost (Train): 0.5212889999489597\n",
            "Iteration 108, Norm of Gradient: 0.08831046476053103, Cost (Train): 0.5205109210742191\n",
            "Iteration 109, Norm of Gradient: 0.08790439705127531, Cost (Train): 0.5197399715194786\n",
            "Iteration 110, Norm of Gradient: 0.08750240582652864, Cost (Train): 0.5189760474715426\n",
            "Iteration 111, Norm of Gradient: 0.0871044324851432, Cost (Train): 0.5182190471096081\n",
            "Iteration 112, Norm of Gradient: 0.0867104194721227, Cost (Train): 0.5174688705589936\n",
            "Iteration 113, Norm of Gradient: 0.08632031025787933, Cost (Train): 0.5167254198460939\n",
            "Iteration 114, Norm of Gradient: 0.0859340493178767, Cost (Train): 0.5159885988545245\n",
            "Iteration 115, Norm of Gradient: 0.0855515821126568, Cost (Train): 0.5152583132824231\n",
            "Iteration 116, Norm of Gradient: 0.08517285506824694, Cost (Train): 0.5145344706008718\n",
            "Iteration 117, Norm of Gradient: 0.08479781555694346, Cost (Train): 0.5138169800134121\n",
            "Iteration 118, Norm of Gradient: 0.08442641187846774, Cost (Train): 0.5131057524166206\n",
            "Iteration 119, Norm of Gradient: 0.08405859324149095, Cost (Train): 0.5124007003617103\n",
            "Iteration 120, Norm of Gradient: 0.08369430974552242, Cost (Train): 0.5117017380171371\n",
            "Iteration 121, Norm of Gradient: 0.08333351236315757, Cost (Train): 0.5110087811321756\n",
            "Iteration 122, Norm of Gradient: 0.08297615292268008, Cost (Train): 0.5103217470014387\n",
            "Iteration 123, Norm of Gradient: 0.08262218409101388, Cost (Train): 0.5096405544303164\n",
            "Iteration 124, Norm of Gradient: 0.0822715593570194, Cost (Train): 0.5089651237013044\n",
            "Iteration 125, Norm of Gradient: 0.08192423301512913, Cost (Train): 0.5082953765411999\n",
            "Iteration 126, Norm of Gradient: 0.08158016014931735, Cost (Train): 0.5076312360891387\n",
            "Iteration 127, Norm of Gradient: 0.08123929661739837, Cost (Train): 0.5069726268654494\n",
            "Iteration 128, Norm of Gradient: 0.08090159903564784, Cost (Train): 0.506319474741303\n",
            "Iteration 129, Norm of Gradient: 0.08056702476374238, Cost (Train): 0.5056717069091358\n",
            "Iteration 130, Norm of Gradient: 0.08023553189001109, Cost (Train): 0.5050292518538201\n",
            "Iteration 131, Norm of Gradient: 0.07990707921699416, Cost (Train): 0.5043920393245676\n",
            "Iteration 132, Norm of Gradient: 0.07958162624730272, Cost (Train): 0.5037600003075389\n",
            "Iteration 133, Norm of Gradient: 0.07925913316977488, Cost (Train): 0.5031330669991445\n",
            "Iteration 134, Norm of Gradient: 0.0789395608459216, Cost (Train): 0.5025111727800144\n",
            "Iteration 135, Norm of Gradient: 0.07862287079665811, Cost (Train): 0.5018942521896189\n",
            "Iteration 136, Norm of Gradient: 0.0783090251893146, Cost (Train): 0.5012822409015226\n",
            "Iteration 137, Norm of Gradient: 0.07799798682492098, Cost (Train): 0.5006750756992542\n",
            "Iteration 138, Norm of Gradient: 0.07768971912576068, Cost (Train): 0.5000726944527737\n",
            "Iteration 139, Norm of Gradient: 0.07738418612318759, Cost (Train): 0.4994750360955228\n",
            "Iteration 140, Norm of Gradient: 0.07708135244570179, Cost (Train): 0.498882040602039\n",
            "Iteration 141, Norm of Gradient: 0.0767811833072777, Cost (Train): 0.4982936489661216\n",
            "Iteration 142, Norm of Gradient: 0.07648364449594058, Cost (Train): 0.49770980317953206\n",
            "Iteration 143, Norm of Gradient: 0.07618870236258579, Cost (Train): 0.4971304462112137\n",
            "Iteration 144, Norm of Gradient: 0.07589632381003587, Cost (Train): 0.4965555219870185\n",
            "Iteration 145, Norm of Gradient: 0.07560647628233043, Cost (Train): 0.4959849753699257\n",
            "Iteration 146, Norm of Gradient: 0.07531912775424415, Cost (Train): 0.49541875214073916\n",
            "Iteration 147, Norm of Gradient: 0.07503424672102814, Cost (Train): 0.49485679897924956\n",
            "Iteration 148, Norm of Gradient: 0.07475180218836956, Cost (Train): 0.49429906344585145\n",
            "Iteration 149, Norm of Gradient: 0.07447176366256536, Cost (Train): 0.4937454939635989\n",
            "Iteration 150, Norm of Gradient: 0.07419410114090509, Cost (Train): 0.493196039800692\n",
            "Iteration 151, Norm of Gradient: 0.07391878510225872, Cost (Train): 0.49265065105337896\n",
            "Iteration 152, Norm of Gradient: 0.0736457864978646, Cost (Train): 0.49210927862926573\n",
            "Iteration 153, Norm of Gradient: 0.0733750767423138, Cost (Train): 0.49157187423101933\n",
            "Iteration 154, Norm of Gradient: 0.07310662770472597, Cost (Train): 0.49103839034045715\n",
            "Iteration 155, Norm of Gradient: 0.07284041170011267, Cost (Train): 0.4905087802030098\n",
            "Iteration 156, Norm of Gradient: 0.07257640148092476, Cost (Train): 0.4899829978125483\n",
            "Iteration 157, Norm of Gradient: 0.07231457022877862, Cost (Train): 0.48946099789656655\n",
            "Iteration 158, Norm of Gradient: 0.07205489154635827, Cost (Train): 0.48894273590170834\n",
            "Iteration 159, Norm of Gradient: 0.071797339449489, Cost (Train): 0.4884281679796308\n",
            "Iteration 160, Norm of Gradient: 0.07154188835937898, Cost (Train): 0.48791725097319577\n",
            "Iteration 161, Norm of Gradient: 0.07128851309502467, Cost (Train): 0.48740994240297897\n",
            "Iteration 162, Norm of Gradient: 0.07103718886577726, Cost (Train): 0.48690620045409017\n",
            "Iteration 163, Norm of Gradient: 0.07078789126406557, Cost (Train): 0.48640598396329476\n",
            "Iteration 164, Norm of Gradient: 0.07054059625827253, Cost (Train): 0.48590925240643135\n",
            "Iteration 165, Norm of Gradient: 0.07029528018576167, Cost (Train): 0.4854159658861142\n",
            "Iteration 166, Norm of Gradient: 0.07005191974605045, Cost (Train): 0.4849260851197159\n",
            "Iteration 167, Norm of Gradient: 0.06981049199412642, Cost (Train): 0.48443957142762245\n",
            "Iteration 168, Norm of Gradient: 0.06957097433390434, Cost (Train): 0.48395638672175256\n",
            "Iteration 169, Norm of Gradient: 0.06933334451181962, Cost (Train): 0.48347649349433514\n",
            "Iteration 170, Norm of Gradient: 0.06909758061055606, Cost (Train): 0.48299985480694\n",
            "Iteration 171, Norm of Gradient: 0.06886366104290463, Cost (Train): 0.48252643427975145\n",
            "Iteration 172, Norm of Gradient: 0.06863156454574997, Cost (Train): 0.48205619608108224\n",
            "Iteration 173, Norm of Gradient: 0.0684012701741823, Cost (Train): 0.4815891049171205\n",
            "Iteration 174, Norm of Gradient: 0.06817275729573156, Cost (Train): 0.4811251260219024\n",
            "Iteration 175, Norm of Gradient: 0.06794600558472115, Cost (Train): 0.48066422514750756\n",
            "Iteration 176, Norm of Gradient: 0.0677209950167385, Cost (Train): 0.48020636855446885\n",
            "Iteration 177, Norm of Gradient: 0.06749770586322022, Cost (Train): 0.479751523002393\n",
            "Iteration 178, Norm of Gradient: 0.0672761186861485, Cost (Train): 0.47929965574078576\n",
            "Iteration 179, Norm of Gradient: 0.06705621433285718, Cost (Train): 0.4788507345000785\n",
            "Iteration 180, Norm of Gradient: 0.06683797393094427, Cost (Train): 0.478404727482846\n",
            "Iteration 181, Norm of Gradient: 0.06662137888328894, Cost (Train): 0.4779616033552189\n",
            "Iteration 182, Norm of Gradient: 0.06640641086317067, Cost (Train): 0.4775213312384775\n",
            "Iteration 183, Norm of Gradient: 0.06619305180948798, Cost (Train): 0.47708388070082886\n",
            "Iteration 184, Norm of Gradient: 0.06598128392207456, Cost (Train): 0.47664922174935825\n",
            "Iteration 185, Norm of Gradient: 0.0657710896571111, Cost (Train): 0.47621732482215456\n",
            "Iteration 186, Norm of Gradient: 0.06556245172263003, Cost (Train): 0.47578816078060127\n",
            "Iteration 187, Norm of Gradient: 0.06535535307411144, Cost (Train): 0.4753617009018328\n",
            "Iteration 188, Norm of Gradient: 0.06514977691016811, Cost (Train): 0.4749379168713496\n",
            "Iteration 189, Norm of Gradient: 0.06494570666831757, Cost (Train): 0.47451678077578907\n",
            "Iteration 190, Norm of Gradient: 0.06474312602083952, Cost (Train): 0.4740982650958488\n",
            "Iteration 191, Norm of Gradient: 0.06454201887071623, Cost (Train): 0.4736823426993575\n",
            "Iteration 192, Norm of Gradient: 0.06434236934765454, Cost (Train): 0.47326898683449165\n",
            "Iteration 193, Norm of Gradient: 0.06414416180418742, Cost (Train): 0.4728581711231325\n",
            "Iteration 194, Norm of Gradient: 0.06394738081185336, Cost (Train): 0.4724498695543617\n",
            "Iteration 195, Norm of Gradient: 0.06375201115745185, Cost (Train): 0.47204405647809056\n",
            "Iteration 196, Norm of Gradient: 0.06355803783937335, Cost (Train): 0.4716407065988216\n",
            "Iteration 197, Norm of Gradient: 0.063365446064002, Cost (Train): 0.47123979496953877\n",
            "Iteration 198, Norm of Gradient: 0.06317422124218956, Cost (Train): 0.4708412969857211\n",
            "Iteration 199, Norm of Gradient: 0.06298434898579884, Cost (Train): 0.4704451883794802\n",
            "Iteration 200, Norm of Gradient: 0.0627958151043154, Cost (Train): 0.47005144521381736\n",
            "Iteration 201, Norm of Gradient: 0.06260860560152565, Cost (Train): 0.4696600438769958\n",
            "Iteration 202, Norm of Gradient: 0.06242270667226013, Cost (Train): 0.4692709610770282\n",
            "Iteration 203, Norm of Gradient: 0.062238104699200714, Cost (Train): 0.46888417383627484\n",
            "Iteration 204, Norm of Gradient: 0.06205478624974956, Cost (Train): 0.46849965948615024\n",
            "Iteration 205, Norm of Gradient: 0.06187273807295975, Cost (Train): 0.4681173956619367\n",
            "Iteration 206, Norm of Gradient: 0.06169194709652478, Cost (Train): 0.46773736029770036\n",
            "Iteration 207, Norm of Gradient: 0.061512400423826996, Cost (Train): 0.4673595316213097\n",
            "Iteration 208, Norm of Gradient: 0.061334085331042724, Cost (Train): 0.46698388814955216\n",
            "Iteration 209, Norm of Gradient: 0.06115698926430331, Cost (Train): 0.46661040868334813\n",
            "Iteration 210, Norm of Gradient: 0.060981099836910746, Cost (Train): 0.4662390723030585\n",
            "Iteration 211, Norm of Gradient: 0.06080640482660679, Cost (Train): 0.46586985836388595\n",
            "Iteration 212, Norm of Gradient: 0.06063289217289417, Cost (Train): 0.4655027464913648\n",
            "Iteration 213, Norm of Gradient: 0.06046054997440921, Cost (Train): 0.4651377165769406\n",
            "Iteration 214, Norm of Gradient: 0.06028936648634423, Cost (Train): 0.46477474877363467\n",
            "Iteration 215, Norm of Gradient: 0.06011933011791913, Cost (Train): 0.46441382349179394\n",
            "Iteration 216, Norm of Gradient: 0.05995042942990067, Cost (Train): 0.46405492139492216\n",
            "Iteration 217, Norm of Gradient: 0.05978265313216892, Cost (Train): 0.46369802339559285\n",
            "Iteration 218, Norm of Gradient: 0.05961599008132931, Cost (Train): 0.46334311065143957\n",
            "Iteration 219, Norm of Gradient: 0.05945042927836988, Cost (Train): 0.46299016456122444\n",
            "Iteration 220, Norm of Gradient: 0.059285959866362224, Cost (Train): 0.4626391667609809\n",
            "Iteration 221, Norm of Gradient: 0.05912257112820569, Cost (Train): 0.4622900991202311\n",
            "Iteration 222, Norm of Gradient: 0.05896025248441354, Cost (Train): 0.46194294373827427\n",
            "Iteration 223, Norm of Gradient: 0.05879899349094048, Cost (Train): 0.4615976829405461\n",
            "Iteration 224, Norm of Gradient: 0.058638783837050366, Cost (Train): 0.46125429927504685\n",
            "Iteration 225, Norm of Gradient: 0.058479613343223526, Cost (Train): 0.4609127755088367\n",
            "Iteration 226, Norm of Gradient: 0.0583214719591029, Cost (Train): 0.46057309462459733\n",
            "Iteration 227, Norm of Gradient: 0.05816434976147764, Cost (Train): 0.4602352398172573\n",
            "Iteration 228, Norm of Gradient: 0.058008236952304296, Cost (Train): 0.45989919449068073\n",
            "Iteration 229, Norm of Gradient: 0.0578531238567639, Cost (Train): 0.4595649422544186\n",
            "Iteration 230, Norm of Gradient: 0.057699000921354784, Cost (Train): 0.4592324669205185\n",
            "Iteration 231, Norm of Gradient: 0.05754585871202016, Cost (Train): 0.45890175250039567\n",
            "Iteration 232, Norm of Gradient: 0.057393687912309796, Cost (Train): 0.45857278320175954\n",
            "Iteration 233, Norm of Gradient: 0.05724247932157504, Cost (Train): 0.458245543425599\n",
            "Iteration 234, Norm of Gradient: 0.057092223853196566, Cost (Train): 0.4579200177632211\n",
            "Iteration 235, Norm of Gradient: 0.056942912532844094, Cost (Train): 0.457596190993346\n",
            "Iteration 236, Norm of Gradient: 0.05679453649676752, Cost (Train): 0.4572740480792524\n",
            "Iteration 237, Norm of Gradient: 0.056647086990118695, Cost (Train): 0.45695357416597726\n",
            "Iteration 238, Norm of Gradient: 0.05650055536530333, Cost (Train): 0.45663475457756497\n",
            "Iteration 239, Norm of Gradient: 0.056354933080362386, Cost (Train): 0.4563175748143667\n",
            "Iteration 240, Norm of Gradient: 0.05621021169738218, Cost (Train): 0.4560020205503881\n",
            "Iteration 241, Norm of Gradient: 0.056066382880933054, Cost (Train): 0.45568807763068564\n",
            "Iteration 242, Norm of Gradient: 0.055923438396535456, Cost (Train): 0.45537573206880805\n",
            "Iteration 243, Norm of Gradient: 0.05578137010915336, Cost (Train): 0.4550649700442853\n",
            "Iteration 244, Norm of Gradient: 0.05564016998171413, Cost (Train): 0.45475577790016086\n",
            "Iteration 245, Norm of Gradient: 0.05549983007365463, Cost (Train): 0.45444814214056833\n",
            "Iteration 246, Norm of Gradient: 0.0553603425394927, Cost (Train): 0.4541420494283517\n",
            "Iteration 247, Norm of Gradient: 0.05522169962742367, Cost (Train): 0.45383748658272643\n",
            "Iteration 248, Norm of Gradient: 0.05508389367794152, Cost (Train): 0.4535344405769828\n",
            "Iteration 249, Norm of Gradient: 0.0549469171224839, Cost (Train): 0.45323289853622867\n",
            "Iteration 250, Norm of Gradient: 0.05481076248210093, Cost (Train): 0.45293284773517306\n",
            "Iteration 251, Norm of Gradient: 0.0546754223661467, Cost (Train): 0.45263427559594666\n",
            "Iteration 252, Norm of Gradient: 0.054540889470993816, Cost (Train): 0.45233716968596227\n",
            "Iteration 253, Norm of Gradient: 0.05440715657876992, Cost (Train): 0.452041517715811\n",
            "Iteration 254, Norm of Gradient: 0.054274216556115834, Cost (Train): 0.4517473075371954\n",
            "Iteration 255, Norm of Gradient: 0.054142062352965314, Cost (Train): 0.45145452714089823\n",
            "Iteration 256, Norm of Gradient: 0.054010687001345294, Cost (Train): 0.4511631646547864\n",
            "Iteration 257, Norm of Gradient: 0.05388008361419706, Cost (Train): 0.45087320834184885\n",
            "Iteration 258, Norm of Gradient: 0.05375024538421708, Cost (Train): 0.4505846465982684\n",
            "Iteration 259, Norm of Gradient: 0.05362116558271781, Cost (Train): 0.45029746795152614\n",
            "Iteration 260, Norm of Gradient: 0.053492837558507714, Cost (Train): 0.4500116610585386\n",
            "Iteration 261, Norm of Gradient: 0.053365254736790095, Cost (Train): 0.44972721470382604\n",
            "Iteration 262, Norm of Gradient: 0.05323841061808066, Cost (Train): 0.449444117797712\n",
            "Iteration 263, Norm of Gradient: 0.05311229877714323, Cost (Train): 0.44916235937455323\n",
            "Iteration 264, Norm of Gradient: 0.05298691286194313, Cost (Train): 0.44888192859099973\n",
            "Iteration 265, Norm of Gradient: 0.052862246592618456, Cost (Train): 0.4486028147242835\n",
            "Iteration 266, Norm of Gradient: 0.05273829376046806, Cost (Train): 0.44832500717053625\n",
            "Iteration 267, Norm of Gradient: 0.0526150482269567, Cost (Train): 0.4480484954431349\n",
            "Iteration 268, Norm of Gradient: 0.05249250392273661, Cost (Train): 0.44777326917107546\n",
            "Iteration 269, Norm of Gradient: 0.05237065484668517, Cost (Train): 0.447499318097373\n",
            "Iteration 270, Norm of Gradient: 0.052249495064958636, Cost (Train): 0.4472266320774883\n",
            "Iteration 271, Norm of Gradient: 0.05212901871006133, Cost (Train): 0.4469552010777809\n",
            "Iteration 272, Norm of Gradient: 0.052009219979930195, Cost (Train): 0.4466850151739878\n",
            "Iteration 273, Norm of Gradient: 0.05189009313703432, Cost (Train): 0.44641606454972604\n",
            "Iteration 274, Norm of Gradient: 0.05177163250748915, Cost (Train): 0.446148339495021\n",
            "Iteration 275, Norm of Gradient: 0.05165383248018526, Cost (Train): 0.4458818304048583\n",
            "Iteration 276, Norm of Gradient: 0.051536687505931156, Cost (Train): 0.445616527777759\n",
            "Iteration 277, Norm of Gradient: 0.05142019209661009, Cost (Train): 0.44535242221437893\n",
            "Iteration 278, Norm of Gradient: 0.05130434082435047, Cost (Train): 0.44508950441612943\n",
            "Iteration 279, Norm of Gradient: 0.051189128320709654, Cost (Train): 0.44482776518382106\n",
            "Iteration 280, Norm of Gradient: 0.05107454927587097, Cost (Train): 0.4445671954163297\n",
            "Iteration 281, Norm of Gradient: 0.05096059843785348, Cost (Train): 0.4443077861092833\n",
            "Iteration 282, Norm of Gradient: 0.05084727061173455, Cost (Train): 0.44404952835376943\n",
            "Iteration 283, Norm of Gradient: 0.05073456065888481, Cost (Train): 0.4437924133350648\n",
            "Iteration 284, Norm of Gradient: 0.050622463496215235, Cost (Train): 0.4435364323313839\n",
            "Iteration 285, Norm of Gradient: 0.050510974095436284, Cost (Train): 0.443281576712648\n",
            "Iteration 286, Norm of Gradient: 0.050400087482328736, Cost (Train): 0.44302783793927364\n",
            "Iteration 287, Norm of Gradient: 0.05028979873602618, Cost (Train): 0.4427752075609802\n",
            "Iteration 288, Norm of Gradient: 0.05018010298830857, Cost (Train): 0.4425236772156166\n",
            "Iteration 289, Norm of Gradient: 0.05007099542290724, Cost (Train): 0.44227323862800594\n",
            "Iteration 290, Norm of Gradient: 0.04996247127482055, Cost (Train): 0.4420238836088088\n",
            "Iteration 291, Norm of Gradient: 0.04985452582964045, Cost (Train): 0.4417756040534041\n",
            "Iteration 292, Norm of Gradient: 0.049747154422889454, Cost (Train): 0.44152839194078786\n",
            "Iteration 293, Norm of Gradient: 0.049640352439367895, Cost (Train): 0.4412822393324877\n",
            "Iteration 294, Norm of Gradient: 0.0495341153125116, Cost (Train): 0.4410371383714967\n",
            "Iteration 295, Norm of Gradient: 0.04942843852375918, Cost (Train): 0.4407930812812212\n",
            "Iteration 296, Norm of Gradient: 0.049323317601929414, Cost (Train): 0.44055006036444666\n",
            "Iteration 297, Norm of Gradient: 0.049218748122607965, Cost (Train): 0.44030806800231864\n",
            "Iteration 298, Norm of Gradient: 0.049114725707543806, Cost (Train): 0.44006709665333965\n",
            "Iteration 299, Norm of Gradient: 0.04901124602405475, Cost (Train): 0.43982713885238145\n",
            "Iteration 300, Norm of Gradient: 0.04890830478444213, Cost (Train): 0.4395881872097125\n",
            "Iteration 301, Norm of Gradient: 0.048805897745414425, Cost (Train): 0.43935023441003995\n",
            "Iteration 302, Norm of Gradient: 0.04870402070751969, Cost (Train): 0.4391132732115668\n",
            "Iteration 303, Norm of Gradient: 0.04860266951458662, Cost (Train): 0.4388772964450628\n",
            "Iteration 304, Norm of Gradient: 0.04850184005317409, Cost (Train): 0.43864229701294954\n",
            "Iteration 305, Norm of Gradient: 0.048401528252028966, Cost (Train): 0.43840826788839976\n",
            "Iteration 306, Norm of Gradient: 0.048301730081552294, Cost (Train): 0.4381752021144501\n",
            "Iteration 307, Norm of Gradient: 0.048202441553273204, Cost (Train): 0.43794309280312665\n",
            "Iteration 308, Norm of Gradient: 0.04810365871933113, Cost (Train): 0.43771193313458423\n",
            "Iteration 309, Norm of Gradient: 0.048005377671965416, Cost (Train): 0.4374817163562585\n",
            "Iteration 310, Norm of Gradient: 0.0479075945430128, Cost (Train): 0.43725243578203027\n",
            "Iteration 311, Norm of Gradient: 0.04781030550341227, Cost (Train): 0.43702408479140287\n",
            "Iteration 312, Norm of Gradient: 0.04771350676271741, Cost (Train): 0.43679665682869134\n",
            "Iteration 313, Norm of Gradient: 0.04761719456861583, Cost (Train): 0.4365701454022237\n",
            "Iteration 314, Norm of Gradient: 0.04752136520645583, Cost (Train): 0.4363445440835539\n",
            "Iteration 315, Norm of Gradient: 0.04742601499878, Cost (Train): 0.4361198465066867\n",
            "Iteration 316, Norm of Gradient: 0.04733114030486583, Cost (Train): 0.435896046367314\n",
            "Iteration 317, Norm of Gradient: 0.047236737520272805, Cost (Train): 0.4356731374220618\n",
            "Iteration 318, Norm of Gradient: 0.04714280307639647, Cost (Train): 0.43545111348774845\n",
            "Iteration 319, Norm of Gradient: 0.04704933344002874, Cost (Train): 0.43522996844065404\n",
            "Iteration 320, Norm of Gradient: 0.046956325112924836, Cost (Train): 0.43500969621580043\n",
            "Iteration 321, Norm of Gradient: 0.046863774631376455, Cost (Train): 0.4347902908062409\n",
            "Iteration 322, Norm of Gradient: 0.04677167856579104, Cost (Train): 0.4345717462623613\n",
            "Iteration 323, Norm of Gradient: 0.04668003352027739, Cost (Train): 0.43435405669119037\n",
            "Iteration 324, Norm of Gradient: 0.04658883613223705, Cost (Train): 0.4341372162557203\n",
            "Iteration 325, Norm of Gradient: 0.04649808307196164, Cost (Train): 0.4339212191742378\n",
            "Iteration 326, Norm of Gradient: 0.04640777104223613, Cost (Train): 0.4337060597196628\n",
            "Iteration 327, Norm of Gradient: 0.04631789677794762, Cost (Train): 0.43349173221889925\n",
            "Iteration 328, Norm of Gradient: 0.04622845704569979, Cost (Train): 0.4332782310521929\n",
            "Iteration 329, Norm of Gradient: 0.04613944864343299, Cost (Train): 0.43306555065249974\n",
            "Iteration 330, Norm of Gradient: 0.04605086840004955, Cost (Train): 0.43285368550486303\n",
            "Iteration 331, Norm of Gradient: 0.04596271317504457, Cost (Train): 0.43264263014579774\n",
            "Iteration 332, Norm of Gradient: 0.04587497985814186, Cost (Train): 0.4324323791626869\n",
            "Iteration 333, Norm of Gradient: 0.04578766536893509, Cost (Train): 0.432222927193183\n",
            "Iteration 334, Norm of Gradient: 0.04570076665653407, Cost (Train): 0.43201426892461986\n",
            "Iteration 335, Norm of Gradient: 0.04561428069921582, Cost (Train): 0.4318063990934326\n",
            "Iteration 336, Norm of Gradient: 0.04552820450408082, Cost (Train): 0.43159931248458505\n",
            "Iteration 337, Norm of Gradient: 0.045442535106713866, Cost (Train): 0.4313930039310052\n",
            "Iteration 338, Norm of Gradient: 0.04535726957084973, Cost (Train): 0.4311874683130298\n",
            "Iteration 339, Norm of Gradient: 0.04527240498804363, Cost (Train): 0.4309827005578547\n",
            "Iteration 340, Norm of Gradient: 0.04518793847734608, Cost (Train): 0.43077869563899457\n",
            "Iteration 341, Norm of Gradient: 0.04510386718498241, Cost (Train): 0.4305754485757494\n",
            "Iteration 342, Norm of Gradient: 0.04502018828403672, Cost (Train): 0.43037295443267765\n",
            "Iteration 343, Norm of Gradient: 0.0449368989741401, Cost (Train): 0.4301712083190784\n",
            "Iteration 344, Norm of Gradient: 0.044853996481163325, Cost (Train): 0.4299702053884789\n",
            "Iteration 345, Norm of Gradient: 0.04477147805691367, Cost (Train): 0.4297699408381301\n",
            "Iteration 346, Norm of Gradient: 0.04468934097883592, Cost (Train): 0.4295704099085089\n",
            "Iteration 347, Norm of Gradient: 0.04460758254971752, Cost (Train): 0.42937160788282724\n",
            "Iteration 348, Norm of Gradient: 0.04452620009739768, Cost (Train): 0.4291735300865478\n",
            "Iteration 349, Norm of Gradient: 0.04444519097448058, Cost (Train): 0.4289761718869059\n",
            "Iteration 350, Norm of Gradient: 0.04436455255805241, Cost (Train): 0.42877952869243907\n",
            "Iteration 351, Norm of Gradient: 0.044284282249402146, Cost (Train): 0.4285835959525213\n",
            "Iteration 352, Norm of Gradient: 0.04420437747374637, Cost (Train): 0.4283883691569054\n",
            "Iteration 353, Norm of Gradient: 0.04412483567995765, Cost (Train): 0.42819384383527004\n",
            "Iteration 354, Norm of Gradient: 0.044045654340296565, Cost (Train): 0.42800001555677347\n",
            "Iteration 355, Norm of Gradient: 0.04396683095014745, Cost (Train): 0.4278068799296136\n",
            "Iteration 356, Norm of Gradient: 0.04388836302775764, Cost (Train): 0.42761443260059323\n",
            "Iteration 357, Norm of Gradient: 0.04381024811398019, Cost (Train): 0.42742266925469186\n",
            "Iteration 358, Norm of Gradient: 0.043732483772020096, Cost (Train): 0.42723158561464225\n",
            "Iteration 359, Norm of Gradient: 0.04365506758718387, Cost (Train): 0.42704117744051373\n",
            "Iteration 360, Norm of Gradient: 0.04357799716663247, Cost (Train): 0.4268514405293002\n",
            "Iteration 361, Norm of Gradient: 0.043501270139137516, Cost (Train): 0.4266623707145138\n",
            "Iteration 362, Norm of Gradient: 0.04342488415484072, Cost (Train): 0.4264739638657841\n",
            "Iteration 363, Norm of Gradient: 0.043348836885016576, Cost (Train): 0.4262862158884628\n",
            "Iteration 364, Norm of Gradient: 0.043273126021838075, Cost (Train): 0.42609912272323264\n",
            "Iteration 365, Norm of Gradient: 0.043197749278145604, Cost (Train): 0.42591268034572277\n",
            "Iteration 366, Norm of Gradient: 0.04312270438721883, Cost (Train): 0.4257268847661282\n",
            "Iteration 367, Norm of Gradient: 0.04304798910255167, Cost (Train): 0.4255417320288345\n",
            "Iteration 368, Norm of Gradient: 0.04297360119763006, Cost (Train): 0.4253572182120477\n",
            "Iteration 369, Norm of Gradient: 0.042899538465712754, Cost (Train): 0.42517333942742835\n",
            "Iteration 370, Norm of Gradient: 0.042825798719614956, Cost (Train): 0.42499009181973113\n",
            "Iteration 371, Norm of Gradient: 0.04275237979149474, Cost (Train): 0.42480747156644827\n",
            "Iteration 372, Norm of Gradient: 0.04267927953264227, Cost (Train): 0.4246254748774585\n",
            "Iteration 373, Norm of Gradient: 0.042606495813271764, Cost (Train): 0.4244440979946796\n",
            "Iteration 374, Norm of Gradient: 0.042534026522316096, Cost (Train): 0.42426333719172615\n",
            "Iteration 375, Norm of Gradient: 0.04246186956722412, Cost (Train): 0.4240831887735715\n",
            "Iteration 376, Norm of Gradient: 0.042390022873760605, Cost (Train): 0.4239036490762137\n",
            "Iteration 377, Norm of Gradient: 0.04231848438580861, Cost (Train): 0.4237247144663458\n",
            "Iteration 378, Norm of Gradient: 0.04224725206517459, Cost (Train): 0.42354638134103123\n",
            "Iteration 379, Norm of Gradient: 0.042176323891395855, Cost (Train): 0.42336864612738206\n",
            "Iteration 380, Norm of Gradient: 0.04210569786155059, Cost (Train): 0.423191505282242\n",
            "Iteration 381, Norm of Gradient: 0.04203537199007017, Cost (Train): 0.4230149552918732\n",
            "Iteration 382, Norm of Gradient: 0.041965344308553996, Cost (Train): 0.4228389926716478\n",
            "Iteration 383, Norm of Gradient: 0.04189561286558661, Cost (Train): 0.42266361396574176\n",
            "Iteration 384, Norm of Gradient: 0.04182617572655712, Cost (Train): 0.42248881574683417\n",
            "Iteration 385, Norm of Gradient: 0.041757030973481005, Cost (Train): 0.4223145946158093\n",
            "Iteration 386, Norm of Gradient: 0.04168817670482394, Cost (Train): 0.42214094720146306\n",
            "Iteration 387, Norm of Gradient: 0.04161961103532819, Cost (Train): 0.42196787016021253\n",
            "Iteration 388, Norm of Gradient: 0.041551332095840846, Cost (Train): 0.42179536017580993\n",
            "Iteration 389, Norm of Gradient: 0.04148333803314441, Cost (Train): 0.4216234139590589\n",
            "Iteration 390, Norm of Gradient: 0.04141562700978944, Cost (Train): 0.4214520282475355\n",
            "Iteration 391, Norm of Gradient: 0.041348197203929314, Cost (Train): 0.42128119980531253\n",
            "Iteration 392, Norm of Gradient: 0.04128104680915702, Cost (Train): 0.42111092542268613\n",
            "Iteration 393, Norm of Gradient: 0.041214174034343944, Cost (Train): 0.4209412019159077\n",
            "Iteration 394, Norm of Gradient: 0.041147577103480805, Cost (Train): 0.4207720261269169\n",
            "Iteration 395, Norm of Gradient: 0.04108125425552037, Cost (Train): 0.42060339492307974\n",
            "Iteration 396, Norm of Gradient: 0.04101520374422229, Cost (Train): 0.4204353051969291\n",
            "Iteration 397, Norm of Gradient: 0.04094942383799966, Cost (Train): 0.4202677538659087\n",
            "Iteration 398, Norm of Gradient: 0.040883912819767644, Cost (Train): 0.42010073787211943\n",
            "Iteration 399, Norm of Gradient: 0.0408186689867939, Cost (Train): 0.41993425418207014\n",
            "Iteration 400, Norm of Gradient: 0.04075369065055079, Cost (Train): 0.4197682997864304\n",
            "Iteration 401, Norm of Gradient: 0.04068897613656944, Cost (Train): 0.4196028716997863\n",
            "Iteration 402, Norm of Gradient: 0.0406245237842956, Cost (Train): 0.4194379669604002\n",
            "Iteration 403, Norm of Gradient: 0.04056033194694728, Cost (Train): 0.41927358262997205\n",
            "Iteration 404, Norm of Gradient: 0.040496398991373965, Cost (Train): 0.4191097157934048\n",
            "Iteration 405, Norm of Gradient: 0.0404327232979178, Cost (Train): 0.4189463635585709\n",
            "Iteration 406, Norm of Gradient: 0.04036930326027616, Cost (Train): 0.4187835230560848\n",
            "Iteration 407, Norm of Gradient: 0.04030613728536618, Cost (Train): 0.4186211914390743\n",
            "Iteration 408, Norm of Gradient: 0.040243223793190695, Cost (Train): 0.4184593658829575\n",
            "Iteration 409, Norm of Gradient: 0.04018056121670586, Cost (Train): 0.4182980435852212\n",
            "Iteration 410, Norm of Gradient: 0.04011814800169042, Cost (Train): 0.41813722176520207\n",
            "Iteration 411, Norm of Gradient: 0.040055982606616504, Cost (Train): 0.4179768976638705\n",
            "Iteration 412, Norm of Gradient: 0.03999406350252191, Cost (Train): 0.41781706854361766\n",
            "Iteration 413, Norm of Gradient: 0.03993238917288408, Cost (Train): 0.4176577316880436\n",
            "Iteration 414, Norm of Gradient: 0.03987095811349536, Cost (Train): 0.41749888440174954\n",
            "Iteration 415, Norm of Gradient: 0.03980976883233989, Cost (Train): 0.41734052401013116\n",
            "Iteration 416, Norm of Gradient: 0.039748819849471984, Cost (Train): 0.41718264785917536\n",
            "Iteration 417, Norm of Gradient: 0.039688109696895825, Cost (Train): 0.4170252533152593\n",
            "Iteration 418, Norm of Gradient: 0.03962763691844669, Cost (Train): 0.41686833776495097\n",
            "Iteration 419, Norm of Gradient: 0.039567400069673526, Cost (Train): 0.416711898614813\n",
            "Iteration 420, Norm of Gradient: 0.03950739771772289, Cost (Train): 0.41655593329120855\n",
            "Iteration 421, Norm of Gradient: 0.03944762844122436, Cost (Train): 0.41640043924010917\n",
            "Iteration 422, Norm of Gradient: 0.03938809083017718, Cost (Train): 0.4162454139269052\n",
            "Iteration 423, Norm of Gradient: 0.03932878348583828, Cost (Train): 0.4160908548362181\n",
            "Iteration 424, Norm of Gradient: 0.03926970502061159, Cost (Train): 0.4159367594717155\n",
            "Iteration 425, Norm of Gradient: 0.039210854057938706, Cost (Train): 0.4157831253559279\n",
            "Iteration 426, Norm of Gradient: 0.03915222923219071, Cost (Train): 0.4156299500300671\n",
            "Iteration 427, Norm of Gradient: 0.039093829188561385, Cost (Train): 0.41547723105384793\n",
            "Iteration 428, Norm of Gradient: 0.03903565258296159, Cost (Train): 0.4153249660053115\n",
            "Iteration 429, Norm of Gradient: 0.03897769808191487, Cost (Train): 0.41517315248064923\n",
            "Iteration 430, Norm of Gradient: 0.03891996436245424, Cost (Train): 0.4150217880940313\n",
            "Iteration 431, Norm of Gradient: 0.038862450112020316, Cost (Train): 0.41487087047743487\n",
            "Iteration 432, Norm of Gradient: 0.03880515402836037, Cost (Train): 0.41472039728047577\n",
            "Iteration 433, Norm of Gradient: 0.03874807481942878, Cost (Train): 0.4145703661702411\n",
            "Iteration 434, Norm of Gradient: 0.03869121120328846, Cost (Train): 0.41442077483112416\n",
            "Iteration 435, Norm of Gradient: 0.03863456190801355, Cost (Train): 0.4142716209646618\n",
            "Iteration 436, Norm of Gradient: 0.038578125671593085, Cost (Train): 0.4141229022893721\n",
            "Iteration 437, Norm of Gradient: 0.03852190124183586, Cost (Train): 0.41397461654059586\n",
            "Iteration 438, Norm of Gradient: 0.03846588737627639, Cost (Train): 0.41382676147033814\n",
            "Iteration 439, Norm of Gradient: 0.03841008284208184, Cost (Train): 0.41367933484711295\n",
            "Iteration 440, Norm of Gradient: 0.03835448641596011, Cost (Train): 0.4135323344557881\n",
            "Iteration 441, Norm of Gradient: 0.03829909688406893, Cost (Train): 0.413385758097434\n",
            "Iteration 442, Norm of Gradient: 0.038243913041925925, Cost (Train): 0.4132396035891717\n",
            "Iteration 443, Norm of Gradient: 0.03818893369431986, Cost (Train): 0.4130938687640252\n",
            "Iteration 444, Norm of Gradient: 0.038134157655222635, Cost (Train): 0.41294855147077253\n",
            "Iteration 445, Norm of Gradient: 0.03807958374770254, Cost (Train): 0.412803649573801\n",
            "Iteration 446, Norm of Gradient: 0.03802521080383824, Cost (Train): 0.41265916095296296\n",
            "Iteration 447, Norm of Gradient: 0.03797103766463397, Cost (Train): 0.4125150835034333\n",
            "Iteration 448, Norm of Gradient: 0.03791706317993538, Cost (Train): 0.4123714151355682\n",
            "Iteration 449, Norm of Gradient: 0.03786328620834666, Cost (Train): 0.41222815377476574\n",
            "Iteration 450, Norm of Gradient: 0.03780970561714832, Cost (Train): 0.412085297361329\n",
            "Iteration 451, Norm of Gradient: 0.037756320282215955, Cost (Train): 0.41194284385032826\n",
            "Iteration 452, Norm of Gradient: 0.03770312908794007, Cost (Train): 0.41180079121146757\n",
            "Iteration 453, Norm of Gradient: 0.03765013092714648, Cost (Train): 0.41165913742895055\n",
            "Iteration 454, Norm of Gradient: 0.03759732470101796, Cost (Train): 0.41151788050134885\n",
            "Iteration 455, Norm of Gradient: 0.03754470931901644, Cost (Train): 0.41137701844147156\n",
            "Iteration 456, Norm of Gradient: 0.03749228369880626, Cost (Train): 0.41123654927623665\n",
            "Iteration 457, Norm of Gradient: 0.0374400467661782, Cost (Train): 0.41109647104654296\n",
            "Iteration 458, Norm of Gradient: 0.0373879974549743, Cost (Train): 0.410956781807144\n",
            "Iteration 459, Norm of Gradient: 0.03733613470701363, Cost (Train): 0.4108174796265236\n",
            "Iteration 460, Norm of Gradient: 0.03728445747201876, Cost (Train): 0.41067856258677177\n",
            "Iteration 461, Norm of Gradient: 0.03723296470754302, Cost (Train): 0.410540028783463\n",
            "Iteration 462, Norm of Gradient: 0.037181655378898684, Cost (Train): 0.41040187632553554\n",
            "Iteration 463, Norm of Gradient: 0.03713052845908585, Cost (Train): 0.4102641033351715\n",
            "Iteration 464, Norm of Gradient: 0.037079582928722044, Cost (Train): 0.410126707947679\n",
            "Iteration 465, Norm of Gradient: 0.03702881777597268, Cost (Train): 0.4099896883113751\n",
            "Iteration 466, Norm of Gradient: 0.03697823199648225, Cost (Train): 0.40985304258747\n",
            "Iteration 467, Norm of Gradient: 0.03692782459330623, Cost (Train): 0.40971676894995274\n",
            "Iteration 468, Norm of Gradient: 0.036877594576843725, Cost (Train): 0.40958086558547785\n",
            "Iteration 469, Norm of Gradient: 0.03682754096477087, Cost (Train): 0.4094453306932535\n",
            "Iteration 470, Norm of Gradient: 0.036777662781974946, Cost (Train): 0.4093101624849307\n",
            "Iteration 471, Norm of Gradient: 0.03672795906048918, Cost (Train): 0.4091753591844933\n",
            "Iteration 472, Norm of Gradient: 0.0366784288394282, Cost (Train): 0.4090409190281498\n",
            "Iteration 473, Norm of Gradient: 0.03662907116492432, Cost (Train): 0.4089068402642262\n",
            "Iteration 474, Norm of Gradient: 0.03657988509006437, Cost (Train): 0.40877312115305936\n",
            "Iteration 475, Norm of Gradient: 0.036530869674827236, Cost (Train): 0.4086397599668923\n",
            "Iteration 476, Norm of Gradient: 0.036482023986022134, Cost (Train): 0.4085067549897699\n",
            "Iteration 477, Norm of Gradient: 0.03643334709722744, Cost (Train): 0.4083741045174366\n",
            "Iteration 478, Norm of Gradient: 0.03638483808873023, Cost (Train): 0.40824180685723394\n",
            "Iteration 479, Norm of Gradient: 0.036336496047466416, Cost (Train): 0.40810986032800023\n",
            "Iteration 480, Norm of Gradient: 0.03628832006696162, Cost (Train): 0.40797826325997083\n",
            "Iteration 481, Norm of Gradient: 0.036240309247272506, Cost (Train): 0.4078470139946796\n",
            "Iteration 482, Norm of Gradient: 0.03619246269492891, Cost (Train): 0.407716110884861\n",
            "Iteration 483, Norm of Gradient: 0.036144779522876376, Cost (Train): 0.4075855522943539\n",
            "Iteration 484, Norm of Gradient: 0.03609725885041956, Cost (Train): 0.40745533659800565\n",
            "Iteration 485, Norm of Gradient: 0.03604989980316594, Cost (Train): 0.4073254621815781\n",
            "Iteration 486, Norm of Gradient: 0.03600270151297032, Cost (Train): 0.4071959274416532\n",
            "Iteration 487, Norm of Gradient: 0.03595566311787987, Cost (Train): 0.40706673078554084\n",
            "Iteration 488, Norm of Gradient: 0.035908783762079635, Cost (Train): 0.4069378706311872\n",
            "Iteration 489, Norm of Gradient: 0.03586206259583878, Cost (Train): 0.4068093454070838\n",
            "Iteration 490, Norm of Gradient: 0.035815498775457276, Cost (Train): 0.4066811535521779\n",
            "Iteration 491, Norm of Gradient: 0.035769091463213144, Cost (Train): 0.4065532935157835\n",
            "Iteration 492, Norm of Gradient: 0.035722839827310296, Cost (Train): 0.40642576375749345\n",
            "Iteration 493, Norm of Gradient: 0.03567674304182692, Cost (Train): 0.4062985627470923\n",
            "Iteration 494, Norm of Gradient: 0.035630800286664314, Cost (Train): 0.40617168896447037\n",
            "Iteration 495, Norm of Gradient: 0.03558501074749637, Cost (Train): 0.4060451408995381\n",
            "Iteration 496, Norm of Gradient: 0.035539373615719454, Cost (Train): 0.4059189170521417\n",
            "Iteration 497, Norm of Gradient: 0.03549388808840292, Cost (Train): 0.40579301593198014\n",
            "Iteration 498, Norm of Gradient: 0.03544855336824, Cost (Train): 0.40566743605852146\n",
            "Iteration 499, Norm of Gradient: 0.03540336866349933, Cost (Train): 0.4055421759609217\n",
            "Iteration 500, Norm of Gradient: 0.03535833318797691, Cost (Train): 0.40541723417794334\n",
            "Iteration 501, Norm of Gradient: 0.035313446160948496, Cost (Train): 0.4052926092578757\n",
            "Iteration 502, Norm of Gradient: 0.03526870680712263, Cost (Train): 0.40516829975845475\n",
            "Iteration 503, Norm of Gradient: 0.035224114356593984, Cost (Train): 0.4050443042467848\n",
            "Iteration 504, Norm of Gradient: 0.03517966804479727, Cost (Train): 0.4049206212992609\n",
            "Iteration 505, Norm of Gradient: 0.03513536711246161, Cost (Train): 0.40479724950149154\n",
            "Iteration 506, Norm of Gradient: 0.035091210805565355, Cost (Train): 0.4046741874482223\n",
            "Iteration 507, Norm of Gradient: 0.03504719837529134, Cost (Train): 0.4045514337432607\n",
            "Iteration 508, Norm of Gradient: 0.035003329077982655, Cost (Train): 0.4044289869994008\n",
            "Iteration 509, Norm of Gradient: 0.03495960217509879, Cost (Train): 0.40430684583834975\n",
            "Iteration 510, Norm of Gradient: 0.034916016933172235, Cost (Train): 0.4041850088906545\n",
            "Iteration 511, Norm of Gradient: 0.03487257262376558, Cost (Train): 0.40406347479562876\n",
            "Iteration 512, Norm of Gradient: 0.034829268523428956, Cost (Train): 0.4039422422012814\n",
            "Iteration 513, Norm of Gradient: 0.03478610391365798, Cost (Train): 0.4038213097642457\n",
            "Iteration 514, Norm of Gradient: 0.03474307808085206, Cost (Train): 0.40370067614970834\n",
            "Iteration 515, Norm of Gradient: 0.034700190316273145, Cost (Train): 0.40358034003134\n",
            "Iteration 516, Norm of Gradient: 0.03465743991600489, Cost (Train): 0.40346030009122624\n",
            "Iteration 517, Norm of Gradient: 0.03461482618091223, Cost (Train): 0.4033405550197991\n",
            "Iteration 518, Norm of Gradient: 0.034572348416601326, Cost (Train): 0.4032211035157695\n",
            "Iteration 519, Norm of Gradient: 0.03453000593337993, Cost (Train): 0.40310194428606\n",
            "Iteration 520, Norm of Gradient: 0.03448779804621814, Cost (Train): 0.4029830760457387\n",
            "Iteration 521, Norm of Gradient: 0.034445724074709604, Cost (Train): 0.4028644975179534\n",
            "Iteration 522, Norm of Gradient: 0.03440378334303292, Cost (Train): 0.40274620743386647\n",
            "Iteration 523, Norm of Gradient: 0.034361975179913684, Cost (Train): 0.4026282045325902\n",
            "Iteration 524, Norm of Gradient: 0.034320298918586685, Cost (Train): 0.4025104875611236\n",
            "Iteration 525, Norm of Gradient: 0.034278753896758565, Cost (Train): 0.4023930552742884\n",
            "Iteration 526, Norm of Gradient: 0.034237339456570884, Cost (Train): 0.40227590643466715\n",
            "Iteration 527, Norm of Gradient: 0.03419605494456346, Cost (Train): 0.4021590398125406\n",
            "Iteration 528, Norm of Gradient: 0.034154899711638105, Cost (Train): 0.40204245418582674\n",
            "Iteration 529, Norm of Gradient: 0.03411387311302278, Cost (Train): 0.40192614834002\n",
            "Iteration 530, Norm of Gradient: 0.03407297450823598, Cost (Train): 0.40181012106813074\n",
            "Iteration 531, Norm of Gradient: 0.034032203261051504, Cost (Train): 0.40169437117062595\n",
            "Iteration 532, Norm of Gradient: 0.033991558739463676, Cost (Train): 0.4015788974553699\n",
            "Iteration 533, Norm of Gradient: 0.03395104031565275, Cost (Train): 0.40146369873756566\n",
            "Iteration 534, Norm of Gradient: 0.033910647365950716, Cost (Train): 0.4013487738396975\n",
            "Iteration 535, Norm of Gradient: 0.03387037927080749, Cost (Train): 0.4012341215914731\n",
            "Iteration 536, Norm of Gradient: 0.03383023541475732, Cost (Train): 0.4011197408297669\n",
            "Iteration 537, Norm of Gradient: 0.033790215186385586, Cost (Train): 0.4010056303985637\n",
            "Iteration 538, Norm of Gradient: 0.033750317978295946, Cost (Train): 0.40089178914890305\n",
            "Iteration 539, Norm of Gradient: 0.03371054318707771, Cost (Train): 0.4007782159388238\n",
            "Iteration 540, Norm of Gradient: 0.03367089021327363, Cost (Train): 0.4006649096333098\n",
            "Iteration 541, Norm of Gradient: 0.0336313584613479, Cost (Train): 0.4005518691042349\n",
            "Iteration 542, Norm of Gradient: 0.03359194733965451, Cost (Train): 0.40043909323031024\n",
            "Iteration 543, Norm of Gradient: 0.033552656260405954, Cost (Train): 0.4003265808970304\n",
            "Iteration 544, Norm of Gradient: 0.03351348463964211, Cost (Train): 0.40021433099662096\n",
            "Iteration 545, Norm of Gradient: 0.03347443189719957, Cost (Train): 0.40010234242798615\n",
            "Iteration 546, Norm of Gradient: 0.033435497456681104, Cost (Train): 0.3999906140966575\n",
            "Iteration 547, Norm of Gradient: 0.03339668074542556, Cost (Train): 0.3998791449147422\n",
            "Iteration 548, Norm of Gradient: 0.03335798119447791, Cost (Train): 0.3997679338008727\n",
            "Iteration 549, Norm of Gradient: 0.03331939823855975, Cost (Train): 0.3996569796801564\n",
            "Iteration 550, Norm of Gradient: 0.033280931316039895, Cost (Train): 0.39954628148412547\n",
            "Iteration 551, Norm of Gradient: 0.03324257986890536, Cost (Train): 0.3994358381506881\n",
            "Iteration 552, Norm of Gradient: 0.033204343342732624, Cost (Train): 0.39932564862407954\n",
            "Iteration 553, Norm of Gradient: 0.03316622118665911, Cost (Train): 0.3992157118548129\n",
            "Iteration 554, Norm of Gradient: 0.03312821285335493, Cost (Train): 0.3991060267996325\n",
            "Iteration 555, Norm of Gradient: 0.033090317798994996, Cost (Train): 0.398996592421465\n",
            "Iteration 556, Norm of Gradient: 0.03305253548323121, Cost (Train): 0.3988874076893735\n",
            "Iteration 557, Norm of Gradient: 0.033014865369165154, Cost (Train): 0.3987784715785099\n",
            "Iteration 558, Norm of Gradient: 0.03297730692332078, Cost (Train): 0.3986697830700693\n",
            "Iteration 559, Norm of Gradient: 0.03293985961561759, Cost (Train): 0.39856134115124403\n",
            "Iteration 560, Norm of Gradient: 0.03290252291934381, Cost (Train): 0.39845314481517824\n",
            "Iteration 561, Norm of Gradient: 0.03286529631113016, Cost (Train): 0.39834519306092286\n",
            "Iteration 562, Norm of Gradient: 0.032828179270923456, Cost (Train): 0.3982374848933914\n",
            "Iteration 563, Norm of Gradient: 0.03279117128196082, Cost (Train): 0.398130019323315\n",
            "Iteration 564, Norm of Gradient: 0.03275427183074391, Cost (Train): 0.39802279536719987\n",
            "Iteration 565, Norm of Gradient: 0.03271748040701348, Cost (Train): 0.39791581204728294\n",
            "Iteration 566, Norm of Gradient: 0.03268079650372408, Cost (Train): 0.3978090683914893\n",
            "Iteration 567, Norm of Gradient: 0.0326442196170192, Cost (Train): 0.3977025634333898\n",
            "Iteration 568, Norm of Gradient: 0.032607749246206345, Cost (Train): 0.39759629621215836\n",
            "Iteration 569, Norm of Gradient: 0.032571384893732624, Cost (Train): 0.3974902657725307\n",
            "Iteration 570, Norm of Gradient: 0.03253512606516034, Cost (Train): 0.3973844711647629\n",
            "Iteration 571, Norm of Gradient: 0.03249897226914297, Cost (Train): 0.3972789114445898\n",
            "Iteration 572, Norm of Gradient: 0.032462923017401245, Cost (Train): 0.39717358567318495\n",
            "Iteration 573, Norm of Gradient: 0.03242697782469955, Cost (Train): 0.39706849291712015\n",
            "Iteration 574, Norm of Gradient: 0.032391136208822405, Cost (Train): 0.3969636322483251\n",
            "Iteration 575, Norm of Gradient: 0.032355397690551346, Cost (Train): 0.39685900274404856\n",
            "Iteration 576, Norm of Gradient: 0.03231976179364184, Cost (Train): 0.39675460348681807\n",
            "Iteration 577, Norm of Gradient: 0.03228422804480051, Cost (Train): 0.3966504335644024\n",
            "Iteration 578, Norm of Gradient: 0.03224879597366254, Cost (Train): 0.3965464920697716\n",
            "Iteration 579, Norm of Gradient: 0.03221346511276931, Cost (Train): 0.39644277810106016\n",
            "Iteration 580, Norm of Gradient: 0.03217823499754613, Cost (Train): 0.3963392907615279\n",
            "Iteration 581, Norm of Gradient: 0.03214310516628035, Cost (Train): 0.3962360291595235\n",
            "Iteration 582, Norm of Gradient: 0.03210807516009951, Cost (Train): 0.3961329924084466\n",
            "Iteration 583, Norm of Gradient: 0.032073144522949786, Cost (Train): 0.3960301796267114\n",
            "Iteration 584, Norm of Gradient: 0.03203831280157456, Cost (Train): 0.3959275899377096\n",
            "Iteration 585, Norm of Gradient: 0.03200357954549325, Cost (Train): 0.395825222469775\n",
            "Iteration 586, Norm of Gradient: 0.03196894430698026, Cost (Train): 0.39572307635614673\n",
            "Iteration 587, Norm of Gradient: 0.03193440664104419, Cost (Train): 0.3956211507349344\n",
            "Iteration 588, Norm of Gradient: 0.031899966105407164, Cost (Train): 0.39551944474908235\n",
            "Iteration 589, Norm of Gradient: 0.03186562226048439, Cost (Train): 0.39541795754633485\n",
            "Iteration 590, Norm of Gradient: 0.03183137466936391, Cost (Train): 0.39531668827920174\n",
            "Iteration 591, Norm of Gradient: 0.03179722289778647, Cost (Train): 0.39521563610492355\n",
            "Iteration 592, Norm of Gradient: 0.03176316651412563, Cost (Train): 0.39511480018543793\n",
            "Iteration 593, Norm of Gradient: 0.03172920508936806, Cost (Train): 0.3950141796873457\n",
            "Iteration 594, Norm of Gradient: 0.031695338197093866, Cost (Train): 0.3949137737818772\n",
            "Iteration 595, Norm of Gradient: 0.03166156541345736, Cost (Train): 0.39481358164485975\n",
            "Iteration 596, Norm of Gradient: 0.031627886317167724, Cost (Train): 0.39471360245668413\n",
            "Iteration 597, Norm of Gradient: 0.031594300489470026, Cost (Train): 0.39461383540227213\n",
            "Iteration 598, Norm of Gradient: 0.031560807514126314, Cost (Train): 0.39451427967104474\n",
            "Iteration 599, Norm of Gradient: 0.03152740697739692, Cost (Train): 0.3944149344568894\n",
            "Iteration 600, Norm of Gradient: 0.03149409846802192, Cost (Train): 0.39431579895812885\n",
            "Iteration 601, Norm of Gradient: 0.03146088157720272, Cost (Train): 0.39421687237748915\n",
            "Iteration 602, Norm of Gradient: 0.03142775589858388, Cost (Train): 0.394118153922069\n",
            "Iteration 603, Norm of Gradient: 0.031394721028235026, Cost (Train): 0.39401964280330826\n",
            "Iteration 604, Norm of Gradient: 0.031361776564632945, Cost (Train): 0.39392133823695785\n",
            "Iteration 605, Norm of Gradient: 0.03132892210864382, Cost (Train): 0.3938232394430488\n",
            "Iteration 606, Norm of Gradient: 0.0312961572635057, Cost (Train): 0.3937253456458625\n",
            "Iteration 607, Norm of Gradient: 0.031263481634811, Cost (Train): 0.39362765607390066\n",
            "Iteration 608, Norm of Gradient: 0.031230894830489245, Cost (Train): 0.39353016995985557\n",
            "Iteration 609, Norm of Gradient: 0.031198396460789913, Cost (Train): 0.393432886540581\n",
            "Iteration 610, Norm of Gradient: 0.03116598613826547, Cost (Train): 0.39333580505706267\n",
            "Iteration 611, Norm of Gradient: 0.03113366347775453, Cost (Train): 0.39323892475438976\n",
            "Iteration 612, Norm of Gradient: 0.03110142809636514, Cost (Train): 0.3931422448817261\n",
            "Iteration 613, Norm of Gradient: 0.031069279613458206, Cost (Train): 0.39304576469228153\n",
            "Iteration 614, Norm of Gradient: 0.031037217650631197, Cost (Train): 0.3929494834432845\n",
            "Iteration 615, Norm of Gradient: 0.031005241831701728, Cost (Train): 0.392853400395953\n",
            "Iteration 616, Norm of Gradient: 0.030973351782691567, Cost (Train): 0.3927575148154681\n",
            "Iteration 617, Norm of Gradient: 0.030941547131810552, Cost (Train): 0.39266182597094557\n",
            "Iteration 618, Norm of Gradient: 0.030909827509440806, Cost (Train): 0.39256633313540895\n",
            "Iteration 619, Norm of Gradient: 0.030878192548120978, Cost (Train): 0.3924710355857629\n",
            "Iteration 620, Norm of Gradient: 0.03084664188253071, Cost (Train): 0.39237593260276593\n",
            "Iteration 621, Norm of Gradient: 0.03081517514947514, Cost (Train): 0.3922810234710042\n",
            "Iteration 622, Norm of Gradient: 0.03078379198786963, Cost (Train): 0.39218630747886496\n",
            "Iteration 623, Norm of Gradient: 0.030752492038724592, Cost (Train): 0.39209178391851063\n",
            "Iteration 624, Norm of Gradient: 0.030721274945130363, Cost (Train): 0.39199745208585296\n",
            "Iteration 625, Norm of Gradient: 0.030690140352242358, Cost (Train): 0.3919033112805271\n",
            "Iteration 626, Norm of Gradient: 0.030659087907266253, Cost (Train): 0.3918093608058663\n",
            "Iteration 627, Norm of Gradient: 0.03062811725944328, Cost (Train): 0.39171559996887684\n",
            "Iteration 628, Norm of Gradient: 0.030597228060035703, Cost (Train): 0.39162202808021285\n",
            "Iteration 629, Norm of Gradient: 0.030566419962312404, Cost (Train): 0.3915286444541512\n",
            "Iteration 630, Norm of Gradient: 0.030535692621534567, Cost (Train): 0.3914354484085674\n",
            "Iteration 631, Norm of Gradient: 0.030505045694941474, Cost (Train): 0.39134243926491097\n",
            "Iteration 632, Norm of Gradient: 0.03047447884173648, Cost (Train): 0.3912496163481809\n",
            "Iteration 633, Norm of Gradient: 0.03044399172307306, Cost (Train): 0.39115697898690216\n",
            "Iteration 634, Norm of Gradient: 0.03041358400204096, Cost (Train): 0.39106452651310175\n",
            "Iteration 635, Norm of Gradient: 0.030383255343652504, Cost (Train): 0.3909722582622846\n",
            "Iteration 636, Norm of Gradient: 0.030353005414828993, Cost (Train): 0.39088017357341087\n",
            "Iteration 637, Norm of Gradient: 0.030322833884387237, Cost (Train): 0.39078827178887215\n",
            "Iteration 638, Norm of Gradient: 0.030292740423026165, Cost (Train): 0.3906965522544688\n",
            "Iteration 639, Norm of Gradient: 0.030262724703313548, Cost (Train): 0.39060501431938666\n",
            "Iteration 640, Norm of Gradient: 0.03023278639967295, Cost (Train): 0.3905136573361748\n",
            "Iteration 641, Norm of Gradient: 0.030202925188370557, Cost (Train): 0.3904224806607229\n",
            "Iteration 642, Norm of Gradient: 0.03017314074750234, Cost (Train): 0.3903314836522388\n",
            "Iteration 643, Norm of Gradient: 0.030143432756981208, Cost (Train): 0.39024066567322635\n",
            "Iteration 644, Norm of Gradient: 0.030113800898524275, Cost (Train): 0.39015002608946386\n",
            "Iteration 645, Norm of Gradient: 0.030084244855640287, Cost (Train): 0.39005956426998206\n",
            "Iteration 646, Norm of Gradient: 0.03005476431361709, Cost (Train): 0.38996927958704214\n",
            "Iteration 647, Norm of Gradient: 0.030025358959509224, Cost (Train): 0.38987917141611506\n",
            "Iteration 648, Norm of Gradient: 0.029996028482125663, Cost (Train): 0.3897892391358599\n",
            "Iteration 649, Norm of Gradient: 0.029966772572017564, Cost (Train): 0.38969948212810257\n",
            "Iteration 650, Norm of Gradient: 0.02993759092146622, Cost (Train): 0.38960989977781557\n",
            "Iteration 651, Norm of Gradient: 0.029908483224471, Cost (Train): 0.38952049147309664\n",
            "Iteration 652, Norm of Gradient: 0.02987944917673753, Cost (Train): 0.38943125660514827\n",
            "Iteration 653, Norm of Gradient: 0.02985048847566583, Cost (Train): 0.3893421945682576\n",
            "Iteration 654, Norm of Gradient: 0.029821600820338644, Cost (Train): 0.38925330475977593\n",
            "Iteration 655, Norm of Gradient: 0.029792785911509783, Cost (Train): 0.3891645865800989\n",
            "Iteration 656, Norm of Gradient: 0.029764043451592694, Cost (Train): 0.389076039432646\n",
            "Iteration 657, Norm of Gradient: 0.02973537314464894, Cost (Train): 0.38898766272384155\n",
            "Iteration 658, Norm of Gradient: 0.02970677469637697, Cost (Train): 0.3888994558630945\n",
            "Iteration 659, Norm of Gradient: 0.029678247814100802, Cost (Train): 0.3888114182627792\n",
            "Iteration 660, Norm of Gradient: 0.02964979220675893, Cost (Train): 0.3887235493382164\n",
            "Iteration 661, Norm of Gradient: 0.029621407584893268, Cost (Train): 0.3886358485076534\n",
            "Iteration 662, Norm of Gradient: 0.02959309366063816, Cost (Train): 0.3885483151922458\n",
            "Iteration 663, Norm of Gradient: 0.02956485014770953, Cost (Train): 0.38846094881603815\n",
            "Iteration 664, Norm of Gradient: 0.029536676761394095, Cost (Train): 0.3883737488059456\n",
            "Iteration 665, Norm of Gradient: 0.02950857321853864, Cost (Train): 0.388286714591735\n",
            "Iteration 666, Norm of Gradient: 0.02948053923753944, Cost (Train): 0.3881998456060069\n",
            "Iteration 667, Norm of Gradient: 0.02945257453833173, Cost (Train): 0.38811314128417695\n",
            "Iteration 668, Norm of Gradient: 0.029424678842379214, Cost (Train): 0.3880266010644579\n",
            "Iteration 669, Norm of Gradient: 0.029396851872663776, Cost (Train): 0.38794022438784187\n",
            "Iteration 670, Norm of Gradient: 0.029369093353675164, Cost (Train): 0.3878540106980819\n",
            "Iteration 671, Norm of Gradient: 0.02934140301140078, Cost (Train): 0.38776795944167514\n",
            "Iteration 672, Norm of Gradient: 0.029313780573315643, Cost (Train): 0.3876820700678445\n",
            "Iteration 673, Norm of Gradient: 0.029286225768372275, Cost (Train): 0.3875963420285216\n",
            "Iteration 674, Norm of Gradient: 0.029258738326990788, Cost (Train): 0.38751077477832957\n",
            "Iteration 675, Norm of Gradient: 0.029231317981049023, Cost (Train): 0.3874253677745656\n",
            "Iteration 676, Norm of Gradient: 0.029203964463872748, Cost (Train): 0.38734012047718436\n",
            "Iteration 677, Norm of Gradient: 0.02917667751022594, Cost (Train): 0.38725503234878045\n",
            "Iteration 678, Norm of Gradient: 0.02914945685630115, Cost (Train): 0.38717010285457243\n",
            "Iteration 679, Norm of Gradient: 0.02912230223970994, Cost (Train): 0.3870853314623857\n",
            "Iteration 680, Norm of Gradient: 0.029095213399473407, Cost (Train): 0.3870007176426361\n",
            "Iteration 681, Norm of Gradient: 0.029068190076012794, Cost (Train): 0.38691626086831354\n",
            "Iteration 682, Norm of Gradient: 0.029041232011140086, Cost (Train): 0.386831960614966\n",
            "Iteration 683, Norm of Gradient: 0.02901433894804883, Cost (Train): 0.3867478163606829\n",
            "Iteration 684, Norm of Gradient: 0.028987510631304916, Cost (Train): 0.3866638275860796\n",
            "Iteration 685, Norm of Gradient: 0.028960746806837433, Cost (Train): 0.3865799937742814\n",
            "Iteration 686, Norm of Gradient: 0.028934047221929667, Cost (Train): 0.3864963144109077\n",
            "Iteration 687, Norm of Gradient: 0.028907411625210114, Cost (Train): 0.38641278898405623\n",
            "Iteration 688, Norm of Gradient: 0.028880839766643563, Cost (Train): 0.38632941698428797\n",
            "Iteration 689, Norm of Gradient: 0.02885433139752229, Cost (Train): 0.38624619790461123\n",
            "Iteration 690, Norm of Gradient: 0.028827886270457275, Cost (Train): 0.38616313124046703\n",
            "Iteration 691, Norm of Gradient: 0.02880150413936955, Cost (Train): 0.38608021648971336\n",
            "Iteration 692, Norm of Gradient: 0.02877518475948148, Cost (Train): 0.38599745315261025\n",
            "Iteration 693, Norm of Gradient: 0.028748927887308297, Cost (Train): 0.38591484073180515\n",
            "Iteration 694, Norm of Gradient: 0.028722733280649573, Cost (Train): 0.38583237873231785\n",
            "Iteration 695, Norm of Gradient: 0.0286966006985808, Cost (Train): 0.38575006666152556\n",
            "Iteration 696, Norm of Gradient: 0.02867052990144501, Cost (Train): 0.38566790402914913\n",
            "Iteration 697, Norm of Gradient: 0.028644520650844493, Cost (Train): 0.3855858903472375\n",
            "Iteration 698, Norm of Gradient: 0.028618572709632584, Cost (Train): 0.3855040251301542\n",
            "Iteration 699, Norm of Gradient: 0.02859268584190546, Cost (Train): 0.38542230789456267\n",
            "Iteration 700, Norm of Gradient: 0.028566859812994064, Cost (Train): 0.38534073815941194\n",
            "Iteration 701, Norm of Gradient: 0.028541094389456068, Cost (Train): 0.3852593154459233\n",
            "Iteration 702, Norm of Gradient: 0.02851538933906784, Cost (Train): 0.38517803927757527\n",
            "Iteration 703, Norm of Gradient: 0.028489744430816612, Cost (Train): 0.385096909180091\n",
            "Iteration 704, Norm of Gradient: 0.028464159434892555, Cost (Train): 0.3850159246814233\n",
            "Iteration 705, Norm of Gradient: 0.028438634122681, Cost (Train): 0.384935085311742\n",
            "Iteration 706, Norm of Gradient: 0.028413168266754725, Cost (Train): 0.38485439060341986\n",
            "Iteration 707, Norm of Gradient: 0.028387761640866246, Cost (Train): 0.3847738400910192\n",
            "Iteration 708, Norm of Gradient: 0.02836241401994022, Cost (Train): 0.3846934333112789\n",
            "Iteration 709, Norm of Gradient: 0.028337125180065866, Cost (Train): 0.38461316980310073\n",
            "Iteration 710, Norm of Gradient: 0.028311894898489516, Cost (Train): 0.38453304910753644\n",
            "Iteration 711, Norm of Gradient: 0.028286722953607076, Cost (Train): 0.3844530707677748\n",
            "Iteration 712, Norm of Gradient: 0.02826160912495673, Cost (Train): 0.38437323432912834\n",
            "Iteration 713, Norm of Gradient: 0.02823655319321156, Cost (Train): 0.38429353933902083\n",
            "Iteration 714, Norm of Gradient: 0.028211554940172287, Cost (Train): 0.3842139853469745\n",
            "Iteration 715, Norm of Gradient: 0.028186614148760034, Cost (Train): 0.3841345719045971\n",
            "Iteration 716, Norm of Gradient: 0.02816173060300919, Cost (Train): 0.38405529856556975\n",
            "Iteration 717, Norm of Gradient: 0.02813690408806027, Cost (Train): 0.3839761648856341\n",
            "Iteration 718, Norm of Gradient: 0.028112134390152872, Cost (Train): 0.38389717042258\n",
            "Iteration 719, Norm of Gradient: 0.02808742129661867, Cost (Train): 0.3838183147362334\n",
            "Iteration 720, Norm of Gradient: 0.02806276459587447, Cost (Train): 0.3837395973884442\n",
            "Iteration 721, Norm of Gradient: 0.028038164077415304, Cost (Train): 0.3836610179430739\n",
            "Iteration 722, Norm of Gradient: 0.028013619531807577, Cost (Train): 0.38358257596598344\n",
            "Iteration 723, Norm of Gradient: 0.0279891307506823, Cost (Train): 0.383504271025022\n",
            "Iteration 724, Norm of Gradient: 0.02796469752672831, Cost (Train): 0.3834261026900141\n",
            "Iteration 725, Norm of Gradient: 0.02794031965368561, Cost (Train): 0.38334807053274866\n",
            "Iteration 726, Norm of Gradient: 0.0279159969263387, Cost (Train): 0.3832701741269672\n",
            "Iteration 727, Norm of Gradient: 0.027891729140510003, Cost (Train): 0.3831924130483518\n",
            "Iteration 728, Norm of Gradient: 0.027867516093053313, Cost (Train): 0.383114786874514\n",
            "Iteration 729, Norm of Gradient: 0.027843357581847302, Cost (Train): 0.3830372951849834\n",
            "Iteration 730, Norm of Gradient: 0.02781925340578909, Cost (Train): 0.3829599375611962\n",
            "Iteration 731, Norm of Gradient: 0.027795203364787798, Cost (Train): 0.3828827135864835\n",
            "Iteration 732, Norm of Gradient: 0.027771207259758266, Cost (Train): 0.3828056228460612\n",
            "Iteration 733, Norm of Gradient: 0.027747264892614705, Cost (Train): 0.3827286649270177\n",
            "Iteration 734, Norm of Gradient: 0.02772337606626444, Cost (Train): 0.38265183941830383\n",
            "Iteration 735, Norm of Gradient: 0.02769954058460174, Cost (Train): 0.38257514591072134\n",
            "Iteration 736, Norm of Gradient: 0.027675758252501608, Cost (Train): 0.3824985839969122\n",
            "Iteration 737, Norm of Gradient: 0.027652028875813688, Cost (Train): 0.3824221532713479\n",
            "Iteration 738, Norm of Gradient: 0.027628352261356185, Cost (Train): 0.3823458533303186\n",
            "Iteration 739, Norm of Gradient: 0.02760472821690986, Cost (Train): 0.3822696837719227\n",
            "Iteration 740, Norm of Gradient: 0.027581156551212006, Cost (Train): 0.3821936441960561\n",
            "Iteration 741, Norm of Gradient: 0.02755763707395055, Cost (Train): 0.38211773420440187\n",
            "Iteration 742, Norm of Gradient: 0.027534169595758117, Cost (Train): 0.38204195340041963\n",
            "Iteration 743, Norm of Gradient: 0.027510753928206226, Cost (Train): 0.3819663013893355\n",
            "Iteration 744, Norm of Gradient: 0.027487389883799444, Cost (Train): 0.3818907777781316\n",
            "Iteration 745, Norm of Gradient: 0.02746407727596965, Cost (Train): 0.38181538217553607\n",
            "Iteration 746, Norm of Gradient: 0.02744081591907026, Cost (Train): 0.38174011419201276\n",
            "Iteration 747, Norm of Gradient: 0.027417605628370614, Cost (Train): 0.38166497343975136\n",
            "Iteration 748, Norm of Gradient: 0.027394446220050297, Cost (Train): 0.3815899595326573\n",
            "Iteration 749, Norm of Gradient: 0.027371337511193524, Cost (Train): 0.381515072086342\n",
            "Iteration 750, Norm of Gradient: 0.02734827931978364, Cost (Train): 0.3814403107181131\n",
            "Iteration 751, Norm of Gradient: 0.027325271464697536, Cost (Train): 0.3813656750469641\n",
            "Iteration 752, Norm of Gradient: 0.027302313765700203, Cost (Train): 0.3812911646935656\n",
            "Iteration 753, Norm of Gradient: 0.027279406043439335, Cost (Train): 0.38121677928025505\n",
            "Iteration 754, Norm of Gradient: 0.02725654811943983, Cost (Train): 0.3811425184310273\n",
            "Iteration 755, Norm of Gradient: 0.027233739816098557, Cost (Train): 0.38106838177152513\n",
            "Iteration 756, Norm of Gradient: 0.02721098095667892, Cost (Train): 0.38099436892902994\n",
            "Iteration 757, Norm of Gradient: 0.02718827136530566, Cost (Train): 0.3809204795324525\n",
            "Iteration 758, Norm of Gradient: 0.027165610866959566, Cost (Train): 0.38084671321232316\n",
            "Iteration 759, Norm of Gradient: 0.027142999287472297, Cost (Train): 0.3807730696007831\n",
            "Iteration 760, Norm of Gradient: 0.0271204364535212, Cost (Train): 0.380699548331575\n",
            "Iteration 761, Norm of Gradient: 0.027097922192624174, Cost (Train): 0.380626149040034\n",
            "Iteration 762, Norm of Gradient: 0.027075456333134594, Cost (Train): 0.3805528713630786\n",
            "Iteration 763, Norm of Gradient: 0.027053038704236268, Cost (Train): 0.3804797149392015\n",
            "Iteration 764, Norm of Gradient: 0.027030669135938358, Cost (Train): 0.3804066794084614\n",
            "Iteration 765, Norm of Gradient: 0.027008347459070457, Cost (Train): 0.3803337644124732\n",
            "Iteration 766, Norm of Gradient: 0.026986073505277607, Cost (Train): 0.38026096959439976\n",
            "Iteration 767, Norm of Gradient: 0.026963847107015434, Cost (Train): 0.3801882945989433\n",
            "Iteration 768, Norm of Gradient: 0.026941668097545175, Cost (Train): 0.38011573907233626\n",
            "Iteration 769, Norm of Gradient: 0.026919536310928936, Cost (Train): 0.38004330266233316\n",
            "Iteration 770, Norm of Gradient: 0.02689745158202483, Cost (Train): 0.3799709850182016\n",
            "Iteration 771, Norm of Gradient: 0.026875413746482224, Cost (Train): 0.37989878579071423\n",
            "Iteration 772, Norm of Gradient: 0.026853422640736972, Cost (Train): 0.3798267046321401\n",
            "Iteration 773, Norm of Gradient: 0.02683147810200675, Cost (Train): 0.3797547411962362\n",
            "Iteration 774, Norm of Gradient: 0.02680957996828634, Cost (Train): 0.3796828951382392\n",
            "Iteration 775, Norm of Gradient: 0.026787728078343034, Cost (Train): 0.37961116611485746\n",
            "Iteration 776, Norm of Gradient: 0.02676592227171199, Cost (Train): 0.37953955378426235\n",
            "Iteration 777, Norm of Gradient: 0.026744162388691674, Cost (Train): 0.37946805780608045\n",
            "Iteration 778, Norm of Gradient: 0.026722448270339334, Cost (Train): 0.37939667784138553\n",
            "Iteration 779, Norm of Gradient: 0.026700779758466454, Cost (Train): 0.37932541355269006\n",
            "Iteration 780, Norm of Gradient: 0.026679156695634313, Cost (Train): 0.37925426460393774\n",
            "Iteration 781, Norm of Gradient: 0.02665757892514952, Cost (Train): 0.3791832306604952\n",
            "Iteration 782, Norm of Gradient: 0.026636046291059607, Cost (Train): 0.3791123113891445\n",
            "Iteration 783, Norm of Gradient: 0.026614558638148624, Cost (Train): 0.37904150645807494\n",
            "Iteration 784, Norm of Gradient: 0.026593115811932827, Cost (Train): 0.3789708155368754\n",
            "Iteration 785, Norm of Gradient: 0.02657171765865632, Cost (Train): 0.3789002382965268\n",
            "Iteration 786, Norm of Gradient: 0.026550364025286783, Cost (Train): 0.3788297744093943\n",
            "Iteration 787, Norm of Gradient: 0.02652905475951121, Cost (Train): 0.37875942354921965\n",
            "Iteration 788, Norm of Gradient: 0.026507789709731656, Cost (Train): 0.3786891853911136\n",
            "Iteration 789, Norm of Gradient: 0.02648656872506107, Cost (Train): 0.3786190596115489\n",
            "Iteration 790, Norm of Gradient: 0.02646539165531907, Cost (Train): 0.37854904588835186\n",
            "Iteration 791, Norm of Gradient: 0.02644425835102788, Cost (Train): 0.3784791439006958\n",
            "Iteration 792, Norm of Gradient: 0.02642316866340812, Cost (Train): 0.37840935332909337\n",
            "Iteration 793, Norm of Gradient: 0.026402122444374797, Cost (Train): 0.37833967385538925\n",
            "Iteration 794, Norm of Gradient: 0.026381119546533213, Cost (Train): 0.37827010516275295\n",
            "Iteration 795, Norm of Gradient: 0.02636015982317492, Cost (Train): 0.37820064693567146\n",
            "Iteration 796, Norm of Gradient: 0.02633924312827378, Cost (Train): 0.3781312988599423\n",
            "Iteration 797, Norm of Gradient: 0.0263183693164819, Cost (Train): 0.3780620606226662\n",
            "Iteration 798, Norm of Gradient: 0.02629753824312579, Cost (Train): 0.3779929319122401\n",
            "Iteration 799, Norm of Gradient: 0.026276749764202318, Cost (Train): 0.37792391241835027\n",
            "Iteration 800, Norm of Gradient: 0.026256003736374975, Cost (Train): 0.3778550018319652\n",
            "Iteration 801, Norm of Gradient: 0.026235300016969852, Cost (Train): 0.3777861998453285\n",
            "Iteration 802, Norm of Gradient: 0.026214638463971932, Cost (Train): 0.37771750615195243\n",
            "Iteration 803, Norm of Gradient: 0.026194018936021177, Cost (Train): 0.3776489204466107\n",
            "Iteration 804, Norm of Gradient: 0.02617344129240881, Cost (Train): 0.3775804424253319\n",
            "Iteration 805, Norm of Gradient: 0.02615290539307352, Cost (Train): 0.37751207178539264\n",
            "Iteration 806, Norm of Gradient: 0.026132411098597763, Cost (Train): 0.3774438082253109\n",
            "Iteration 807, Norm of Gradient: 0.026111958270203992, Cost (Train): 0.37737565144483926\n",
            "Iteration 808, Norm of Gradient: 0.02609154676975106, Cost (Train): 0.3773076011449585\n",
            "Iteration 809, Norm of Gradient: 0.02607117645973048, Cost (Train): 0.3772396570278707\n",
            "Iteration 810, Norm of Gradient: 0.026050847203262843, Cost (Train): 0.3771718187969932\n",
            "Iteration 811, Norm of Gradient: 0.026030558864094198, Cost (Train): 0.3771040861569516\n",
            "Iteration 812, Norm of Gradient: 0.02601031130659246, Cost (Train): 0.3770364588135738\n",
            "Iteration 813, Norm of Gradient: 0.025990104395743854, Cost (Train): 0.3769689364738829\n",
            "Iteration 814, Norm of Gradient: 0.025969937997149417, Cost (Train): 0.37690151884609185\n",
            "Iteration 815, Norm of Gradient: 0.025949811977021398, Cost (Train): 0.3768342056395961\n",
            "Iteration 816, Norm of Gradient: 0.02592972620217989, Cost (Train): 0.3767669965649681\n",
            "Iteration 817, Norm of Gradient: 0.025909680540049267, Cost (Train): 0.3766998913339505\n",
            "Iteration 818, Norm of Gradient: 0.0258896748586548, Cost (Train): 0.37663288965945046\n",
            "Iteration 819, Norm of Gradient: 0.02586970902661922, Cost (Train): 0.37656599125553314\n",
            "Iteration 820, Norm of Gradient: 0.025849782913159324, Cost (Train): 0.37649919583741576\n",
            "Iteration 821, Norm of Gradient: 0.025829896388082633, Cost (Train): 0.37643250312146126\n",
            "Iteration 822, Norm of Gradient: 0.025810049321783997, Cost (Train): 0.3763659128251728\n",
            "Iteration 823, Norm of Gradient: 0.02579024158524232, Cost (Train): 0.376299424667187\n",
            "Iteration 824, Norm of Gradient: 0.025770473050017218, Cost (Train): 0.37623303836726885\n",
            "Iteration 825, Norm of Gradient: 0.02575074358824576, Cost (Train): 0.3761667536463051\n",
            "Iteration 826, Norm of Gradient: 0.02573105307263921, Cost (Train): 0.37610057022629856\n",
            "Iteration 827, Norm of Gradient: 0.02571140137647977, Cost (Train): 0.3760344878303626\n",
            "Iteration 828, Norm of Gradient: 0.025691788373617405, Cost (Train): 0.3759685061827148\n",
            "Iteration 829, Norm of Gradient: 0.02567221393846662, Cost (Train): 0.37590262500867155\n",
            "Iteration 830, Norm of Gradient: 0.025652677946003295, Cost (Train): 0.3758368440346423\n",
            "Iteration 831, Norm of Gradient: 0.02563318027176152, Cost (Train): 0.3757711629881237\n",
            "Iteration 832, Norm of Gradient: 0.02561372079183051, Cost (Train): 0.37570558159769385\n",
            "Iteration 833, Norm of Gradient: 0.025594299382851446, Cost (Train): 0.37564009959300726\n",
            "Iteration 834, Norm of Gradient: 0.02557491592201442, Cost (Train): 0.37557471670478854\n",
            "Iteration 835, Norm of Gradient: 0.025555570287055362, Cost (Train): 0.3755094326648271\n",
            "Iteration 836, Norm of Gradient: 0.02553626235625298, Cost (Train): 0.37544424720597214\n",
            "Iteration 837, Norm of Gradient: 0.02551699200842574, Cost (Train): 0.3753791600621262\n",
            "Iteration 838, Norm of Gradient: 0.025497759122928888, Cost (Train): 0.37531417096824043\n",
            "Iteration 839, Norm of Gradient: 0.025478563579651406, Cost (Train): 0.37524927966030913\n",
            "Iteration 840, Norm of Gradient: 0.02545940525901311, Cost (Train): 0.3751844858753642\n",
            "Iteration 841, Norm of Gradient: 0.025440284041961643, Cost (Train): 0.37511978935146956\n",
            "Iteration 842, Norm of Gradient: 0.025421199809969597, Cost (Train): 0.3750551898277164\n",
            "Iteration 843, Norm of Gradient: 0.02540215244503157, Cost (Train): 0.3749906870442175\n",
            "Iteration 844, Norm of Gradient: 0.02538314182966131, Cost (Train): 0.37492628074210216\n",
            "Iteration 845, Norm of Gradient: 0.025364167846888813, Cost (Train): 0.3748619706635108\n",
            "Iteration 846, Norm of Gradient: 0.02534523038025748, Cost (Train): 0.37479775655159003\n",
            "Iteration 847, Norm of Gradient: 0.025326329313821318, Cost (Train): 0.3747336381504874\n",
            "Iteration 848, Norm of Gradient: 0.02530746453214207, Cost (Train): 0.37466961520534614\n",
            "Iteration 849, Norm of Gradient: 0.025288635920286483, Cost (Train): 0.3746056874623004\n",
            "Iteration 850, Norm of Gradient: 0.025269843363823454, Cost (Train): 0.37454185466846984\n",
            "Iteration 851, Norm of Gradient: 0.02525108674882135, Cost (Train): 0.37447811657195496\n",
            "Iteration 852, Norm of Gradient: 0.025232365961845235, Cost (Train): 0.3744144729218318\n",
            "Iteration 853, Norm of Gradient: 0.025213680889954116, Cost (Train): 0.3743509234681473\n",
            "Iteration 854, Norm of Gradient: 0.025195031420698313, Cost (Train): 0.3742874679619141\n",
            "Iteration 855, Norm of Gradient: 0.02517641744211668, Cost (Train): 0.3742241061551056\n",
            "Iteration 856, Norm of Gradient: 0.02515783884273402, Cost (Train): 0.37416083780065174\n",
            "Iteration 857, Norm of Gradient: 0.02513929551155837, Cost (Train): 0.3740976626524332\n",
            "Iteration 858, Norm of Gradient: 0.02512078733807842, Cost (Train): 0.37403458046527716\n",
            "Iteration 859, Norm of Gradient: 0.025102314212260834, Cost (Train): 0.37397159099495286\n",
            "Iteration 860, Norm of Gradient: 0.02508387602454771, Cost (Train): 0.37390869399816573\n",
            "Iteration 861, Norm of Gradient: 0.025065472665853968, Cost (Train): 0.3738458892325542\n",
            "Iteration 862, Norm of Gradient: 0.025047104027564753, Cost (Train): 0.3737831764566836\n",
            "Iteration 863, Norm of Gradient: 0.025028770001532953, Cost (Train): 0.37372055543004246\n",
            "Iteration 864, Norm of Gradient: 0.025010470480076614, Cost (Train): 0.3736580259130373\n",
            "Iteration 865, Norm of Gradient: 0.024992205355976422, Cost (Train): 0.3735955876669885\n",
            "Iteration 866, Norm of Gradient: 0.024973974522473238, Cost (Train): 0.3735332404541254\n",
            "Iteration 867, Norm of Gradient: 0.024955777873265573, Cost (Train): 0.3734709840375817\n",
            "Iteration 868, Norm of Gradient: 0.024937615302507148, Cost (Train): 0.3734088181813915\n",
            "Iteration 869, Norm of Gradient: 0.02491948670480442, Cost (Train): 0.3733467426504841\n",
            "Iteration 870, Norm of Gradient: 0.02490139197521416, Cost (Train): 0.37328475721068\n",
            "Iteration 871, Norm of Gradient: 0.02488333100924102, Cost (Train): 0.37322286162868623\n",
            "Iteration 872, Norm of Gradient: 0.024865303702835125, Cost (Train): 0.3731610556720922\n",
            "Iteration 873, Norm of Gradient: 0.024847309952389703, Cost (Train): 0.373099339109365\n",
            "Iteration 874, Norm of Gradient: 0.024829349654738687, Cost (Train): 0.37303771170984534\n",
            "Iteration 875, Norm of Gradient: 0.02481142270715436, Cost (Train): 0.3729761732437432\n",
            "Iteration 876, Norm of Gradient: 0.024793529007345, Cost (Train): 0.3729147234821333\n",
            "Iteration 877, Norm of Gradient: 0.024775668453452603, Cost (Train): 0.3728533621969508\n",
            "Iteration 878, Norm of Gradient: 0.02475784094405047, Cost (Train): 0.3727920891609877\n",
            "Iteration 879, Norm of Gradient: 0.024740046378141, Cost (Train): 0.3727309041478877\n",
            "Iteration 880, Norm of Gradient: 0.02472228465515336, Cost (Train): 0.37266980693214263\n",
            "Iteration 881, Norm of Gradient: 0.024704555674941196, Cost (Train): 0.3726087972890882\n",
            "Iteration 882, Norm of Gradient: 0.024686859337780434, Cost (Train): 0.37254787499489966\n",
            "Iteration 883, Norm of Gradient: 0.024669195544366977, Cost (Train): 0.37248703982658765\n",
            "Iteration 884, Norm of Gradient: 0.024651564195814518, Cost (Train): 0.37242629156199447\n",
            "Iteration 885, Norm of Gradient: 0.024633965193652293, Cost (Train): 0.3723656299797896\n",
            "Iteration 886, Norm of Gradient: 0.02461639843982291, Cost (Train): 0.3723050548594661\n",
            "Iteration 887, Norm of Gradient: 0.024598863836680137, Cost (Train): 0.3722445659813358\n",
            "Iteration 888, Norm of Gradient: 0.02458136128698676, Cost (Train): 0.3721841631265266\n",
            "Iteration 889, Norm of Gradient: 0.024563890693912387, Cost (Train): 0.3721238460769769\n",
            "Iteration 890, Norm of Gradient: 0.024546451961031328, Cost (Train): 0.3720636146154331\n",
            "Iteration 891, Norm of Gradient: 0.024529044992320448, Cost (Train): 0.3720034685254447\n",
            "Iteration 892, Norm of Gradient: 0.02451166969215708, Cost (Train): 0.37194340759136096\n",
            "Iteration 893, Norm of Gradient: 0.02449432596531687, Cost (Train): 0.3718834315983266\n",
            "Iteration 894, Norm of Gradient: 0.02447701371697172, Cost (Train): 0.3718235403322785\n",
            "Iteration 895, Norm of Gradient: 0.024459732852687707, Cost (Train): 0.37176373357994114\n",
            "Iteration 896, Norm of Gradient: 0.024442483278422997, Cost (Train): 0.37170401112882356\n",
            "Iteration 897, Norm of Gradient: 0.024425264900525816, Cost (Train): 0.37164437276721474\n",
            "Iteration 898, Norm of Gradient: 0.024408077625732404, Cost (Train): 0.3715848182841807\n",
            "Iteration 899, Norm of Gradient: 0.024390921361164955, Cost (Train): 0.37152534746956023\n",
            "Iteration 900, Norm of Gradient: 0.02437379601432967, Cost (Train): 0.37146596011396116\n",
            "Iteration 901, Norm of Gradient: 0.024356701493114686, Cost (Train): 0.3714066560087569\n",
            "Iteration 902, Norm of Gradient: 0.024339637705788127, Cost (Train): 0.37134743494608263\n",
            "Iteration 903, Norm of Gradient: 0.024322604560996112, Cost (Train): 0.37128829671883185\n",
            "Iteration 904, Norm of Gradient: 0.02430560196776079, Cost (Train): 0.3712292411206522\n",
            "Iteration 905, Norm of Gradient: 0.0242886298354784, Cost (Train): 0.37117026794594254\n",
            "Iteration 906, Norm of Gradient: 0.02427168807391731, Cost (Train): 0.3711113769898489\n",
            "Iteration 907, Norm of Gradient: 0.024254776593216103, Cost (Train): 0.37105256804826114\n",
            "Iteration 908, Norm of Gradient: 0.02423789530388166, Cost (Train): 0.37099384091780924\n",
            "Iteration 909, Norm of Gradient: 0.024221044116787244, Cost (Train): 0.37093519539585984\n",
            "Iteration 910, Norm of Gradient: 0.024204222943170615, Cost (Train): 0.3708766312805128\n",
            "Iteration 911, Norm of Gradient: 0.024187431694632156, Cost (Train): 0.37081814837059773\n",
            "Iteration 912, Norm of Gradient: 0.024170670283132984, Cost (Train): 0.3707597464656703\n",
            "Iteration 913, Norm of Gradient: 0.024153938620993118, Cost (Train): 0.3707014253660091\n",
            "Iteration 914, Norm of Gradient: 0.024137236620889593, Cost (Train): 0.3706431848726118\n",
            "Iteration 915, Norm of Gradient: 0.024120564195854673, Cost (Train): 0.3705850247871923\n",
            "Iteration 916, Norm of Gradient: 0.02410392125927398, Cost (Train): 0.370526944912177\n",
            "Iteration 917, Norm of Gradient: 0.024087307724884724, Cost (Train): 0.37046894505070155\n",
            "Iteration 918, Norm of Gradient: 0.02407072350677387, Cost (Train): 0.37041102500660716\n",
            "Iteration 919, Norm of Gradient: 0.02405416851937635, Cost (Train): 0.3703531845844379\n",
            "Iteration 920, Norm of Gradient: 0.024037642677473305, Cost (Train): 0.3702954235894368\n",
            "Iteration 921, Norm of Gradient: 0.024021145896190282, Cost (Train): 0.370237741827543\n",
            "Iteration 922, Norm of Gradient: 0.024004678090995532, Cost (Train): 0.3701801391053883\n",
            "Iteration 923, Norm of Gradient: 0.023988239177698174, Cost (Train): 0.3701226152302937\n",
            "Iteration 924, Norm of Gradient: 0.023971829072446552, Cost (Train): 0.3700651700102666\n",
            "Iteration 925, Norm of Gradient: 0.02395544769172646, Cost (Train): 0.37000780325399724\n",
            "Iteration 926, Norm of Gradient: 0.023939094952359408, Cost (Train): 0.36995051477085583\n",
            "Iteration 927, Norm of Gradient: 0.023922770771500963, Cost (Train): 0.3698933043708889\n",
            "Iteration 928, Norm of Gradient: 0.023906475066639028, Cost (Train): 0.3698361718648166\n",
            "Iteration 929, Norm of Gradient: 0.023890207755592172, Cost (Train): 0.36977911706402944\n",
            "Iteration 930, Norm of Gradient: 0.023873968756507928, Cost (Train): 0.3697221397805849\n",
            "Iteration 931, Norm of Gradient: 0.023857757987861147, Cost (Train): 0.3696652398272048\n",
            "Iteration 932, Norm of Gradient: 0.023841575368452363, Cost (Train): 0.36960841701727165\n",
            "Iteration 933, Norm of Gradient: 0.023825420817406098, Cost (Train): 0.3695516711648263\n",
            "Iteration 934, Norm of Gradient: 0.0238092942541693, Cost (Train): 0.3694950020845641\n",
            "Iteration 935, Norm of Gradient: 0.023793195598509627, Cost (Train): 0.36943840959183244\n",
            "Iteration 936, Norm of Gradient: 0.023777124770513927, Cost (Train): 0.3693818935026275\n",
            "Iteration 937, Norm of Gradient: 0.023761081690586563, Cost (Train): 0.3693254536335913\n",
            "Iteration 938, Norm of Gradient: 0.02374506627944785, Cost (Train): 0.3692690898020087\n",
            "Iteration 939, Norm of Gradient: 0.023729078458132453, Cost (Train): 0.36921280182580457\n",
            "Iteration 940, Norm of Gradient: 0.023713118147987843, Cost (Train): 0.3691565895235405\n",
            "Iteration 941, Norm of Gradient: 0.023697185270672685, Cost (Train): 0.36910045271441233\n",
            "Iteration 942, Norm of Gradient: 0.023681279748155315, Cost (Train): 0.3690443912182468\n",
            "Iteration 943, Norm of Gradient: 0.02366540150271218, Cost (Train): 0.3689884048554989\n",
            "Iteration 944, Norm of Gradient: 0.023649550456926283, Cost (Train): 0.36893249344724915\n",
            "Iteration 945, Norm of Gradient: 0.023633726533685694, Cost (Train): 0.3688766568152003\n",
            "Iteration 946, Norm of Gradient: 0.023617929656181993, Cost (Train): 0.3688208947816748\n",
            "Iteration 947, Norm of Gradient: 0.023602159747908757, Cost (Train): 0.36876520716961186\n",
            "Iteration 948, Norm of Gradient: 0.023586416732660116, Cost (Train): 0.3687095938025645\n",
            "Iteration 949, Norm of Gradient: 0.023570700534529154, Cost (Train): 0.36865405450469746\n",
            "Iteration 950, Norm of Gradient: 0.02355501107790655, Cost (Train): 0.3685985891007831\n",
            "Iteration 951, Norm of Gradient: 0.023539348287478983, Cost (Train): 0.36854319741619984\n",
            "Iteration 952, Norm of Gradient: 0.023523712088227747, Cost (Train): 0.368487879276929\n",
            "Iteration 953, Norm of Gradient: 0.023508102405427268, Cost (Train): 0.36843263450955177\n",
            "Iteration 954, Norm of Gradient: 0.023492519164643635, Cost (Train): 0.3683774629412469\n",
            "Iteration 955, Norm of Gradient: 0.023476962291733208, Cost (Train): 0.36832236439978777\n",
            "Iteration 956, Norm of Gradient: 0.023461431712841107, Cost (Train): 0.36826733871353984\n",
            "Iteration 957, Norm of Gradient: 0.02344592735439988, Cost (Train): 0.3682123857114577\n",
            "Iteration 958, Norm of Gradient: 0.02343044914312802, Cost (Train): 0.3681575052230829\n",
            "Iteration 959, Norm of Gradient: 0.023414997006028577, Cost (Train): 0.36810269707854065\n",
            "Iteration 960, Norm of Gradient: 0.023399570870387774, Cost (Train): 0.3680479611085378\n",
            "Iteration 961, Norm of Gradient: 0.0233841706637736, Cost (Train): 0.36799329714435997\n",
            "Iteration 962, Norm of Gradient: 0.023368796314034424, Cost (Train): 0.3679387050178687\n",
            "Iteration 963, Norm of Gradient: 0.023353447749297637, Cost (Train): 0.3678841845614993\n",
            "Iteration 964, Norm of Gradient: 0.02333812489796827, Cost (Train): 0.3678297356082581\n",
            "Iteration 965, Norm of Gradient: 0.023322827688727656, Cost (Train): 0.3677753579917197\n",
            "Iteration 966, Norm of Gradient: 0.02330755605053206, Cost (Train): 0.3677210515460247\n",
            "Iteration 967, Norm of Gradient: 0.023292309912611317, Cost (Train): 0.36766681610587704\n",
            "Iteration 968, Norm of Gradient: 0.023277089204467565, Cost (Train): 0.3676126515065414\n",
            "Iteration 969, Norm of Gradient: 0.02326189385587385, Cost (Train): 0.36755855758384093\n",
            "Iteration 970, Norm of Gradient: 0.023246723796872846, Cost (Train): 0.3675045341741546\n",
            "Iteration 971, Norm of Gradient: 0.02323157895777552, Cost (Train): 0.3674505811144147\n",
            "Iteration 972, Norm of Gradient: 0.023216459269159835, Cost (Train): 0.36739669824210447\n",
            "Iteration 973, Norm of Gradient: 0.023201364661869485, Cost (Train): 0.3673428853952556\n",
            "Iteration 974, Norm of Gradient: 0.02318629506701254, Cost (Train): 0.36728914241244565\n",
            "Iteration 975, Norm of Gradient: 0.023171250415960246, Cost (Train): 0.367235469132796\n",
            "Iteration 976, Norm of Gradient: 0.02315623064034567, Cost (Train): 0.3671818653959692\n",
            "Iteration 977, Norm of Gradient: 0.023141235672062492, Cost (Train): 0.3671283310421663\n",
            "Iteration 978, Norm of Gradient: 0.02312626544326375, Cost (Train): 0.3670748659121251\n",
            "Iteration 979, Norm of Gradient: 0.0231113198863605, Cost (Train): 0.3670214698471173\n",
            "Iteration 980, Norm of Gradient: 0.023096398934020716, Cost (Train): 0.3669681426889463\n",
            "Iteration 981, Norm of Gradient: 0.02308150251916792, Cost (Train): 0.3669148842799446\n",
            "Iteration 982, Norm of Gradient: 0.023066630574980016, Cost (Train): 0.36686169446297223\n",
            "Iteration 983, Norm of Gradient: 0.023051783034888078, Cost (Train): 0.3668085730814134\n",
            "Iteration 984, Norm of Gradient: 0.023036959832575088, Cost (Train): 0.36675551997917494\n",
            "Iteration 985, Norm of Gradient: 0.023022160901974766, Cost (Train): 0.3667025350006836\n",
            "Iteration 986, Norm of Gradient: 0.02300738617727034, Cost (Train): 0.36664961799088425\n",
            "Iteration 987, Norm of Gradient: 0.02299263559289338, Cost (Train): 0.36659676879523695\n",
            "Iteration 988, Norm of Gradient: 0.022977909083522596, Cost (Train): 0.3665439872597152\n",
            "Iteration 989, Norm of Gradient: 0.02296320658408264, Cost (Train): 0.3664912732308035\n",
            "Iteration 990, Norm of Gradient: 0.02294852802974294, Cost (Train): 0.36643862655549525\n",
            "Iteration 991, Norm of Gradient: 0.022933873355916568, Cost (Train): 0.36638604708129024\n",
            "Iteration 992, Norm of Gradient: 0.022919242498259034, Cost (Train): 0.3663335346561927\n",
            "Iteration 993, Norm of Gradient: 0.02290463539266713, Cost (Train): 0.36628108912870927\n",
            "Iteration 994, Norm of Gradient: 0.022890051975277836, Cost (Train): 0.3662287103478461\n",
            "Iteration 995, Norm of Gradient: 0.022875492182467105, Cost (Train): 0.36617639816310765\n",
            "Iteration 996, Norm of Gradient: 0.022860955950848787, Cost (Train): 0.3661241524244937\n",
            "Iteration 997, Norm of Gradient: 0.022846443217273468, Cost (Train): 0.3660719729824978\n",
            "Iteration 998, Norm of Gradient: 0.02283195391882735, Cost (Train): 0.3660198596881044\n",
            "Iteration 999, Norm of Gradient: 0.02281748799283117, Cost (Train): 0.36596781239278786\n",
            "Terminated after 1000 iterations, with norm of the gradient equal to 0.02281748799283117\n",
            "The weight found: [-0.01581484 -0.02260578  0.00555916 ... -0.20458525 -0.03817849\n",
            "  0.04789373]\n",
            "x_train shape after bias term: (25000, 1877)\n",
            "self.w shape: (1877,)\n",
            "Iteration 0, Norm of Gradient: 0.18085673335543803, Cost (Train): 0.6899036929210999\n",
            "Iteration 1, Norm of Gradient: 0.178004490629001, Cost (Train): 0.6867560996566342\n",
            "Iteration 2, Norm of Gradient: 0.17575297490588979, Cost (Train): 0.6836844361706759\n",
            "Iteration 3, Norm of Gradient: 0.17385004038780102, Cost (Train): 0.6806771139796537\n",
            "Iteration 4, Norm of Gradient: 0.1721516355914082, Cost (Train): 0.6777272285707464\n",
            "Iteration 5, Norm of Gradient: 0.17057600599413705, Cost (Train): 0.6748304961806547\n",
            "Iteration 6, Norm of Gradient: 0.1690769243238518, Cost (Train): 0.6719841018963146\n",
            "Iteration 7, Norm of Gradient: 0.16762830079326466, Cost (Train): 0.6691860569079092\n",
            "Iteration 8, Norm of Gradient: 0.1662154183786112, Cost (Train): 0.6664348400050392\n",
            "Iteration 9, Norm of Gradient: 0.16482996784187928, Cost (Train): 0.6637291976818338\n",
            "Iteration 10, Norm of Gradient: 0.16346724391581294, Cost (Train): 0.6610680327161036\n",
            "Iteration 11, Norm of Gradient: 0.162124564879174, Cost (Train): 0.6584503420846851\n",
            "Iteration 12, Norm of Gradient: 0.16080038279224798, Cost (Train): 0.6558751823801449\n",
            "Iteration 13, Norm of Gradient: 0.15949378302436945, Cost (Train): 0.6533416505497402\n",
            "Iteration 14, Norm of Gradient: 0.1582042029967064, Cost (Train): 0.6508488731646427\n",
            "Iteration 15, Norm of Gradient: 0.15693127429218096, Cost (Train): 0.648396000432525\n",
            "Iteration 16, Norm of Gradient: 0.15567473416184543, Cost (Train): 0.6459822028427099\n",
            "Iteration 17, Norm of Gradient: 0.15443437605336505, Cost (Train): 0.6436066692678323\n",
            "Iteration 18, Norm of Gradient: 0.15321002207282228, Cost (Train): 0.6412686058671896\n",
            "Iteration 19, Norm of Gradient: 0.15200150776777183, Cost (Train): 0.6389672354275754\n",
            "Iteration 20, Norm of Gradient: 0.15080867382587143, Cost (Train): 0.6367017969393902\n",
            "Iteration 21, Norm of Gradient: 0.14963136164945975, Cost (Train): 0.634471545296102\n",
            "Iteration 22, Norm of Gradient: 0.14846941109714223, Cost (Train): 0.6322757510554334\n",
            "Iteration 23, Norm of Gradient: 0.1473226594317827, Cost (Train): 0.630113700228641\n",
            "Iteration 24, Norm of Gradient: 0.1461909409351066, Cost (Train): 0.6279846940798247\n",
            "Iteration 25, Norm of Gradient: 0.14507408688575038, Cost (Train): 0.6258880489258398\n",
            "Iteration 26, Norm of Gradient: 0.14397192573064815, Cost (Train): 0.6238230959321589\n",
            "Iteration 27, Norm of Gradient: 0.14288428335446354, Cost (Train): 0.6217891809026559\n",
            "Iteration 28, Norm of Gradient: 0.1418109833938457, Cost (Train): 0.619785664062705\n",
            "Iteration 29, Norm of Gradient: 0.14075184756694567, Cost (Train): 0.6178119198357476\n",
            "Iteration 30, Norm of Gradient: 0.13970669600193075, Cost (Train): 0.6158673366138685\n",
            "Iteration 31, Norm of Gradient: 0.13867534755571753, Cost (Train): 0.6139513165231046\n",
            "Iteration 32, Norm of Gradient: 0.13765762011834834, Cost (Train): 0.6120632751842787\n",
            "Iteration 33, Norm of Gradient: 0.13665333090080106, Cost (Train): 0.610202641470163\n",
            "Iteration 34, Norm of Gradient: 0.13566229670535218, Cost (Train): 0.6083688572597534\n",
            "Iteration 35, Norm of Gradient: 0.13468433417835246, Cost (Train): 0.6065613771903915\n",
            "Iteration 36, Norm of Gradient: 0.13371926004569643, Cost (Train): 0.6047796684084289\n",
            "Iteration 37, Norm of Gradient: 0.13276689133148972, Cost (Train): 0.603023210319074\n",
            "Iteration 38, Norm of Gradient: 0.13182704556054492, Cost (Train): 0.6012914943360113\n",
            "Iteration 39, Norm of Gradient: 0.13089954094539888, Cost (Train): 0.5995840236313345\n",
            "Iteration 40, Norm of Gradient: 0.1299841965585719, Cost (Train): 0.5979003128862831\n",
            "Iteration 41, Norm of Gradient: 0.1290808324907976, Cost (Train): 0.5962398880432314\n",
            "Iteration 42, Norm of Gradient: 0.12818926999594688, Cost (Train): 0.5946022860593332\n",
            "Iteration 43, Norm of Gradient: 0.12730933162335759, Cost (Train): 0.5929870546621824\n",
            "Iteration 44, Norm of Gradient: 0.12644084133826664, Cost (Train): 0.5913937521078185\n",
            "Iteration 45, Norm of Gradient: 0.12558362463101708, Cost (Train): 0.5898219469413643\n",
            "Iteration 46, Norm of Gradient: 0.12473750861569699, Cost (Train): 0.5882712177605502\n",
            "Iteration 47, Norm of Gradient: 0.12390232211883764, Cost (Train): 0.5867411529823573\n",
            "Iteration 48, Norm of Gradient: 0.12307789575878036, Cost (Train): 0.5852313506129689\n",
            "Iteration 49, Norm of Gradient: 0.12226406201629189, Cost (Train): 0.5837414180212077\n",
            "Iteration 50, Norm of Gradient: 0.12146065529698791, Cost (Train): 0.5822709717156012\n",
            "Iteration 51, Norm of Gradient: 0.12066751198609528, Cost (Train): 0.580819637125202\n",
            "Iteration 52, Norm of Gradient: 0.11988447049606403, Cost (Train): 0.579387048384266\n",
            "Iteration 53, Norm of Gradient: 0.11911137130750987, Cost (Train): 0.5779728481208712\n",
            "Iteration 54, Norm of Gradient: 0.11834805700395008, Cost (Train): 0.5765766872495481\n",
            "Iteration 55, Norm of Gradient: 0.11759437230076765, Cost (Train): 0.57519822476797\n",
            "Iteration 56, Norm of Gradient: 0.1168501640688181, Cost (Train): 0.5738371275577413\n",
            "Iteration 57, Norm of Gradient: 0.11611528135307063, Cost (Train): 0.572493070189308\n",
            "Iteration 58, Norm of Gradient: 0.1153895753866531, Cost (Train): 0.571165734731002\n",
            "Iteration 59, Norm of Gradient: 0.11467289960065116, Cost (Train): 0.5698548105622204\n",
            "Iteration 60, Norm of Gradient: 0.11396510962998958, Cost (Train): 0.5685599941907309\n",
            "Iteration 61, Norm of Gradient: 0.113266063315707, Cost (Train): 0.5672809890740868\n",
            "Iteration 62, Norm of Gradient: 0.11257562070391439, Cost (Train): 0.5660175054451259\n",
            "Iteration 63, Norm of Gradient: 0.11189364404171247, Cost (Train): 0.5647692601415216\n",
            "Iteration 64, Norm of Gradient: 0.11121999777032396, Cost (Train): 0.5635359764393473\n",
            "Iteration 65, Norm of Gradient: 0.11055454851568264, Cost (Train): 0.5623173838906107\n",
            "Iteration 66, Norm of Gradient: 0.10989716507670384, Cost (Train): 0.5611132181647099\n",
            "Iteration 67, Norm of Gradient: 0.10924771841144838, Cost (Train): 0.5599232208937582\n",
            "Iteration 68, Norm of Gradient: 0.10860608162137665, Cost (Train): 0.5587471395217193\n",
            "Iteration 69, Norm of Gradient: 0.10797212993387736, Cost (Train): 0.5575847271572976\n",
            "Iteration 70, Norm of Gradient: 0.10734574068324279, Cost (Train): 0.5564357424305161\n",
            "Iteration 71, Norm of Gradient: 0.10672679329025031, Cost (Train): 0.5552999493529215\n",
            "Iteration 72, Norm of Gradient: 0.10611516924049956, Cost (Train): 0.5541771171813492\n",
            "Iteration 73, Norm of Gradient: 0.10551075206164444, Cost (Train): 0.5530670202851763\n",
            "Iteration 74, Norm of Gradient: 0.10491342729964727, Cost (Train): 0.5519694380170023\n",
            "Iteration 75, Norm of Gradient: 0.10432308249417614, Cost (Train): 0.5508841545866773\n",
            "Iteration 76, Norm of Gradient: 0.1037396071532559, Cost (Train): 0.5498109589386142\n",
            "Iteration 77, Norm of Gradient: 0.10316289272727515, Cost (Train): 0.5487496446323132\n",
            "Iteration 78, Norm of Gradient: 0.10259283258244409, Cost (Train): 0.5477000097260234\n",
            "Iteration 79, Norm of Gradient: 0.10202932197379248, Cost (Train): 0.5466618566634753\n",
            "Iteration 80, Norm of Gradient: 0.10147225801778696, Cost (Train): 0.5456349921636101\n",
            "Iteration 81, Norm of Gradient: 0.10092153966464325, Cost (Train): 0.544619227113237\n",
            "Iteration 82, Norm of Gradient: 0.10037706767040253, Cost (Train): 0.5436143764625485\n",
            "Iteration 83, Norm of Gradient: 0.09983874456883489, Cost (Train): 0.5426202591234223\n",
            "Iteration 84, Norm of Gradient: 0.09930647464322727, Cost (Train): 0.5416366978704441\n",
            "Iteration 85, Norm of Gradient: 0.09878016389811096, Cost (Train): 0.5406635192445817\n",
            "Iteration 86, Norm of Gradient: 0.09825972003097547, Cost (Train): 0.5397005534594426\n",
            "Iteration 87, Norm of Gradient: 0.09774505240401485, Cost (Train): 0.5387476343100521\n",
            "Iteration 88, Norm of Gradient: 0.09723607201594608, Cost (Train): 0.5378045990840838\n",
            "Iteration 89, Norm of Gradient: 0.09673269147393758, Cost (Train): 0.5368712884754807\n",
            "Iteration 90, Norm of Gradient: 0.0962348249656804, Cost (Train): 0.5359475465004032\n",
            "Iteration 91, Norm of Gradient: 0.09574238823163318, Cost (Train): 0.5350332204154432\n",
            "Iteration 92, Norm of Gradient: 0.09525529853746854, Cost (Train): 0.5341281606380427\n",
            "Iteration 93, Norm of Gradient: 0.09477347464674485, Cost (Train): 0.5332322206690591\n",
            "Iteration 94, Norm of Gradient: 0.09429683679382661, Cost (Train): 0.5323452570174192\n",
            "Iteration 95, Norm of Gradient: 0.09382530665707292, Cost (Train): 0.5314671291268036\n",
            "Iteration 96, Norm of Gradient: 0.09335880733231113, Cost (Train): 0.5305976993043091\n",
            "Iteration 97, Norm of Gradient: 0.092897263306613, Cost (Train): 0.5297368326510301\n",
            "Iteration 98, Norm of Gradient: 0.09244060043238524, Cost (Train): 0.5288843969945126\n",
            "Iteration 99, Norm of Gradient: 0.09198874590178796, Cost (Train): 0.5280402628230239\n",
            "Iteration 100, Norm of Gradient: 0.09154162822149108, Cost (Train): 0.527204303221589\n",
            "Iteration 101, Norm of Gradient: 0.0910991771877773, Cost (Train): 0.5263763938097459\n",
            "Iteration 102, Norm of Gradient: 0.09066132386200056, Cost (Train): 0.5255564126809706\n",
            "Iteration 103, Norm of Gradient: 0.09022800054640444, Cost (Train): 0.524744240343726\n",
            "Iteration 104, Norm of Gradient: 0.08979914076030807, Cost (Train): 0.5239397596640871\n",
            "Iteration 105, Norm of Gradient: 0.08937467921666165, Cost (Train): 0.5231428558099008\n",
            "Iteration 106, Norm of Gradient: 0.08895455179897643, Cost (Train): 0.5223534161964368\n",
            "Iteration 107, Norm of Gradient: 0.08853869553863063, Cost (Train): 0.5215713304334845\n",
            "Iteration 108, Norm of Gradient: 0.08812704859255303, Cost (Train): 0.5207964902738597\n",
            "Iteration 109, Norm of Gradient: 0.08771955022128503, Cost (Train): 0.5200287895632778\n",
            "Iteration 110, Norm of Gradient: 0.08731614076742139, Cost (Train): 0.5192681241915559\n",
            "Iteration 111, Norm of Gradient: 0.0869167616344292, Cost (Train): 0.5185143920451067\n",
            "Iteration 112, Norm of Gradient: 0.08652135526584324, Cost (Train): 0.5177674929606868\n",
            "Iteration 113, Norm of Gradient: 0.08612986512483763, Cost (Train): 0.5170273286803647\n",
            "Iteration 114, Norm of Gradient: 0.08574223567417007, Cost (Train): 0.5162938028076731\n",
            "Iteration 115, Norm of Gradient: 0.08535841235649745, Cost (Train): 0.5155668207649117\n",
            "Iteration 116, Norm of Gradient: 0.08497834157505907, Cost (Train): 0.5148462897515699\n",
            "Iteration 117, Norm of Gradient: 0.08460197067472502, Cost (Train): 0.5141321187038348\n",
            "Iteration 118, Norm of Gradient: 0.08422924792340566, Cost (Train): 0.5134242182551558\n",
            "Iteration 119, Norm of Gradient: 0.08386012249381875, Cost (Train): 0.5127225006978355\n",
            "Iteration 120, Norm of Gradient: 0.08349454444561004, Cost (Train): 0.5120268799456164\n",
            "Iteration 121, Norm of Gradient: 0.08313246470782326, Cost (Train): 0.5113372714972378\n",
            "Iteration 122, Norm of Gradient: 0.0827738350617149, Cost (Train): 0.510653592400933\n",
            "Iteration 123, Norm of Gradient: 0.08241860812390922, Cost (Train): 0.509975761219842\n",
            "Iteration 124, Norm of Gradient: 0.08206673732988935, Cost (Train): 0.5093036979983125\n",
            "Iteration 125, Norm of Gradient: 0.0817181769178184, Cost (Train): 0.508637324229065\n",
            "Iteration 126, Norm of Gradient: 0.08137288191268717, Cost (Train): 0.5079765628211982\n",
            "Iteration 127, Norm of Gradient: 0.08103080811078218, Cost (Train): 0.5073213380690089\n",
            "Iteration 128, Norm of Gradient: 0.08069191206446959, Cost (Train): 0.5066715756216061\n",
            "Iteration 129, Norm of Gradient: 0.08035615106728985, Cost (Train): 0.5060272024532954\n",
            "Iteration 130, Norm of Gradient: 0.08002348313935755, Cost (Train): 0.5053881468347124\n",
            "Iteration 131, Norm of Gradient: 0.07969386701306172, Cost (Train): 0.504754338304685\n",
            "Iteration 132, Norm of Gradient: 0.07936726211906085, Cost (Train): 0.5041257076428023\n",
            "Iteration 133, Norm of Gradient: 0.07904362857256801, Cost (Train): 0.5035021868426731\n",
            "Iteration 134, Norm of Gradient: 0.07872292715992027, Cost (Train): 0.5028837090858524\n",
            "Iteration 135, Norm of Gradient: 0.07840511932542747, Cost (Train): 0.5022702087164185\n",
            "Iteration 136, Norm of Gradient: 0.07809016715849539, Cost (Train): 0.5016616212161823\n",
            "Iteration 137, Norm of Gradient: 0.07777803338101746, Cost (Train): 0.5010578831805116\n",
            "Iteration 138, Norm of Gradient: 0.07746868133503072, Cost (Train): 0.5004589322947538\n",
            "Iteration 139, Norm of Gradient: 0.07716207497063025, Cost (Train): 0.4998647073112377\n",
            "Iteration 140, Norm of Gradient: 0.07685817883413758, Cost (Train): 0.49927514802684453\n",
            "Iteration 141, Norm of Gradient: 0.07655695805651723, Cost (Train): 0.4986901952611243\n",
            "Iteration 142, Norm of Gradient: 0.07625837834203773, Cost (Train): 0.4981097908349505\n",
            "Iteration 143, Norm of Gradient: 0.0759624059571709, Cost (Train): 0.4975338775496922\n",
            "Iteration 144, Norm of Gradient: 0.07566900771972529, Cost (Train): 0.49696239916689466\n",
            "Iteration 145, Norm of Gradient: 0.07537815098820903, Cost (Train): 0.4963953003884494\n",
            "Iteration 146, Norm of Gradient: 0.07508980365141678, Cost (Train): 0.4958325268372445\n",
            "Iteration 147, Norm of Gradient: 0.07480393411823699, Cost (Train): 0.49527402503828105\n",
            "Iteration 148, Norm of Gradient: 0.07452051130767405, Cost (Train): 0.494719742400241\n",
            "Iteration 149, Norm of Gradient: 0.07423950463908141, Cost (Train): 0.49416962719749813\n",
            "Iteration 150, Norm of Gradient: 0.07396088402260106, Cost (Train): 0.49362362855255565\n",
            "Iteration 151, Norm of Gradient: 0.07368461984980475, Cost (Train): 0.4930816964189036\n",
            "Iteration 152, Norm of Gradient: 0.07341068298453303, Cost (Train): 0.492543781564281\n",
            "Iteration 153, Norm of Gradient: 0.0731390447539278, Cost (Train): 0.492009835554334\n",
            "Iteration 154, Norm of Gradient: 0.07286967693965389, Cost (Train): 0.49147981073665903\n",
            "Iteration 155, Norm of Gradient: 0.07260255176930605, Cost (Train): 0.49095366022522074\n",
            "Iteration 156, Norm of Gradient: 0.0723376419079971, Cost (Train): 0.4904313378851337\n",
            "Iteration 157, Norm of Gradient: 0.07207492045012329, Cost (Train): 0.4899127983178003\n",
            "Iteration 158, Norm of Gradient: 0.07181436091130322, Cost (Train): 0.48939799684639446\n",
            "Iteration 159, Norm of Gradient: 0.07155593722048625, Cost (Train): 0.48888688950168013\n",
            "Iteration 160, Norm of Gradient: 0.07129962371222695, Cost (Train): 0.48837943300816\n",
            "Iteration 161, Norm of Gradient: 0.0710453951191217, Cost (Train): 0.48787558477054144\n",
            "Iteration 162, Norm of Gradient: 0.07079322656440427, Cost (Train): 0.4873753028605142\n",
            "Iteration 163, Norm of Gradient: 0.07054309355469647, Cost (Train): 0.4868785460038307\n",
            "Iteration 164, Norm of Gradient: 0.07029497197291054, Cost (Train): 0.4863852735676807\n",
            "Iteration 165, Norm of Gradient: 0.07004883807130029, Cost (Train): 0.485895445548354\n",
            "Iteration 166, Norm of Gradient: 0.06980466846465709, Cost (Train): 0.48540902255918233\n",
            "Iteration 167, Norm of Gradient: 0.06956244012364811, Cost (Train): 0.48492596581875363\n",
            "Iteration 168, Norm of Gradient: 0.0693221303682933, Cost (Train): 0.48444623713939244\n",
            "Iteration 169, Norm of Gradient: 0.06908371686157797, Cost (Train): 0.48396979891589903\n",
            "Iteration 170, Norm of Gradient: 0.06884717760319835, Cost (Train): 0.48349661411453987\n",
            "Iteration 171, Norm of Gradient: 0.06861249092343687, Cost (Train): 0.48302664626228475\n",
            "Iteration 172, Norm of Gradient: 0.06837963547716414, Cost (Train): 0.482559859436283\n",
            "Iteration 173, Norm of Gradient: 0.0681485902379653, Cost (Train): 0.4820962182535726\n",
            "Iteration 174, Norm of Gradient: 0.06791933449238743, Cost (Train): 0.48163568786101757\n",
            "Iteration 175, Norm of Gradient: 0.06769184783430596, Cost (Train): 0.4811782339254667\n",
            "Iteration 176, Norm of Gradient: 0.0674661101594065, Cost (Train): 0.4807238226241285\n",
            "Iteration 177, Norm of Gradient: 0.06724210165978059, Cost (Train): 0.4802724206351563\n",
            "Iteration 178, Norm of Gradient: 0.06701980281863205, Cost (Train): 0.47982399512844043\n",
            "Iteration 179, Norm of Gradient: 0.06679919440509183, Cost (Train): 0.47937851375659846\n",
            "Iteration 180, Norm of Gradient: 0.06658025746913881, Cost (Train): 0.4789359446461631\n",
            "Iteration 181, Norm of Gradient: 0.0663629733366242, Cost (Train): 0.47849625638895893\n",
            "Iteration 182, Norm of Gradient: 0.06614732360439751, Cost (Train): 0.47805941803366564\n",
            "Iteration 183, Norm of Gradient: 0.06593329013553126, Cost (Train): 0.47762539907756235\n",
            "Iteration 184, Norm of Gradient: 0.0657208550546427, Cost (Train): 0.4771941694584489\n",
            "Iteration 185, Norm of Gradient: 0.06551000074331051, Cost (Train): 0.47676569954673964\n",
            "Iteration 186, Norm of Gradient: 0.06530070983558367, Cost (Train): 0.47633996013772456\n",
            "Iteration 187, Norm of Gradient: 0.06509296521358134, Cost (Train): 0.47591692244399536\n",
            "Iteration 188, Norm of Gradient: 0.06488675000318088, Cost (Train): 0.47549655808803226\n",
            "Iteration 189, Norm of Gradient: 0.06468204756979294, Cost (Train): 0.4750788390949453\n",
            "Iteration 190, Norm of Gradient: 0.06447884151422091, Cost (Train): 0.4746637378853694\n",
            "Iteration 191, Norm of Gradient: 0.06427711566860321, Cost (Train): 0.474251227268508\n",
            "Iteration 192, Norm of Gradient: 0.06407685409243666, Cost (Train): 0.4738412804353218\n",
            "Iteration 193, Norm of Gradient: 0.06387804106867914, Cost (Train): 0.4734338709518592\n",
            "Iteration 194, Norm of Gradient: 0.06368066109992956, Cost (Train): 0.4730289727527251\n",
            "Iteration 195, Norm of Gradient: 0.06348469890468343, Cost (Train): 0.47262656013468457\n",
            "Iteration 196, Norm of Gradient: 0.06329013941366307, Cost (Train): 0.47222660775039915\n",
            "Iteration 197, Norm of Gradient: 0.06309696776621958, Cost (Train): 0.47182909060229095\n",
            "Iteration 198, Norm of Gradient: 0.0629051693068063, Cost (Train): 0.4714339840365328\n",
            "Iteration 199, Norm of Gradient: 0.06271472958152109, Cost (Train): 0.47104126373716126\n",
            "Iteration 200, Norm of Gradient: 0.06252563433471679, Cost (Train): 0.4706509057203091\n",
            "Iteration 201, Norm of Gradient: 0.062337869505677906, Cost (Train): 0.4702628863285556\n",
            "Iteration 202, Norm of Gradient: 0.06215142122536188, Cost (Train): 0.4698771822253897\n",
            "Iteration 203, Norm of Gradient: 0.061966275813204366, Cost (Train): 0.46949377038978674\n",
            "Iteration 204, Norm of Gradient: 0.06178241977398616, Cost (Train): 0.4691126281108925\n",
            "Iteration 205, Norm of Gradient: 0.06159983979476094, Cost (Train): 0.46873373298281457\n",
            "Iteration 206, Norm of Gradient: 0.06141852274184263, Cost (Train): 0.4683570628995181\n",
            "Iteration 207, Norm of Gradient: 0.061238455657850695, Cost (Train): 0.4679825960498225\n",
            "Iteration 208, Norm of Gradient: 0.06105962575881234, Cost (Train): 0.4676103109124979\n",
            "Iteration 209, Norm of Gradient: 0.0608820204313205, Cost (Train): 0.4672401862514584\n",
            "Iteration 210, Norm of Gradient: 0.06070562722974611, Cost (Train): 0.4668722011110506\n",
            "Iteration 211, Norm of Gradient: 0.06053043387350374, Cost (Train): 0.4665063348114347\n",
            "Iteration 212, Norm of Gradient: 0.06035642824436939, Cost (Train): 0.4661425669440558\n",
            "Iteration 213, Norm of Gradient: 0.06018359838384923, Cost (Train): 0.465780877367204\n",
            "Iteration 214, Norm of Gradient: 0.06001193249059832, Cost (Train): 0.4654212462016616\n",
            "Iteration 215, Norm of Gradient: 0.05984141891788826, Cost (Train): 0.46506365382643367\n",
            "Iteration 216, Norm of Gradient: 0.05967204617112249, Cost (Train): 0.4647080808745624\n",
            "Iteration 217, Norm of Gradient: 0.05950380290539859, Cost (Train): 0.464354508229022\n",
            "Iteration 218, Norm of Gradient: 0.05933667792311619, Cost (Train): 0.46400291701869284\n",
            "Iteration 219, Norm of Gradient: 0.0591706601716299, Cost (Train): 0.4636532886144116\n",
            "Iteration 220, Norm of Gradient: 0.059005738740946015, Cost (Train): 0.4633056046250995\n",
            "Iteration 221, Norm of Gradient: 0.05884190286146224, Cost (Train): 0.46295984689396197\n",
            "Iteration 222, Norm of Gradient: 0.0586791419017495, Cost (Train): 0.46261599749476184\n",
            "Iteration 223, Norm of Gradient: 0.058517445366374786, Cost (Train): 0.4622740387281632\n",
            "Iteration 224, Norm of Gradient: 0.05835680289376449, Cost (Train): 0.46193395311814434\n",
            "Iteration 225, Norm of Gradient: 0.05819720425410703, Cost (Train): 0.4615957234084774\n",
            "Iteration 226, Norm of Gradient: 0.05803863934729433, Cost (Train): 0.461259332559276\n",
            "Iteration 227, Norm of Gradient: 0.05788109820090088, Cost (Train): 0.4609247637436063\n",
            "Iteration 228, Norm of Gradient: 0.05772457096820005, Cost (Train): 0.46059200034416103\n",
            "Iteration 229, Norm of Gradient: 0.05756904792621655, Cost (Train): 0.46026102594999685\n",
            "Iteration 230, Norm of Gradient: 0.05741451947381446, Cost (Train): 0.4599318243533313\n",
            "Iteration 231, Norm of Gradient: 0.05726097612982006, Cost (Train): 0.45960437954639943\n",
            "Iteration 232, Norm of Gradient: 0.057108408531178746, Cost (Train): 0.4592786757183679\n",
            "Iteration 233, Norm of Gradient: 0.05695680743114517, Cost (Train): 0.4589546972523075\n",
            "Iteration 234, Norm of Gradient: 0.056806163697506316, Cost (Train): 0.45863242872221877\n",
            "Iteration 235, Norm of Gradient: 0.0566564683108363, Cost (Train): 0.45831185489011456\n",
            "Iteration 236, Norm of Gradient: 0.05650771236278273, Cost (Train): 0.45799296070315415\n",
            "Iteration 237, Norm of Gradient: 0.05635988705438369, Cost (Train): 0.45767573129083\n",
            "Iteration 238, Norm of Gradient: 0.056212983694414805, Cost (Train): 0.4573601519622061\n",
            "Iteration 239, Norm of Gradient: 0.05606699369776584, Cost (Train): 0.4570462082032051\n",
            "Iteration 240, Norm of Gradient: 0.055921908583846075, Cost (Train): 0.4567338856739458\n",
            "Iteration 241, Norm of Gradient: 0.05577771997501808, Cost (Train): 0.4564231702061269\n",
            "Iteration 242, Norm of Gradient: 0.0556344195950591, Cost (Train): 0.4561140478004597\n",
            "Iteration 243, Norm of Gradient: 0.05549199926764968, Cost (Train): 0.45580650462414474\n",
            "Iteration 244, Norm of Gradient: 0.05535045091488886, Cost (Train): 0.4555005270083941\n",
            "Iteration 245, Norm of Gradient: 0.055209766555835434, Cost (Train): 0.4551961014459988\n",
            "Iteration 246, Norm of Gradient: 0.055069938305074784, Cost (Train): 0.4548932145889375\n",
            "Iteration 247, Norm of Gradient: 0.054930958371310776, Cost (Train): 0.45459185324602913\n",
            "Iteration 248, Norm of Gradient: 0.05479281905598215, Cost (Train): 0.45429200438062567\n",
            "Iteration 249, Norm of Gradient: 0.05465551275190293, Cost (Train): 0.4539936551083462\n",
            "Iteration 250, Norm of Gradient: 0.0545190319419266, Cost (Train): 0.45369679269485047\n",
            "Iteration 251, Norm of Gradient: 0.054383369197633105, Cost (Train): 0.45340140455365124\n",
            "Iteration 252, Norm of Gradient: 0.05424851717803873, Cost (Train): 0.453107478243965\n",
            "Iteration 253, Norm of Gradient: 0.05411446862832804, Cost (Train): 0.4528150014685999\n",
            "Iteration 254, Norm of Gradient: 0.05398121637860769, Cost (Train): 0.45252396207188\n",
            "Iteration 255, Norm of Gradient: 0.05384875334268151, Cost (Train): 0.4522343480376061\n",
            "Iteration 256, Norm of Gradient: 0.053717072516846494, Cost (Train): 0.451946147487051\n",
            "Iteration 257, Norm of Gradient: 0.053586166978709354, Cost (Train): 0.4516593486769896\n",
            "Iteration 258, Norm of Gradient: 0.05345602988602315, Cost (Train): 0.45137393999776265\n",
            "Iteration 259, Norm of Gradient: 0.05332665447554367, Cost (Train): 0.45108990997137355\n",
            "Iteration 260, Norm of Gradient: 0.053198034061905035, Cost (Train): 0.4508072472496174\n",
            "Iteration 261, Norm of Gradient: 0.05307016203651441, Cost (Train): 0.45052594061224277\n",
            "Iteration 262, Norm of Gradient: 0.05294303186646526, Cost (Train): 0.45024597896514285\n",
            "Iteration 263, Norm of Gradient: 0.052816637093468795, Cost (Train): 0.449967351338579\n",
            "Iteration 264, Norm of Gradient: 0.052690971332803274, Cost (Train): 0.44969004688543357\n",
            "Iteration 265, Norm of Gradient: 0.05256602827228098, Cost (Train): 0.4494140548794911\n",
            "Iteration 266, Norm of Gradient: 0.0524418016712323, Cost (Train): 0.4491393647137505\n",
            "Iteration 267, Norm of Gradient: 0.05231828535950667, Cost (Train): 0.44886596589876254\n",
            "Iteration 268, Norm of Gradient: 0.05219547323649006, Cost (Train): 0.4485938480609981\n",
            "Iteration 269, Norm of Gradient: 0.05207335927013881, Cost (Train): 0.44832300094124156\n",
            "Iteration 270, Norm of Gradient: 0.05195193749602932, Cost (Train): 0.4480534143930111\n",
            "Iteration 271, Norm of Gradient: 0.05183120201642324, Cost (Train): 0.44778507838100573\n",
            "Iteration 272, Norm of Gradient: 0.05171114699934826, Cost (Train): 0.4475179829795775\n",
            "Iteration 273, Norm of Gradient: 0.05159176667769373, Cost (Train): 0.4472521183712289\n",
            "Iteration 274, Norm of Gradient: 0.05147305534832106, Cost (Train): 0.446987474845135\n",
            "Iteration 275, Norm of Gradient: 0.05135500737118871, Cost (Train): 0.4467240427956892\n",
            "Iteration 276, Norm of Gradient: 0.05123761716849129, Cost (Train): 0.4464618127210741\n",
            "Iteration 277, Norm of Gradient: 0.05112087922381267, Cost (Train): 0.4462007752218537\n",
            "Iteration 278, Norm of Gradient: 0.05100478808129267, Cost (Train): 0.4459409209995904\n",
            "Iteration 279, Norm of Gradient: 0.05088933834480734, Cost (Train): 0.44568224085548236\n",
            "Iteration 280, Norm of Gradient: 0.05077452467716225, Cost (Train): 0.44542472568902486\n",
            "Iteration 281, Norm of Gradient: 0.05066034179929888, Cost (Train): 0.44516836649669184\n",
            "Iteration 282, Norm of Gradient: 0.05054678448951353, Cost (Train): 0.4449131543706387\n",
            "Iteration 283, Norm of Gradient: 0.050433847582688866, Cost (Train): 0.4446590804974271\n",
            "Iteration 284, Norm of Gradient: 0.05032152596953761, Cost (Train): 0.4444061361567681\n",
            "Iteration 285, Norm of Gradient: 0.05020981459585815, Cost (Train): 0.4441543127202872\n",
            "Iteration 286, Norm of Gradient: 0.05009870846180223, Cost (Train): 0.4439036016503081\n",
            "Iteration 287, Norm of Gradient: 0.049988202621153804, Cost (Train): 0.44365399449865567\n",
            "Iteration 288, Norm of Gradient: 0.04987829218061964, Cost (Train): 0.4434054829054782\n",
            "Iteration 289, Norm of Gradient: 0.049768972299130854, Cost (Train): 0.4431580585980876\n",
            "Iteration 290, Norm of Gradient: 0.049660238187155446, Cost (Train): 0.442911713389819\n",
            "Iteration 291, Norm of Gradient: 0.04955208510602157, Cost (Train): 0.44266643917890697\n",
            "Iteration 292, Norm of Gradient: 0.04944450836725149, Cost (Train): 0.44242222794737995\n",
            "Iteration 293, Norm of Gradient: 0.049337503331905765, Cost (Train): 0.4421790717599719\n",
            "Iteration 294, Norm of Gradient: 0.0492310654099376, Cost (Train): 0.4419369627630509\n",
            "Iteration 295, Norm of Gradient: 0.049125190059557435, Cost (Train): 0.44169589318356367\n",
            "Iteration 296, Norm of Gradient: 0.04901987278660698, Cost (Train): 0.44145585532799797\n",
            "Iteration 297, Norm of Gradient: 0.04891510914394333, Cost (Train): 0.4412168415813595\n",
            "Iteration 298, Norm of Gradient: 0.04881089473083207, Cost (Train): 0.4409788444061649\n",
            "Iteration 299, Norm of Gradient: 0.048707225192350274, Cost (Train): 0.4407418563414514\n",
            "Iteration 300, Norm of Gradient: 0.048604096218797986, Cost (Train): 0.4405058700017996\n",
            "Iteration 301, Norm of Gradient: 0.04850150354511933, Cost (Train): 0.44027087807637305\n",
            "Iteration 302, Norm of Gradient: 0.048399442950331883, Cost (Train): 0.44003687332797126\n",
            "Iteration 303, Norm of Gradient: 0.04829791025696515, Cost (Train): 0.4398038485920981\n",
            "Iteration 304, Norm of Gradient: 0.048196901330507184, Cost (Train): 0.43957179677604297\n",
            "Iteration 305, Norm of Gradient: 0.04809641207885996, Cost (Train): 0.43934071085797777\n",
            "Iteration 306, Norm of Gradient: 0.047996438451802605, Cost (Train): 0.439110583886065\n",
            "Iteration 307, Norm of Gradient: 0.047896976440462924, Cost (Train): 0.4388814089775823\n",
            "Iteration 308, Norm of Gradient: 0.04779802207679683, Cost (Train): 0.43865317931805736\n",
            "Iteration 309, Norm of Gradient: 0.04769957143307547, Cost (Train): 0.43842588816041733\n",
            "Iteration 310, Norm of Gradient: 0.04760162062138014, Cost (Train): 0.43819952882415075\n",
            "Iteration 311, Norm of Gradient: 0.04750416579310467, Cost (Train): 0.4379740946944817\n",
            "Iteration 312, Norm of Gradient: 0.0474072031384652, Cost (Train): 0.4377495792215561\n",
            "Iteration 313, Norm of Gradient: 0.04731072888601719, Cost (Train): 0.4375259759196408\n",
            "Iteration 314, Norm of Gradient: 0.04721473930217981, Cost (Train): 0.4373032783663335\n",
            "Iteration 315, Norm of Gradient: 0.04711923069076692, Cost (Train): 0.43708148020178533\n",
            "Iteration 316, Norm of Gradient: 0.04702419939252546, Cost (Train): 0.43686057512793375\n",
            "Iteration 317, Norm of Gradient: 0.046929641784680155, Cost (Train): 0.4366405569077482\n",
            "Iteration 318, Norm of Gradient: 0.04683555428048534, Cost (Train): 0.4364214193644857\n",
            "Iteration 319, Norm of Gradient: 0.04674193332878293, Cost (Train): 0.4362031563809569\n",
            "Iteration 320, Norm of Gradient: 0.04664877541356712, Cost (Train): 0.4359857618988048\n",
            "Iteration 321, Norm of Gradient: 0.046556077053555356, Cost (Train): 0.43576922991779155\n",
            "Iteration 322, Norm of Gradient: 0.04646383480176543, Cost (Train): 0.43555355449509714\n",
            "Iteration 323, Norm of Gradient: 0.04637204524509885, Cost (Train): 0.4353387297446285\n",
            "Iteration 324, Norm of Gradient: 0.046280705003930146, Cost (Train): 0.43512474983633675\n",
            "Iteration 325, Norm of Gradient: 0.046189810731702134, Cost (Train): 0.4349116089955465\n",
            "Iteration 326, Norm of Gradient: 0.046099359114527005, Cost (Train): 0.43469930150229297\n",
            "Iteration 327, Norm of Gradient: 0.046009346870793076, Cost (Train): 0.4344878216906699\n",
            "Iteration 328, Norm of Gradient: 0.04591977075077733, Cost (Train): 0.43427716394818605\n",
            "Iteration 329, Norm of Gradient: 0.04583062753626328, Cost (Train): 0.43406732271513143\n",
            "Iteration 330, Norm of Gradient: 0.045741914040164415, Cost (Train): 0.4338582924839516\n",
            "Iteration 331, Norm of Gradient: 0.04565362710615304, Cost (Train): 0.433650067798632\n",
            "Iteration 332, Norm of Gradient: 0.04556576360829432, Cost (Train): 0.4334426432540908\n",
            "Iteration 333, Norm of Gradient: 0.04547832045068542, Cost (Train): 0.4332360134955789\n",
            "Iteration 334, Norm of Gradient: 0.04539129456709993, Cost (Train): 0.4330301732180905\n",
            "Iteration 335, Norm of Gradient: 0.045304682920637114, Cost (Train): 0.432825117165781\n",
            "Iteration 336, Norm of Gradient: 0.04521848250337629, Cost (Train): 0.4326208401313922\n",
            "Iteration 337, Norm of Gradient: 0.045132690336035855, Cost (Train): 0.4324173369556875\n",
            "Iteration 338, Norm of Gradient: 0.045047303467637224, Cost (Train): 0.43221460252689275\n",
            "Iteration 339, Norm of Gradient: 0.04496231897517344, Cost (Train): 0.43201263178014704\n",
            "Iteration 340, Norm of Gradient: 0.04487773396328231, Cost (Train): 0.43181141969695963\n",
            "Iteration 341, Norm of Gradient: 0.044793545563924224, Cost (Train): 0.4316109613046747\n",
            "Iteration 342, Norm of Gradient: 0.044709750936064344, Cost (Train): 0.4314112516759443\n",
            "Iteration 343, Norm of Gradient: 0.04462634726535928, Cost (Train): 0.43121228592820726\n",
            "Iteration 344, Norm of Gradient: 0.0445433317638479, Cost (Train): 0.43101405922317704\n",
            "Iteration 345, Norm of Gradient: 0.044460701669646746, Cost (Train): 0.4308165667663343\n",
            "Iteration 346, Norm of Gradient: 0.0443784542466492, Cost (Train): 0.43061980380642867\n",
            "Iteration 347, Norm of Gradient: 0.044296586784229215, Cost (Train): 0.4304237656349864\n",
            "Iteration 348, Norm of Gradient: 0.044215096596948764, Cost (Train): 0.4302284475858238\n",
            "Iteration 349, Norm of Gradient: 0.04413398102426948, Cost (Train): 0.4300338450345693\n",
            "Iteration 350, Norm of Gradient: 0.04405323743026809, Cost (Train): 0.4298399533981904\n",
            "Iteration 351, Norm of Gradient: 0.04397286320335589, Cost (Train): 0.42964676813452746\n",
            "Iteration 352, Norm of Gradient: 0.04389285575600183, Cost (Train): 0.42945428474183434\n",
            "Iteration 353, Norm of Gradient: 0.043813212524459536, Cost (Train): 0.4292624987583243\n",
            "Iteration 354, Norm of Gradient: 0.043733930968497814, Cost (Train): 0.429071405761723\n",
            "Iteration 355, Norm of Gradient: 0.043655008571134975, Cost (Train): 0.4288810013688266\n",
            "Iteration 356, Norm of Gradient: 0.043576442838376674, Cost (Train): 0.4286912812350667\n",
            "Iteration 357, Norm of Gradient: 0.043498231298957184, Cost (Train): 0.4285022410540799\n",
            "Iteration 358, Norm of Gradient: 0.04342037150408424, Cost (Train): 0.4283138765572847\n",
            "Iteration 359, Norm of Gradient: 0.043342861027187256, Cost (Train): 0.4281261835134623\n",
            "Iteration 360, Norm of Gradient: 0.043265697463668845, Cost (Train): 0.42793915772834445\n",
            "Iteration 361, Norm of Gradient: 0.043188878430659786, Cost (Train): 0.4277527950442058\n",
            "Iteration 362, Norm of Gradient: 0.043112401566777064, Cost (Train): 0.42756709133946236\n",
            "Iteration 363, Norm of Gradient: 0.04303626453188524, Cost (Train): 0.42738204252827433\n",
            "Iteration 364, Norm of Gradient: 0.042960465006860966, Cost (Train): 0.427197644560155\n",
            "Iteration 365, Norm of Gradient: 0.042885000693360494, Cost (Train): 0.42701389341958473\n",
            "Iteration 366, Norm of Gradient: 0.04280986931359042, Cost (Train): 0.42683078512562916\n",
            "Iteration 367, Norm of Gradient: 0.04273506861008128, Cost (Train): 0.426648315731564\n",
            "Iteration 368, Norm of Gradient: 0.04266059634546425, Cost (Train): 0.4264664813245026\n",
            "Iteration 369, Norm of Gradient: 0.042586450302250604, Cost (Train): 0.42628527802503047\n",
            "Iteration 370, Norm of Gradient: 0.04251262828261422, Cost (Train): 0.4261047019868431\n",
            "Iteration 371, Norm of Gradient: 0.042439128108176766, Cost (Train): 0.42592474939638864\n",
            "Iteration 372, Norm of Gradient: 0.042365947619795855, Cost (Train): 0.4257454164725168\n",
            "Iteration 373, Norm of Gradient: 0.042293084677355736, Cost (Train): 0.4255666994661295\n",
            "Iteration 374, Norm of Gradient: 0.042220537159560846, Cost (Train): 0.4253885946598386\n",
            "Iteration 375, Norm of Gradient: 0.04214830296373198, Cost (Train): 0.42521109836762655\n",
            "Iteration 376, Norm of Gradient: 0.04207638000560506, Cost (Train): 0.4250342069345119\n",
            "Iteration 377, Norm of Gradient: 0.042004766219132586, Cost (Train): 0.42485791673621903\n",
            "Iteration 378, Norm of Gradient: 0.04193345955628747, Cost (Train): 0.42468222417885226\n",
            "Iteration 379, Norm of Gradient: 0.04186245798686954, Cost (Train): 0.4245071256985735\n",
            "Iteration 380, Norm of Gradient: 0.04179175949831449, Cost (Train): 0.42433261776128495\n",
            "Iteration 381, Norm of Gradient: 0.041721362095505095, Cost (Train): 0.4241586968623149\n",
            "Iteration 382, Norm of Gradient: 0.041651263800585106, Cost (Train): 0.4239853595261084\n",
            "Iteration 383, Norm of Gradient: 0.0415814626527753, Cost (Train): 0.4238126023059208\n",
            "Iteration 384, Norm of Gradient: 0.04151195670819192, Cost (Train): 0.42364042178351635\n",
            "Iteration 385, Norm of Gradient: 0.041442744039667426, Cost (Train): 0.4234688145688697\n",
            "Iteration 386, Norm of Gradient: 0.0413738227365735, Cost (Train): 0.42329777729987106\n",
            "Iteration 387, Norm of Gradient: 0.0413051909046463, Cost (Train): 0.42312730664203607\n",
            "Iteration 388, Norm of Gradient: 0.04123684666581383, Cost (Train): 0.4229573992882183\n",
            "Iteration 389, Norm of Gradient: 0.04116878815802556, Cost (Train): 0.42278805195832575\n",
            "Iteration 390, Norm of Gradient: 0.04110101353508421, Cost (Train): 0.42261926139904105\n",
            "Iteration 391, Norm of Gradient: 0.041033520966479456, Cost (Train): 0.42245102438354426\n",
            "Iteration 392, Norm of Gradient: 0.04096630863722392, Cost (Train): 0.4222833377112406\n",
            "Iteration 393, Norm of Gradient: 0.040899374747691085, Cost (Train): 0.4221161982074903\n",
            "Iteration 394, Norm of Gradient: 0.04083271751345524, Cost (Train): 0.421949602723342\n",
            "Iteration 395, Norm of Gradient: 0.0407663351651334, Cost (Train): 0.4217835481352701\n",
            "Iteration 396, Norm of Gradient: 0.040700225948229196, Cost (Train): 0.4216180313449147\n",
            "Iteration 397, Norm of Gradient: 0.040634388122978767, Cost (Train): 0.4214530492788248\n",
            "Iteration 398, Norm of Gradient: 0.04056881996419829, Cost (Train): 0.42128859888820486\n",
            "Iteration 399, Norm of Gradient: 0.040503519761133755, Cost (Train): 0.42112467714866436\n",
            "Iteration 400, Norm of Gradient: 0.04043848581731232, Cost (Train): 0.42096128105997027\n",
            "Iteration 401, Norm of Gradient: 0.040373716450395496, Cost (Train): 0.4207984076458034\n",
            "Iteration 402, Norm of Gradient: 0.040309209992034294, Cost (Train): 0.4206360539535157\n",
            "Iteration 403, Norm of Gradient: 0.04024496478772593, Cost (Train): 0.4204742170538931\n",
            "Iteration 404, Norm of Gradient: 0.040180979196672455, Cost (Train): 0.4203128940409192\n",
            "Iteration 405, Norm of Gradient: 0.040117251591641, Cost (Train): 0.4201520820315425\n",
            "Iteration 406, Norm of Gradient: 0.04005378035882568, Cost (Train): 0.419991778165447\n",
            "Iteration 407, Norm of Gradient: 0.03999056389771133, Cost (Train): 0.4198319796048247\n",
            "Iteration 408, Norm of Gradient: 0.03992760062093862, Cost (Train): 0.41967268353415094\n",
            "Iteration 409, Norm of Gradient: 0.03986488895417117, Cost (Train): 0.41951388715996296\n",
            "Iteration 410, Norm of Gradient: 0.03980242733596383, Cost (Train): 0.4193555877106412\n",
            "Iteration 411, Norm of Gradient: 0.039740214217632894, Cost (Train): 0.4191977824361917\n",
            "Iteration 412, Norm of Gradient: 0.039678248063127704, Cost (Train): 0.4190404686080336\n",
            "Iteration 413, Norm of Gradient: 0.039616527348903786, Cost (Train): 0.41888364351878643\n",
            "Iteration 414, Norm of Gradient: 0.039555050563797614, Cost (Train): 0.4187273044820622\n",
            "Iteration 415, Norm of Gradient: 0.03949381620890259, Cost (Train): 0.4185714488322589\n",
            "Iteration 416, Norm of Gradient: 0.03943282279744697, Cost (Train): 0.41841607392435615\n",
            "Iteration 417, Norm of Gradient: 0.039372068854672704, Cost (Train): 0.4182611771337141\n",
            "Iteration 418, Norm of Gradient: 0.03931155291771609, Cost (Train): 0.41810675585587387\n",
            "Iteration 419, Norm of Gradient: 0.03925127353548974, Cost (Train): 0.4179528075063613\n",
            "Iteration 420, Norm of Gradient: 0.03919122926856577, Cost (Train): 0.417799329520492\n",
            "Iteration 421, Norm of Gradient: 0.03913141868906067, Cost (Train): 0.4176463193531789\n",
            "Iteration 422, Norm of Gradient: 0.03907184038052125, Cost (Train): 0.4174937744787432\n",
            "Iteration 423, Norm of Gradient: 0.039012492937812074, Cost (Train): 0.4173416923907256\n",
            "Iteration 424, Norm of Gradient: 0.03895337496700417, Cost (Train): 0.417190070601701\n",
            "Iteration 425, Norm of Gradient: 0.03889448508526504, Cost (Train): 0.41703890664309556\n",
            "Iteration 426, Norm of Gradient: 0.03883582192074993, Cost (Train): 0.4168881980650047\n",
            "Iteration 427, Norm of Gradient: 0.03877738411249445, Cost (Train): 0.4167379424360145\n",
            "Iteration 428, Norm of Gradient: 0.03871917031030834, Cost (Train): 0.41658813734302397\n",
            "Iteration 429, Norm of Gradient: 0.038661179174670456, Cost (Train): 0.4164387803910704\n",
            "Iteration 430, Norm of Gradient: 0.03860340937662513, Cost (Train): 0.4162898692031562\n",
            "Iteration 431, Norm of Gradient: 0.03854585959767956, Cost (Train): 0.4161414014200774\n",
            "Iteration 432, Norm of Gradient: 0.03848852852970243, Cost (Train): 0.4159933747002553\n",
            "Iteration 433, Norm of Gradient: 0.03843141487482375, Cost (Train): 0.4158457867195685\n",
            "Iteration 434, Norm of Gradient: 0.038374517345335764, Cost (Train): 0.4156986351711878\n",
            "Iteration 435, Norm of Gradient: 0.03831783466359507, Cost (Train): 0.41555191776541334\n",
            "Iteration 436, Norm of Gradient: 0.038261365561925796, Cost (Train): 0.41540563222951227\n",
            "Iteration 437, Norm of Gradient: 0.03820510878252394, Cost (Train): 0.4152597763075593\n",
            "Iteration 438, Norm of Gradient: 0.038149063077362766, Cost (Train): 0.4151143477602791\n",
            "Iteration 439, Norm of Gradient: 0.038093227208099226, Cost (Train): 0.41496934436488986\n",
            "Iteration 440, Norm of Gradient: 0.0380375999459816, Cost (Train): 0.41482476391494877\n",
            "Iteration 441, Norm of Gradient: 0.03798218007175797, Cost (Train): 0.4146806042201997\n",
            "Iteration 442, Norm of Gradient: 0.03792696637558598, Cost (Train): 0.41453686310642246\n",
            "Iteration 443, Norm of Gradient: 0.03787195765694333, Cost (Train): 0.41439353841528276\n",
            "Iteration 444, Norm of Gradient: 0.037817152724539546, Cost (Train): 0.4142506280041858\n",
            "Iteration 445, Norm of Gradient: 0.037762550396228535, Cost (Train): 0.4141081297461294\n",
            "Iteration 446, Norm of Gradient: 0.03770814949892229, Cost (Train): 0.4139660415295604\n",
            "Iteration 447, Norm of Gradient: 0.03765394886850543, Cost (Train): 0.4138243612582313\n",
            "Iteration 448, Norm of Gradient: 0.03759994734975075, Cost (Train): 0.41368308685106\n",
            "Iteration 449, Norm of Gradient: 0.037546143796235776, Cost (Train): 0.4135422162419896\n",
            "Iteration 450, Norm of Gradient: 0.03749253707026011, Cost (Train): 0.4134017473798507\n",
            "Iteration 451, Norm of Gradient: 0.0374391260427638, Cost (Train): 0.41326167822822535\n",
            "Iteration 452, Norm of Gradient: 0.03738590959324665, Cost (Train): 0.41312200676531124\n",
            "Iteration 453, Norm of Gradient: 0.03733288660968828, Cost (Train): 0.41298273098378885\n",
            "Iteration 454, Norm of Gradient: 0.037280055988469216, Cost (Train): 0.41284384889068954\n",
            "Iteration 455, Norm of Gradient: 0.03722741663429275, Cost (Train): 0.4127053585072646\n",
            "Iteration 456, Norm of Gradient: 0.03717496746010774, Cost (Train): 0.41256725786885634\n",
            "Iteration 457, Norm of Gradient: 0.03712270738703223, Cost (Train): 0.41242954502477047\n",
            "Iteration 458, Norm of Gradient: 0.037070635344277875, Cost (Train): 0.41229221803814986\n",
            "Iteration 459, Norm of Gradient: 0.03701875026907524, Cost (Train): 0.4121552749858493\n",
            "Iteration 460, Norm of Gradient: 0.03696705110659991, Cost (Train): 0.4120187139583121\n",
            "Iteration 461, Norm of Gradient: 0.03691553680989941, Cost (Train): 0.41188253305944805\n",
            "Iteration 462, Norm of Gradient: 0.03686420633982087, Cost (Train): 0.411746730406512\n",
            "Iteration 463, Norm of Gradient: 0.03681305866493956, Cost (Train): 0.4116113041299849\n",
            "Iteration 464, Norm of Gradient: 0.03676209276148818, Cost (Train): 0.4114762523734552\n",
            "Iteration 465, Norm of Gradient: 0.03671130761328686, Cost (Train): 0.4113415732935016\n",
            "Iteration 466, Norm of Gradient: 0.036660702211673946, Cost (Train): 0.41120726505957755\n",
            "Iteration 467, Norm of Gradient: 0.03661027555543765, Cost (Train): 0.4110733258538966\n",
            "Iteration 468, Norm of Gradient: 0.0365600266507482, Cost (Train): 0.4109397538713192\n",
            "Iteration 469, Norm of Gradient: 0.03650995451109098, Cost (Train): 0.41080654731924066\n",
            "Iteration 470, Norm of Gradient: 0.036460058157200194, Cost (Train): 0.41067370441747997\n",
            "Iteration 471, Norm of Gradient: 0.036410336616993326, Cost (Train): 0.41054122339817034\n",
            "Iteration 472, Norm of Gradient: 0.03636078892550636, Cost (Train): 0.4104091025056505\n",
            "Iteration 473, Norm of Gradient: 0.0363114141248295, Cost (Train): 0.41027733999635724\n",
            "Iteration 474, Norm of Gradient: 0.036262211264043887, Cost (Train): 0.4101459341387198\n",
            "Iteration 475, Norm of Gradient: 0.03621317939915861, Cost (Train): 0.4100148832130536\n",
            "Iteration 476, Norm of Gradient: 0.03616431759304872, Cost (Train): 0.4098841855114574\n",
            "Iteration 477, Norm of Gradient: 0.036115624915393774, Cost (Train): 0.4097538393377093\n",
            "Iteration 478, Norm of Gradient: 0.03606710044261691, Cost (Train): 0.4096238430071657\n",
            "Iteration 479, Norm of Gradient: 0.036018743257824844, Cost (Train): 0.40949419484666033\n",
            "Iteration 480, Norm of Gradient: 0.03597055245074821, Cost (Train): 0.4093648931944042\n",
            "Iteration 481, Norm of Gradient: 0.03592252711768275, Cost (Train): 0.4092359363998873\n",
            "Iteration 482, Norm of Gradient: 0.035874666361430996, Cost (Train): 0.409107322823781\n",
            "Iteration 483, Norm of Gradient: 0.0358269692912446, Cost (Train): 0.4089790508378411\n",
            "Iteration 484, Norm of Gradient: 0.035779435022767335, Cost (Train): 0.4088511188248123\n",
            "Iteration 485, Norm of Gradient: 0.03573206267797858, Cost (Train): 0.4087235251783338\n",
            "Iteration 486, Norm of Gradient: 0.03568485138513745, Cost (Train): 0.40859626830284557\n",
            "Iteration 487, Norm of Gradient: 0.0356378002787276, Cost (Train): 0.4084693466134956\n",
            "Iteration 488, Norm of Gradient: 0.035590908499402364, Cost (Train): 0.4083427585360481\n",
            "Iteration 489, Norm of Gradient: 0.0355441751939308, Cost (Train): 0.40821650250679303\n",
            "Iteration 490, Norm of Gradient: 0.03549759951514397, Cost (Train): 0.4080905769724561\n",
            "Iteration 491, Norm of Gradient: 0.035451180621882, Cost (Train): 0.4079649803901099\n",
            "Iteration 492, Norm of Gradient: 0.03540491767894162, Cost (Train): 0.40783971122708557\n",
            "Iteration 493, Norm of Gradient: 0.03535880985702417, Cost (Train): 0.4077147679608866\n",
            "Iteration 494, Norm of Gradient: 0.035312856332684335, Cost (Train): 0.4075901490791015\n",
            "Iteration 495, Norm of Gradient: 0.03526705628827914, Cost (Train): 0.40746585307931943\n",
            "Iteration 496, Norm of Gradient: 0.035221408911917725, Cost (Train): 0.40734187846904524\n",
            "Iteration 497, Norm of Gradient: 0.0351759133974115, Cost (Train): 0.40721822376561584\n",
            "Iteration 498, Norm of Gradient: 0.03513056894422485, Cost (Train): 0.40709488749611805\n",
            "Iteration 499, Norm of Gradient: 0.03508537475742629, Cost (Train): 0.4069718681973059\n",
            "Iteration 500, Norm of Gradient: 0.03504033004764026, Cost (Train): 0.40684916441552016\n",
            "Iteration 501, Norm of Gradient: 0.03499543403099922, Cost (Train): 0.40672677470660784\n",
            "Iteration 502, Norm of Gradient: 0.034950685929096365, Cost (Train): 0.40660469763584284\n",
            "Iteration 503, Norm of Gradient: 0.03490608496893882, Cost (Train): 0.4064829317778471\n",
            "Iteration 504, Norm of Gradient: 0.03486163038290127, Cost (Train): 0.4063614757165132\n",
            "Iteration 505, Norm of Gradient: 0.034817321408680026, Cost (Train): 0.4062403280449271\n",
            "Iteration 506, Norm of Gradient: 0.034773157289247646, Cost (Train): 0.4061194873652913\n",
            "Iteration 507, Norm of Gradient: 0.034729137272807964, Cost (Train): 0.40599895228885013\n",
            "Iteration 508, Norm of Gradient: 0.034685260612751574, Cost (Train): 0.40587872143581444\n",
            "Iteration 509, Norm of Gradient: 0.03464152656761175, Cost (Train): 0.40575879343528837\n",
            "Iteration 510, Norm of Gradient: 0.03459793440102085, Cost (Train): 0.40563916692519475\n",
            "Iteration 511, Norm of Gradient: 0.03455448338166713, Cost (Train): 0.4055198405522035\n",
            "Iteration 512, Norm of Gradient: 0.034511172783252005, Cost (Train): 0.40540081297165975\n",
            "Iteration 513, Norm of Gradient: 0.0344680018844477, Cost (Train): 0.4052820828475122\n",
            "Iteration 514, Norm of Gradient: 0.034424969968855385, Cost (Train): 0.40516364885224326\n",
            "Iteration 515, Norm of Gradient: 0.03438207632496371, Cost (Train): 0.40504550966679903\n",
            "Iteration 516, Norm of Gradient: 0.034339320246107696, Cost (Train): 0.4049276639805202\n",
            "Iteration 517, Norm of Gradient: 0.03429670103042812, Cost (Train): 0.4048101104910738\n",
            "Iteration 518, Norm of Gradient: 0.034254217980831225, Cost (Train): 0.40469284790438564\n",
            "Iteration 519, Norm of Gradient: 0.03421187040494895, Cost (Train): 0.4045758749345731\n",
            "Iteration 520, Norm of Gradient: 0.034169657615099346, Cost (Train): 0.4044591903038786\n",
            "Iteration 521, Norm of Gradient: 0.03412757892824765, Cost (Train): 0.4043427927426045\n",
            "Iteration 522, Norm of Gradient: 0.0340856336659675, Cost (Train): 0.40422668098904724\n",
            "Iteration 523, Norm of Gradient: 0.03404382115440268, Cost (Train): 0.4041108537894335\n",
            "Iteration 524, Norm of Gradient: 0.03400214072422919, Cost (Train): 0.40399530989785604\n",
            "Iteration 525, Norm of Gradient: 0.03396059171061774, Cost (Train): 0.40388004807621086\n",
            "Iteration 526, Norm of Gradient: 0.03391917345319644, Cost (Train): 0.403765067094134\n",
            "Iteration 527, Norm of Gradient: 0.03387788529601418, Cost (Train): 0.40365036572894014\n",
            "Iteration 528, Norm of Gradient: 0.033836726587504015, Cost (Train): 0.4035359427655612\n",
            "Iteration 529, Norm of Gradient: 0.0337956966804471, Cost (Train): 0.4034217969964852\n",
            "Iteration 530, Norm of Gradient: 0.03375479493193702, Cost (Train): 0.40330792722169617\n",
            "Iteration 531, Norm of Gradient: 0.033714020703344276, Cost (Train): 0.4031943322486149\n",
            "Iteration 532, Norm of Gradient: 0.03367337336028134, Cost (Train): 0.4030810108920391\n",
            "Iteration 533, Norm of Gradient: 0.03363285227256784, Cost (Train): 0.4029679619740862\n",
            "Iteration 534, Norm of Gradient: 0.03359245681419627, Cost (Train): 0.40285518432413375\n",
            "Iteration 535, Norm of Gradient: 0.033552186363297876, Cost (Train): 0.4027426767787638\n",
            "Iteration 536, Norm of Gradient: 0.033512040302108984, Cost (Train): 0.40263043818170446\n",
            "Iteration 537, Norm of Gradient: 0.033472018016937596, Cost (Train): 0.4025184673837749\n",
            "Iteration 538, Norm of Gradient: 0.03343211889813032, Cost (Train): 0.4024067632428291\n",
            "Iteration 539, Norm of Gradient: 0.03339234234003958, Cost (Train): 0.40229532462370027\n",
            "Iteration 540, Norm of Gradient: 0.03335268774099127, Cost (Train): 0.4021841503981471\n",
            "Iteration 541, Norm of Gradient: 0.03331315450325255, Cost (Train): 0.4020732394447984\n",
            "Iteration 542, Norm of Gradient: 0.033273742033000056, Cost (Train): 0.40196259064910045\n",
            "Iteration 543, Norm of Gradient: 0.033234449740288385, Cost (Train): 0.40185220290326346\n",
            "Iteration 544, Norm of Gradient: 0.03319527703901888, Cost (Train): 0.4017420751062086\n",
            "Iteration 545, Norm of Gradient: 0.03315622334690873, Cost (Train): 0.4016322061635165\n",
            "Iteration 546, Norm of Gradient: 0.0331172880854603, Cost (Train): 0.4015225949873752\n",
            "Iteration 547, Norm of Gradient: 0.03307847067993088, Cost (Train): 0.40141324049652877\n",
            "Iteration 548, Norm of Gradient: 0.03303977055930258, Cost (Train): 0.4013041416162269\n",
            "Iteration 549, Norm of Gradient: 0.03300118715625259, Cost (Train): 0.401195297278175\n",
            "Iteration 550, Norm of Gradient: 0.0329627199071237, Cost (Train): 0.40108670642048355\n",
            "Iteration 551, Norm of Gradient: 0.03292436825189513, Cost (Train): 0.40097836798761943\n",
            "Iteration 552, Norm of Gradient: 0.03288613163415356, Cost (Train): 0.4008702809303572\n",
            "Iteration 553, Norm of Gradient: 0.03284800950106455, Cost (Train): 0.4007624442057302\n",
            "Iteration 554, Norm of Gradient: 0.03281000130334406, Cost (Train): 0.4006548567769831\n",
            "Iteration 555, Norm of Gradient: 0.032772106495230435, Cost (Train): 0.400547517613524\n",
            "Iteration 556, Norm of Gradient: 0.032734324534456516, Cost (Train): 0.4004404256908776\n",
            "Iteration 557, Norm of Gradient: 0.032696654882222, Cost (Train): 0.4003335799906387\n",
            "Iteration 558, Norm of Gradient: 0.03265909700316623, Cost (Train): 0.40022697950042574\n",
            "Iteration 559, Norm of Gradient: 0.03262165036534097, Cost (Train): 0.4001206232138351\n",
            "Iteration 560, Norm of Gradient: 0.032584314440183726, Cost (Train): 0.4000145101303961\n",
            "Iteration 561, Norm of Gradient: 0.032547088702491046, Cost (Train): 0.3999086392555253\n",
            "Iteration 562, Norm of Gradient: 0.0325099726303923, Cost (Train): 0.399803009600483\n",
            "Iteration 563, Norm of Gradient: 0.0324729657053235, Cost (Train): 0.39969762018232824\n",
            "Iteration 564, Norm of Gradient: 0.032436067412001536, Cost (Train): 0.3995924700238756\n",
            "Iteration 565, Norm of Gradient: 0.03239927723839854, Cost (Train): 0.3994875581536517\n",
            "Iteration 566, Norm of Gradient: 0.03236259467571652, Cost (Train): 0.3993828836058524\n",
            "Iteration 567, Norm of Gradient: 0.03232601921836222, Cost (Train): 0.3992784454203\n",
            "Iteration 568, Norm of Gradient: 0.03228955036392226, Cost (Train): 0.39917424264240164\n",
            "Iteration 569, Norm of Gradient: 0.03225318761313844, Cost (Train): 0.39907027432310693\n",
            "Iteration 570, Norm of Gradient: 0.03221693046988325, Cost (Train): 0.3989665395188671\n",
            "Iteration 571, Norm of Gradient: 0.03218077844113574, Cost (Train): 0.3988630372915935\n",
            "Iteration 572, Norm of Gradient: 0.03214473103695747, Cost (Train): 0.3987597667086174\n",
            "Iteration 573, Norm of Gradient: 0.032108787770468696, Cost (Train): 0.39865672684264947\n",
            "Iteration 574, Norm of Gradient: 0.03207294815782492, Cost (Train): 0.39855391677173996\n",
            "Iteration 575, Norm of Gradient: 0.03203721171819343, Cost (Train): 0.39845133557923923\n",
            "Iteration 576, Norm of Gradient: 0.03200157797373021, Cost (Train): 0.3983489823537582\n",
            "Iteration 577, Norm of Gradient: 0.03196604644955703, Cost (Train): 0.39824685618912997\n",
            "Iteration 578, Norm of Gradient: 0.03193061667373872, Cost (Train): 0.3981449561843712\n",
            "Iteration 579, Norm of Gradient: 0.031895288177260624, Cost (Train): 0.3980432814436436\n",
            "Iteration 580, Norm of Gradient: 0.03186006049400639, Cost (Train): 0.39794183107621683\n",
            "Iteration 581, Norm of Gradient: 0.03182493316073574, Cost (Train): 0.3978406041964301\n",
            "Iteration 582, Norm of Gradient: 0.031789905717062694, Cost (Train): 0.3977395999236557\n",
            "Iteration 583, Norm of Gradient: 0.031754977705433736, Cost (Train): 0.3976388173822622\n",
            "Iteration 584, Norm of Gradient: 0.0317201486711064, Cost (Train): 0.3975382557015776\n",
            "Iteration 585, Norm of Gradient: 0.03168541816212794, Cost (Train): 0.3974379140158533\n",
            "Iteration 586, Norm of Gradient: 0.031650785729314145, Cost (Train): 0.39733779146422843\n",
            "Iteration 587, Norm of Gradient: 0.03161625092622846, Cost (Train): 0.397237887190694\n",
            "Iteration 588, Norm of Gradient: 0.03158181330916118, Cost (Train): 0.3971382003440579\n",
            "Iteration 589, Norm of Gradient: 0.031547472437108995, Cost (Train): 0.39703873007791024\n",
            "Iteration 590, Norm of Gradient: 0.031513227871754465, Cost (Train): 0.396939475550588\n",
            "Iteration 591, Norm of Gradient: 0.031479079177445926, Cost (Train): 0.39684043592514157\n",
            "Iteration 592, Norm of Gradient: 0.03144502592117743, Cost (Train): 0.39674161036930006\n",
            "Iteration 593, Norm of Gradient: 0.03141106767256891, Cost (Train): 0.39664299805543823\n",
            "Iteration 594, Norm of Gradient: 0.031377204003846526, Cost (Train): 0.3965445981605426\n",
            "Iteration 595, Norm of Gradient: 0.03134343448982317, Cost (Train): 0.3964464098661786\n",
            "Iteration 596, Norm of Gradient: 0.0313097587078791, Cost (Train): 0.3963484323584575\n",
            "Iteration 597, Norm of Gradient: 0.031276176237942904, Cost (Train): 0.3962506648280041\n",
            "Iteration 598, Norm of Gradient: 0.0312426866624724, Cost (Train): 0.3961531064699245\n",
            "Iteration 599, Norm of Gradient: 0.031209289566435913, Cost (Train): 0.3960557564837739\n",
            "Iteration 600, Norm of Gradient: 0.031175984537293578, Cost (Train): 0.3959586140735249\n",
            "Iteration 601, Norm of Gradient: 0.03114277116497888, Cost (Train): 0.3958616784475362\n",
            "Iteration 602, Norm of Gradient: 0.03110964904188034, Cost (Train): 0.3957649488185218\n",
            "Iteration 603, Norm of Gradient: 0.03107661776282334, Cost (Train): 0.39566842440351924\n",
            "Iteration 604, Norm of Gradient: 0.03104367692505215, Cost (Train): 0.39557210442385954\n",
            "Iteration 605, Norm of Gradient: 0.031010826128212054, Cost (Train): 0.39547598810513707\n",
            "Iteration 606, Norm of Gradient: 0.030978064974331688, Cost (Train): 0.3953800746771788\n",
            "Iteration 607, Norm of Gradient: 0.030945393067805478, Cost (Train): 0.39528436337401496\n",
            "Iteration 608, Norm of Gradient: 0.03091281001537632, Cost (Train): 0.39518885343384924\n",
            "Iteration 609, Norm of Gradient: 0.03088031542611829, Cost (Train): 0.3950935440990293\n",
            "Iteration 610, Norm of Gradient: 0.03084790891141959, Cost (Train): 0.3949984346160183\n",
            "Iteration 611, Norm of Gradient: 0.03081559008496562, Cost (Train): 0.39490352423536534\n",
            "Iteration 612, Norm of Gradient: 0.030783358562722202, Cost (Train): 0.39480881221167724\n",
            "Iteration 613, Norm of Gradient: 0.030751213962918925, Cost (Train): 0.39471429780358985\n",
            "Iteration 614, Norm of Gradient: 0.030719155906032646, Cost (Train): 0.39461998027374073\n",
            "Iteration 615, Norm of Gradient: 0.03068718401477117, Cost (Train): 0.3945258588887405\n",
            "Iteration 616, Norm of Gradient: 0.03065529791405702, Cost (Train): 0.39443193291914525\n",
            "Iteration 617, Norm of Gradient: 0.030623497231011324, Cost (Train): 0.3943382016394295\n",
            "Iteration 618, Norm of Gradient: 0.030591781594937948, Cost (Train): 0.394244664327959\n",
            "Iteration 619, Norm of Gradient: 0.03056015063730768, Cost (Train): 0.39415132026696353\n",
            "Iteration 620, Norm of Gradient: 0.030528603991742537, Cost (Train): 0.3940581687425101\n",
            "Iteration 621, Norm of Gradient: 0.030497141294000267, Cost (Train): 0.39396520904447685\n",
            "Iteration 622, Norm of Gradient: 0.03046576218195893, Cost (Train): 0.39387244046652675\n",
            "Iteration 623, Norm of Gradient: 0.030434466295601682, Cost (Train): 0.393779862306081\n",
            "Iteration 624, Norm of Gradient: 0.030403253277001576, Cost (Train): 0.3936874738642939\n",
            "Iteration 625, Norm of Gradient: 0.030372122770306598, Cost (Train): 0.3935952744460266\n",
            "Iteration 626, Norm of Gradient: 0.030341074421724797, Cost (Train): 0.39350326335982233\n",
            "Iteration 627, Norm of Gradient: 0.0303101078795095, Cost (Train): 0.3934114399178806\n",
            "Iteration 628, Norm of Gradient: 0.0302792227939447, Cost (Train): 0.39331980343603246\n",
            "Iteration 629, Norm of Gradient: 0.03024841881733057, Cost (Train): 0.39322835323371613\n",
            "Iteration 630, Norm of Gradient: 0.03021769560396908, Cost (Train): 0.3931370886339515\n",
            "Iteration 631, Norm of Gradient: 0.03018705281014971, Cost (Train): 0.3930460089633167\n",
            "Iteration 632, Norm of Gradient: 0.030156490094135333, Cost (Train): 0.39295511355192336\n",
            "Iteration 633, Norm of Gradient: 0.03012600711614821, Cost (Train): 0.3928644017333928\n",
            "Iteration 634, Norm of Gradient: 0.03009560353835606, Cost (Train): 0.39277387284483245\n",
            "Iteration 635, Norm of Gradient: 0.030065279024858307, Cost (Train): 0.3926835262268119\n",
            "Iteration 636, Norm of Gradient: 0.03003503324167238, Cost (Train): 0.3925933612233396\n",
            "Iteration 637, Norm of Gradient: 0.030004865856720166, Cost (Train): 0.3925033771818402\n",
            "Iteration 638, Norm of Gradient: 0.029974776539814597, Cost (Train): 0.39241357345313066\n",
            "Iteration 639, Norm of Gradient: 0.02994476496264629, Cost (Train): 0.39232394939139825\n",
            "Iteration 640, Norm of Gradient: 0.02991483079877034, Cost (Train): 0.3922345043541771\n",
            "Iteration 641, Norm of Gradient: 0.02988497372359324, Cost (Train): 0.392145237702327\n",
            "Iteration 642, Norm of Gradient: 0.029855193414359806, Cost (Train): 0.3920561488000095\n",
            "Iteration 643, Norm of Gradient: 0.029825489550140392, Cost (Train): 0.39196723701466746\n",
            "Iteration 644, Norm of Gradient: 0.029795861811818015, Cost (Train): 0.3918785017170017\n",
            "Iteration 645, Norm of Gradient: 0.029766309882075754, Cost (Train): 0.39178994228095065\n",
            "Iteration 646, Norm of Gradient: 0.029736833445384112, Cost (Train): 0.3917015580836674\n",
            "Iteration 647, Norm of Gradient: 0.02970743218798858, Cost (Train): 0.39161334850549984\n",
            "Iteration 648, Norm of Gradient: 0.029678105797897297, Cost (Train): 0.39152531292996784\n",
            "Iteration 649, Norm of Gradient: 0.029648853964868722, Cost (Train): 0.3914374507437437\n",
            "Iteration 650, Norm of Gradient: 0.029619676380399536, Cost (Train): 0.3913497613366304\n",
            "Iteration 651, Norm of Gradient: 0.02959057273771255, Cost (Train): 0.3912622441015411\n",
            "Iteration 652, Norm of Gradient: 0.029561542731744748, Cost (Train): 0.3911748984344786\n",
            "Iteration 653, Norm of Gradient: 0.029532586059135415, Cost (Train): 0.3910877237345152\n",
            "Iteration 654, Norm of Gradient: 0.029503702418214376, Cost (Train): 0.3910007194037725\n",
            "Iteration 655, Norm of Gradient: 0.029474891508990336, Cost (Train): 0.39091388484740075\n",
            "Iteration 656, Norm of Gradient: 0.029446153033139275, Cost (Train): 0.3908272194735598\n",
            "Iteration 657, Norm of Gradient: 0.029417486693993012, Cost (Train): 0.3907407226933989\n",
            "Iteration 658, Norm of Gradient: 0.02938889219652776, Cost (Train): 0.39065439392103707\n",
            "Iteration 659, Norm of Gradient: 0.029360369247352892, Cost (Train): 0.3905682325735442\n",
            "Iteration 660, Norm of Gradient: 0.029331917554699688, Cost (Train): 0.39048223807092136\n",
            "Iteration 661, Norm of Gradient: 0.029303536828410262, Cost (Train): 0.3903964098360817\n",
            "Iteration 662, Norm of Gradient: 0.029275226779926507, Cost (Train): 0.3903107472948317\n",
            "Iteration 663, Norm of Gradient: 0.029246987122279182, Cost (Train): 0.39022524987585233\n",
            "Iteration 664, Norm of Gradient: 0.029218817570077067, Cost (Train): 0.39013991701068024\n",
            "Iteration 665, Norm of Gradient: 0.029190717839496203, Cost (Train): 0.39005474813368957\n",
            "Iteration 666, Norm of Gradient: 0.029162687648269237, Cost (Train): 0.38996974268207335\n",
            "Iteration 667, Norm of Gradient: 0.029134726715674786, Cost (Train): 0.38988490009582505\n",
            "Iteration 668, Norm of Gradient: 0.029106834762527036, Cost (Train): 0.38980021981772145\n",
            "Iteration 669, Norm of Gradient: 0.029079011511165223, Cost (Train): 0.3897157012933032\n",
            "Iteration 670, Norm of Gradient: 0.029051256685443377, Cost (Train): 0.38963134397085875\n",
            "Iteration 671, Norm of Gradient: 0.029023570010720035, Cost (Train): 0.38954714730140544\n",
            "Iteration 672, Norm of Gradient: 0.028995951213848105, Cost (Train): 0.38946311073867224\n",
            "Iteration 673, Norm of Gradient: 0.028968400023164743, Cost (Train): 0.3893792337390829\n",
            "Iteration 674, Norm of Gradient: 0.028940916168481396, Cost (Train): 0.38929551576173826\n",
            "Iteration 675, Norm of Gradient: 0.028913499381073845, Cost (Train): 0.3892119562683991\n",
            "Iteration 676, Norm of Gradient: 0.028886149393672383, Cost (Train): 0.3891285547234695\n",
            "Iteration 677, Norm of Gradient: 0.02885886594045202, Cost (Train): 0.3890453105939797\n",
            "Iteration 678, Norm of Gradient: 0.028831648757022835, Cost (Train): 0.3889622233495693\n",
            "Iteration 679, Norm of Gradient: 0.02880449758042033, Cost (Train): 0.38887929246247166\n",
            "Iteration 680, Norm of Gradient: 0.0287774121490959, Cost (Train): 0.38879651740749566\n",
            "Iteration 681, Norm of Gradient: 0.028750392202907427, Cost (Train): 0.3887138976620111\n",
            "Iteration 682, Norm of Gradient: 0.028723437483109786, Cost (Train): 0.3886314327059316\n",
            "Iteration 683, Norm of Gradient: 0.028696547732345645, Cost (Train): 0.38854912202169856\n",
            "Iteration 684, Norm of Gradient: 0.028669722694636177, Cost (Train): 0.38846696509426576\n",
            "Iteration 685, Norm of Gradient: 0.02864296211537188, Cost (Train): 0.38838496141108275\n",
            "Iteration 686, Norm of Gradient: 0.028616265741303513, Cost (Train): 0.38830311046207977\n",
            "Iteration 687, Norm of Gradient: 0.028589633320533107, Cost (Train): 0.3882214117396519\n",
            "Iteration 688, Norm of Gradient: 0.028563064602504937, Cost (Train): 0.38813986473864365\n",
            "Iteration 689, Norm of Gradient: 0.028536559337996686, Cost (Train): 0.3880584689563335\n",
            "Iteration 690, Norm of Gradient: 0.028510117279110653, Cost (Train): 0.3879772238924189\n",
            "Iteration 691, Norm of Gradient: 0.028483738179264983, Cost (Train): 0.3878961290490009\n",
            "Iteration 692, Norm of Gradient: 0.02845742179318499, Cost (Train): 0.3878151839305694\n",
            "Iteration 693, Norm of Gradient: 0.028431167876894606, Cost (Train): 0.3877343880439881\n",
            "Iteration 694, Norm of Gradient: 0.028404976187707768, Cost (Train): 0.3876537408984796\n",
            "Iteration 695, Norm of Gradient: 0.02837884648421999, Cost (Train): 0.38757324200561116\n",
            "Iteration 696, Norm of Gradient: 0.02835277852629997, Cost (Train): 0.38749289087927985\n",
            "Iteration 697, Norm of Gradient: 0.0283267720750812, Cost (Train): 0.3874126870356978\n",
            "Iteration 698, Norm of Gradient: 0.028300826892953757, Cost (Train): 0.3873326299933789\n",
            "Iteration 699, Norm of Gradient: 0.028274942743556034, Cost (Train): 0.38725271927312327\n",
            "Iteration 700, Norm of Gradient: 0.028249119391766622, Cost (Train): 0.3871729543980041\n",
            "Iteration 701, Norm of Gradient: 0.028223356603696233, Cost (Train): 0.3870933348933532\n",
            "Iteration 702, Norm of Gradient: 0.028197654146679636, Cost (Train): 0.3870138602867474\n",
            "Iteration 703, Norm of Gradient: 0.02817201178926776, Cost (Train): 0.38693453010799417\n",
            "Iteration 704, Norm of Gradient: 0.028146429301219737, Cost (Train): 0.3868553438891187\n",
            "Iteration 705, Norm of Gradient: 0.028120906453495107, Cost (Train): 0.38677630116434986\n",
            "Iteration 706, Norm of Gradient: 0.02809544301824599, Cost (Train): 0.38669740147010656\n",
            "Iteration 707, Norm of Gradient: 0.02807003876880945, Cost (Train): 0.38661864434498494\n",
            "Iteration 708, Norm of Gradient: 0.028044693479699737, Cost (Train): 0.3865400293297443\n",
            "Iteration 709, Norm of Gradient: 0.02801940692660077, Cost (Train): 0.3864615559672945\n",
            "Iteration 710, Norm of Gradient: 0.027994178886358544, Cost (Train): 0.3863832238026827\n",
            "Iteration 711, Norm of Gradient: 0.027969009136973685, Cost (Train): 0.38630503238308034\n",
            "Iteration 712, Norm of Gradient: 0.02794389745759397, Cost (Train): 0.38622698125777033\n",
            "Iteration 713, Norm of Gradient: 0.02791884362850701, Cost (Train): 0.3861490699781339\n",
            "Iteration 714, Norm of Gradient: 0.027893847431132903, Cost (Train): 0.3860712980976384\n",
            "Iteration 715, Norm of Gradient: 0.027868908648017, Cost (Train): 0.38599366517182426\n",
            "Iteration 716, Norm of Gradient: 0.02784402706282267, Cost (Train): 0.3859161707582927\n",
            "Iteration 717, Norm of Gradient: 0.027819202460324197, Cost (Train): 0.38583881441669315\n",
            "Iteration 718, Norm of Gradient: 0.027794434626399602, Cost (Train): 0.38576159570871116\n",
            "Iteration 719, Norm of Gradient: 0.02776972334802372, Cost (Train): 0.38568451419805594\n",
            "Iteration 720, Norm of Gradient: 0.027745068413261104, Cost (Train): 0.385607569450448\n",
            "Iteration 721, Norm of Gradient: 0.027720469611259162, Cost (Train): 0.3855307610336076\n",
            "Iteration 722, Norm of Gradient: 0.027695926732241236, Cost (Train): 0.38545408851724255\n",
            "Iteration 723, Norm of Gradient: 0.027671439567499772, Cost (Train): 0.38537755147303576\n",
            "Iteration 724, Norm of Gradient: 0.02764700790938957, Cost (Train): 0.3853011494746345\n",
            "Iteration 725, Norm of Gradient: 0.027622631551321036, Cost (Train): 0.3852248820976377\n",
            "Iteration 726, Norm of Gradient: 0.02759831028775348, Cost (Train): 0.38514874891958517\n",
            "Iteration 727, Norm of Gradient: 0.027574043914188547, Cost (Train): 0.3850727495199449\n",
            "Iteration 728, Norm of Gradient: 0.027549832227163576, Cost (Train): 0.3849968834801029\n",
            "Iteration 729, Norm of Gradient: 0.027525675024245123, Cost (Train): 0.38492115038335106\n",
            "Iteration 730, Norm of Gradient: 0.027501572104022442, Cost (Train): 0.3848455498148756\n",
            "Iteration 731, Norm of Gradient: 0.027477523266101075, Cost (Train): 0.384770081361747\n",
            "Iteration 732, Norm of Gradient: 0.027453528311096466, Cost (Train): 0.38469474461290726\n",
            "Iteration 733, Norm of Gradient: 0.027429587040627604, Cost (Train): 0.38461953915916003\n",
            "Iteration 734, Norm of Gradient: 0.02740569925731077, Cost (Train): 0.38454446459315955\n",
            "Iteration 735, Norm of Gradient: 0.027381864764753263, Cost (Train): 0.3844695205093992\n",
            "Iteration 736, Norm of Gradient: 0.02735808336754723, Cost (Train): 0.38439470650420093\n",
            "Iteration 737, Norm of Gradient: 0.027334354871263494, Cost (Train): 0.3843200221757046\n",
            "Iteration 738, Norm of Gradient: 0.02731067908244544, Cost (Train): 0.3842454671238574\n",
            "Iteration 739, Norm of Gradient: 0.02728705580860302, Cost (Train): 0.38417104095040266\n",
            "Iteration 740, Norm of Gradient: 0.027263484858206642, Cost (Train): 0.3840967432588704\n",
            "Iteration 741, Norm of Gradient: 0.02723996604068126, Cost (Train): 0.3840225736545659\n",
            "Iteration 742, Norm of Gradient: 0.02721649916640046, Cost (Train): 0.38394853174455973\n",
            "Iteration 743, Norm of Gradient: 0.027193084046680558, Cost (Train): 0.38387461713767757\n",
            "Iteration 744, Norm of Gradient: 0.027169720493774707, Cost (Train): 0.3838008294444898\n",
            "Iteration 745, Norm of Gradient: 0.0271464083208672, Cost (Train): 0.38372716827730147\n",
            "Iteration 746, Norm of Gradient: 0.02712314734206766, Cost (Train): 0.38365363325014207\n",
            "Iteration 747, Norm of Gradient: 0.027099937372405302, Cost (Train): 0.3835802239787557\n",
            "Iteration 748, Norm of Gradient: 0.027076778227823346, Cost (Train): 0.38350694008059094\n",
            "Iteration 749, Norm of Gradient: 0.027053669725173285, Cost (Train): 0.3834337811747916\n",
            "Iteration 750, Norm of Gradient: 0.027030611682209407, Cost (Train): 0.38336074688218563\n",
            "Iteration 751, Norm of Gradient: 0.027007603917583153, Cost (Train): 0.38328783682527695\n",
            "Iteration 752, Norm of Gradient: 0.026984646250837664, Cost (Train): 0.3832150506282348\n",
            "Iteration 753, Norm of Gradient: 0.026961738502402296, Cost (Train): 0.3831423879168844\n",
            "Iteration 754, Norm of Gradient: 0.026938880493587208, Cost (Train): 0.3830698483186974\n",
            "Iteration 755, Norm of Gradient: 0.026916072046577966, Cost (Train): 0.3829974314627825\n",
            "Iteration 756, Norm of Gradient: 0.02689331298443019, Cost (Train): 0.38292513697987623\n",
            "Iteration 757, Norm of Gradient: 0.02687060313106426, Cost (Train): 0.3828529645023332\n",
            "Iteration 758, Norm of Gradient: 0.02684794231126001, Cost (Train): 0.3827809136641172\n",
            "Iteration 759, Norm of Gradient: 0.026825330350651575, Cost (Train): 0.38270898410079185\n",
            "Iteration 760, Norm of Gradient: 0.026802767075722083, Cost (Train): 0.38263717544951154\n",
            "Iteration 761, Norm of Gradient: 0.0267802523137986, Cost (Train): 0.38256548734901263\n",
            "Iteration 762, Norm of Gradient: 0.026757785893046938, Cost (Train): 0.38249391943960354\n",
            "Iteration 763, Norm of Gradient: 0.026735367642466642, Cost (Train): 0.38242247136315705\n",
            "Iteration 764, Norm of Gradient: 0.026712997391885856, Cost (Train): 0.38235114276310056\n",
            "Iteration 765, Norm of Gradient: 0.026690674971956378, Cost (Train): 0.3822799332844077\n",
            "Iteration 766, Norm of Gradient: 0.026668400214148697, Cost (Train): 0.382208842573589\n",
            "Iteration 767, Norm of Gradient: 0.026646172950746984, Cost (Train): 0.3821378702786841\n",
            "Iteration 768, Norm of Gradient: 0.02662399301484425, Cost (Train): 0.3820670160492526\n",
            "Iteration 769, Norm of Gradient: 0.026601860240337446, Cost (Train): 0.3819962795363651\n",
            "Iteration 770, Norm of Gradient: 0.02657977446192265, Cost (Train): 0.3819256603925956\n",
            "Iteration 771, Norm of Gradient: 0.02655773551509026, Cost (Train): 0.3818551582720123\n",
            "Iteration 772, Norm of Gradient: 0.026535743236120218, Cost (Train): 0.38178477283016954\n",
            "Iteration 773, Norm of Gradient: 0.026513797462077292, Cost (Train): 0.38171450372409943\n",
            "Iteration 774, Norm of Gradient: 0.02649189803080639, Cost (Train): 0.3816443506123034\n",
            "Iteration 775, Norm of Gradient: 0.026470044780927852, Cost (Train): 0.38157431315474416\n",
            "Iteration 776, Norm of Gradient: 0.026448237551832877, Cost (Train): 0.3815043910128375\n",
            "Iteration 777, Norm of Gradient: 0.026426476183678885, Cost (Train): 0.38143458384944434\n",
            "Iteration 778, Norm of Gradient: 0.026404760517384964, Cost (Train): 0.38136489132886214\n",
            "Iteration 779, Norm of Gradient: 0.026383090394627346, Cost (Train): 0.38129531311681747\n",
            "Iteration 780, Norm of Gradient: 0.02636146565783488, Cost (Train): 0.3812258488804578\n",
            "Iteration 781, Norm of Gradient: 0.026339886150184583, Cost (Train): 0.3811564982883434\n",
            "Iteration 782, Norm of Gradient: 0.026318351715597217, Cost (Train): 0.38108726101044044\n",
            "Iteration 783, Norm of Gradient: 0.02629686219873283, Cost (Train): 0.3810181367181117\n",
            "Iteration 784, Norm of Gradient: 0.02627541744498645, Cost (Train): 0.3809491250841101\n",
            "Iteration 785, Norm of Gradient: 0.02625401730048368, Cost (Train): 0.38088022578257064\n",
            "Iteration 786, Norm of Gradient: 0.026232661612076432, Cost (Train): 0.38081143848900223\n",
            "Iteration 787, Norm of Gradient: 0.02621135022733858, Cost (Train): 0.38074276288028125\n",
            "Iteration 788, Norm of Gradient: 0.02619008299456178, Cost (Train): 0.3806741986346428\n",
            "Iteration 789, Norm of Gradient: 0.026168859762751212, Cost (Train): 0.3806057454316741\n",
            "Iteration 790, Norm of Gradient: 0.02614768038162136, Cost (Train): 0.3805374029523066\n",
            "Iteration 791, Norm of Gradient: 0.026126544701591883, Cost (Train): 0.3804691708788087\n",
            "Iteration 792, Norm of Gradient: 0.026105452573783466, Cost (Train): 0.38040104889477877\n",
            "Iteration 793, Norm of Gradient: 0.02608440385001372, Cost (Train): 0.3803330366851373\n",
            "Iteration 794, Norm of Gradient: 0.026063398382793087, Cost (Train): 0.38026513393611994\n",
            "Iteration 795, Norm of Gradient: 0.026042436025320796, Cost (Train): 0.38019734033527064\n",
            "Iteration 796, Norm of Gradient: 0.026021516631480862, Cost (Train): 0.3801296555714339\n",
            "Iteration 797, Norm of Gradient: 0.026000640055838034, Cost (Train): 0.38006207933474817\n",
            "Iteration 798, Norm of Gradient: 0.025979806153633905, Cost (Train): 0.3799946113166387\n",
            "Iteration 799, Norm of Gradient: 0.025959014780782916, Cost (Train): 0.37992725120981014\n",
            "Iteration 800, Norm of Gradient: 0.02593826579386847, Cost (Train): 0.37985999870824033\n",
            "Iteration 801, Norm of Gradient: 0.02591755905013902, Cost (Train): 0.37979285350717285\n",
            "Iteration 802, Norm of Gradient: 0.025896894407504253, Cost (Train): 0.37972581530311045\n",
            "Iteration 803, Norm of Gradient: 0.025876271724531205, Cost (Train): 0.3796588837938081\n",
            "Iteration 804, Norm of Gradient: 0.02585569086044052, Cost (Train): 0.3795920586782661\n",
            "Iteration 805, Norm of Gradient: 0.025835151675102613, Cost (Train): 0.37952533965672386\n",
            "Iteration 806, Norm of Gradient: 0.025814654029033945, Cost (Train): 0.37945872643065254\n",
            "Iteration 807, Norm of Gradient: 0.025794197783393286, Cost (Train): 0.37939221870274925\n",
            "Iteration 808, Norm of Gradient: 0.02577378279997802, Cost (Train): 0.37932581617692923\n",
            "Iteration 809, Norm of Gradient: 0.025753408941220468, Cost (Train): 0.379259518558321\n",
            "Iteration 810, Norm of Gradient: 0.025733076070184224, Cost (Train): 0.3791933255532584\n",
            "Iteration 811, Norm of Gradient: 0.02571278405056055, Cost (Train): 0.3791272368692746\n",
            "Iteration 812, Norm of Gradient: 0.025692532746664762, Cost (Train): 0.3790612522150961\n",
            "Iteration 813, Norm of Gradient: 0.025672322023432635, Cost (Train): 0.3789953713006358\n",
            "Iteration 814, Norm of Gradient: 0.025652151746416902, Cost (Train): 0.37892959383698704\n",
            "Iteration 815, Norm of Gradient: 0.025632021781783657, Cost (Train): 0.37886391953641696\n",
            "Iteration 816, Norm of Gradient: 0.02561193199630891, Cost (Train): 0.37879834811236057\n",
            "Iteration 817, Norm of Gradient: 0.025591882257375062, Cost (Train): 0.37873287927941457\n",
            "Iteration 818, Norm of Gradient: 0.025571872432967498, Cost (Train): 0.3786675127533309\n",
            "Iteration 819, Norm of Gradient: 0.025551902391671075, Cost (Train): 0.37860224825101074\n",
            "Iteration 820, Norm of Gradient: 0.025531972002666808, Cost (Train): 0.3785370854904984\n",
            "Iteration 821, Norm of Gradient: 0.025512081135728407, Cost (Train): 0.3784720241909758\n",
            "Iteration 822, Norm of Gradient: 0.025492229661218935, Cost (Train): 0.37840706407275515\n",
            "Iteration 823, Norm of Gradient: 0.025472417450087503, Cost (Train): 0.37834220485727466\n",
            "Iteration 824, Norm of Gradient: 0.025452644373865865, Cost (Train): 0.37827744626709114\n",
            "Iteration 825, Norm of Gradient: 0.025432910304665213, Cost (Train): 0.3782127880258751\n",
            "Iteration 826, Norm of Gradient: 0.025413215115172864, Cost (Train): 0.3781482298584045\n",
            "Iteration 827, Norm of Gradient: 0.025393558678648984, Cost (Train): 0.3780837714905587\n",
            "Iteration 828, Norm of Gradient: 0.02537394086892338, Cost (Train): 0.3780194126493134\n",
            "Iteration 829, Norm of Gradient: 0.025354361560392325, Cost (Train): 0.377955153062734\n",
            "Iteration 830, Norm of Gradient: 0.02533482062801528, Cost (Train): 0.37789099245997065\n",
            "Iteration 831, Norm of Gradient: 0.02531531794731184, Cost (Train): 0.3778269305712522\n",
            "Iteration 832, Norm of Gradient: 0.02529585339435849, Cost (Train): 0.3777629671278807\n",
            "Iteration 833, Norm of Gradient: 0.02527642684578554, Cost (Train): 0.3776991018622255\n",
            "Iteration 834, Norm of Gradient: 0.025257038178773998, Cost (Train): 0.3776353345077185\n",
            "Iteration 835, Norm of Gradient: 0.02523768727105249, Cost (Train): 0.37757166479884746\n",
            "Iteration 836, Norm of Gradient: 0.025218374000894217, Cost (Train): 0.3775080924711515\n",
            "Iteration 837, Norm of Gradient: 0.025199098247113867, Cost (Train): 0.3774446172612152\n",
            "Iteration 838, Norm of Gradient: 0.02517985988906466, Cost (Train): 0.37738123890666314\n",
            "Iteration 839, Norm of Gradient: 0.02516065880663528, Cost (Train): 0.3773179571461548\n",
            "Iteration 840, Norm of Gradient: 0.025141494880246928, Cost (Train): 0.37725477171937905\n",
            "Iteration 841, Norm of Gradient: 0.025122367990850367, Cost (Train): 0.37719168236704864\n",
            "Iteration 842, Norm of Gradient: 0.025103278019922962, Cost (Train): 0.3771286888308953\n",
            "Iteration 843, Norm of Gradient: 0.02508422484946576, Cost (Train): 0.377065790853664\n",
            "Iteration 844, Norm of Gradient: 0.025065208362000598, Cost (Train): 0.37700298817910866\n",
            "Iteration 845, Norm of Gradient: 0.025046228440567187, Cost (Train): 0.37694028055198564\n",
            "Iteration 846, Norm of Gradient: 0.02502728496872029, Cost (Train): 0.37687766771804954\n",
            "Iteration 847, Norm of Gradient: 0.025008377830526857, Cost (Train): 0.37681514942404815\n",
            "Iteration 848, Norm of Gradient: 0.024989506910563175, Cost (Train): 0.3767527254177165\n",
            "Iteration 849, Norm of Gradient: 0.024970672093912096, Cost (Train): 0.3766903954477728\n",
            "Iteration 850, Norm of Gradient: 0.024951873266160236, Cost (Train): 0.3766281592639127\n",
            "Iteration 851, Norm of Gradient: 0.024933110313395162, Cost (Train): 0.3765660166168046\n",
            "Iteration 852, Norm of Gradient: 0.024914383122202705, Cost (Train): 0.37650396725808505\n",
            "Iteration 853, Norm of Gradient: 0.02489569157966417, Cost (Train): 0.37644201094035284\n",
            "Iteration 854, Norm of Gradient: 0.02487703557335365, Cost (Train): 0.3763801474171652\n",
            "Iteration 855, Norm of Gradient: 0.024858414991335304, Cost (Train): 0.3763183764430321\n",
            "Iteration 856, Norm of Gradient: 0.024839829722160676, Cost (Train): 0.37625669777341186\n",
            "Iteration 857, Norm of Gradient: 0.02482127965486605, Cost (Train): 0.37619511116470633\n",
            "Iteration 858, Norm of Gradient: 0.02480276467896978, Cost (Train): 0.37613361637425585\n",
            "Iteration 859, Norm of Gradient: 0.024784284684469682, Cost (Train): 0.37607221316033496\n",
            "Iteration 860, Norm of Gradient: 0.02476583956184037, Cost (Train): 0.37601090128214704\n",
            "Iteration 861, Norm of Gradient: 0.024747429202030727, Cost (Train): 0.3759496804998201\n",
            "Iteration 862, Norm of Gradient: 0.024729053496461272, Cost (Train): 0.375888550574402\n",
            "Iteration 863, Norm of Gradient: 0.02471071233702163, Cost (Train): 0.37582751126785624\n",
            "Iteration 864, Norm of Gradient: 0.024692405616067968, Cost (Train): 0.37576656234305617\n",
            "Iteration 865, Norm of Gradient: 0.024674133226420446, Cost (Train): 0.3757057035637816\n",
            "Iteration 866, Norm of Gradient: 0.024655895061360764, Cost (Train): 0.37564493469471405\n",
            "Iteration 867, Norm of Gradient: 0.024637691014629592, Cost (Train): 0.37558425550143154\n",
            "Iteration 868, Norm of Gradient: 0.02461952098042414, Cost (Train): 0.3755236657504051\n",
            "Iteration 869, Norm of Gradient: 0.02460138485339566, Cost (Train): 0.37546316520899325\n",
            "Iteration 870, Norm of Gradient: 0.02458328252864704, Cost (Train): 0.37540275364543874\n",
            "Iteration 871, Norm of Gradient: 0.024565213901730294, Cost (Train): 0.3753424308288629\n",
            "Iteration 872, Norm of Gradient: 0.024547178868644222, Cost (Train): 0.37528219652926237\n",
            "Iteration 873, Norm of Gradient: 0.024529177325831972, Cost (Train): 0.37522205051750396\n",
            "Iteration 874, Norm of Gradient: 0.024511209170178634, Cost (Train): 0.37516199256532073\n",
            "Iteration 875, Norm of Gradient: 0.02449327429900892, Cost (Train): 0.37510202244530766\n",
            "Iteration 876, Norm of Gradient: 0.024475372610084745, Cost (Train): 0.3750421399309172\n",
            "Iteration 877, Norm of Gradient: 0.024457504001602945, Cost (Train): 0.374982344796455\n",
            "Iteration 878, Norm of Gradient: 0.024439668372192885, Cost (Train): 0.3749226368170761\n",
            "Iteration 879, Norm of Gradient: 0.024421865620914217, Cost (Train): 0.37486301576878023\n",
            "Iteration 880, Norm of Gradient: 0.02440409564725452, Cost (Train): 0.37480348142840775\n",
            "Iteration 881, Norm of Gradient: 0.024386358351127062, Cost (Train): 0.3747440335736361\n",
            "Iteration 882, Norm of Gradient: 0.024368653632868508, Cost (Train): 0.3746846719829748\n",
            "Iteration 883, Norm of Gradient: 0.02435098139323667, Cost (Train): 0.37462539643576165\n",
            "Iteration 884, Norm of Gradient: 0.02433334153340828, Cost (Train): 0.3745662067121592\n",
            "Iteration 885, Norm of Gradient: 0.02431573395497673, Cost (Train): 0.3745071025931497\n",
            "Iteration 886, Norm of Gradient: 0.02429815855994991, Cost (Train): 0.37444808386053197\n",
            "Iteration 887, Norm of Gradient: 0.024280615250747976, Cost (Train): 0.3743891502969173\n",
            "Iteration 888, Norm of Gradient: 0.024263103930201167, Cost (Train): 0.3743303016857247\n",
            "Iteration 889, Norm of Gradient: 0.02424562450154765, Cost (Train): 0.37427153781117806\n",
            "Iteration 890, Norm of Gradient: 0.02422817686843137, Cost (Train): 0.37421285845830127\n",
            "Iteration 891, Norm of Gradient: 0.024210760934899867, Cost (Train): 0.37415426341291474\n",
            "Iteration 892, Norm of Gradient: 0.024193376605402207, Cost (Train): 0.3740957524616319\n",
            "Iteration 893, Norm of Gradient: 0.024176023784786796, Cost (Train): 0.37403732539185447\n",
            "Iteration 894, Norm of Gradient: 0.024158702378299358, Cost (Train): 0.37397898199176927\n",
            "Iteration 895, Norm of Gradient: 0.02414141229158077, Cost (Train): 0.3739207220503443\n",
            "Iteration 896, Norm of Gradient: 0.024124153430665044, Cost (Train): 0.37386254535732505\n",
            "Iteration 897, Norm of Gradient: 0.024106925701977224, Cost (Train): 0.37380445170323023\n",
            "Iteration 898, Norm of Gradient: 0.02408972901233135, Cost (Train): 0.3737464408793485\n",
            "Iteration 899, Norm of Gradient: 0.02407256326892843, Cost (Train): 0.37368851267773484\n",
            "Iteration 900, Norm of Gradient: 0.024055428379354406, Cost (Train): 0.37363066689120644\n",
            "Iteration 901, Norm of Gradient: 0.024038324251578154, Cost (Train): 0.37357290331333937\n",
            "Iteration 902, Norm of Gradient: 0.02402125079394945, Cost (Train): 0.37351522173846463\n",
            "Iteration 903, Norm of Gradient: 0.024004207915197015, Cost (Train): 0.3734576219616648\n",
            "Iteration 904, Norm of Gradient: 0.023987195524426557, Cost (Train): 0.3734001037787704\n",
            "Iteration 905, Norm of Gradient: 0.023970213531118757, Cost (Train): 0.373342666986356\n",
            "Iteration 906, Norm of Gradient: 0.023953261845127347, Cost (Train): 0.373285311381737\n",
            "Iteration 907, Norm of Gradient: 0.02393634037667718, Cost (Train): 0.3732280367629658\n",
            "Iteration 908, Norm of Gradient: 0.023919449036362294, Cost (Train): 0.37317084292882874\n",
            "Iteration 909, Norm of Gradient: 0.023902587735144004, Cost (Train): 0.37311372967884193\n",
            "Iteration 910, Norm of Gradient: 0.02388575638434897, Cost (Train): 0.3730566968132483\n",
            "Iteration 911, Norm of Gradient: 0.02386895489566736, Cost (Train): 0.372999744133014\n",
            "Iteration 912, Norm of Gradient: 0.023852183181150927, Cost (Train): 0.3729428714398249\n",
            "Iteration 913, Norm of Gradient: 0.02383544115321116, Cost (Train): 0.37288607853608285\n",
            "Iteration 914, Norm of Gradient: 0.023818728724617422, Cost (Train): 0.372829365224903\n",
            "Iteration 915, Norm of Gradient: 0.02380204580849511, Cost (Train): 0.37277273131010996\n",
            "Iteration 916, Norm of Gradient: 0.02378539231832382, Cost (Train): 0.37271617659623435\n",
            "Iteration 917, Norm of Gradient: 0.023768768167935533, Cost (Train): 0.3726597008885097\n",
            "Iteration 918, Norm of Gradient: 0.02375217327151279, Cost (Train): 0.37260330399286895\n",
            "Iteration 919, Norm of Gradient: 0.023735607543586897, Cost (Train): 0.3725469857159414\n",
            "Iteration 920, Norm of Gradient: 0.023719070899036136, Cost (Train): 0.37249074586504904\n",
            "Iteration 921, Norm of Gradient: 0.023702563253084008, Cost (Train): 0.3724345842482035\n",
            "Iteration 922, Norm of Gradient: 0.023686084521297425, Cost (Train): 0.3723785006741033\n",
            "Iteration 923, Norm of Gradient: 0.023669634619585007, Cost (Train): 0.37232249495212927\n",
            "Iteration 924, Norm of Gradient: 0.023653213464195286, Cost (Train): 0.37226656689234283\n",
            "Iteration 925, Norm of Gradient: 0.023636820971715, Cost (Train): 0.3722107163054819\n",
            "Iteration 926, Norm of Gradient: 0.02362045705906735, Cost (Train): 0.3721549430029581\n",
            "Iteration 927, Norm of Gradient: 0.023604121643510327, Cost (Train): 0.3720992467968534\n",
            "Iteration 928, Norm of Gradient: 0.023587814642634954, Cost (Train): 0.372043627499917\n",
            "Iteration 929, Norm of Gradient: 0.023571535974363636, Cost (Train): 0.3719880849255625\n",
            "Iteration 930, Norm of Gradient: 0.023555285556948462, Cost (Train): 0.3719326188878644\n",
            "Iteration 931, Norm of Gradient: 0.02353906330896952, Cost (Train): 0.37187722920155514\n",
            "Iteration 932, Norm of Gradient: 0.023522869149333263, Cost (Train): 0.3718219156820224\n",
            "Iteration 933, Norm of Gradient: 0.023506702997270833, Cost (Train): 0.3717666781453054\n",
            "Iteration 934, Norm of Gradient: 0.023490564772336416, Cost (Train): 0.3717115164080922\n",
            "Iteration 935, Norm of Gradient: 0.02347445439440566, Cost (Train): 0.37165643028771717\n",
            "Iteration 936, Norm of Gradient: 0.02345837178367397, Cost (Train): 0.37160141960215687\n",
            "Iteration 937, Norm of Gradient: 0.023442316860654958, Cost (Train): 0.37154648417002833\n",
            "Iteration 938, Norm of Gradient: 0.02342628954617884, Cost (Train): 0.371491623810585\n",
            "Iteration 939, Norm of Gradient: 0.023410289761390785, Cost (Train): 0.3714368383437147\n",
            "Iteration 940, Norm of Gradient: 0.023394317427749403, Cost (Train): 0.37138212758993594\n",
            "Iteration 941, Norm of Gradient: 0.023378372467025135, Cost (Train): 0.37132749137039556\n",
            "Iteration 942, Norm of Gradient: 0.023362454801298678, Cost (Train): 0.3712729295068655\n",
            "Iteration 943, Norm of Gradient: 0.023346564352959454, Cost (Train): 0.37121844182174035\n",
            "Iteration 944, Norm of Gradient: 0.023330701044704055, Cost (Train): 0.37116402813803384\n",
            "Iteration 945, Norm of Gradient: 0.023314864799534703, Cost (Train): 0.3711096882793764\n",
            "Iteration 946, Norm of Gradient: 0.023299055540757733, Cost (Train): 0.37105542207001274\n",
            "Iteration 947, Norm of Gradient: 0.023283273191982055, Cost (Train): 0.37100122933479773\n",
            "Iteration 948, Norm of Gradient: 0.02326751767711766, Cost (Train): 0.3709471098991952\n",
            "Iteration 949, Norm of Gradient: 0.02325178892037411, Cost (Train): 0.37089306358927404\n",
            "Iteration 950, Norm of Gradient: 0.02323608684625908, Cost (Train): 0.37083909023170586\n",
            "Iteration 951, Norm of Gradient: 0.023220411379576804, Cost (Train): 0.37078518965376245\n",
            "Iteration 952, Norm of Gradient: 0.02320476244542669, Cost (Train): 0.3707313616833122\n",
            "Iteration 953, Norm of Gradient: 0.023189139969201786, Cost (Train): 0.3706776061488186\n",
            "Iteration 954, Norm of Gradient: 0.023173543876587346, Cost (Train): 0.37062392287933654\n",
            "Iteration 955, Norm of Gradient: 0.023157974093559384, Cost (Train): 0.37057031170450994\n",
            "Iteration 956, Norm of Gradient: 0.023142430546383267, Cost (Train): 0.37051677245456927\n",
            "Iteration 957, Norm of Gradient: 0.02312691316161221, Cost (Train): 0.37046330496032875\n",
            "Iteration 958, Norm of Gradient: 0.023111421866085922, Cost (Train): 0.3704099090531835\n",
            "Iteration 959, Norm of Gradient: 0.02309595658692916, Cost (Train): 0.37035658456510734\n",
            "Iteration 960, Norm of Gradient: 0.023080517251550345, Cost (Train): 0.3703033313286498\n",
            "Iteration 961, Norm of Gradient: 0.023065103787640113, Cost (Train): 0.3702501491769336\n",
            "Iteration 962, Norm of Gradient: 0.023049716123170003, Cost (Train): 0.37019703794365233\n",
            "Iteration 963, Norm of Gradient: 0.02303435418639103, Cost (Train): 0.3701439974630675\n",
            "Iteration 964, Norm of Gradient: 0.023019017905832304, Cost (Train): 0.3700910275700061\n",
            "Iteration 965, Norm of Gradient: 0.023003707210299693, Cost (Train): 0.3700381280998584\n",
            "Iteration 966, Norm of Gradient: 0.02298842202887445, Cost (Train): 0.36998529888857495\n",
            "Iteration 967, Norm of Gradient: 0.02297316229091187, Cost (Train): 0.36993253977266416\n",
            "Iteration 968, Norm of Gradient: 0.022957927926039966, Cost (Train): 0.3698798505891901\n",
            "Iteration 969, Norm of Gradient: 0.022942718864158083, Cost (Train): 0.36982723117576966\n",
            "Iteration 970, Norm of Gradient: 0.022927535035435643, Cost (Train): 0.3697746813705701\n",
            "Iteration 971, Norm of Gradient: 0.02291237637031077, Cost (Train): 0.36972220101230713\n",
            "Iteration 972, Norm of Gradient: 0.022897242799489015, Cost (Train): 0.3696697899402417\n",
            "Iteration 973, Norm of Gradient: 0.02288213425394205, Cost (Train): 0.369617447994178\n",
            "Iteration 974, Norm of Gradient: 0.022867050664906352, Cost (Train): 0.36956517501446096\n",
            "Iteration 975, Norm of Gradient: 0.022851991963881935, Cost (Train): 0.36951297084197393\n",
            "Iteration 976, Norm of Gradient: 0.022836958082631073, Cost (Train): 0.36946083531813606\n",
            "Iteration 977, Norm of Gradient: 0.022821948953177015, Cost (Train): 0.36940876828490016\n",
            "Iteration 978, Norm of Gradient: 0.022806964507802722, Cost (Train): 0.36935676958475017\n",
            "Iteration 979, Norm of Gradient: 0.022792004679049634, Cost (Train): 0.36930483906069894\n",
            "Iteration 980, Norm of Gradient: 0.022777069399716385, Cost (Train): 0.3692529765562857\n",
            "Iteration 981, Norm of Gradient: 0.02276215860285758, Cost (Train): 0.3692011819155741\n",
            "Iteration 982, Norm of Gradient: 0.022747272221782546, Cost (Train): 0.3691494549831494\n",
            "Iteration 983, Norm of Gradient: 0.02273241019005414, Cost (Train): 0.3690977956041166\n",
            "Iteration 984, Norm of Gradient: 0.02271757244148747, Cost (Train): 0.36904620362409774\n",
            "Iteration 985, Norm of Gradient: 0.022702758910148734, Cost (Train): 0.36899467888923004\n",
            "Iteration 986, Norm of Gradient: 0.022687969530353982, Cost (Train): 0.3689432212461636\n",
            "Iteration 987, Norm of Gradient: 0.022673204236667917, Cost (Train): 0.36889183054205854\n",
            "Iteration 988, Norm of Gradient: 0.022658462963902722, Cost (Train): 0.3688405066245835\n",
            "Iteration 989, Norm of Gradient: 0.022643745647116853, Cost (Train): 0.36878924934191337\n",
            "Iteration 990, Norm of Gradient: 0.02262905222161387, Cost (Train): 0.3687380585427263\n",
            "Iteration 991, Norm of Gradient: 0.022614382622941246, Cost (Train): 0.3686869340762024\n",
            "Iteration 992, Norm of Gradient: 0.022599736786889254, Cost (Train): 0.36863587579202095\n",
            "Iteration 993, Norm of Gradient: 0.022585114649489722, Cost (Train): 0.3685848835403588\n",
            "Iteration 994, Norm of Gradient: 0.022570516147014958, Cost (Train): 0.36853395717188747\n",
            "Iteration 995, Norm of Gradient: 0.022555941215976545, Cost (Train): 0.3684830965377715\n",
            "Iteration 996, Norm of Gradient: 0.02254138979312427, Cost (Train): 0.3684323014896662\n",
            "Iteration 997, Norm of Gradient: 0.02252686181544491, Cost (Train): 0.36838157187971565\n",
            "Iteration 998, Norm of Gradient: 0.022512357220161164, Cost (Train): 0.36833090756055\n",
            "Iteration 999, Norm of Gradient: 0.022497875944730503, Cost (Train): 0.3682803083852843\n",
            "Terminated after 1000 iterations, with norm of the gradient equal to 0.022497875944730503\n",
            "The weight found: [-0.03473372 -0.02716593 -0.01017575 ... -0.20521069 -0.03812566\n",
            "  0.03710947]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC for Logistic Regression:\n",
            "20% of data: 0.90729184\n",
            "40% of data: 0.9135669632000001\n",
            "60% of data: 0.9164716224\n",
            "80% of data: 0.9179134528\n",
            "100% of data: 0.9185314816000001\n",
            "\n",
            "AUROC for Decision Tree:\n",
            "20% of data: 0.65508\n",
            "40% of data: 0.6702400000000001\n",
            "60% of data: 0.6646799999999999\n",
            "80% of data: 0.6748000000000001\n",
            "100% of data: 0.67276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# AUROC values\n",
        "auroc_logistic = [0.904596736, 0.9130913920000001, 0.916180576, 0.9180692224000001, 0.9185314816000001]\n",
        "auroc_dt = [0.65492, 0.6574000000000001, 0.6676399999999999, 0.6674399999999999, 0.6732400000000001]\n",
        "\n",
        "# Training set sizes\n",
        "training_sizes = [20, 40, 60, 80, 100]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting lines for logistic regression and decision tree\n",
        "plt.plot(training_sizes, auroc_logistic, marker='o', color='b', label='Logistic Regression')\n",
        "plt.plot(training_sizes, auroc_dt, marker='o', color='r', label='Decision Tree')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Training Set Size (%)', fontweight='bold')\n",
        "plt.ylabel('AUROC', fontweight='bold')\n",
        "plt.title('AUROC for Logistic Regression and Decision Tree')\n",
        "\n",
        "# Adding legend\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "apZJbyIGX-oQ",
        "outputId": "8eb7cd40-fe0d-4975-e5e8-d7fe24f772cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilinear Regression on 20 News Dataset"
      ],
      "metadata": {
        "id": "HuVTmbmLCRdh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HtIUU4EVYIwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(1234)\n",
        "\n",
        "# Define 5 categories we want to use (randomly selected)\n",
        "categories = ['comp.graphics', 'misc.forsale', 'rec.sport.baseball', 'sci.med', 'talk.politics.guns']\n",
        "\n",
        "# Fetch the data for the selected categories\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=False, random_state=42)\n",
        "\n",
        "# Tokenize the text data and count word occurrences\n",
        "count_vect = CountVectorizer(lowercase=True, stop_words='english', max_df=0.5, min_df=0.01)\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_test_counts = count_vect.transform(twenty_test.data)\n",
        "\n",
        "# Calculate TF-IDF features\n",
        "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
      ],
      "metadata": {
        "id": "ZYvYB6DWzOJG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mutual Information Calculation to Select Top Features"
      ],
      "metadata": {
        "id": "2COvmR7zvkXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Mutual Information for feature selection\n",
        "mutual_info = mutual_info_classif(X_train_tfidf.toarray(), twenty_train.target)"
      ],
      "metadata": {
        "id": "unoiS82Z0Gdt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get top M feature words for each class\n",
        "def top_feature_words_per_class(mutual_info, features, labels, top_m=10):\n",
        "\n",
        "    top_words_per_class = {}\n",
        "\n",
        "    for i, label in enumerate(np.unique(labels)):\n",
        "        # Get indices of features with highest mutual information for this class\n",
        "        top_indices = np.argsort(mutual_info)[::-1][:top_m]\n",
        "        # Get corresponding words\n",
        "        top_words = [features[i] for i in top_indices]\n",
        "        top_words_per_class[label] = top_words\n",
        "    return top_words_per_class\n",
        "\n",
        "# Get feature names from CountVectorizer\n",
        "feature_names = np.array(count_vect.get_feature_names_out())\n",
        "\n",
        "# Get top feature words per class\n",
        "top_words_per_class = top_feature_words_per_class(mutual_info, feature_names, twenty_train.target, top_m=100)\n",
        "\n",
        "# Take the union of top feature words across classes\n",
        "top_feature_words = set()\n",
        "for words in top_words_per_class.values():\n",
        "    top_feature_words.update(words)\n",
        "\n",
        "# Get indices of selected features\n",
        "selected_feature_indices = [np.where(feature_names == word)[0][0] for word in top_feature_words]\n",
        "\n",
        "# Filter the training and test data to include only selected features\n",
        "X_train_selected = X_train_tfidf[:, selected_feature_indices]\n",
        "X_test_selected = X_test_tfidf[:, selected_feature_indices]\n",
        "\n",
        "\n",
        "X_train_selected = X_train_selected.toarray()\n",
        "X_test_selected = X_test_selected.toarray()\n",
        "# Split the data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_selected, twenty_train.target, test_size=0.2, random_state=42)\n",
        "\n",
        "X_test = X_test_selected\n",
        "y_test = twenty_test.target"
      ],
      "metadata": {
        "id": "_fwv1SvbzlGV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use LabelBinarizer to one-hot encode the target variables\n",
        "label_binarizer = LabelBinarizer()\n",
        "y_train = label_binarizer.fit_transform(y_train)\n",
        "y_val = label_binarizer.transform(y_val)\n",
        "y_test = label_binarizer.transform(y_test)\n",
        "\n",
        "print(\"\\nX_train shape:\", X_train.shape, type(X_train))\n",
        "print(\"y_train shape:\", y_train.shape, type(y_train))\n",
        "print(\"X_val shape:\", X_val.shape, type(X_val))\n",
        "print(\"\\ny_val shape:\", y_val.shape, type(y_val))\n",
        "print(\"X_test shape:\", X_test.shape, type(X_test))\n",
        "print(\"y_test shape:\", y_test.shape, type(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AXVT8C10YBR",
        "outputId": "395f88e1-6eb9-4621-ed21-a1bb3465e089"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "X_train shape: (2324, 100) <class 'numpy.ndarray'>\n",
            "y_train shape: (2324, 5) <class 'numpy.ndarray'>\n",
            "X_val shape: (582, 100) <class 'numpy.ndarray'>\n",
            "\n",
            "y_val shape: (582, 5) <class 'numpy.ndarray'>\n",
            "X_test shape: (1936, 100) <class 'numpy.ndarray'>\n",
            "y_test shape: (1936, 5) <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Multinomial_logistic:\n",
        "    def __init__(self, nFeatures, nClasses):\n",
        "        self.W = np.random.rand(nFeatures, nClasses)\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = np.dot(X, self.W)\n",
        "        y_pred = np.exp(scores)\n",
        "        return y_pred / y_pred.sum(axis=1).reshape(X.shape[0], 1)\n",
        "\n",
        "    def grad(self, X, y):\n",
        "        return np.matmul(X.transpose(), self.predict(X) - y)\n",
        "\n",
        "    def ce(self, X, y):\n",
        "        # Calculate the predicted probabilities\n",
        "        y_pred = self.predict(X)\n",
        "        # Calculate the cross-entropy loss\n",
        "        crossentropy = -np.sum (y * np.log (y_pred)) # sum of all individual losses for each data point\n",
        "        return crossentropy\n",
        "\n",
        "    # Modify fit to add stopping criteria\n",
        "    def fit(self, X, y, X_valid=None, y_valid=None, lr=0.005, niter=100, tol=1e-4):\n",
        "        losses_train = np.zeros(niter)\n",
        "        losses_valid = np.zeros(niter)\n",
        "        prev_loss_valid = float('inf')\n",
        "        stopping_point = 0\n",
        "        for i in range(niter):\n",
        "            loss_train = self.ce(X, y)/X.shape[0]\n",
        "            losses_train[i] = loss_train\n",
        "            if X_valid is not None and y_valid is not None:\n",
        "                loss_valid = self.ce(X_valid, y_valid)/X_valid.shape[0]\n",
        "                losses_valid[i] = loss_valid\n",
        "                # printing removed to avoid long logging\n",
        "                # print(f\"iter {i}: {loss_train:.3f}; {loss_valid:.3f}\")\n",
        "                # Check for convergence based on validation loss\n",
        "                if abs(loss_valid - prev_loss_valid) < tol:\n",
        "                    print(\"Convergence reached. Stopping training.\")\n",
        "                    stopping_point = i\n",
        "                    break\n",
        "                if loss_valid < prev_loss_valid: # selecting only the best validation loss\n",
        "                  prev_loss_valid = loss_valid\n",
        "                  print('Best validation loss: {}'.format(prev_loss_valid))\n",
        "            else:\n",
        "                # print(f\"iter {i}: {loss_train:.3f}\")\n",
        "                pass\n",
        "            self.W = self.W - lr * self.grad(X, y)\n",
        "        return losses_train, losses_valid, stopping_point\n",
        "\n",
        "    def check_grad(self, X, y):\n",
        "        N, C = y.shape\n",
        "        D = X.shape[1]\n",
        "\n",
        "        diff = np.zeros((D, C))\n",
        "\n",
        "        W = self.W.copy()\n",
        "\n",
        "        for i in range(D):\n",
        "            for j in range(C):\n",
        "                epsilon = np.zeros((D, C))\n",
        "                epsilon[i, j] = np.random.rand() * 1e-4\n",
        "\n",
        "                self.W = self.W + epsilon\n",
        "                J1 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                self.W = self.W - epsilon\n",
        "                J2 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                numeric_grad = (J1 - J2) / (2 * epsilon[i, j])\n",
        "                derived_grad = self.grad(X, y)[i, j]\n",
        "\n",
        "                diff[i, j] = np.square(derived_grad - numeric_grad).sum() / \\\n",
        "                             np.square(derived_grad + numeric_grad).sum()\n",
        "\n",
        "        return diff.sum()\n"
      ],
      "metadata": {
        "id": "oaa3zXCr0dM8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = X_train.shape[1] # number of features\n",
        "C = y_train.shape[1] # number of classes\n",
        "mlr = Multinomial_logistic(D, C)"
      ],
      "metadata": {
        "id": "GCPY96Tl1GD4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the gradient\n",
        "gradient_diff = mlr.check_grad(X_train, y_train)\n",
        "print('Checking Gradient Diff: {}'.format(gradient_diff))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsWLnW0s1eTp",
        "outputId": "ac36984a-8f71-41d5-ba81-d5bf93a1415d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking Gradient Diff: 1.4222267162669176e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ce_train, ce_valid, stopping_point = mlr.fit(X_train, y_train, X_val, y_val, niter=5000)\n",
        "print('Stopping Iteration at step {} after the training converged'.format(stopping_point))\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(ce_train[:stopping_point], label='train')\n",
        "plt.plot(ce_valid[:stopping_point], label='valid')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Categorical Cross-Entropy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"training_ce.png\", bbox_inches=\"tight\", dpi=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TsoE4zvPWpT7",
        "outputId": "606da5b1-68a6-4645-f2bb-189aaed98cca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation loss: 1.617428365830718\n",
            "Best validation loss: 1.593572555629945\n",
            "Best validation loss: 1.5704206755991392\n",
            "Best validation loss: 1.5479789221026936\n",
            "Best validation loss: 1.5262474105573216\n",
            "Best validation loss: 1.5052204621299705\n",
            "Best validation loss: 1.4848872405012747\n",
            "Best validation loss: 1.4652326241057083\n",
            "Best validation loss: 1.446238183190232\n",
            "Best validation loss: 1.427883144534151\n",
            "Best validation loss: 1.4101452588695078\n",
            "Best validation loss: 1.3930015237305708\n",
            "Best validation loss: 1.3764287474401409\n",
            "Best validation loss: 1.360403963144656\n",
            "Best validation loss: 1.3449047146745858\n",
            "Best validation loss: 1.3299092405905926\n",
            "Best validation loss: 1.3153965819682079\n",
            "Best validation loss: 1.3013466358684922\n",
            "Best validation loss: 1.2877401718901156\n",
            "Best validation loss: 1.2745588247942108\n",
            "Best validation loss: 1.2617850724537227\n",
            "Best validation loss: 1.2494022054526266\n",
            "Best validation loss: 1.2373942924994936\n",
            "Best validation loss: 1.225746144294081\n",
            "Best validation loss: 1.2144432774475302\n",
            "Best validation loss: 1.2034718793734327\n",
            "Best validation loss: 1.1928187746313685\n",
            "Best validation loss: 1.1824713929360864\n",
            "Best validation loss: 1.1724177388866344\n",
            "Best validation loss: 1.1626463633808306\n",
            "Best validation loss: 1.153146336634803\n",
            "Best validation loss: 1.143907222707692\n",
            "Best validation loss: 1.1349190554268591\n",
            "Best validation loss: 1.1261723156123142\n",
            "Best validation loss: 1.1176579095063217\n",
            "Best validation loss: 1.10936714832292\n",
            "Best validation loss: 1.1012917288409838\n",
            "Best validation loss: 1.0934237149727979\n",
            "Best validation loss: 1.0857555202475644\n",
            "Best validation loss: 1.0782798911557565\n",
            "Best validation loss: 1.0709898913057663\n",
            "Best validation loss: 1.0638788863489732\n",
            "Best validation loss: 1.056940529633295\n",
            "Best validation loss: 1.0501687485485662\n",
            "Best validation loss: 1.0435577315298596\n",
            "Best validation loss: 1.0371019156871972\n",
            "Best validation loss: 1.0307959750320808\n",
            "Best validation loss: 1.0246348092729625\n",
            "Best validation loss: 1.018613533153279\n",
            "Best validation loss: 1.012727466306954\n",
            "Best validation loss: 1.0069721236074582\n",
            "Best validation loss: 1.0013432059875729\n",
            "Best validation loss: 0.9958365917079789\n",
            "Best validation loss: 0.990448328053706\n",
            "Best validation loss: 0.9851746234383406\n",
            "Best validation loss: 0.9800118398967027\n",
            "Best validation loss: 0.9749564859474913\n",
            "Best validation loss: 0.9700052098081591\n",
            "Best validation loss: 0.9651547929449952\n",
            "Best validation loss: 0.9604021439421176\n",
            "Best validation loss: 0.955744292673765\n",
            "Best validation loss: 0.951178384764937\n",
            "Best validation loss: 0.9467016763260985\n",
            "Best validation loss: 0.9423115289482891\n",
            "Best validation loss: 0.9380054049455863\n",
            "Best validation loss: 0.9337808628324861\n",
            "Best validation loss: 0.9296355530243182\n",
            "Best validation loss: 0.9255672137493914\n",
            "Best validation loss: 0.9215736671620859\n",
            "Best validation loss: 0.9176528156466439\n",
            "Best validation loss: 0.913802638301897\n",
            "Best validation loss: 0.9100211875976579\n",
            "Best validation loss: 0.9063065861939618\n",
            "Best validation loss: 0.9026570239147855\n",
            "Best validation loss: 0.8990707548682958\n",
            "Best validation loss: 0.8955460947060853\n",
            "Best validation loss: 0.8920814180142396\n",
            "Best validation loss: 0.8886751558294492\n",
            "Best validation loss: 0.885325793273734\n",
            "Best validation loss: 0.8820318673016785\n",
            "Best validation loss: 0.8787919645544036\n",
            "Best validation loss: 0.8756047193147967\n",
            "Best validation loss: 0.872468811558815\n",
            "Best validation loss: 0.8693829650979511\n",
            "Best validation loss: 0.8663459458082077\n",
            "Best validation loss: 0.8633565599411769\n",
            "Best validation loss: 0.8604136525130575\n",
            "Best validation loss: 0.8575161057676548\n",
            "Best validation loss: 0.8546628377096331\n",
            "Best validation loss: 0.8518528007044743\n",
            "Best validation loss: 0.8490849801417985\n",
            "Best validation loss: 0.8463583931588691\n",
            "Best validation loss: 0.8436720874212847\n",
            "Best validation loss: 0.84102513995801\n",
            "Best validation loss: 0.8384166560480542\n",
            "Best validation loss: 0.83584576815625\n",
            "Best validation loss: 0.833311634915716\n",
            "Best validation loss: 0.8308134401547148\n",
            "Best validation loss: 0.8283503919657448\n",
            "Best validation loss: 0.8259217218148092\n",
            "Best validation loss: 0.8235266836889208\n",
            "Best validation loss: 0.8211645532799996\n",
            "Best validation loss: 0.8188346272034154\n",
            "Best validation loss: 0.8165362222495235\n",
            "Best validation loss: 0.8142686746666226\n",
            "Best validation loss: 0.8120313394738479\n",
            "Best validation loss: 0.8098235898025872\n",
            "Best validation loss: 0.8076448162650858\n",
            "Best validation loss: 0.8054944263489654\n",
            "Best validation loss: 0.8033718438364577\n",
            "Best validation loss: 0.8012765082472062\n",
            "Best validation loss: 0.7992078743035528\n",
            "Best validation loss: 0.7971654114172793\n",
            "Best validation loss: 0.7951486031968243\n",
            "Best validation loss: 0.793156946974049\n",
            "Best validation loss: 0.7911899533496664\n",
            "Best validation loss: 0.7892471457564982\n",
            "Best validation loss: 0.7873280600397611\n",
            "Best validation loss: 0.7854322440536253\n",
            "Best validation loss: 0.7835592572733249\n",
            "Best validation loss: 0.781708670422136\n",
            "Best validation loss: 0.7798800651125708\n",
            "Best validation loss: 0.7780730335011682\n",
            "Best validation loss: 0.7762871779562892\n",
            "Best validation loss: 0.7745221107383617\n",
            "Best validation loss: 0.772777453692031\n",
            "Best validation loss: 0.7710528379497177\n",
            "Best validation loss: 0.7693479036460896\n",
            "Best validation loss: 0.7676622996429927\n",
            "Best validation loss: 0.7659956832643957\n",
            "Best validation loss: 0.7643477200409345\n",
            "Best validation loss: 0.7627180834636527\n",
            "Best validation loss: 0.7611064547465588\n",
            "Best validation loss: 0.7595125225976384\n",
            "Best validation loss: 0.7579359829979698\n",
            "Best validation loss: 0.7563765389886209\n",
            "Best validation loss: 0.7548339004650018\n",
            "Best validation loss: 0.7533077839783796\n",
            "Best validation loss: 0.7517979125442648\n",
            "Best validation loss: 0.7503040154573941\n",
            "Best validation loss: 0.7488258281130498\n",
            "Best validation loss: 0.7473630918344614\n",
            "Best validation loss: 0.7459155537060552\n",
            "Best validation loss: 0.7444829664123184\n",
            "Best validation loss: 0.7430650880820602\n",
            "Best validation loss: 0.7416616821378637\n",
            "Best validation loss: 0.740272517150523\n",
            "Best validation loss: 0.7388973666982805\n",
            "Best validation loss: 0.7375360092306757\n",
            "Best validation loss: 0.7361882279368345\n",
            "Best validation loss: 0.7348538106180276\n",
            "Best validation loss: 0.7335325495643423\n",
            "Best validation loss: 0.7322242414353082\n",
            "Best validation loss: 0.7309286871443342\n",
            "Best validation loss: 0.7296456917468143\n",
            "Best validation loss: 0.7283750643317642\n",
            "Best validation loss: 0.727116617916866\n",
            "Best validation loss: 0.7258701693467884\n",
            "Best validation loss: 0.7246355391946704\n",
            "Best validation loss: 0.7234125516666503\n",
            "Best validation loss: 0.7222010345093322\n",
            "Best validation loss: 0.7210008189200842\n",
            "Best validation loss: 0.7198117394600684\n",
            "Best validation loss: 0.7186336339699052\n",
            "Best validation loss: 0.717466343487879\n",
            "Best validation loss: 0.7163097121705977\n",
            "Best validation loss: 0.7151635872160175\n",
            "Best validation loss: 0.7140278187887547\n",
            "Best validation loss: 0.7129022599476015\n",
            "Best validation loss: 0.7117867665751731\n",
            "Best validation loss: 0.7106811973096119\n",
            "Best validation loss: 0.7095854134782777\n",
            "Best validation loss: 0.7084992790333601\n",
            "Best validation loss: 0.7074226604893425\n",
            "Best validation loss: 0.706355426862263\n",
            "Best validation loss: 0.7052974496107041\n",
            "Best validation loss: 0.7042486025784598\n",
            "Best validation loss: 0.7032087619388234\n",
            "Best validation loss: 0.7021778061404396\n",
            "Best validation loss: 0.7011556158546722\n",
            "Best validation loss: 0.7001420739244391\n",
            "Best validation loss: 0.6991370653144631\n",
            "Best validation loss: 0.6981404770628962\n",
            "Best validation loss: 0.6971521982342717\n",
            "Best validation loss: 0.6961721198737418\n",
            "Best validation loss: 0.6952001349625596\n",
            "Best validation loss: 0.6942361383747656\n",
            "Best validation loss: 0.6932800268350429\n",
            "Best validation loss: 0.6923316988776996\n",
            "Best validation loss: 0.6913910548067498\n",
            "Best validation loss: 0.6904579966570515\n",
            "Best validation loss: 0.689532428156476\n",
            "Best validation loss: 0.6886142546890702\n",
            "Best validation loss: 0.6877033832591861\n",
            "Best validation loss: 0.6867997224565459\n",
            "Best validation loss: 0.6859031824222144\n",
            "Best validation loss: 0.6850136748154508\n",
            "Best validation loss: 0.6841311127814148\n",
            "Best validation loss: 0.6832554109196988\n",
            "Best validation loss: 0.6823864852536642\n",
            "Best validation loss: 0.6815242532005563\n",
            "Best validation loss: 0.6806686335423746\n",
            "Best validation loss: 0.6798195463974764\n",
            "Best validation loss: 0.6789769131928934\n",
            "Best validation loss: 0.6781406566373375\n",
            "Best validation loss: 0.6773107006948789\n",
            "Best validation loss: 0.6764869705592751\n",
            "Best validation loss: 0.675669392628932\n",
            "Best validation loss: 0.6748578944824802\n",
            "Best validation loss: 0.6740524048549467\n",
            "Best validation loss: 0.6732528536145069\n",
            "Best validation loss: 0.6724591717398007\n",
            "Best validation loss: 0.671671291297794\n",
            "Best validation loss: 0.6708891454221728\n",
            "Best validation loss: 0.6701126682922546\n",
            "Best validation loss: 0.6693417951124011\n",
            "Best validation loss: 0.6685764620919205\n",
            "Best validation loss: 0.6678166064254436\n",
            "Best validation loss: 0.667062166273762\n",
            "Best validation loss: 0.6663130807451154\n",
            "Best validation loss: 0.6655692898769159\n",
            "Best validation loss: 0.6648307346178953\n",
            "Best validation loss: 0.6640973568106687\n",
            "Best validation loss: 0.6633690991746979\n",
            "Best validation loss: 0.6626459052896456\n",
            "Best validation loss: 0.6619277195791127\n",
            "Best validation loss: 0.6612144872947437\n",
            "Best validation loss: 0.6605061545006943\n",
            "Best validation loss: 0.6598026680584487\n",
            "Best validation loss: 0.6591039756119804\n",
            "Best validation loss: 0.6584100255732444\n",
            "Best validation loss: 0.6577207671079937\n",
            "Best validation loss: 0.6570361501219117\n",
            "Best validation loss: 0.6563561252470528\n",
            "Best validation loss: 0.6556806438285796\n",
            "Best validation loss: 0.6550096579117953\n",
            "Best validation loss: 0.6543431202294585\n",
            "Best validation loss: 0.6536809841893739\n",
            "Best validation loss: 0.6530232038622547\n",
            "Best validation loss: 0.6523697339698477\n",
            "Best validation loss: 0.651720529873313\n",
            "Best validation loss: 0.6510755475618556\n",
            "Best validation loss: 0.6504347436415993\n",
            "Best validation loss: 0.6497980753246984\n",
            "Best validation loss: 0.649165500418681\n",
            "Best validation loss: 0.6485369773160174\n",
            "Best validation loss: 0.6479124649839105\n",
            "Best validation loss: 0.6472919229542974\n",
            "Best validation loss: 0.6466753113140652\n",
            "Best validation loss: 0.6460625906954665\n",
            "Best validation loss: 0.6454537222667375\n",
            "Best validation loss: 0.6448486677229085\n",
            "Best validation loss: 0.6442473892768057\n",
            "Best validation loss: 0.6436498496502371\n",
            "Best validation loss: 0.6430560120653593\n",
            "Best validation loss: 0.6424658402362211\n",
            "Best validation loss: 0.641879298360479\n",
            "Best validation loss: 0.6412963511112806\n",
            "Best validation loss: 0.6407169636293125\n",
            "Best validation loss: 0.6401411015150078\n",
            "Best validation loss: 0.6395687308209107\n",
            "Best validation loss: 0.638999818044193\n",
            "Best validation loss: 0.6384343301193212\n",
            "Best validation loss: 0.6378722344108672\n",
            "Best validation loss: 0.6373134987064636\n",
            "Best validation loss: 0.6367580912098959\n",
            "Best validation loss: 0.6362059805343325\n",
            "Best validation loss: 0.6356571356956855\n",
            "Best validation loss: 0.6351115261061024\n",
            "Best validation loss: 0.6345691215675847\n",
            "Best validation loss: 0.6340298922657285\n",
            "Best validation loss: 0.6334938087635883\n",
            "Best validation loss: 0.6329608419956577\n",
            "Best validation loss: 0.6324309632619659\n",
            "Best validation loss: 0.6319041442222877\n",
            "Best validation loss: 0.6313803568904635\n",
            "Best validation loss: 0.6308595736288266\n",
            "Best validation loss: 0.6303417671427388\n",
            "Best validation loss: 0.6298269104752253\n",
            "Best validation loss: 0.6293149770017136\n",
            "Best validation loss: 0.6288059404248701\n",
            "Best validation loss: 0.6282997747695338\n",
            "Best validation loss: 0.6277964543777431\n",
            "Best validation loss: 0.6272959539038566\n",
            "Best validation loss: 0.6267982483097635\n",
            "Best validation loss: 0.6263033128601827\n",
            "Best validation loss: 0.6258111231180482\n",
            "Best validation loss: 0.6253216549399793\n",
            "Best validation loss: 0.6248348844718339\n",
            "Best validation loss: 0.6243507881443422\n",
            "Best validation loss: 0.623869342668821\n",
            "Best validation loss: 0.6233905250329641\n",
            "Best validation loss: 0.6229143124967096\n",
            "Best validation loss: 0.6224406825881822\n",
            "Best validation loss: 0.6219696130997067\n",
            "Best validation loss: 0.6215010820838945\n",
            "Best validation loss: 0.6210350678497992\n",
            "Best validation loss: 0.6205715489591401\n",
            "Best validation loss: 0.6201105042225935\n",
            "Best validation loss: 0.6196519126961493\n",
            "Best validation loss: 0.6191957536775313\n",
            "Best validation loss: 0.6187420067026816\n",
            "Best validation loss: 0.6182906515423049\n",
            "Best validation loss: 0.6178416681984743\n",
            "Best validation loss: 0.6173950369012963\n",
            "Best validation loss: 0.616950738105633\n",
            "Best validation loss: 0.6165087524878815\n",
            "Best validation loss: 0.6160690609428088\n",
            "Best validation loss: 0.6156316445804403\n",
            "Best validation loss: 0.6151964847230039\n",
            "Best validation loss: 0.6147635629019237\n",
            "Best validation loss: 0.6143328608548665\n",
            "Best validation loss: 0.6139043605228386\n",
            "Best validation loss: 0.6134780440473306\n",
            "Best validation loss: 0.6130538937675124\n",
            "Best validation loss: 0.6126318922174734\n",
            "Best validation loss: 0.6122120221235101\n",
            "Best validation loss: 0.61179426640146\n",
            "Best validation loss: 0.6113786081540776\n",
            "Best validation loss: 0.6109650306684575\n",
            "Best validation loss: 0.6105535174134965\n",
            "Best validation loss: 0.6101440520374014\n",
            "Best validation loss: 0.6097366183652351\n",
            "Best validation loss: 0.6093312003965049\n",
            "Best validation loss: 0.6089277823027897\n",
            "Best validation loss: 0.6085263484254056\n",
            "Best validation loss: 0.6081268832731115\n",
            "Best validation loss: 0.607729371519849\n",
            "Best validation loss: 0.6073337980025221\n",
            "Best validation loss: 0.606940147718811\n",
            "Best validation loss: 0.6065484058250223\n",
            "Best validation loss: 0.606158557633972\n",
            "Best validation loss: 0.6057705886129051\n",
            "Best validation loss: 0.6053844843814462\n",
            "Best validation loss: 0.6050002307095846\n",
            "Best validation loss: 0.6046178135156904\n",
            "Best validation loss: 0.6042372188645618\n",
            "Best validation loss: 0.6038584329655053\n",
            "Best validation loss: 0.6034814421704437\n",
            "Best validation loss: 0.6031062329720555\n",
            "Best validation loss: 0.6027327920019434\n",
            "Best validation loss: 0.6023611060288314\n",
            "Best validation loss: 0.6019911619567888\n",
            "Best validation loss: 0.6016229468234848\n",
            "Best validation loss: 0.6012564477984667\n",
            "Best validation loss: 0.6008916521814676\n",
            "Best validation loss: 0.6005285474007389\n",
            "Best validation loss: 0.6001671210114091\n",
            "Best validation loss: 0.5998073606938671\n",
            "Best validation loss: 0.5994492542521715\n",
            "Best validation loss: 0.5990927896124825\n",
            "Best validation loss: 0.59873795482152\n",
            "Best validation loss: 0.5983847380450426\n",
            "Best validation loss: 0.5980331275663523\n",
            "Best validation loss: 0.5976831117848198\n",
            "Best validation loss: 0.5973346792144327\n",
            "Best validation loss: 0.5969878184823667\n",
            "Best validation loss: 0.5966425183275758\n",
            "Best validation loss: 0.5962987675994064\n",
            "Best validation loss: 0.59595655525623\n",
            "Best validation loss: 0.595615870364098\n",
            "Best validation loss: 0.5952767020954143\n",
            "Best validation loss: 0.5949390397276301\n",
            "Best validation loss: 0.5946028726419561\n",
            "Best validation loss: 0.594268190322095\n",
            "Best validation loss: 0.5939349823529915\n",
            "Best validation loss: 0.5936032384196013\n",
            "Best validation loss: 0.5932729483056786\n",
            "Best validation loss: 0.5929441018925805\n",
            "Best validation loss: 0.5926166891580886\n",
            "Best validation loss: 0.5922907001752491\n",
            "Best validation loss: 0.5919661251112283\n",
            "Best validation loss: 0.5916429542261853\n",
            "Best validation loss: 0.591321177872161\n",
            "Best validation loss: 0.5910007864919831\n",
            "Best validation loss: 0.5906817706181862\n",
            "Best validation loss: 0.5903641208719482\n",
            "Best validation loss: 0.5900478279620421\n",
            "Best validation loss: 0.5897328826838013\n",
            "Best validation loss: 0.5894192759181006\n",
            "Best validation loss: 0.5891069986303523\n",
            "Best validation loss: 0.5887960418695152\n",
            "Best validation loss: 0.5884863967671182\n",
            "Best validation loss: 0.5881780545362981\n",
            "Best validation loss: 0.5878710064708497\n",
            "Best validation loss: 0.5875652439442902\n",
            "Best validation loss: 0.5872607584089371\n",
            "Best validation loss: 0.586957541394997\n",
            "Best validation loss: 0.58665558450967\n",
            "Best validation loss: 0.586354879436263\n",
            "Best validation loss: 0.5860554179333191\n",
            "Best validation loss: 0.5857571918337552\n",
            "Best validation loss: 0.5854601930440155\n",
            "Best validation loss: 0.5851644135432323\n",
            "Best validation loss: 0.5848698453824026\n",
            "Best validation loss: 0.5845764806835724\n",
            "Best validation loss: 0.584284311639034\n",
            "Best validation loss: 0.5839933305105349\n",
            "Best validation loss: 0.583703529628495\n",
            "Best validation loss: 0.5834149013912365\n",
            "Best validation loss: 0.5831274382642236\n",
            "Best validation loss: 0.5828411327793128\n",
            "Best validation loss: 0.5825559775340119\n",
            "Best validation loss: 0.5822719651907509\n",
            "Best validation loss: 0.5819890884761612\n",
            "Best validation loss: 0.5817073401803654\n",
            "Best validation loss: 0.5814267131562759\n",
            "Best validation loss: 0.5811472003189032\n",
            "Best validation loss: 0.5808687946446732\n",
            "Best validation loss: 0.5805914891707536\n",
            "Best validation loss: 0.5803152769943897\n",
            "Best validation loss: 0.5800401512722476\n",
            "Best validation loss: 0.5797661052197678\n",
            "Best validation loss: 0.5794931321105259\n",
            "Best validation loss: 0.5792212252756027\n",
            "Best validation loss: 0.5789503781029616\n",
            "Best validation loss: 0.5786805840368344\n",
            "Best validation loss: 0.5784118365771163\n",
            "Best validation loss: 0.5781441292787661\n",
            "Best validation loss: 0.5778774557512174\n",
            "Best validation loss: 0.5776118096577947\n",
            "Best validation loss: 0.5773471847151387\n",
            "Best validation loss: 0.5770835746926379\n",
            "Best validation loss: 0.5768209734118689\n",
            "Best validation loss: 0.5765593747460419\n",
            "Best validation loss: 0.5762987726194554\n",
            "Best validation loss: 0.576039161006956\n",
            "Best validation loss: 0.5757805339334067\n",
            "Best validation loss: 0.5755228854731603\n",
            "Best validation loss: 0.5752662097495416\n",
            "Best validation loss: 0.5750105009343335\n",
            "Best validation loss: 0.574755753247272\n",
            "Best validation loss: 0.5745019609555458\n",
            "Best validation loss: 0.5742491183733041\n",
            "Best validation loss: 0.5739972198611677\n",
            "Best validation loss: 0.5737462598257498\n",
            "Best validation loss: 0.5734962327191792\n",
            "Best validation loss: 0.5732471330386324\n",
            "Best validation loss: 0.5729989553258698\n",
            "Best validation loss: 0.572751694166778\n",
            "Best validation loss: 0.5725053441909179\n",
            "Best validation loss: 0.572259900071079\n",
            "Best validation loss: 0.5720153565228372\n",
            "Best validation loss: 0.5717717083041212\n",
            "Best validation loss: 0.5715289502147811\n",
            "Best validation loss: 0.5712870770961638\n",
            "Best validation loss: 0.5710460838306939\n",
            "Best validation loss: 0.5708059653414591\n",
            "Best validation loss: 0.5705667165918001\n",
            "Best validation loss: 0.5703283325849071\n",
            "Best validation loss: 0.5700908083634195\n",
            "Best validation loss: 0.5698541390090318\n",
            "Best validation loss: 0.5696183196421031\n",
            "Best validation loss: 0.569383345421272\n",
            "Best validation loss: 0.5691492115430762\n",
            "Best validation loss: 0.5689159132415763\n",
            "Best validation loss: 0.568683445787984\n",
            "Best validation loss: 0.5684518044902954\n",
            "Best validation loss: 0.5682209846929281\n",
            "Best validation loss: 0.5679909817763625\n",
            "Best validation loss: 0.5677617911567884\n",
            "Best validation loss: 0.5675334082857538\n",
            "Best validation loss: 0.5673058286498205\n",
            "Best validation loss: 0.5670790477702211\n",
            "Best validation loss: 0.5668530612025223\n",
            "Best validation loss: 0.5666278645362902\n",
            "Best validation loss: 0.5664034533947613\n",
            "Best validation loss: 0.5661798234345161\n",
            "Best validation loss: 0.5659569703451571\n",
            "Best validation loss: 0.5657348898489901\n",
            "Best validation loss: 0.56551357770071\n",
            "Best validation loss: 0.5652930296870892\n",
            "Best validation loss: 0.5650732416266705\n",
            "Best validation loss: 0.5648542093694635\n",
            "Best validation loss: 0.5646359287966437\n",
            "Best validation loss: 0.5644183958202557\n",
            "Best validation loss: 0.5642016063829198\n",
            "Best validation loss: 0.5639855564575419\n",
            "Best validation loss: 0.5637702420470269\n",
            "Best validation loss: 0.5635556591839945\n",
            "Best validation loss: 0.5633418039305\n",
            "Best validation loss: 0.5631286723777562\n",
            "Best validation loss: 0.5629162606458599\n",
            "Best validation loss: 0.5627045648835209\n",
            "Best validation loss: 0.5624935812677947\n",
            "Best validation loss: 0.5622833060038169\n",
            "Best validation loss: 0.5620737353245425\n",
            "Best validation loss: 0.5618648654904863\n",
            "Best validation loss: 0.5616566927894668\n",
            "Best validation loss: 0.5614492135363545\n",
            "Best validation loss: 0.56124242407282\n",
            "Best validation loss: 0.561036320767088\n",
            "Best validation loss: 0.5608309000136916\n",
            "Best validation loss: 0.5606261582332313\n",
            "Best validation loss: 0.5604220918721349\n",
            "Best validation loss: 0.5602186974024216\n",
            "Best validation loss: 0.5600159713214676\n",
            "Best validation loss: 0.5598139101517751\n",
            "Best validation loss: 0.5596125104407431\n",
            "Best validation loss: 0.559411768760441\n",
            "Best validation loss: 0.5592116817073854\n",
            "Best validation loss: 0.5590122459023185\n",
            "Best validation loss: 0.5588134579899884\n",
            "Best validation loss: 0.5586153146389332\n",
            "Best validation loss: 0.5584178125412675\n",
            "Best validation loss: 0.558220948412469\n",
            "Best validation loss: 0.55802471899117\n",
            "Best validation loss: 0.5578291210389494\n",
            "Best validation loss: 0.5576341513401291\n",
            "Best validation loss: 0.5574398067015685\n",
            "Best validation loss: 0.5572460839524667\n",
            "Best validation loss: 0.5570529799441625\n",
            "Best validation loss: 0.5568604915499379\n",
            "Best validation loss: 0.5566686156648247\n",
            "Best validation loss: 0.5564773492054116\n",
            "Best validation loss: 0.5562866891096541\n",
            "Best validation loss: 0.5560966323366872\n",
            "Best validation loss: 0.5559071758666384\n",
            "Best validation loss: 0.5557183167004437\n",
            "Best validation loss: 0.5555300518596661\n",
            "Best validation loss: 0.5553423783863143\n",
            "Best validation loss: 0.5551552933426657\n",
            "Best validation loss: 0.5549687938110884\n",
            "Best validation loss: 0.554782876893868\n",
            "Best validation loss: 0.5545975397130336\n",
            "Best validation loss: 0.5544127794101885\n",
            "Best validation loss: 0.5542285931463389\n",
            "Best validation loss: 0.554044978101728\n",
            "Best validation loss: 0.5538619314756703\n",
            "Best validation loss: 0.5536794504863868\n",
            "Best validation loss: 0.5534975323708436\n",
            "Best validation loss: 0.5533161743845911\n",
            "Best validation loss: 0.5531353738016049\n",
            "Best validation loss: 0.552955127914129\n",
            "Best validation loss: 0.5527754340325197\n",
            "Best validation loss: 0.5525962894850918\n",
            "Best validation loss: 0.5524176916179666\n",
            "Best validation loss: 0.55223963779492\n",
            "Best validation loss: 0.5520621253972345\n",
            "Best validation loss: 0.5518851518235505\n",
            "Best validation loss: 0.5517087144897201\n",
            "Best validation loss: 0.5515328108286623\n",
            "Best validation loss: 0.5513574382902208\n",
            "Best validation loss: 0.5511825943410199\n",
            "Best validation loss: 0.5510082764643264\n",
            "Best validation loss: 0.5508344821599093\n",
            "Best validation loss: 0.5506612089439022\n",
            "Best validation loss: 0.5504884543486677\n",
            "Best validation loss: 0.5503162159226622\n",
            "Best validation loss: 0.5501444912303023\n",
            "Best validation loss: 0.549973277851833\n",
            "Best validation loss: 0.5498025733831968\n",
            "Best validation loss: 0.5496323754359037\n",
            "Best validation loss: 0.5494626816369038\n",
            "Best validation loss: 0.5492934896284598\n",
            "Best validation loss: 0.5491247970680216\n",
            "Best validation loss: 0.5489566016281014\n",
            "Best validation loss: 0.5487889009961511\n",
            "Best validation loss: 0.5486216928744402\n",
            "Best validation loss: 0.5484549749799348\n",
            "Best validation loss: 0.5482887450441778\n",
            "Best validation loss: 0.5481230008131713\n",
            "Best validation loss: 0.5479577400472587\n",
            "Best validation loss: 0.5477929605210089\n",
            "Best validation loss: 0.547628660023101\n",
            "Best validation loss: 0.5474648363562111\n",
            "Best validation loss: 0.5473014873368989\n",
            "Best validation loss: 0.5471386107954966\n",
            "Best validation loss: 0.5469762045759983\n",
            "Best validation loss: 0.5468142665359498\n",
            "Best validation loss: 0.5466527945463416\n",
            "Best validation loss: 0.5464917864915005\n",
            "Best validation loss: 0.5463312402689834\n",
            "Best validation loss: 0.5461711537894723\n",
            "Best validation loss: 0.54601152497667\n",
            "Best validation loss: 0.5458523517671969\n",
            "Best validation loss: 0.5456936321104882\n",
            "Best validation loss: 0.5455353639686934\n",
            "Best validation loss: 0.5453775453165755\n",
            "Best validation loss: 0.5452201741414113\n",
            "Best validation loss: 0.5450632484428941\n",
            "Best validation loss: 0.544906766233035\n",
            "Best validation loss: 0.5447507255360673\n",
            "Best validation loss: 0.5445951243883497\n",
            "Best validation loss: 0.5444399608382734\n",
            "Best validation loss: 0.5442852329461663\n",
            "Best validation loss: 0.5441309387842013\n",
            "Best validation loss: 0.5439770764363034\n",
            "Best validation loss: 0.54382364399806\n",
            "Best validation loss: 0.5436706395766284\n",
            "Best validation loss: 0.543518061290648\n",
            "Best validation loss: 0.5433659072701508\n",
            "Best validation loss: 0.543214175656474\n",
            "Best validation loss: 0.5430628646021727\n",
            "Best validation loss: 0.5429119722709339\n",
            "Best validation loss: 0.5427614968374911\n",
            "Best validation loss: 0.5426114364875393\n",
            "Best validation loss: 0.542461789417652\n",
            "Best validation loss: 0.5423125538351968\n",
            "Best validation loss: 0.5421637279582548\n",
            "Best validation loss: 0.5420153100155382\n",
            "Best validation loss: 0.5418672982463091\n",
            "Best validation loss: 0.5417196909003\n",
            "Best validation loss: 0.5415724862376353\n",
            "Best validation loss: 0.541425682528751\n",
            "Best validation loss: 0.5412792780543184\n",
            "Best validation loss: 0.541133271105166\n",
            "Best validation loss: 0.5409876599822038\n",
            "Best validation loss: 0.5408424429963468\n",
            "Best validation loss: 0.5406976184684408\n",
            "Best validation loss: 0.5405531847291876\n",
            "Best validation loss: 0.5404091401190712\n",
            "Best validation loss: 0.5402654829882847\n",
            "Best validation loss: 0.5401222116966589\n",
            "Best validation loss: 0.5399793246135894\n",
            "Best validation loss: 0.5398368201179662\n",
            "Best validation loss: 0.5396946965981031\n",
            "Best validation loss: 0.5395529524516679\n",
            "Best validation loss: 0.5394115860856135\n",
            "Best validation loss: 0.539270595916109\n",
            "Best validation loss: 0.5391299803684722\n",
            "Best validation loss: 0.5389897378771016\n",
            "Best validation loss: 0.5388498668854107\n",
            "Best validation loss: 0.538710365845761\n",
            "Best validation loss: 0.5385712332193967\n",
            "Best validation loss: 0.5384324674763802\n",
            "Best validation loss: 0.538294067095527\n",
            "Best validation loss: 0.5381560305643424\n",
            "Best validation loss: 0.5380183563789581\n",
            "Best validation loss: 0.5378810430440698\n",
            "Best validation loss: 0.5377440890728745\n",
            "Best validation loss: 0.5376074929870093\n",
            "Best validation loss: 0.5374712533164905\n",
            "Best validation loss: 0.5373353685996525\n",
            "Best validation loss: 0.5371998373830884\n",
            "Best validation loss: 0.5370646582215906\n",
            "Best validation loss: 0.5369298296780913\n",
            "Best validation loss: 0.5367953503236047\n",
            "Best validation loss: 0.5366612187371683\n",
            "Best validation loss: 0.5365274335057867\n",
            "Best validation loss: 0.5363939932243732\n",
            "Best validation loss: 0.5362608964956947\n",
            "Best validation loss: 0.5361281419303152\n",
            "Best validation loss: 0.5359957281465401\n",
            "Best validation loss: 0.5358636537703617\n",
            "Best validation loss: 0.5357319174354046\n",
            "Best validation loss: 0.5356005177828717\n",
            "Best validation loss: 0.5354694534614898\n",
            "Best validation loss: 0.5353387231274582\n",
            "Best validation loss: 0.5352083254443947\n",
            "Best validation loss: 0.5350782590832837\n",
            "Best validation loss: 0.5349485227224245\n",
            "Best validation loss: 0.5348191150473803\n",
            "Best validation loss: 0.5346900347509268\n",
            "Best validation loss: 0.534561280533002\n",
            "Best validation loss: 0.5344328511006566\n",
            "Best validation loss: 0.5343047451680034\n",
            "Best validation loss: 0.5341769614561693\n",
            "Best validation loss: 0.5340494986932457\n",
            "Best validation loss: 0.5339223556142402\n",
            "Best validation loss: 0.5337955309610295\n",
            "Best validation loss: 0.5336690234823107\n",
            "Best validation loss: 0.533542831933555\n",
            "Best validation loss: 0.5334169550769606\n",
            "Best validation loss: 0.5332913916814066\n",
            "Best validation loss: 0.5331661405224066\n",
            "Best validation loss: 0.5330412003820637\n",
            "Best validation loss: 0.5329165700490249\n",
            "Best validation loss: 0.5327922483184361\n",
            "Best validation loss: 0.5326682339918984\n",
            "Best validation loss: 0.5325445258774233\n",
            "Best validation loss: 0.5324211227893892\n",
            "Best validation loss: 0.5322980235484981\n",
            "Best validation loss: 0.5321752269817329\n",
            "Best validation loss: 0.5320527319223138\n",
            "Best validation loss: 0.5319305372096576\n",
            "Best validation loss: 0.531808641689334\n",
            "Best validation loss: 0.5316870442130249\n",
            "Best validation loss: 0.5315657436384837\n",
            "Best validation loss: 0.5314447388294932\n",
            "Best validation loss: 0.5313240286558255\n",
            "Best validation loss: 0.5312036119932023\n",
            "Best validation loss: 0.5310834877232543\n",
            "Best validation loss: 0.5309636547334816\n",
            "Best validation loss: 0.5308441119172149\n",
            "Best validation loss: 0.530724858173576\n",
            "Best validation loss: 0.5306058924074397\n",
            "Best validation loss: 0.5304872135293951\n",
            "Best validation loss: 0.5303688204557079\n",
            "Best validation loss: 0.5302507121082819\n",
            "Best validation loss: 0.5301328874146227\n",
            "Best validation loss: 0.5300153453078001\n",
            "Best validation loss: 0.5298980847264111\n",
            "Best validation loss: 0.5297811046145434\n",
            "Best validation loss: 0.5296644039217393\n",
            "Best validation loss: 0.5295479816029606\n",
            "Best validation loss: 0.529431836618551\n",
            "Best validation loss: 0.5293159679342028\n",
            "Best validation loss: 0.5292003745209204\n",
            "Best validation loss: 0.5290850553549866\n",
            "Best validation loss: 0.528970009417927\n",
            "Best validation loss: 0.5288552356964766\n",
            "Best validation loss: 0.5287407331825453\n",
            "Best validation loss: 0.5286265008731846\n",
            "Best validation loss: 0.5285125377705542\n",
            "Best validation loss: 0.5283988428818877\n",
            "Best validation loss: 0.5282854152194616\n",
            "Best validation loss: 0.5281722538005612\n",
            "Best validation loss: 0.5280593576474488\n",
            "Best validation loss: 0.5279467257873312\n",
            "Best validation loss: 0.527834357252328\n",
            "Best validation loss: 0.5277222510794402\n",
            "Best validation loss: 0.5276104063105186\n",
            "Best validation loss: 0.5274988219922323\n",
            "Best validation loss: 0.5273874971760387\n",
            "Best validation loss: 0.5272764309181518\n",
            "Best validation loss: 0.5271656222795122\n",
            "Best validation loss: 0.5270550703257577\n",
            "Best validation loss: 0.5269447741271917\n",
            "Best validation loss: 0.526834732758755\n",
            "Best validation loss: 0.5267249452999955\n",
            "Best validation loss: 0.5266154108350387\n",
            "Best validation loss: 0.5265061284525605\n",
            "Best validation loss: 0.5263970972457551\n",
            "Best validation loss: 0.5262883163123099\n",
            "Best validation loss: 0.5261797847543744\n",
            "Best validation loss: 0.5260715016785341\n",
            "Best validation loss: 0.5259634661957807\n",
            "Best validation loss: 0.5258556774214863\n",
            "Best validation loss: 0.5257481344753739\n",
            "Best validation loss: 0.525640836481492\n",
            "Best validation loss: 0.5255337825681857\n",
            "Best validation loss: 0.5254269718680716\n",
            "Best validation loss: 0.5253204035180096\n",
            "Best validation loss: 0.5252140766590774\n",
            "Best validation loss: 0.5251079904365437\n",
            "Best validation loss: 0.5250021439998419\n",
            "Best validation loss: 0.5248965365025455\n",
            "Best validation loss: 0.5247911671023409\n",
            "Best validation loss: 0.5246860349610023\n",
            "Best validation loss: 0.5245811392443673\n",
            "Best validation loss: 0.5244764791223104\n",
            "Best validation loss: 0.5243720537687194\n",
            "Best validation loss: 0.5242678623614694\n",
            "Best validation loss: 0.5241639040823993\n",
            "Best validation loss: 0.5240601781172869\n",
            "Best validation loss: 0.5239566836558249\n",
            "Best validation loss: 0.5238534198915965\n",
            "Best validation loss: 0.5237503860220526\n",
            "Best validation loss: 0.5236475812484864\n",
            "Best validation loss: 0.523545004776012\n",
            "Best validation loss: 0.5234426558135393\n",
            "Best validation loss: 0.5233405335737522\n",
            "Best validation loss: 0.5232386372730847\n",
            "Best validation loss: 0.5231369661316986\n",
            "Best validation loss: 0.5230355193734609\n",
            "Best validation loss: 0.522934296225921\n",
            "Best validation loss: 0.5228332959202892\n",
            "Best validation loss: 0.522732517691413\n",
            "Best validation loss: 0.5226319607777572\n",
            "Best validation loss: 0.5225316244213805\n",
            "Best validation loss: 0.5224315078679144\n",
            "Convergence reached. Stopping training.\n",
            "Stopping Iteration at step 762 after the training converged\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "y_pred_test = mlr.predict(X_test)\n",
        "predicted_classes = np.argmax(y_pred_test, axis=1)\n",
        "actual_classes_test = np.argmax(y_test, axis=1)\n",
        "actual_classes_train = np.argmax(y_train, axis=1)\n",
        "print('MLR Accuracy: {}'.format(accuracy_score(actual_classes_test, predicted_classes)))\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, actual_classes_train)\n",
        "dt_pred = clf.predict(X_test)\n",
        "print('DT Accuracy: {}'.format(accuracy_score(actual_classes_test, dt_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmLuycMAo88u",
        "outputId": "6d21f40c-2257-4fcb-c460-a4ced413797c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLR Accuracy: 0.7954545454545454\n",
            "DT Accuracy: 0.7386363636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy as a function of the training size"
      ],
      "metadata": {
        "id": "OfUO8Zjctd7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_dict_mlr = {}\n",
        "train_acc_dict_dt = {}\n",
        "\n",
        "for train_size in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]:\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train_selected, twenty_train.target, test_size=1-train_size, random_state=42)\n",
        "  # print(X_train.shape)\n",
        "  # print(y_train.shape)\n",
        "\n",
        "  label_binarizer = LabelBinarizer()\n",
        "  y_train = label_binarizer.fit_transform(y_train)\n",
        "  y_val = label_binarizer.transform(y_val)\n",
        "\n",
        "  D = X_train.shape[1] # number of features\n",
        "  C = y_train.shape[1] # number of classes\n",
        "  mlr = Multinomial_logistic(D, C)\n",
        "  _, _, _ = mlr.fit(X_train, y_train, X_val, y_val, niter=5000)\n",
        "\n",
        "  y_pred_test = mlr.predict(X_test)\n",
        "  predicted_classes = np.argmax(y_pred_test, axis=1)\n",
        "  acc = accuracy_score(actual_classes_test, predicted_classes)\n",
        "\n",
        "  train_acc_dict_mlr[train_size] = acc\n",
        "\n",
        "  actual_classes_train = np.argmax(y_train, axis=1)\n",
        "  clf = DecisionTreeClassifier(random_state=42)\n",
        "  clf.fit(X_train, actual_classes_train)\n",
        "  dt_pred = clf.predict(X_test)\n",
        "  dt_accuracy = accuracy_score(actual_classes_test, dt_pred)\n",
        "\n",
        "  train_acc_dict_dt[train_size] = dt_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhTBuDCQtcFw",
        "outputId": "23dca001-1144-4423-9a2b-4d3103601d5b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Best validation loss: 0.5885703365890169\n",
            "Best validation loss: 0.5883943476677113\n",
            "Best validation loss: 0.5882187659400714\n",
            "Best validation loss: 0.5880435899455911\n",
            "Best validation loss: 0.5878688182308774\n",
            "Best validation loss: 0.5876944493496074\n",
            "Best validation loss: 0.5875204818624846\n",
            "Best validation loss: 0.5873469143371967\n",
            "Best validation loss: 0.5871737453483716\n",
            "Best validation loss: 0.5870009734775371\n",
            "Best validation loss: 0.5868285973130765\n",
            "Best validation loss: 0.5866566154501891\n",
            "Best validation loss: 0.5864850264908474\n",
            "Best validation loss: 0.5863138290437567\n",
            "Best validation loss: 0.5861430217243142\n",
            "Best validation loss: 0.585972603154568\n",
            "Best validation loss: 0.5858025719631778\n",
            "Best validation loss: 0.5856329267853743\n",
            "Best validation loss: 0.5854636662629197\n",
            "Best validation loss: 0.5852947890440687\n",
            "Best validation loss: 0.5851262937835294\n",
            "Best validation loss: 0.5849581791424239\n",
            "Best validation loss: 0.5847904437882508\n",
            "Best validation loss: 0.5846230863948464\n",
            "Best validation loss: 0.5844561056423468\n",
            "Best validation loss: 0.5842895002171506\n",
            "Best validation loss: 0.584123268811881\n",
            "Best validation loss: 0.5839574101253492\n",
            "Best validation loss: 0.5837919228625174\n",
            "Best validation loss: 0.5836268057344617\n",
            "Best validation loss: 0.5834620574583369\n",
            "Best validation loss: 0.5832976767573395\n",
            "Best validation loss: 0.5831336623606719\n",
            "Best validation loss: 0.5829700130035076\n",
            "Best validation loss: 0.5828067274269554\n",
            "Best validation loss: 0.5826438043780241\n",
            "Best validation loss: 0.5824812426095883\n",
            "Best validation loss: 0.5823190408803537\n",
            "Best validation loss: 0.5821571979548222\n",
            "Best validation loss: 0.5819957126032589\n",
            "Best validation loss: 0.5818345836016574\n",
            "Best validation loss: 0.5816738097317066\n",
            "Best validation loss: 0.5815133897807574\n",
            "Best validation loss: 0.5813533225417894\n",
            "Best validation loss: 0.5811936068133781\n",
            "Best validation loss: 0.5810342413996623\n",
            "Best validation loss: 0.5808752251103119\n",
            "Best validation loss: 0.580716556760495\n",
            "Best validation loss: 0.5805582351708473\n",
            "Best validation loss: 0.5804002591674384\n",
            "Best validation loss: 0.5802426275817424\n",
            "Best validation loss: 0.5800853392506049\n",
            "Best validation loss: 0.5799283930162128\n",
            "Best validation loss: 0.5797717877260635\n",
            "Best validation loss: 0.5796155222329338\n",
            "Best validation loss: 0.5794595953948496\n",
            "Best validation loss: 0.5793040060750562\n",
            "Best validation loss: 0.5791487531419872\n",
            "Best validation loss: 0.5789938354692361\n",
            "Best validation loss: 0.5788392519355252\n",
            "Best validation loss: 0.5786850014246775\n",
            "Best validation loss: 0.5785310828255866\n",
            "Best validation loss: 0.5783774950321887\n",
            "Best validation loss: 0.5782242369434323\n",
            "Best validation loss: 0.5780713074632513\n",
            "Best validation loss: 0.5779187055005351\n",
            "Best validation loss: 0.5777664299691019\n",
            "Best validation loss: 0.5776144797876691\n",
            "Best validation loss: 0.5774628538798268\n",
            "Best validation loss: 0.5773115511740095\n",
            "Best validation loss: 0.5771605706034686\n",
            "Best validation loss: 0.5770099111062457\n",
            "Best validation loss: 0.576859571625145\n",
            "Best validation loss: 0.5767095511077065\n",
            "Best validation loss: 0.5765598485061799\n",
            "Best validation loss: 0.5764104627774971\n",
            "Best validation loss: 0.5762613928832463\n",
            "Best validation loss: 0.5761126377896463\n",
            "Best validation loss: 0.5759641964675197\n",
            "Best validation loss: 0.5758160678922681\n",
            "Best validation loss: 0.575668251043845\n",
            "Best validation loss: 0.575520744906732\n",
            "Best validation loss: 0.5753735484699123\n",
            "Best validation loss: 0.575226660726846\n",
            "Best validation loss: 0.5750800806754452\n",
            "Best validation loss: 0.5749338073180493\n",
            "Best validation loss: 0.5747878396613999\n",
            "Best validation loss: 0.5746421767166171\n",
            "Best validation loss: 0.5744968174991745\n",
            "Best validation loss: 0.5743517610288754\n",
            "Best validation loss: 0.574207006329829\n",
            "Best validation loss: 0.574062552430426\n",
            "Best validation loss: 0.5739183983633158\n",
            "Best validation loss: 0.5737745431653821\n",
            "Best validation loss: 0.5736309858777203\n",
            "Best validation loss: 0.573487725545614\n",
            "Best validation loss: 0.573344761218512\n",
            "Best validation loss: 0.573202091950005\n",
            "Best validation loss: 0.5730597167978041\n",
            "Best validation loss: 0.5729176348237168\n",
            "Best validation loss: 0.5727758450936252\n",
            "Best validation loss: 0.5726343466774642\n",
            "Best validation loss: 0.5724931386491983\n",
            "Best validation loss: 0.5723522200868006\n",
            "Best validation loss: 0.5722115900722303\n",
            "Best validation loss: 0.5720712476914115\n",
            "Best validation loss: 0.5719311920342112\n",
            "Best validation loss: 0.5717914221944181\n",
            "Best validation loss: 0.5716519372697213\n",
            "Best validation loss: 0.5715127363616894\n",
            "Best validation loss: 0.5713738185757486\n",
            "Best validation loss: 0.5712351830211634\n",
            "Best validation loss: 0.5710968288110145\n",
            "Best validation loss: 0.5709587550621784\n",
            "Best validation loss: 0.5708209608953078\n",
            "Best validation loss: 0.5706834454348102\n",
            "Best validation loss: 0.5705462078088285\n",
            "Best validation loss: 0.5704092471492205\n",
            "Best validation loss: 0.5702725625915392\n",
            "Best validation loss: 0.5701361532750128\n",
            "Best validation loss: 0.5700000183425251\n",
            "Best validation loss: 0.5698641569405959\n",
            "Best validation loss: 0.5697285682193618\n",
            "Best validation loss: 0.5695932513325568\n",
            "Best validation loss: 0.5694582054374926\n",
            "Best validation loss: 0.5693234296950404\n",
            "Best validation loss: 0.5691889232696115\n",
            "Best validation loss: 0.5690546853291384\n",
            "Best validation loss: 0.5689207150450565\n",
            "Best validation loss: 0.568787011592285\n",
            "Best validation loss: 0.5686535741492089\n",
            "Best validation loss: 0.5685204018976608\n",
            "Best validation loss: 0.5683874940229016\n",
            "Best validation loss: 0.5682548497136038\n",
            "Best validation loss: 0.5681224681618328\n",
            "Best validation loss: 0.5679903485630289\n",
            "Best validation loss: 0.56785849011599\n",
            "Best validation loss: 0.5677268920228534\n",
            "Best validation loss: 0.5675955534890789\n",
            "Best validation loss: 0.5674644737234307\n",
            "Best validation loss: 0.5673336519379607\n",
            "Best validation loss: 0.567203087347991\n",
            "Best validation loss: 0.5670727791720963\n",
            "Best validation loss: 0.5669427266320881\n",
            "Best validation loss: 0.566812928952997\n",
            "Best validation loss: 0.5666833853630554\n",
            "Best validation loss: 0.5665540950936824\n",
            "Best validation loss: 0.5664250573794658\n",
            "Best validation loss: 0.5662962714581461\n",
            "Best validation loss: 0.5661677365706003\n",
            "Best validation loss: 0.5660394519608256\n",
            "Best validation loss: 0.5659114168759231\n",
            "Best validation loss: 0.5657836305660815\n",
            "Best validation loss: 0.5656560922845619\n",
            "Best validation loss: 0.5655288012876813\n",
            "Best validation loss: 0.5654017568347971\n",
            "Best validation loss: 0.5652749581882909\n",
            "Best validation loss: 0.565148404613554\n",
            "Best validation loss: 0.5650220953789707\n",
            "Best validation loss: 0.5648960297559044\n",
            "Best validation loss: 0.5647702070186804\n",
            "Best validation loss: 0.5646446264445725\n",
            "Best validation loss: 0.5645192873137871\n",
            "Best validation loss: 0.5643941889094478\n",
            "Best validation loss: 0.5642693305175819\n",
            "Best validation loss: 0.5641447114271033\n",
            "Best validation loss: 0.5640203309298006\n",
            "Best validation loss: 0.5638961883203202\n",
            "Best validation loss: 0.5637722828961522\n",
            "Best validation loss: 0.5636486139576172\n",
            "Best validation loss: 0.5635251808078504\n",
            "Best validation loss: 0.5634019827527883\n",
            "Best validation loss: 0.5632790191011536\n",
            "Best validation loss: 0.5631562891644425\n",
            "Best validation loss: 0.5630337922569092\n",
            "Best validation loss: 0.5629115276955525\n",
            "Best validation loss: 0.5627894948001027\n",
            "Best validation loss: 0.5626676928930064\n",
            "Best validation loss: 0.5625461212994143\n",
            "Best validation loss: 0.5624247793471665\n",
            "Best validation loss: 0.5623036663667792\n",
            "Best validation loss: 0.562182781691432\n",
            "Best validation loss: 0.5620621246569535\n",
            "Best validation loss: 0.5619416946018084\n",
            "Best validation loss: 0.5618214908670853\n",
            "Best validation loss: 0.5617015127964815\n",
            "Best validation loss: 0.5615817597362922\n",
            "Best validation loss: 0.5614622310353958\n",
            "Best validation loss: 0.561342926045242\n",
            "Best validation loss: 0.5612238441198389\n",
            "Best validation loss: 0.5611049846157397\n",
            "Best validation loss: 0.5609863468920305\n",
            "Best validation loss: 0.560867930310318\n",
            "Best validation loss: 0.5607497342347167\n",
            "Best validation loss: 0.560631758031836\n",
            "Best validation loss: 0.5605140010707683\n",
            "Best validation loss: 0.5603964627230772\n",
            "Best validation loss: 0.5602791423627846\n",
            "Best validation loss: 0.5601620393663586\n",
            "Best validation loss: 0.5600451531127016\n",
            "Best validation loss: 0.5599284829831389\n",
            "Best validation loss: 0.5598120283614055\n",
            "Best validation loss: 0.5596957886336351\n",
            "Best validation loss: 0.5595797631883488\n",
            "Best validation loss: 0.5594639514164418\n",
            "Best validation loss: 0.5593483527111737\n",
            "Best validation loss: 0.5592329664681552\n",
            "Best validation loss: 0.5591177920853381\n",
            "Best validation loss: 0.5590028289630022\n",
            "Best validation loss: 0.5588880765037461\n",
            "Best validation loss: 0.5587735341124734\n",
            "Best validation loss: 0.5586592011963836\n",
            "Best validation loss: 0.5585450771649596\n",
            "Best validation loss: 0.5584311614299574\n",
            "Best validation loss: 0.5583174534053946\n",
            "Best validation loss: 0.5582039525075395\n",
            "Best validation loss: 0.5580906581549001\n",
            "Best validation loss: 0.5579775697682139\n",
            "Best validation loss: 0.5578646867704358\n",
            "Best validation loss: 0.5577520085867291\n",
            "Best validation loss: 0.5576395346444534\n",
            "Best validation loss: 0.5575272643731545\n",
            "Best validation loss: 0.557415197204554\n",
            "Best validation loss: 0.5573033325725393\n",
            "Best validation loss: 0.5571916699131515\n",
            "Best validation loss: 0.557080208664577\n",
            "Best validation loss: 0.5569689482671362\n",
            "Best validation loss: 0.5568578881632733\n",
            "Best validation loss: 0.5567470277975465\n",
            "Best validation loss: 0.5566363666166175\n",
            "Best validation loss: 0.5565259040692414\n",
            "Best validation loss: 0.5564156396062571\n",
            "Best validation loss: 0.5563055726805773\n",
            "Best validation loss: 0.5561957027471784\n",
            "Best validation loss: 0.5560860292630904\n",
            "Best validation loss: 0.5559765516873879\n",
            "Best validation loss: 0.5558672694811797\n",
            "Best validation loss: 0.5557581821075994\n",
            "Best validation loss: 0.5556492890317959\n",
            "Best validation loss: 0.5555405897209237\n",
            "Best validation loss: 0.5554320836441334\n",
            "Best validation loss: 0.5553237702725624\n",
            "Best validation loss: 0.5552156490793251\n",
            "Best validation loss: 0.5551077195395043\n",
            "Best validation loss: 0.5549999811301412\n",
            "Best validation loss: 0.5548924333302264\n",
            "Best validation loss: 0.5547850756206911\n",
            "Best validation loss: 0.5546779074843974\n",
            "Best validation loss: 0.5545709284061298\n",
            "Best validation loss: 0.5544641378725851\n",
            "Best validation loss: 0.5543575353723652\n",
            "Best validation loss: 0.5542511203959665\n",
            "Best validation loss: 0.5541448924357718\n",
            "Best validation loss: 0.5540388509860417\n",
            "Best validation loss: 0.5539329955429051\n",
            "Best validation loss: 0.5538273256043511\n",
            "Best validation loss: 0.55372184067022\n",
            "Best validation loss: 0.5536165402421951\n",
            "Best validation loss: 0.5535114238237934\n",
            "Best validation loss: 0.5534064909203577\n",
            "Best validation loss: 0.553301741039048\n",
            "Best validation loss: 0.5531971736888325\n",
            "Best validation loss: 0.5530927883804804\n",
            "Best validation loss: 0.5529885846265524\n",
            "Best validation loss: 0.5528845619413929\n",
            "Best validation loss: 0.5527807198411219\n",
            "Best validation loss: 0.5526770578436262\n",
            "Best validation loss: 0.5525735754685523\n",
            "Best validation loss: 0.5524702722372972\n",
            "Best validation loss: 0.5523671476730005\n",
            "Best validation loss: 0.5522642013005377\n",
            "Best validation loss: 0.5521614326465099\n",
            "Best validation loss: 0.5520588412392381\n",
            "Best validation loss: 0.551956426608754\n",
            "Best validation loss: 0.5518541882867926\n",
            "Best validation loss: 0.5517521258067842\n",
            "Best validation loss: 0.5516502387038472\n",
            "Best validation loss: 0.5515485265147793\n",
            "Best validation loss: 0.551446988778051\n",
            "Best validation loss: 0.5513456250337972\n",
            "Best validation loss: 0.5512444348238098\n",
            "Best validation loss: 0.5511434176915309\n",
            "Best validation loss: 0.5510425731820433\n",
            "Best validation loss: 0.5509419008420656\n",
            "Best validation loss: 0.5508414002199431\n",
            "Best validation loss: 0.5507410708656406\n",
            "Best validation loss: 0.5506409123307354\n",
            "Convergence reached. Stopping training.\n",
            "Best validation loss: 1.6090881510321673\n",
            "Best validation loss: 1.5944172587560677\n",
            "Best validation loss: 1.5800137603369249\n",
            "Best validation loss: 1.5658789659012837\n",
            "Best validation loss: 1.552013319143015\n",
            "Best validation loss: 1.5384164129974165\n",
            "Best validation loss: 1.5250870276344666\n",
            "Best validation loss: 1.5120231875421275\n",
            "Best validation loss: 1.499222233475241\n",
            "Best validation loss: 1.4866809045330696\n",
            "Best validation loss: 1.4743954255964784\n",
            "Best validation loss: 1.4623615957343907\n",
            "Best validation loss: 1.4505748738649442\n",
            "Best validation loss: 1.4390304587999037\n",
            "Best validation loss: 1.4277233616895717\n",
            "Best validation loss: 1.416648469722156\n",
            "Best validation loss: 1.4058006006508577\n",
            "Best validation loss: 1.395174548290112\n",
            "Best validation loss: 1.3847651195321082\n",
            "Best validation loss: 1.37456716369769\n",
            "Best validation loss: 1.3645755951745953\n",
            "Best validation loss: 1.3547854103381038\n",
            "Best validation loss: 1.3451916997216395\n",
            "Best validation loss: 1.335789656331825\n",
            "Best validation loss: 1.3265745809035943\n",
            "Best validation loss: 1.3175418847811242\n",
            "Best validation loss: 1.3086870910001143\n",
            "Best validation loss: 1.3000058340431253\n",
            "Best validation loss: 1.2914938586462696\n",
            "Best validation loss: 1.2831470179543027\n",
            "Best validation loss: 1.27496127125251\n",
            "Best validation loss: 1.2669326814470891\n",
            "Best validation loss: 1.2590574124199454\n",
            "Best validation loss: 1.2513317263475137\n",
            "Best validation loss: 1.2437519810450453\n",
            "Best validation loss: 1.2363146273762906\n",
            "Best validation loss: 1.229016206752497\n",
            "Best validation loss: 1.221853348732962\n",
            "Best validation loss: 1.2148227687311282\n",
            "Best validation loss: 1.2079212658245972\n",
            "Best validation loss: 1.2011457206638096\n",
            "Best validation loss: 1.1944930934720077\n",
            "Best validation loss: 1.1879604221280335\n",
            "Best validation loss: 1.181544820323225\n",
            "Best validation loss: 1.1752434757838737\n",
            "Best validation loss: 1.1690536485512828\n",
            "Best validation loss: 1.1629726693121951\n",
            "Best validation loss: 1.1569979377732282\n",
            "Best validation loss: 1.1511269210738246\n",
            "Best validation loss: 1.1453571522330876\n",
            "Best validation loss: 1.1396862286266924\n",
            "Best validation loss: 1.1341118104907932\n",
            "Best validation loss: 1.1286316194505202\n",
            "Best validation loss: 1.1232434370712356\n",
            "Best validation loss: 1.1179451034312156\n",
            "Best validation loss: 1.112734515714848\n",
            "Best validation loss: 1.1076096268257822\n",
            "Best validation loss: 1.1025684440197445\n",
            "Best validation loss: 1.0976090275569634\n",
            "Best validation loss: 1.0927294893743111\n",
            "Best validation loss: 1.0879279917773998\n",
            "Best validation loss: 1.0832027461529599\n",
            "Best validation loss: 1.0785520117018785\n",
            "Best validation loss: 1.0739740941933122\n",
            "Best validation loss: 1.069467344740291\n",
            "Best validation loss: 1.0650301585972268\n",
            "Best validation loss: 1.0606609739797122\n",
            "Best validation loss: 1.0563582709069586\n",
            "Best validation loss: 1.0521205700671947\n",
            "Best validation loss: 1.0479464317062848\n",
            "Best validation loss: 1.0438344545397813\n",
            "Best validation loss: 1.039783274688583\n",
            "Best validation loss: 1.035791564638299\n",
            "Best validation loss: 1.0318580322223883\n",
            "Best validation loss: 1.0279814196290737\n",
            "Best validation loss: 1.0241605024319969\n",
            "Best validation loss: 1.0203940886445217\n",
            "Best validation loss: 1.016681017797559\n",
            "Best validation loss: 1.0130201600407354\n",
            "Best validation loss: 1.0094104152667072\n",
            "Best validation loss: 1.0058507122583606\n",
            "Best validation loss: 1.0023400078586389\n",
            "Best validation loss: 0.9988772861626786\n",
            "Best validation loss: 0.9954615577319404\n",
            "Best validation loss: 0.9920918588299716\n",
            "Best validation loss: 0.9887672506794395\n",
            "Best validation loss: 0.9854868187400443\n",
            "Best validation loss: 0.982249672006915\n",
            "Best validation loss: 0.979054942329072\n",
            "Best validation loss: 0.9759017837475389\n",
            "Best validation loss: 0.9727893718526723\n",
            "Best validation loss: 0.9697169031602743\n",
            "Best validation loss: 0.966683594506055\n",
            "Best validation loss: 0.9636886824579953\n",
            "Best validation loss: 0.9607314227461773\n",
            "Best validation loss: 0.9578110897096364\n",
            "Best validation loss: 0.9549269757597988\n",
            "Best validation loss: 0.952078390860066\n",
            "Best validation loss: 0.9492646620211149\n",
            "Best validation loss: 0.946485132811485\n",
            "Best validation loss: 0.9437391628830303\n",
            "Best validation loss: 0.941026127510816\n",
            "Best validation loss: 0.9383454171470551\n",
            "Best validation loss: 0.9356964369886736\n",
            "Best validation loss: 0.9330786065581148\n",
            "Best validation loss: 0.930491359296989\n",
            "Best validation loss: 0.9279341421721897\n",
            "Best validation loss: 0.9254064152941041\n",
            "Best validation loss: 0.9229076515465494\n",
            "Best validation loss: 0.9204373362280872\n",
            "Best validation loss: 0.9179949667043557\n",
            "Best validation loss: 0.9155800520710945\n",
            "Best validation loss: 0.9131921128275217\n",
            "Best validation loss: 0.9108306805597456\n",
            "Best validation loss: 0.908495297633896\n",
            "Best validation loss: 0.9061855168986743\n",
            "Best validation loss: 0.9039009013970192\n",
            "Best validation loss: 0.901641024086607\n",
            "Best validation loss: 0.8994054675689027\n",
            "Best validation loss: 0.8971938238264913\n",
            "Best validation loss: 0.8950056939684258\n",
            "Best validation loss: 0.8928406879833355\n",
            "Best validation loss: 0.8906984245000483\n",
            "Best validation loss: 0.8885785305554837\n",
            "Best validation loss: 0.8864806413695863\n",
            "Best validation loss: 0.8844044001270699\n",
            "Best validation loss: 0.8823494577657579\n",
            "Best validation loss: 0.8803154727713057\n",
            "Best validation loss: 0.8783021109780974\n",
            "Best validation loss: 0.8763090453761231\n",
            "Best validation loss: 0.8743359559236391\n",
            "Best validation loss: 0.87238252936543\n",
            "Best validation loss: 0.8704484590564868\n",
            "Best validation loss: 0.8685334447909303\n",
            "Best validation loss: 0.8666371926360125\n",
            "Best validation loss: 0.8647594147710259\n",
            "Best validation loss: 0.8628998293309726\n",
            "Best validation loss: 0.8610581602548303\n",
            "Best validation loss: 0.8592341371382775\n",
            "Best validation loss: 0.8574274950907258\n",
            "Best validation loss: 0.8556379745965285\n",
            "Best validation loss: 0.853865321380229\n",
            "Best validation loss: 0.8521092862757185\n",
            "Best validation loss: 0.850369625099182\n",
            "Best validation loss: 0.8486460985257089\n",
            "Best validation loss: 0.8469384719694535\n",
            "Best validation loss: 0.8452465154672304\n",
            "Best validation loss: 0.8435700035654402\n",
            "Best validation loss: 0.841908715210214\n",
            "Best validation loss: 0.8402624336406832\n",
            "Best validation loss: 0.8386309462852655\n",
            "Best validation loss: 0.8370140446608837\n",
            "Best validation loss: 0.8354115242750175\n",
            "Best validation loss: 0.8338231845304995\n",
            "Best validation loss: 0.8322488286329762\n",
            "Best validation loss: 0.8306882635009436\n",
            "Best validation loss: 0.8291412996782811\n",
            "Best validation loss: 0.8276077512492049\n",
            "Best validation loss: 0.8260874357555676\n",
            "Best validation loss: 0.8245801741164283\n",
            "Best validation loss: 0.8230857905498263\n",
            "Best validation loss: 0.8216041124966901\n",
            "Best validation loss: 0.8201349705468134\n",
            "Best validation loss: 0.8186781983668395\n",
            "Best validation loss: 0.8172336326301887\n",
            "Best validation loss: 0.8158011129488721\n",
            "Best validation loss: 0.8143804818071321\n",
            "Best validation loss: 0.8129715844968584\n",
            "Best validation loss: 0.8115742690547205\n",
            "Best validation loss: 0.8101883862009708\n",
            "Best validation loss: 0.8088137892798615\n",
            "Best validation loss: 0.8074503342016337\n",
            "Best validation loss: 0.8060978793860284\n",
            "Best validation loss: 0.8047562857072718\n",
            "Best validation loss: 0.8034254164404964\n",
            "Best validation loss: 0.8021051372095528\n",
            "Best validation loss: 0.800795315936168\n",
            "Best validation loss: 0.7994958227904172\n",
            "Best validation loss: 0.7982065301424649\n",
            "Best validation loss: 0.7969273125155405\n",
            "Best validation loss: 0.7956580465401126\n",
            "Best validation loss: 0.7943986109092274\n",
            "Best validation loss: 0.7931488863349739\n",
            "Best validation loss: 0.7919087555060498\n",
            "Best validation loss: 0.7906781030463903\n",
            "Best validation loss: 0.789456815474832\n",
            "Best validation loss: 0.7882447811657838\n",
            "Best validation loss: 0.7870418903108708\n",
            "Best validation loss: 0.7858480348815307\n",
            "Best validation loss: 0.7846631085925294\n",
            "Best validation loss: 0.7834870068663732\n",
            "Best validation loss: 0.7823196267985918\n",
            "Best validation loss: 0.7811608671238656\n",
            "Best validation loss: 0.7800106281829777\n",
            "Best validation loss: 0.7788688118905639\n",
            "Best validation loss: 0.7777353217036399\n",
            "Best validation loss: 0.7766100625908832\n",
            "Best validation loss: 0.7754929410026521\n",
            "Best validation loss: 0.7743838648417166\n",
            "Best validation loss: 0.7732827434346846\n",
            "Best validation loss: 0.7721894875041058\n",
            "Best validation loss: 0.7711040091412293\n",
            "Best validation loss: 0.7700262217794044\n",
            "Best validation loss: 0.7689560401681014\n",
            "Best validation loss: 0.7678933803475371\n",
            "Best validation loss: 0.766838159623891\n",
            "Best validation loss: 0.7657902965450946\n",
            "Best validation loss: 0.7647497108771758\n",
            "Best validation loss: 0.7637163235811502\n",
            "Best validation loss: 0.7626900567904377\n",
            "Best validation loss: 0.761670833788796\n",
            "Best validation loss: 0.760658578988752\n",
            "Best validation loss: 0.7596532179105254\n",
            "Best validation loss: 0.7586546771614222\n",
            "Best validation loss: 0.7576628844156948\n",
            "Best validation loss: 0.7566777683948505\n",
            "Best validation loss: 0.7556992588483985\n",
            "Best validation loss: 0.7547272865350271\n",
            "Best validation loss: 0.7537617832041926\n",
            "Best validation loss: 0.7528026815781189\n",
            "Best validation loss: 0.7518499153341884\n",
            "Best validation loss: 0.7509034190877216\n",
            "Best validation loss: 0.7499631283751275\n",
            "Best validation loss: 0.7490289796374258\n",
            "Best validation loss: 0.7481009102041196\n",
            "Best validation loss: 0.7471788582774215\n",
            "Best validation loss: 0.7462627629168138\n",
            "Best validation loss: 0.7453525640239417\n",
            "Best validation loss: 0.7444482023278283\n",
            "Best validation loss: 0.7435496193704035\n",
            "Best validation loss: 0.7426567574923396\n",
            "Best validation loss: 0.7417695598191849\n",
            "Best validation loss: 0.7408879702477896\n",
            "Best validation loss: 0.7400119334330164\n",
            "Best validation loss: 0.7391413947747271\n",
            "Best validation loss: 0.7382763004050407\n",
            "Best validation loss: 0.7374165971758552\n",
            "Best validation loss: 0.7365622326466282\n",
            "Best validation loss: 0.7357131550724066\n",
            "Best validation loss: 0.7348693133921038\n",
            "Best validation loss: 0.7340306572170157\n",
            "Best validation loss: 0.7331971368195696\n",
            "Best validation loss: 0.7323687031223035\n",
            "Best validation loss: 0.7315453076870654\n",
            "Best validation loss: 0.7307269027044324\n",
            "Best validation loss: 0.7299134409833412\n",
            "Best validation loss: 0.7291048759409259\n",
            "Best validation loss: 0.7283011615925586\n",
            "Best validation loss: 0.7275022525420889\n",
            "Best validation loss: 0.7267081039722736\n",
            "Best validation loss: 0.7259186716353991\n",
            "Best validation loss: 0.7251339118440854\n",
            "Best validation loss: 0.7243537814622704\n",
            "Best validation loss: 0.7235782378963718\n",
            "Best validation loss: 0.7228072390866196\n",
            "Best validation loss: 0.722040743498558\n",
            "Best validation loss: 0.7212787101147087\n",
            "Best validation loss: 0.7205210984263984\n",
            "Best validation loss: 0.719767868425741\n",
            "Best validation loss: 0.7190189805977726\n",
            "Best validation loss: 0.7182743959127389\n",
            "Best validation loss: 0.7175340758185268\n",
            "Best validation loss: 0.7167979822332405\n",
            "Best validation loss: 0.7160660775379164\n",
            "Best validation loss: 0.7153383245693777\n",
            "Best validation loss: 0.7146146866132195\n",
            "Best validation loss: 0.7138951273969284\n",
            "Best validation loss: 0.7131796110831268\n",
            "Best validation loss: 0.7124681022629458\n",
            "Best validation loss: 0.711760565949518\n",
            "Best validation loss: 0.7110569675715931\n",
            "Best validation loss: 0.710357272967267\n",
            "Best validation loss: 0.7096614483778305\n",
            "Best validation loss: 0.7089694604417259\n",
            "Best validation loss: 0.7082812761886165\n",
            "Best validation loss: 0.707596863033562\n",
            "Best validation loss: 0.7069161887713009\n",
            "Best validation loss: 0.7062392215706341\n",
            "Best validation loss: 0.7055659299689105\n",
            "Best validation loss: 0.704896282866611\n",
            "Best validation loss: 0.7042302495220301\n",
            "Best validation loss: 0.7035677995460493\n",
            "Best validation loss: 0.7029089028970079\n",
            "Best validation loss: 0.7022535298756597\n",
            "Best validation loss: 0.701601651120221\n",
            "Best validation loss: 0.7009532376015063\n",
            "Best validation loss: 0.7003082606181473\n",
            "Best validation loss: 0.6996666917918962\n",
            "Best validation loss: 0.6990285030630111\n",
            "Best validation loss: 0.6983936666857212\n",
            "Best validation loss: 0.6977621552237696\n",
            "Best validation loss: 0.6971339415460347\n",
            "Best validation loss: 0.6965089988222252\n",
            "Best validation loss: 0.6958873005186497\n",
            "Best validation loss: 0.6952688203940584\n",
            "Best validation loss: 0.6946535324955556\n",
            "Best validation loss: 0.6940414111545816\n",
            "Best validation loss: 0.6934324309829627\n",
            "Best validation loss: 0.6928265668690274\n",
            "Best validation loss: 0.6922237939737885\n",
            "Best validation loss: 0.6916240877271894\n",
            "Best validation loss: 0.6910274238244121\n",
            "Best validation loss: 0.6904337782222478\n",
            "Best validation loss: 0.6898431271355278\n",
            "Best validation loss: 0.6892554470336123\n",
            "Best validation loss: 0.6886707146369392\n",
            "Best validation loss: 0.688088906913628\n",
            "Best validation loss: 0.6875100010761399\n",
            "Best validation loss: 0.6869339745779938\n",
            "Best validation loss: 0.6863608051105338\n",
            "Best validation loss: 0.6857904705997514\n",
            "Best validation loss: 0.6852229492031581\n",
            "Best validation loss: 0.6846582193067082\n",
            "Best validation loss: 0.684096259521773\n",
            "Best validation loss: 0.6835370486821619\n",
            "Best validation loss: 0.6829805658411918\n",
            "Best validation loss: 0.6824267902688044\n",
            "Best validation loss: 0.6818757014487277\n",
            "Best validation loss: 0.6813272790756845\n",
            "Best validation loss: 0.6807815030526431\n",
            "Best validation loss: 0.6802383534881138\n",
            "Best validation loss: 0.6796978106934863\n",
            "Best validation loss: 0.6791598551804094\n",
            "Best validation loss: 0.6786244676582119\n",
            "Best validation loss: 0.6780916290313644\n",
            "Best validation loss: 0.6775613203969794\n",
            "Best validation loss: 0.6770335230423509\n",
            "Best validation loss: 0.676508218442533\n",
            "Best validation loss: 0.6759853882579543\n",
            "Best validation loss: 0.6754650143320705\n",
            "Best validation loss: 0.6749470786890517\n",
            "Best validation loss: 0.674431563531507\n",
            "Best validation loss: 0.6739184512382419\n",
            "Best validation loss: 0.6734077243620512\n",
            "Best validation loss: 0.6728993656275452\n",
            "Best validation loss: 0.6723933579290082\n",
            "Best validation loss: 0.671889684328291\n",
            "Best validation loss: 0.6713883280527323\n",
            "Best validation loss: 0.6708892724931151\n",
            "Best validation loss: 0.6703925012016508\n",
            "Best validation loss: 0.6698979978899939\n",
            "Best validation loss: 0.6694057464272882\n",
            "Best validation loss: 0.6689157308382397\n",
            "Best validation loss: 0.6684279353012194\n",
            "Best validation loss: 0.667942344146394\n",
            "Best validation loss: 0.6674589418538837\n",
            "Best validation loss: 0.6669777130519485\n",
            "Best validation loss: 0.6664986425151989\n",
            "Best validation loss: 0.6660217151628341\n",
            "Best validation loss: 0.6655469160569069\n",
            "Best validation loss: 0.6650742304006116\n",
            "Best validation loss: 0.6646036435365991\n",
            "Best validation loss: 0.6641351409453136\n",
            "Best validation loss: 0.6636687082433571\n",
            "Best validation loss: 0.6632043311818735\n",
            "Best validation loss: 0.662741995644959\n",
            "Best validation loss: 0.6622816876480935\n",
            "Best validation loss: 0.6618233933365955\n",
            "Best validation loss: 0.6613670989840978\n",
            "Best validation loss: 0.6609127909910467\n",
            "Best validation loss: 0.6604604558832202\n",
            "Best validation loss: 0.6600100803102693\n",
            "Best validation loss: 0.6595616510442788\n",
            "Best validation loss: 0.6591151549783483\n",
            "Best validation loss: 0.6586705791251938\n",
            "Best validation loss: 0.6582279106157681\n",
            "Best validation loss: 0.6577871366979013\n",
            "Best validation loss: 0.6573482447349593\n",
            "Best validation loss: 0.6569112222045219\n",
            "Best validation loss: 0.6564760566970784\n",
            "Best validation loss: 0.656042735914742\n",
            "Best validation loss: 0.6556112476699819\n",
            "Best validation loss: 0.6551815798843716\n",
            "Best validation loss: 0.654753720587356\n",
            "Best validation loss: 0.6543276579150348\n",
            "Best validation loss: 0.6539033801089621\n",
            "Best validation loss: 0.6534808755149627\n",
            "Best validation loss: 0.6530601325819653\n",
            "Best validation loss: 0.6526411398608494\n",
            "Best validation loss: 0.6522238860033103\n",
            "Best validation loss: 0.6518083597607378\n",
            "Best validation loss: 0.6513945499831105\n",
            "Best validation loss: 0.6509824456179049\n",
            "Best validation loss: 0.6505720357090191\n",
            "Best validation loss: 0.6501633093957111\n",
            "Best validation loss: 0.649756255911551\n",
            "Best validation loss: 0.6493508645833871\n",
            "Best validation loss: 0.6489471248303262\n",
            "Best validation loss: 0.6485450261627266\n",
            "Best validation loss: 0.6481445581812051\n",
            "Best validation loss: 0.6477457105756571\n",
            "Best validation loss: 0.6473484731242887\n",
            "Best validation loss: 0.6469528356926629\n",
            "Best validation loss: 0.6465587882327569\n",
            "Best validation loss: 0.6461663207820324\n",
            "Best validation loss: 0.6457754234625184\n",
            "Best validation loss: 0.6453860864799048\n",
            "Best validation loss: 0.6449983001226487\n",
            "Best validation loss: 0.6446120547610924\n",
            "Best validation loss: 0.6442273408465913\n",
            "Best validation loss: 0.6438441489106551\n",
            "Best validation loss: 0.6434624695640984\n",
            "Best validation loss: 0.6430822934962029\n",
            "Best validation loss: 0.6427036114738903\n",
            "Best validation loss: 0.6423264143409059\n",
            "Best validation loss: 0.6419506930170116\n",
            "Best validation loss: 0.6415764384971906\n",
            "Best validation loss: 0.641203641850861\n",
            "Best validation loss: 0.6408322942211003\n",
            "Best validation loss: 0.6404623868238789\n",
            "Best validation loss: 0.6400939109473032\n",
            "Best validation loss: 0.6397268579508691\n",
            "Best validation loss: 0.6393612192647242\n",
            "Best validation loss: 0.6389969863889391\n",
            "Best validation loss: 0.6386341508927881\n",
            "Best validation loss: 0.6382727044140386\n",
            "Best validation loss: 0.6379126386582501\n",
            "Best validation loss: 0.6375539453980803\n",
            "Best validation loss: 0.6371966164726015\n",
            "Best validation loss: 0.6368406437866247\n",
            "Best validation loss: 0.6364860193100313\n",
            "Best validation loss: 0.6361327350771145\n",
            "Best validation loss: 0.6357807831859275\n",
            "Best validation loss: 0.63543015579764\n",
            "Best validation loss: 0.6350808451359035\n",
            "Best validation loss: 0.6347328434862225\n",
            "Best validation loss: 0.6343861431953355\n",
            "Best validation loss: 0.6340407366706011\n",
            "Best validation loss: 0.633696616379394\n",
            "Best validation loss: 0.6333537748485059\n",
            "Best validation loss: 0.633012204663556\n",
            "Best validation loss: 0.6326718984684065\n",
            "Best validation loss: 0.6323328489645866\n",
            "Best validation loss: 0.6319950489107222\n",
            "Best validation loss: 0.6316584911219736\n",
            "Best validation loss: 0.6313231684694786\n",
            "Best validation loss: 0.6309890738798037\n",
            "Best validation loss: 0.6306562003344008\n",
            "Best validation loss: 0.6303245408690702\n",
            "Best validation loss: 0.6299940885734303\n",
            "Best validation loss: 0.6296648365903945\n",
            "Best validation loss: 0.6293367781156521\n",
            "Best validation loss: 0.6290099063971573\n",
            "Best validation loss: 0.6286842147346234\n",
            "Best validation loss: 0.6283596964790225\n",
            "Best validation loss: 0.6280363450320915\n",
            "Best validation loss: 0.6277141538458444\n",
            "Best validation loss: 0.627393116422089\n",
            "Best validation loss: 0.6270732263119503\n",
            "Best validation loss: 0.6267544771153989\n",
            "Best validation loss: 0.626436862480785\n",
            "Best validation loss: 0.6261203761043773\n",
            "Best validation loss: 0.6258050117299085\n",
            "Best validation loss: 0.6254907631481246\n",
            "Best validation loss: 0.6251776241963392\n",
            "Best validation loss: 0.6248655887579952\n",
            "Best validation loss: 0.6245546507622289\n",
            "Best validation loss: 0.6242448041834401\n",
            "Best validation loss: 0.6239360430408669\n",
            "Best validation loss: 0.6236283613981666\n",
            "Best validation loss: 0.6233217533629984\n",
            "Best validation loss: 0.6230162130866144\n",
            "Best validation loss: 0.6227117347634523\n",
            "Best validation loss: 0.6224083126307346\n",
            "Best validation loss: 0.6221059409680706\n",
            "Best validation loss: 0.6218046140970647\n",
            "Best validation loss: 0.6215043263809279\n",
            "Best validation loss: 0.6212050722240937\n",
            "Best validation loss: 0.6209068460718384\n",
            "Best validation loss: 0.6206096424099065\n",
            "Best validation loss: 0.6203134557641383\n",
            "Best validation loss: 0.6200182807001038\n",
            "Best validation loss: 0.6197241118227392\n",
            "Best validation loss: 0.6194309437759876\n",
            "Best validation loss: 0.6191387712424448\n",
            "Best validation loss: 0.6188475889430067\n",
            "Best validation loss: 0.6185573916365236\n",
            "Best validation loss: 0.6182681741194552\n",
            "Best validation loss: 0.6179799312255317\n",
            "Best validation loss: 0.6176926578254176\n",
            "Best validation loss: 0.6174063488263789\n",
            "Best validation loss: 0.6171209991719547\n",
            "Best validation loss: 0.6168366038416319\n",
            "Best validation loss: 0.616553157850523\n",
            "Best validation loss: 0.6162706562490485\n",
            "Best validation loss: 0.6159890941226219\n",
            "Best validation loss: 0.615708466591338\n",
            "Best validation loss: 0.6154287688096651\n",
            "Best validation loss: 0.61514999596614\n",
            "Best validation loss: 0.6148721432830666\n",
            "Best validation loss: 0.6145952060162178\n",
            "Best validation loss: 0.6143191794545397\n",
            "Best validation loss: 0.6140440589198609\n",
            "Best validation loss: 0.613769839766602\n",
            "Best validation loss: 0.613496517381491\n",
            "Best validation loss: 0.6132240871832803\n",
            "Best validation loss: 0.6129525446224661\n",
            "Best validation loss: 0.6126818851810127\n",
            "Best validation loss: 0.6124121043720777\n",
            "Best validation loss: 0.6121431977397415\n",
            "Best validation loss: 0.6118751608587385\n",
            "Best validation loss: 0.6116079893341922\n",
            "Best validation loss: 0.6113416788013524\n",
            "Best validation loss: 0.6110762249253352\n",
            "Best validation loss: 0.6108116234008658\n",
            "Best validation loss: 0.6105478699520238\n",
            "Best validation loss: 0.6102849603319921\n",
            "Best validation loss: 0.6100228903228064\n",
            "Best validation loss: 0.6097616557351099\n",
            "Best validation loss: 0.6095012524079075\n",
            "Best validation loss: 0.6092416762083258\n",
            "Best validation loss: 0.6089829230313728\n",
            "Best validation loss: 0.6087249887997014\n",
            "Best validation loss: 0.6084678694633756\n",
            "Best validation loss: 0.6082115609996382\n",
            "Best validation loss: 0.6079560594126814\n",
            "Best validation loss: 0.6077013607334192\n",
            "Best validation loss: 0.6074474610192632\n",
            "Best validation loss: 0.6071943563538996\n",
            "Best validation loss: 0.6069420428470689\n",
            "Best validation loss: 0.606690516634348\n",
            "Best validation loss: 0.606439773876934\n",
            "Best validation loss: 0.6061898107614304\n",
            "Best validation loss: 0.6059406234996358\n",
            "Best validation loss: 0.6056922083283346\n",
            "Best validation loss: 0.605444561509089\n",
            "Best validation loss: 0.6051976793280341\n",
            "Best validation loss: 0.6049515580956749\n",
            "Best validation loss: 0.6047061941466845\n",
            "Best validation loss: 0.6044615838397056\n",
            "Best validation loss: 0.6042177235571525\n",
            "Best validation loss: 0.6039746097050166\n",
            "Best validation loss: 0.6037322387126723\n",
            "Best validation loss: 0.6034906070326864\n",
            "Best validation loss: 0.6032497111406278\n",
            "Best validation loss: 0.6030095475348801\n",
            "Best validation loss: 0.6027701127364563\n",
            "Best validation loss: 0.602531403288814\n",
            "Best validation loss: 0.6022934157576733\n",
            "Best validation loss: 0.6020561467308371\n",
            "Best validation loss: 0.6018195928180118\n",
            "Best validation loss: 0.6015837506506305\n",
            "Best validation loss: 0.6013486168816781\n",
            "Best validation loss: 0.6011141881855173\n",
            "Best validation loss: 0.6008804612577173\n",
            "Best validation loss: 0.6006474328148835\n",
            "Best validation loss: 0.6004150995944888\n",
            "Best validation loss: 0.6001834583547072\n",
            "Best validation loss: 0.599952505874248\n",
            "Best validation loss: 0.5997222389521925\n",
            "Best validation loss: 0.5994926544078317\n",
            "Best validation loss: 0.599263749080506\n",
            "Best validation loss: 0.599035519829446\n",
            "Best validation loss: 0.5988079635336152\n",
            "Best validation loss: 0.5985810770915537\n",
            "Best validation loss: 0.5983548574212239\n",
            "Best validation loss: 0.5981293014598577\n",
            "Best validation loss: 0.5979044061638044\n",
            "Best validation loss: 0.597680168508381\n",
            "Best validation loss: 0.5974565854877235\n",
            "Best validation loss: 0.5972336541146394\n",
            "Best validation loss: 0.5970113714204615\n",
            "Best validation loss: 0.5967897344549041\n",
            "Best validation loss: 0.5965687402859191\n",
            "Best validation loss: 0.5963483859995545\n",
            "Best validation loss: 0.5961286686998143\n",
            "Best validation loss: 0.5959095855085178\n",
            "Best validation loss: 0.5956911335651638\n",
            "Best validation loss: 0.5954733100267922\n",
            "Best validation loss: 0.5952561120678498\n",
            "Best validation loss: 0.5950395368800556\n",
            "Best validation loss: 0.5948235816722681\n",
            "Best validation loss: 0.5946082436703539\n",
            "Best validation loss: 0.5943935201170572\n",
            "Best validation loss: 0.5941794082718702\n",
            "Best validation loss: 0.5939659054109058\n",
            "Best validation loss: 0.5937530088267706\n",
            "Best validation loss: 0.5935407158284387\n",
            "Best validation loss: 0.5933290237411276\n",
            "Best validation loss: 0.5931179299061755\n",
            "Best validation loss: 0.5929074316809175\n",
            "Best validation loss: 0.5926975264385662\n",
            "Best validation loss: 0.5924882115680904\n",
            "Best validation loss: 0.5922794844740967\n",
            "Best validation loss: 0.5920713425767121\n",
            "Best validation loss: 0.5918637833114663\n",
            "Best validation loss: 0.5916568041291765\n",
            "Best validation loss: 0.591450402495833\n",
            "Best validation loss: 0.5912445758924854\n",
            "Best validation loss: 0.5910393218151291\n",
            "Best validation loss: 0.5908346377745954\n",
            "Best validation loss: 0.5906305212964397\n",
            "Best validation loss: 0.590426969920832\n",
            "Best validation loss: 0.5902239812024488\n",
            "Best validation loss: 0.5900215527103652\n",
            "Best validation loss: 0.589819682027948\n",
            "Best validation loss: 0.5896183667527505\n",
            "Best validation loss: 0.589417604496407\n",
            "Best validation loss: 0.5892173928845298\n",
            "Best validation loss: 0.5890177295566057\n",
            "Best validation loss: 0.588818612165894\n",
            "Best validation loss: 0.5886200383793256\n",
            "Best validation loss: 0.5884220058774028\n",
            "Best validation loss: 0.5882245123540998\n",
            "Best validation loss: 0.5880275555167641\n",
            "Best validation loss: 0.5878311330860198\n",
            "Best validation loss: 0.5876352427956693\n",
            "Best validation loss: 0.587439882392599\n",
            "Best validation loss: 0.5872450496366833\n",
            "Best validation loss: 0.5870507423006907\n",
            "Best validation loss: 0.5868569581701902\n",
            "Best validation loss: 0.5866636950434596\n",
            "Best validation loss: 0.5864709507313919\n",
            "Best validation loss: 0.5862787230574065\n",
            "Best validation loss: 0.5860870098573571\n",
            "Best validation loss: 0.585895808979444\n",
            "Best validation loss: 0.5857051182841244\n",
            "Best validation loss: 0.5855149356440243\n",
            "Best validation loss: 0.5853252589438523\n",
            "Best validation loss: 0.5851360860803133\n",
            "Best validation loss: 0.5849474149620215\n",
            "Best validation loss: 0.5847592435094171\n",
            "Best validation loss: 0.5845715696546813\n",
            "Best validation loss: 0.5843843913416533\n",
            "Best validation loss: 0.5841977065257472\n",
            "Best validation loss: 0.5840115131738701\n",
            "Best validation loss: 0.5838258092643412\n",
            "Best validation loss: 0.5836405927868109\n",
            "Best validation loss: 0.5834558617421802\n",
            "Best validation loss: 0.5832716141425229\n",
            "Best validation loss: 0.5830878480110058\n",
            "Best validation loss: 0.5829045613818113\n",
            "Best validation loss: 0.5827217523000602\n",
            "Best validation loss: 0.5825394188217347\n",
            "Best validation loss: 0.582357559013603\n",
            "Best validation loss: 0.5821761709531433\n",
            "Best validation loss: 0.5819952527284702\n",
            "Best validation loss: 0.5818148024382594\n",
            "Best validation loss: 0.581634818191675\n",
            "Best validation loss: 0.5814552981082963\n",
            "Best validation loss: 0.5812762403180464\n",
            "Best validation loss: 0.5810976429611194\n",
            "Best validation loss: 0.5809195041879104\n",
            "Best validation loss: 0.5807418221589443\n",
            "Best validation loss: 0.5805645950448065\n",
            "Best validation loss: 0.5803878210260738\n",
            "Best validation loss: 0.5802114982932448\n",
            "Best validation loss: 0.5800356250466726\n",
            "Best validation loss: 0.5798601994964975\n",
            "Best validation loss: 0.5796852198625788\n",
            "Best validation loss: 0.5795106843744297\n",
            "Best validation loss: 0.5793365912711503\n",
            "Best validation loss: 0.5791629388013634\n",
            "Best validation loss: 0.5789897252231484\n",
            "Best validation loss: 0.5788169488039777\n",
            "Best validation loss: 0.5786446078206536\n",
            "Best validation loss: 0.5784727005592434\n",
            "Best validation loss: 0.5783012253150183\n",
            "Best validation loss: 0.5781301803923902\n",
            "Best validation loss: 0.5779595641048503\n",
            "Best validation loss: 0.5777893747749081\n",
            "Best validation loss: 0.5776196107340306\n",
            "Best validation loss: 0.5774502703225813\n",
            "Best validation loss: 0.5772813518897619\n",
            "Best validation loss: 0.5771128537935521\n",
            "Best validation loss: 0.5769447744006508\n",
            "Best validation loss: 0.5767771120864182\n",
            "Best validation loss: 0.5766098652348178\n",
            "Best validation loss: 0.5764430322383592\n",
            "Best validation loss: 0.576276611498041\n",
            "Best validation loss: 0.5761106014232938\n",
            "Best validation loss: 0.5759450004319256\n",
            "Best validation loss: 0.5757798069500655\n",
            "Best validation loss: 0.5756150194121078\n",
            "Best validation loss: 0.5754506362606588\n",
            "Best validation loss: 0.5752866559464815\n",
            "Best validation loss: 0.5751230769284421\n",
            "Best validation loss: 0.5749598976734573\n",
            "Best validation loss: 0.57479711665644\n",
            "Best validation loss: 0.574634732360248\n",
            "Best validation loss: 0.574472743275631\n",
            "Best validation loss: 0.57431114790118\n",
            "Best validation loss: 0.5741499447432745\n",
            "Best validation loss: 0.5739891323160329\n",
            "Best validation loss: 0.5738287091412612\n",
            "Best validation loss: 0.5736686737484039\n",
            "Best validation loss: 0.5735090246744927\n",
            "Best validation loss: 0.5733497604640988\n",
            "Best validation loss: 0.5731908796692831\n",
            "Best validation loss: 0.5730323808495478\n",
            "Best validation loss: 0.5728742625717882\n",
            "Best validation loss: 0.5727165234102453\n",
            "Best validation loss: 0.5725591619464574\n",
            "Best validation loss: 0.5724021767692147\n",
            "Best validation loss: 0.5722455664745107\n",
            "Best validation loss: 0.5720893296654974\n",
            "Best validation loss: 0.5719334649524385\n",
            "Best validation loss: 0.5717779709526639\n",
            "Best validation loss: 0.571622846290525\n",
            "Best validation loss: 0.5714680895973487\n",
            "Best validation loss: 0.5713136995113943\n",
            "Best validation loss: 0.5711596746778078\n",
            "Best validation loss: 0.5710060137485788\n",
            "Best validation loss: 0.5708527153824974\n",
            "Best validation loss: 0.5706997782451094\n",
            "Best validation loss: 0.5705472010086756\n",
            "Best validation loss: 0.5703949823521272\n",
            "Best validation loss: 0.5702431209610247\n",
            "Best validation loss: 0.570091615527516\n",
            "Best validation loss: 0.5699404647502945\n",
            "Best validation loss: 0.5697896673345574\n",
            "Best validation loss: 0.5696392219919659\n",
            "Best validation loss: 0.5694891274406034\n",
            "Best validation loss: 0.569339382404936\n",
            "Best validation loss: 0.5691899856157715\n",
            "Best validation loss: 0.5690409358102209\n",
            "Best validation loss: 0.568892231731658\n",
            "Best validation loss: 0.5687438721296807\n",
            "Best validation loss: 0.5685958557600718\n",
            "Best validation loss: 0.5684481813847607\n",
            "Best validation loss: 0.5683008477717856\n",
            "Best validation loss: 0.5681538536952543\n",
            "Best validation loss: 0.5680071979353079\n",
            "Best validation loss: 0.5678608792780822\n",
            "Best validation loss: 0.5677148965156714\n",
            "Best validation loss: 0.5675692484460908\n",
            "Best validation loss: 0.5674239338732405\n",
            "Best validation loss: 0.5672789516068688\n",
            "Best validation loss: 0.5671343004625364\n",
            "Best validation loss: 0.5669899792615803\n",
            "Best validation loss: 0.566845986831079\n",
            "Best validation loss: 0.5667023220038161\n",
            "Best validation loss: 0.5665589836182465\n",
            "Best validation loss: 0.566415970518461\n",
            "Best validation loss: 0.5662732815541519\n",
            "Best validation loss: 0.5661309155805792\n",
            "Best validation loss: 0.5659888714585356\n",
            "Best validation loss: 0.5658471480543144\n",
            "Best validation loss: 0.5657057442396743\n",
            "Best validation loss: 0.5655646588918074\n",
            "Best validation loss: 0.5654238908933059\n",
            "Best validation loss: 0.5652834391321289\n",
            "Best validation loss: 0.5651433025015703\n",
            "Best validation loss: 0.5650034799002268\n",
            "Best validation loss: 0.5648639702319653\n",
            "Best validation loss: 0.5647247724058915\n",
            "Best validation loss: 0.5645858853363183\n",
            "Best validation loss: 0.5644473079427337\n",
            "Best validation loss: 0.5643090391497713\n",
            "Best validation loss: 0.5641710778871776\n",
            "Best validation loss: 0.5640334230897828\n",
            "Best validation loss: 0.5638960736974694\n",
            "Best validation loss: 0.5637590286551424\n",
            "Best validation loss: 0.5636222869126994\n",
            "Best validation loss: 0.5634858474250003\n",
            "Best validation loss: 0.5633497091518382\n",
            "Best validation loss: 0.5632138710579102\n",
            "Best validation loss: 0.5630783321127873\n",
            "Best validation loss: 0.5629430912908865\n",
            "Best validation loss: 0.5628081475714416\n",
            "Best validation loss: 0.5626734999384746\n",
            "Best validation loss: 0.5625391473807676\n",
            "Best validation loss: 0.5624050888918344\n",
            "Best validation loss: 0.562271323469893\n",
            "Best validation loss: 0.5621378501178372\n",
            "Best validation loss: 0.5620046678432098\n",
            "Best validation loss: 0.5618717756581747\n",
            "Best validation loss: 0.5617391725794901\n",
            "Best validation loss: 0.5616068576284814\n",
            "Best validation loss: 0.5614748298310144\n",
            "Best validation loss: 0.5613430882174687\n",
            "Best validation loss: 0.5612116318227116\n",
            "Best validation loss: 0.5610804596860716\n",
            "Best validation loss: 0.5609495708513124\n",
            "Best validation loss: 0.560818964366607\n",
            "Best validation loss: 0.5606886392845127\n",
            "Best validation loss: 0.5605585946619446\n",
            "Best validation loss: 0.5604288295601509\n",
            "Best validation loss: 0.560299343044688\n",
            "Best validation loss: 0.5601701341853947\n",
            "Best validation loss: 0.5600412020563685\n",
            "Best validation loss: 0.5599125457359403\n",
            "Best validation loss: 0.5597841643066498\n",
            "Best validation loss: 0.559656056855222\n",
            "Best validation loss: 0.5595282224725425\n",
            "Best validation loss: 0.5594006602536341\n",
            "Best validation loss: 0.5592733692976322\n",
            "Best validation loss: 0.5591463487077619\n",
            "Best validation loss: 0.5590195975913147\n",
            "Best validation loss: 0.5588931150596244\n",
            "Best validation loss: 0.558766900228045\n",
            "Best validation loss: 0.5586409522159268\n",
            "Best validation loss: 0.5585152701465945\n",
            "Best validation loss: 0.5583898531473237\n",
            "Best validation loss: 0.5582647003493192\n",
            "Best validation loss: 0.5581398108876922\n",
            "Best validation loss: 0.5580151839014381\n",
            "Best validation loss: 0.5578908185334148\n",
            "Best validation loss: 0.5577667139303206\n",
            "Best validation loss: 0.5576428692426725\n",
            "Best validation loss: 0.5575192836247845\n",
            "Best validation loss: 0.5573959562347468\n",
            "Best validation loss: 0.5572728862344033\n",
            "Best validation loss: 0.5571500727893318\n",
            "Best validation loss: 0.5570275150688222\n",
            "Best validation loss: 0.5569052122458557\n",
            "Best validation loss: 0.5567831634970843\n",
            "Best validation loss: 0.5566613680028102\n",
            "Best validation loss: 0.5565398249469655\n",
            "Best validation loss: 0.5564185335170914\n",
            "Best validation loss: 0.5562974929043184\n",
            "Best validation loss: 0.5561767023033463\n",
            "Best validation loss: 0.5560561609124243\n",
            "Best validation loss: 0.5559358679333312\n",
            "Best validation loss: 0.5558158225713555\n",
            "Best validation loss: 0.5556960240352766\n",
            "Best validation loss: 0.5555764715373446\n",
            "Best validation loss: 0.5554571642932614\n",
            "Best validation loss: 0.5553381015221623\n",
            "Best validation loss: 0.5552192824465956\n",
            "Best validation loss: 0.5551007062925051\n",
            "Best validation loss: 0.5549823722892103\n",
            "Best validation loss: 0.5548642796693888\n",
            "Best validation loss: 0.5547464276690569\n",
            "Best validation loss: 0.5546288155275519\n",
            "Best validation loss: 0.5545114424875136\n",
            "Best validation loss: 0.554394307794866\n",
            "Best validation loss: 0.5542774106987997\n",
            "Best validation loss: 0.5541607504517538\n",
            "Best validation loss: 0.5540443263093981\n",
            "Best validation loss: 0.5539281375306158\n",
            "Best validation loss: 0.5538121833774852\n",
            "Best validation loss: 0.5536964631152631\n",
            "Best validation loss: 0.5535809760123672\n",
            "Best validation loss: 0.5534657213403589\n",
            "Best validation loss: 0.5533506983739263\n",
            "Best validation loss: 0.5532359063908666\n",
            "Best validation loss: 0.5531213446720709\n",
            "Best validation loss: 0.5530070125015051\n",
            "Best validation loss: 0.5528929091661956\n",
            "Best validation loss: 0.5527790339562113\n",
            "Best validation loss: 0.5526653861646477\n",
            "Best validation loss: 0.5525519650876102\n",
            "Best validation loss: 0.5524387700241985\n",
            "Best validation loss: 0.5523258002764901\n",
            "Best validation loss: 0.5522130551495248\n",
            "Best validation loss: 0.5521005339512877\n",
            "Best validation loss: 0.5519882359926945\n",
            "Best validation loss: 0.5518761605875755\n",
            "Best validation loss: 0.55176430705266\n",
            "Best validation loss: 0.5516526747075604\n",
            "Best validation loss: 0.5515412628747576\n",
            "Best validation loss: 0.551430070879585\n",
            "Best validation loss: 0.5513190980502137\n",
            "Best validation loss: 0.5512083437176375\n",
            "Best validation loss: 0.5510978072156576\n",
            "Best validation loss: 0.5509874878808675\n",
            "Best validation loss: 0.5508773850526391\n",
            "Best validation loss: 0.550767498073107\n",
            "Best validation loss: 0.5506578262871542\n",
            "Best validation loss: 0.5505483690423983\n",
            "Best validation loss: 0.5504391256891756\n",
            "Best validation loss: 0.5503300955805284\n",
            "Best validation loss: 0.5502212780721893\n",
            "Best validation loss: 0.5501126725225679\n",
            "Best validation loss: 0.550004278292737\n",
            "Best validation loss: 0.5498960947464173\n",
            "Best validation loss: 0.549788121249965\n",
            "Best validation loss: 0.5496803571723572\n",
            "Best validation loss: 0.549572801885178\n",
            "Best validation loss: 0.549465454762606\n",
            "Best validation loss: 0.5493583151813991\n",
            "Best validation loss: 0.5492513825208823\n",
            "Best validation loss: 0.549144656162934\n",
            "Best validation loss: 0.5490381354919726\n",
            "Best validation loss: 0.5489318198949432\n",
            "Best validation loss: 0.5488257087613049\n",
            "Best validation loss: 0.5487198014830169\n",
            "Best validation loss: 0.5486140974545268\n",
            "Best validation loss: 0.5485085960727567\n",
            "Best validation loss: 0.5484032967370908\n",
            "Best validation loss: 0.5482981988493622\n",
            "Best validation loss: 0.5481933018138413\n",
            "Best validation loss: 0.5480886050372223\n",
            "Best validation loss: 0.5479841079286111\n",
            "Best validation loss: 0.5478798098995127\n",
            "Best validation loss: 0.547775710363819\n",
            "Best validation loss: 0.5476718087377966\n",
            "Best validation loss: 0.5475681044400746\n",
            "Best validation loss: 0.5474645968916325\n",
            "Best validation loss: 0.5473612855157878\n",
            "Best validation loss: 0.5472581697381844\n",
            "Best validation loss: 0.5471552489867814\n",
            "Best validation loss: 0.5470525226918396\n",
            "Best validation loss: 0.5469499902859112\n",
            "Best validation loss: 0.5468476512038277\n",
            "Best validation loss: 0.5467455048826885\n",
            "Best validation loss: 0.5466435507618485\n",
            "Best validation loss: 0.546541788282908\n",
            "Best validation loss: 0.5464402168897002\n",
            "Best validation loss: 0.5463388360282807\n",
            "Best validation loss: 0.5462376451469155\n",
            "Best validation loss: 0.5461366436960704\n",
            "Best validation loss: 0.5460358311283996\n",
            "Best validation loss: 0.5459352068987351\n",
            "Best validation loss: 0.5458347704640749\n",
            "Best validation loss: 0.5457345212835729\n",
            "Best validation loss: 0.5456344588185279\n",
            "Convergence reached. Stopping training.\n",
            "Best validation loss: 1.6299269931233153\n",
            "Best validation loss: 1.6116452698657677\n",
            "Best validation loss: 1.5937338791978515\n",
            "Best validation loss: 1.576198747419828\n",
            "Best validation loss: 1.5590441114597942\n",
            "Best validation loss: 1.5422724303781699\n",
            "Best validation loss: 1.5258843691622512\n",
            "Best validation loss: 1.5098788537811227\n",
            "Best validation loss: 1.4942531894629765\n",
            "Best validation loss: 1.4790032287007755\n",
            "Best validation loss: 1.4641235723703274\n",
            "Best validation loss: 1.4496077867582589\n",
            "Best validation loss: 1.4354486209052135\n",
            "Best validation loss: 1.4216382117759387\n",
            "Best validation loss: 1.4081682685643706\n",
            "Best validation loss: 1.3950302312139677\n",
            "Best validation loss: 1.3822154014814965\n",
            "Best validation loss: 1.3697150473396509\n",
            "Best validation loss: 1.357520483150065\n",
            "Best validation loss: 1.3456231289323901\n",
            "Best validation loss: 1.3340145523662805\n",
            "Best validation loss: 1.3226864970670424\n",
            "Best validation loss: 1.3116309003310183\n",
            "Best validation loss: 1.3008399030789946\n",
            "Best validation loss: 1.2903058542244963\n",
            "Best validation loss: 1.2800213112156276\n",
            "Best validation loss: 1.2699790380756344\n",
            "Best validation loss: 1.2601720019119171\n",
            "Best validation loss: 1.250593368577319\n",
            "Best validation loss: 1.2412364979458304\n",
            "Best validation loss: 1.232094939098714\n",
            "Best validation loss: 1.223162425596704\n",
            "Best validation loss: 1.2144328709298955\n",
            "Best validation loss: 1.2059003641807697\n",
            "Best validation loss: 1.197559165900364\n",
            "Best validation loss: 1.1894037041770689\n",
            "Best validation loss: 1.1814285708674008\n",
            "Best validation loss: 1.1736285179549022\n",
            "Best validation loss: 1.1659984540044683\n",
            "Best validation loss: 1.1585334406831127\n",
            "Best validation loss: 1.1512286893231125\n",
            "Best validation loss: 1.1440795575088192\n",
            "Best validation loss: 1.1370815456736139\n",
            "Best validation loss: 1.1302302936981852\n",
            "Best validation loss: 1.1235215775053984\n",
            "Best validation loss: 1.1169513056503908\n",
            "Best validation loss: 1.1105155159072144\n",
            "Best validation loss: 1.104210371855408\n",
            "Best validation loss: 1.0980321594713267\n",
            "Best validation loss: 1.0919772837300843\n",
            "Best validation loss: 1.0860422652244968\n",
            "Best validation loss: 1.0802237368077048\n",
            "Best validation loss: 1.0745184402661123\n",
            "Best validation loss: 1.0689232230290788\n",
            "Best validation loss: 1.063435034921439\n",
            "Best validation loss: 1.0580509249644687\n",
            "Best validation loss: 1.0527680382303803\n",
            "Best validation loss: 1.04758361275487\n",
            "Best validation loss: 1.0424949765116587\n",
            "Best validation loss: 1.0374995444523891\n",
            "Best validation loss: 1.0325948156146822\n",
            "Best validation loss: 1.0277783703006171\n",
            "Best validation loss: 1.0230478673273966\n",
            "Best validation loss: 1.01840104135149\n",
            "Best validation loss: 1.0138357002671137\n",
            "Best validation loss: 1.009349722679528\n",
            "Best validation loss: 1.0049410554532578\n",
            "Best validation loss: 1.0006077113350524\n",
            "Best validation loss: 0.9963477666511068\n",
            "Best validation loss: 0.9921593590778323\n",
            "Best validation loss: 0.9880406854852432\n",
            "Best validation loss: 0.9839899998518611\n",
            "Best validation loss: 0.9800056112498667\n",
            "Best validation loss: 0.9760858818991134\n",
            "Best validation loss: 0.9722292252885024\n",
            "Best validation loss: 0.9684341043631439\n",
            "Best validation loss: 0.9646990297756449\n",
            "Best validation loss: 0.9610225581998334\n",
            "Best validation loss: 0.9574032907051726\n",
            "Best validation loss: 0.9538398711901145\n",
            "Best validation loss: 0.9503309848726096\n",
            "Best validation loss: 0.9468753568360048\n",
            "Best validation loss: 0.943471750628556\n",
            "Best validation loss: 0.940118966914799\n",
            "Best validation loss: 0.9368158421770396\n",
            "Best validation loss: 0.9335612474652546\n",
            "Best validation loss: 0.930354087193713\n",
            "Best validation loss: 0.9271932979826721\n",
            "Best validation loss: 0.9240778475435287\n",
            "Best validation loss: 0.921006733605851\n",
            "Best validation loss: 0.9179789828847511\n",
            "Best validation loss: 0.9149936500871027\n",
            "Best validation loss: 0.9120498169551534\n",
            "Best validation loss: 0.9091465913461164\n",
            "Best validation loss: 0.906283106346376\n",
            "Best validation loss: 0.9034585194189872\n",
            "Best validation loss: 0.9006720115831826\n",
            "Best validation loss: 0.8979227866246574\n",
            "Best validation loss: 0.8952100703354314\n",
            "Best validation loss: 0.8925331097821406\n",
            "Best validation loss: 0.8898911726016444\n",
            "Best validation loss: 0.8872835463228818\n",
            "Best validation loss: 0.8847095377139415\n",
            "Best validation loss: 0.8821684721533573\n",
            "Best validation loss: 0.8796596930246718\n",
            "Best validation loss: 0.8771825611333528\n",
            "Best validation loss: 0.8747364541451773\n",
            "Best validation loss: 0.8723207660452384\n",
            "Best validation loss: 0.8699349066167565\n",
            "Best validation loss: 0.8675783009389162\n",
            "Best validation loss: 0.8652503889029738\n",
            "Best validation loss: 0.8629506247459189\n",
            "Best validation loss: 0.860678476600992\n",
            "Best validation loss: 0.8584334260643983\n",
            "Best validation loss: 0.8562149677775799\n",
            "Best validation loss: 0.8540226090244304\n",
            "Best validation loss: 0.8518558693428725\n",
            "Best validation loss: 0.8497142801502292\n",
            "Best validation loss: 0.8475973843818531\n",
            "Best validation loss: 0.845504736142495\n",
            "Best validation loss: 0.8434359003699156\n",
            "Best validation loss: 0.8413904525102623\n",
            "Best validation loss: 0.8393679782047606\n",
            "Best validation loss: 0.8373680729872747\n",
            "Best validation loss: 0.8353903419923238\n",
            "Best validation loss: 0.8334343996731489\n",
            "Best validation loss: 0.831499869529445\n",
            "Best validation loss: 0.829586383844388\n",
            "Best validation loss: 0.8276935834306031\n",
            "Best validation loss: 0.8258211173847325\n",
            "Best validation loss: 0.823968642850277\n",
            "Best validation loss: 0.8221358247883996\n",
            "Best validation loss: 0.8203223357563881\n",
            "Best validation loss: 0.8185278556934923\n",
            "Best validation loss: 0.8167520717138558\n",
            "Best validation loss: 0.8149946779062812\n",
            "Best validation loss: 0.8132553751405723\n",
            "Best validation loss: 0.8115338708802085\n",
            "Best validation loss: 0.8098298790011194\n",
            "Best validation loss: 0.8081431196163351\n",
            "Best validation loss: 0.806473318906294\n",
            "Best validation loss: 0.8048202089546043\n",
            "Best validation loss: 0.8031835275890565\n",
            "Best validation loss: 0.8015630182277037\n",
            "Best validation loss: 0.7999584297298158\n",
            "Best validation loss: 0.7983695162515417\n",
            "Best validation loss: 0.7967960371061058\n",
            "Best validation loss: 0.7952377566283759\n",
            "Best validation loss: 0.7936944440436501\n",
            "Best validation loss: 0.7921658733405101\n",
            "Best validation loss: 0.7906518231475985\n",
            "Best validation loss: 0.7891520766141816\n",
            "Best validation loss: 0.7876664212943643\n",
            "Best validation loss: 0.786194649034831\n",
            "Best validation loss: 0.7847365558659887\n",
            "Best validation loss: 0.7832919418963933\n",
            "Best validation loss: 0.7818606112103479\n",
            "Best validation loss: 0.7804423717685639\n",
            "Best validation loss: 0.7790370353117765\n",
            "Best validation loss: 0.7776444172672186\n",
            "Best validation loss: 0.7762643366578524\n",
            "Best validation loss: 0.7748966160142681\n",
            "Best validation loss: 0.7735410812891554\n",
            "Best validation loss: 0.772197561774268\n",
            "Best validation loss: 0.770865890019791\n",
            "Best validation loss: 0.7695459017560357\n",
            "Best validation loss: 0.768237435817382\n",
            "Best validation loss: 0.7669403340683967\n",
            "Best validation loss: 0.7656544413320528\n",
            "Best validation loss: 0.7643796053199834\n",
            "Best validation loss: 0.7631156765647037\n",
            "Best validation loss: 0.7618625083537354\n",
            "Best validation loss: 0.7606199566655727\n",
            "Best validation loss: 0.7593878801074327\n",
            "Best validation loss: 0.7581661398547289\n",
            "Best validation loss: 0.7569545995922152\n",
            "Best validation loss: 0.755753125456747\n",
            "Best validation loss: 0.7545615859816083\n",
            "Best validation loss: 0.7533798520423534\n",
            "Best validation loss: 0.7522077968041172\n",
            "Best validation loss: 0.7510452956703496\n",
            "Best validation loss: 0.7498922262329245\n",
            "Best validation loss: 0.7487484682235868\n",
            "Best validation loss: 0.7476139034666907\n",
            "Best validation loss: 0.7464884158331935\n",
            "Best validation loss: 0.7453718911958637\n",
            "Best validation loss: 0.7442642173856671\n",
            "Best validation loss: 0.7431652841492948\n",
            "Best validation loss: 0.7420749831077997\n",
            "Best validation loss: 0.740993207716306\n",
            "Best validation loss: 0.73991985322476\n",
            "Best validation loss: 0.7388548166396931\n",
            "Best validation loss: 0.7377979966869624\n",
            "Best validation loss: 0.7367492937754444\n",
            "Best validation loss: 0.7357086099616507\n",
            "Best validation loss: 0.7346758489152394\n",
            "Best validation loss: 0.733650915885396\n",
            "Best validation loss: 0.7326337176680572\n",
            "Best validation loss: 0.7316241625739555\n",
            "Best validation loss: 0.7306221603974575\n",
            "Best validation loss: 0.7296276223861761\n",
            "Best validation loss: 0.7286404612113319\n",
            "Best validation loss: 0.7276605909388427\n",
            "Best validation loss: 0.726687927001123\n",
            "Best validation loss: 0.7257223861695672\n",
            "Best validation loss: 0.7247638865277057\n",
            "Best validation loss: 0.723812347445006\n",
            "Best validation loss: 0.7228676895513066\n",
            "Best validation loss: 0.7219298347118651\n",
            "Best validation loss: 0.720998706002999\n",
            "Best validation loss: 0.7200742276883104\n",
            "Best validation loss: 0.7191563251954706\n",
            "Best validation loss: 0.7182449250935538\n",
            "Best validation loss: 0.7173399550709044\n",
            "Best validation loss: 0.7164413439135213\n",
            "Best validation loss: 0.7155490214839471\n",
            "Best validation loss: 0.7146629187006459\n",
            "Best validation loss: 0.7137829675178613\n",
            "Best validation loss: 0.7129091009059355\n",
            "Best validation loss: 0.7120412528320821\n",
            "Best validation loss: 0.711179358241597\n",
            "Best validation loss: 0.7103233530394983\n",
            "Best validation loss: 0.7094731740725829\n",
            "Best validation loss: 0.7086287591118867\n",
            "Best validation loss: 0.7077900468355419\n",
            "Best validation loss: 0.7069569768120169\n",
            "Best validation loss: 0.7061294894837318\n",
            "Best validation loss: 0.7053075261510375\n",
            "Best validation loss: 0.7044910289565505\n",
            "Best validation loss: 0.703679940869833\n",
            "Best validation loss: 0.7028742056724107\n",
            "Best validation loss: 0.7020737679431177\n",
            "Best validation loss: 0.7012785730437634\n",
            "Best validation loss: 0.7004885671051082\n",
            "Best validation loss: 0.6997036970131463\n",
            "Best validation loss: 0.6989239103956806\n",
            "Best validation loss: 0.6981491556091907\n",
            "Best validation loss: 0.6973793817259789\n",
            "Best validation loss: 0.6966145385215917\n",
            "Best validation loss: 0.6958545764625087\n",
            "Best validation loss: 0.6950994466940931\n",
            "Best validation loss: 0.6943491010287941\n",
            "Best validation loss: 0.693603491934601\n",
            "Best validation loss: 0.6928625725237358\n",
            "Best validation loss: 0.6921262965415838\n",
            "Best validation loss: 0.6913946183558539\n",
            "Best validation loss: 0.6906674929459634\n",
            "Best validation loss: 0.689944875892643\n",
            "Best validation loss: 0.6892267233677533\n",
            "Best validation loss: 0.6885129921243128\n",
            "Best validation loss: 0.6878036394867287\n",
            "Best validation loss: 0.6870986233412246\n",
            "Best validation loss: 0.6863979021264658\n",
            "Best validation loss: 0.6857014348243711\n",
            "Best validation loss: 0.6850091809511112\n",
            "Best validation loss: 0.6843211005482875\n",
            "Best validation loss: 0.6836371541742868\n",
            "Best validation loss: 0.682957302895809\n",
            "Best validation loss: 0.6822815082795614\n",
            "Best validation loss: 0.6816097323841206\n",
            "Best validation loss: 0.6809419377519506\n",
            "Best validation loss: 0.6802780874015819\n",
            "Best validation loss: 0.6796181448199405\n",
            "Best validation loss: 0.6789620739548287\n",
            "Best validation loss: 0.678309839207551\n",
            "Best validation loss: 0.6776614054256828\n",
            "Best validation loss: 0.6770167378959803\n",
            "Best validation loss: 0.6763758023374253\n",
            "Best validation loss: 0.6757385648944038\n",
            "Best validation loss: 0.675104992130015\n",
            "Best validation loss: 0.6744750510195087\n",
            "Best validation loss: 0.6738487089438459\n",
            "Best validation loss: 0.6732259336833823\n",
            "Best validation loss: 0.6726066934116703\n",
            "Best validation loss: 0.6719909566893785\n",
            "Best validation loss: 0.6713786924583236\n",
            "Best validation loss: 0.6707698700356153\n",
            "Best validation loss: 0.670164459107909\n",
            "Best validation loss: 0.6695624297257667\n",
            "Best validation loss: 0.6689637522981198\n",
            "Best validation loss: 0.668368397586836\n",
            "Best validation loss: 0.6677763367013853\n",
            "Best validation loss: 0.667187541093604\n",
            "Best validation loss: 0.6666019825525531\n",
            "Best validation loss: 0.6660196331994721\n",
            "Best validation loss: 0.6654404654828224\n",
            "Best validation loss: 0.664864452173422\n",
            "Best validation loss: 0.6642915663596669\n",
            "Best validation loss: 0.663721781442838\n",
            "Best validation loss: 0.6631550711324935\n",
            "Best validation loss: 0.6625914094419414\n",
            "Best validation loss: 0.6620307706837948\n",
            "Best validation loss: 0.6614731294656045\n",
            "Best validation loss: 0.6609184606855694\n",
            "Best validation loss: 0.660366739528322\n",
            "Best validation loss: 0.6598179414607888\n",
            "Best validation loss: 0.6592720422281225\n",
            "Best validation loss: 0.6587290178497054\n",
            "Best validation loss: 0.6581888446152221\n",
            "Best validation loss: 0.6576514990808014\n",
            "Best validation loss: 0.6571169580652236\n",
            "Best validation loss: 0.6565851986461937\n",
            "Best validation loss: 0.6560561981566791\n",
            "Best validation loss: 0.6555299341813096\n",
            "Best validation loss: 0.6550063845528388\n",
            "Best validation loss: 0.6544855273486663\n",
            "Best validation loss: 0.6539673408874186\n",
            "Best validation loss: 0.6534518037255878\n",
            "Best validation loss: 0.6529388946542276\n",
            "Best validation loss: 0.6524285926957037\n",
            "Best validation loss: 0.651920877100501\n",
            "Best validation loss: 0.6514157273440809\n",
            "Best validation loss: 0.6509131231237949\n",
            "Best validation loss: 0.6504130443558455\n",
            "Best validation loss: 0.6499154711723012\n",
            "Best validation loss: 0.6494203839181584\n",
            "Best validation loss: 0.648927763148452\n",
            "Best validation loss: 0.6484375896254156\n",
            "Best validation loss: 0.6479498443156846\n",
            "Best validation loss: 0.6474645083875489\n",
            "Best validation loss: 0.6469815632082468\n",
            "Best validation loss: 0.6465009903413061\n",
            "Best validation loss: 0.6460227715439258\n",
            "Best validation loss: 0.6455468887644011\n",
            "Best validation loss: 0.6450733241395901\n",
            "Best validation loss: 0.6446020599924206\n",
            "Best validation loss: 0.6441330788294374\n",
            "Best validation loss: 0.6436663633383881\n",
            "Best validation loss: 0.643201896385848\n",
            "Best validation loss: 0.6427396610148824\n",
            "Best validation loss: 0.6422796404427459\n",
            "Best validation loss: 0.6418218180586175\n",
            "Best validation loss: 0.6413661774213731\n",
            "Best validation loss: 0.6409127022573908\n",
            "Best validation loss: 0.6404613764583919\n",
            "Best validation loss: 0.640012184079315\n",
            "Best validation loss: 0.6395651093362242\n",
            "Best validation loss: 0.6391201366042479\n",
            "Best validation loss: 0.6386772504155518\n",
            "Best validation loss: 0.6382364354573409\n",
            "Best validation loss: 0.6377976765698945\n",
            "Best validation loss: 0.6373609587446296\n",
            "Best validation loss: 0.6369262671221948\n",
            "Best validation loss: 0.6364935869905931\n",
            "Best validation loss: 0.6360629037833336\n",
            "Best validation loss: 0.6356342030776109\n",
            "Best validation loss: 0.6352074705925115\n",
            "Best validation loss: 0.6347826921872495\n",
            "Best validation loss: 0.6343598538594261\n",
            "Best validation loss: 0.6339389417433168\n",
            "Best validation loss: 0.6335199421081842\n",
            "Best validation loss: 0.6331028413566161\n",
            "Best validation loss: 0.6326876260228873\n",
            "Best validation loss: 0.6322742827713472\n",
            "Best validation loss: 0.6318627983948307\n",
            "Best validation loss: 0.6314531598130921\n",
            "Best validation loss: 0.6310453540712627\n",
            "Best validation loss: 0.6306393683383312\n",
            "Best validation loss: 0.6302351899056461\n",
            "Best validation loss: 0.6298328061854396\n",
            "Best validation loss: 0.6294322047093746\n",
            "Best validation loss: 0.6290333731271105\n",
            "Best validation loss: 0.6286362992048917\n",
            "Best validation loss: 0.6282409708241564\n",
            "Best validation loss: 0.627847375980164\n",
            "Best validation loss: 0.6274555027806443\n",
            "Best validation loss: 0.6270653394444646\n",
            "Best validation loss: 0.6266768743003166\n",
            "Best validation loss: 0.6262900957854219\n",
            "Best validation loss: 0.6259049924442559\n",
            "Best validation loss: 0.6255215529272904\n",
            "Best validation loss: 0.6251397659897528\n",
            "Best validation loss: 0.6247596204904047\n",
            "Best validation loss: 0.6243811053903352\n",
            "Best validation loss: 0.6240042097517748\n",
            "Best validation loss: 0.6236289227369215\n",
            "Best validation loss: 0.6232552336067879\n",
            "Best validation loss: 0.6228831317200606\n",
            "Best validation loss: 0.622512606531978\n",
            "Best validation loss: 0.6221436475932226\n",
            "Best validation loss: 0.6217762445488292\n",
            "Best validation loss: 0.6214103871371072\n",
            "Best validation loss: 0.6210460651885793\n",
            "Best validation loss: 0.6206832686249342\n",
            "Best validation loss: 0.6203219874579924\n",
            "Best validation loss: 0.6199622117886883\n",
            "Best validation loss: 0.6196039318060651\n",
            "Best validation loss: 0.6192471377862832\n",
            "Best validation loss: 0.6188918200916426\n",
            "Best validation loss: 0.6185379691696182\n",
            "Best validation loss: 0.6181855755519094\n",
            "Best validation loss: 0.6178346298535\n",
            "Best validation loss: 0.6174851227717331\n",
            "Best validation loss: 0.6171370450853977\n",
            "Best validation loss: 0.616790387653827\n",
            "Best validation loss: 0.6164451414160098\n",
            "Best validation loss: 0.6161012973897133\n",
            "Best validation loss: 0.6157588466706163\n",
            "Best validation loss: 0.6154177804314566\n",
            "Best validation loss: 0.6150780899211873\n",
            "Best validation loss: 0.6147397664641455\n",
            "Best validation loss: 0.6144028014592315\n",
            "Best validation loss: 0.6140671863790994\n",
            "Best validation loss: 0.6137329127693569\n",
            "Best validation loss: 0.6133999722477778\n",
            "Best validation loss: 0.6130683565035222\n",
            "Best validation loss: 0.6127380572963695\n",
            "Best validation loss: 0.6124090664559596\n",
            "Best validation loss: 0.612081375881044\n",
            "Best validation loss: 0.611754977538748\n",
            "Best validation loss: 0.6114298634638401\n",
            "Best validation loss: 0.6111060257580142\n",
            "Best validation loss: 0.6107834565891773\n",
            "Best validation loss: 0.6104621481907486\n",
            "Best validation loss: 0.6101420928609675\n",
            "Best validation loss: 0.6098232829622099\n",
            "Best validation loss: 0.6095057109203126\n",
            "Best validation loss: 0.6091893692239079\n",
            "Best validation loss: 0.6088742504237654\n",
            "Best validation loss: 0.6085603471321425\n",
            "Best validation loss: 0.6082476520221429\n",
            "Best validation loss: 0.607936157827084\n",
            "Best validation loss: 0.6076258573398712\n",
            "Best validation loss: 0.6073167434123802\n",
            "Best validation loss: 0.6070088089548482\n",
            "Best validation loss: 0.6067020469352711\n",
            "Best validation loss: 0.6063964503788093\n",
            "Best validation loss: 0.6060920123672007\n",
            "Best validation loss: 0.60578872603818\n",
            "Best validation loss: 0.6054865845849072\n",
            "Best validation loss: 0.6051855812554013\n",
            "Best validation loss: 0.6048857093519823\n",
            "Best validation loss: 0.6045869622307181\n",
            "Best validation loss: 0.6042893333008809\n",
            "Best validation loss: 0.6039928160244086\n",
            "Best validation loss: 0.6036974039153726\n",
            "Best validation loss: 0.6034030905394532\n",
            "Best validation loss: 0.6031098695134202\n",
            "Best validation loss: 0.6028177345046205\n",
            "Best validation loss: 0.6025266792304728\n",
            "Best validation loss: 0.6022366974579652\n",
            "Best validation loss: 0.6019477830031634\n",
            "Best validation loss: 0.601659929730721\n",
            "Best validation loss: 0.6013731315533979\n",
            "Best validation loss: 0.6010873824315831\n",
            "Best validation loss: 0.6008026763728248\n",
            "Best validation loss: 0.6005190074313642\n",
            "Best validation loss: 0.6002363697076762\n",
            "Best validation loss: 0.5999547573480155\n",
            "Best validation loss: 0.5996741645439676\n",
            "Best validation loss: 0.5993945855320046\n",
            "Best validation loss: 0.5991160145930483\n",
            "Best validation loss: 0.598838446052036\n",
            "Best validation loss: 0.5985618742774926\n",
            "Best validation loss: 0.5982862936811084\n",
            "Best validation loss: 0.5980116987173204\n",
            "Best validation loss: 0.5977380838828993\n",
            "Best validation loss: 0.5974654437165411\n",
            "Best validation loss: 0.5971937727984641\n",
            "Best validation loss: 0.5969230657500094\n",
            "Best validation loss: 0.5966533172332473\n",
            "Best validation loss: 0.5963845219505869\n",
            "Best validation loss: 0.5961166746443916\n",
            "Best validation loss: 0.5958497700965977\n",
            "Best validation loss: 0.5955838031283393\n",
            "Best validation loss: 0.595318768599575\n",
            "Best validation loss: 0.5950546614087205\n",
            "Best validation loss: 0.5947914764922857\n",
            "Best validation loss: 0.5945292088245144\n",
            "Best validation loss: 0.5942678534170299\n",
            "Best validation loss: 0.5940074053184827\n",
            "Best validation loss: 0.5937478596142044\n",
            "Best validation loss: 0.5934892114258634\n",
            "Best validation loss: 0.5932314559111259\n",
            "Best validation loss: 0.5929745882633207\n",
            "Best validation loss: 0.5927186037111061\n",
            "Best validation loss: 0.5924634975181431\n",
            "Best validation loss: 0.5922092649827703\n",
            "Best validation loss: 0.5919559014376832\n",
            "Best validation loss: 0.591703402249617\n",
            "Best validation loss: 0.5914517628190331\n",
            "Best validation loss: 0.5912009785798084\n",
            "Best validation loss: 0.5909510449989293\n",
            "Best validation loss: 0.5907019575761874\n",
            "Best validation loss: 0.5904537118438807\n",
            "Best validation loss: 0.5902063033665158\n",
            "Best validation loss: 0.5899597277405152\n",
            "Best validation loss: 0.5897139805939267\n",
            "Best validation loss: 0.5894690575861364\n",
            "Best validation loss: 0.5892249544075859\n",
            "Best validation loss: 0.5889816667794898\n",
            "Best validation loss: 0.5887391904535592\n",
            "Best validation loss: 0.5884975212117266\n",
            "Best validation loss: 0.5882566548658746\n",
            "Best validation loss: 0.5880165872575669\n",
            "Best validation loss: 0.5877773142577826\n",
            "Best validation loss: 0.5875388317666528\n",
            "Best validation loss: 0.5873011357132014\n",
            "Best validation loss: 0.587064222055087\n",
            "Best validation loss: 0.5868280867783489\n",
            "Best validation loss: 0.5865927258971547\n",
            "Best validation loss: 0.586358135453552\n",
            "Best validation loss: 0.5861243115172211\n",
            "Best validation loss: 0.5858912501852319\n",
            "Best validation loss: 0.5856589475818023\n",
            "Best validation loss: 0.5854273998580596\n",
            "Best validation loss: 0.5851966031918043\n",
            "Best validation loss: 0.5849665537872771\n",
            "Best validation loss: 0.5847372478749269\n",
            "Best validation loss: 0.5845086817111822\n",
            "Best validation loss: 0.5842808515782255\n",
            "Best validation loss: 0.5840537537837689\n",
            "Best validation loss: 0.5838273846608322\n",
            "Best validation loss: 0.5836017405675243\n",
            "Best validation loss: 0.5833768178868263\n",
            "Best validation loss: 0.5831526130263757\n",
            "Best validation loss: 0.5829291224182553\n",
            "Best validation loss: 0.5827063425187822\n",
            "Best validation loss: 0.5824842698082999\n",
            "Best validation loss: 0.5822629007909725\n",
            "Best validation loss: 0.5820422319945809\n",
            "Best validation loss: 0.5818222599703207\n",
            "Best validation loss: 0.5816029812926035\n",
            "Best validation loss: 0.5813843925588582\n",
            "Best validation loss: 0.5811664903893368\n",
            "Best validation loss: 0.5809492714269198\n",
            "Best validation loss: 0.5807327323369252\n",
            "Best validation loss: 0.5805168698069189\n",
            "Best validation loss: 0.5803016805465266\n",
            "Best validation loss: 0.5800871612872492\n",
            "Best validation loss: 0.5798733087822776\n",
            "Best validation loss: 0.5796601198063116\n",
            "Best validation loss: 0.5794475911553794\n",
            "Best validation loss: 0.5792357196466594\n",
            "Best validation loss: 0.579024502118304\n",
            "Best validation loss: 0.5788139354292641\n",
            "Best validation loss: 0.5786040164591169\n",
            "Best validation loss: 0.5783947421078942\n",
            "Best validation loss: 0.5781861092959132\n",
            "Best validation loss: 0.5779781149636082\n",
            "Best validation loss: 0.577770756071365\n",
            "Best validation loss: 0.5775640295993558\n",
            "Best validation loss: 0.5773579325473773\n",
            "Best validation loss: 0.5771524619346882\n",
            "Best validation loss: 0.5769476147998504\n",
            "Best validation loss: 0.5767433882005707\n",
            "Best validation loss: 0.5765397792135446\n",
            "Best validation loss: 0.5763367849343005\n",
            "Best validation loss: 0.5761344024770476\n",
            "Best validation loss: 0.5759326289745222\n",
            "Best validation loss: 0.5757314615778393\n",
            "Best validation loss: 0.5755308974563421\n",
            "Best validation loss: 0.5753309337974549\n",
            "Best validation loss: 0.5751315678065376\n",
            "Best validation loss: 0.5749327967067401\n",
            "Best validation loss: 0.5747346177388601\n",
            "Best validation loss: 0.5745370281612004\n",
            "Best validation loss: 0.5743400252494287\n",
            "Best validation loss: 0.5741436062964389\n",
            "Best validation loss: 0.5739477686122131\n",
            "Best validation loss: 0.5737525095236846\n",
            "Best validation loss: 0.5735578263746038\n",
            "Best validation loss: 0.5733637165254035\n",
            "Best validation loss: 0.5731701773530671\n",
            "Best validation loss: 0.5729772062509966\n",
            "Best validation loss: 0.5727848006288833\n",
            "Best validation loss: 0.5725929579125784\n",
            "Best validation loss: 0.5724016755439663\n",
            "Best validation loss: 0.5722109509808372\n",
            "Best validation loss: 0.5720207816967627\n",
            "Best validation loss: 0.5718311651809723\n",
            "Best validation loss: 0.5716420989382291\n",
            "Best validation loss: 0.5714535804887101\n",
            "Best validation loss: 0.571265607367884\n",
            "Best validation loss: 0.5710781771263933\n",
            "Best validation loss: 0.5708912873299355\n",
            "Best validation loss: 0.5707049355591459\n",
            "Best validation loss: 0.5705191194094817\n",
            "Best validation loss: 0.5703338364911072\n",
            "Best validation loss: 0.5701490844287805\n",
            "Best validation loss: 0.5699648608617395\n",
            "Best validation loss: 0.5697811634435912\n",
            "Best validation loss: 0.569597989842201\n",
            "Best validation loss: 0.5694153377395824\n",
            "Best validation loss: 0.5692332048317894\n",
            "Best validation loss: 0.5690515888288077\n",
            "Best validation loss: 0.5688704874544492\n",
            "Best validation loss: 0.5686898984462453\n",
            "Best validation loss: 0.5685098195553433\n",
            "Best validation loss: 0.5683302485464019\n",
            "Best validation loss: 0.5681511831974891\n",
            "Best validation loss: 0.5679726212999798\n",
            "Best validation loss: 0.5677945606584553\n",
            "Best validation loss: 0.5676169990906033\n",
            "Best validation loss: 0.5674399344271187\n",
            "Best validation loss: 0.5672633645116057\n",
            "Best validation loss: 0.5670872872004807\n",
            "Best validation loss: 0.5669117003628753\n",
            "Best validation loss: 0.5667366018805416\n",
            "Best validation loss: 0.5665619896477572\n",
            "Best validation loss: 0.5663878615712312\n",
            "Best validation loss: 0.5662142155700121\n",
            "Best validation loss: 0.5660410495753946\n",
            "Best validation loss: 0.5658683615308294\n",
            "Best validation loss: 0.5656961493918325\n",
            "Best validation loss: 0.5655244111258949\n",
            "Best validation loss: 0.565353144712395\n",
            "Best validation loss: 0.565182348142509\n",
            "Best validation loss: 0.5650120194191255\n",
            "Best validation loss: 0.564842156556758\n",
            "Best validation loss: 0.5646727575814589\n",
            "Best validation loss: 0.5645038205307356\n",
            "Best validation loss: 0.5643353434534655\n",
            "Best validation loss: 0.5641673244098129\n",
            "Best validation loss: 0.5639997614711466\n",
            "Best validation loss: 0.5638326527199574\n",
            "Best validation loss: 0.5636659962497772\n",
            "Best validation loss: 0.5634997901650985\n",
            "Best validation loss: 0.5633340325812949\n",
            "Best validation loss: 0.5631687216245406\n",
            "Best validation loss: 0.5630038554317341\n",
            "Best validation loss: 0.5628394321504191\n",
            "Best validation loss: 0.5626754499387079\n",
            "Best validation loss: 0.5625119069652045\n",
            "Best validation loss: 0.5623488014089297\n",
            "Best validation loss: 0.5621861314592459\n",
            "Best validation loss: 0.5620238953157821\n",
            "Best validation loss: 0.5618620911883608\n",
            "Best validation loss: 0.5617007172969246\n",
            "Best validation loss: 0.5615397718714639\n",
            "Best validation loss: 0.5613792531519447\n",
            "Best validation loss: 0.561219159388238\n",
            "Best validation loss: 0.5610594888400486\n",
            "Best validation loss: 0.5609002397768453\n",
            "Best validation loss: 0.560741410477792\n",
            "Best validation loss: 0.5605829992316776\n",
            "Best validation loss: 0.5604250043368494\n",
            "Best validation loss: 0.5602674241011443\n",
            "Best validation loss: 0.560110256841822\n",
            "Best validation loss: 0.5599535008854991\n",
            "Best validation loss: 0.5597971545680824\n",
            "Best validation loss: 0.5596412162347036\n",
            "Best validation loss: 0.5594856842396557\n",
            "Best validation loss: 0.5593305569463268\n",
            "Best validation loss: 0.5591758327271382\n",
            "Best validation loss: 0.5590215099634797\n",
            "Best validation loss: 0.5588675870456483\n",
            "Best validation loss: 0.5587140623727855\n",
            "Best validation loss: 0.5585609343528152\n",
            "Best validation loss: 0.5584082014023837\n",
            "Best validation loss: 0.5582558619467984\n",
            "Best validation loss: 0.5581039144199679\n",
            "Best validation loss: 0.5579523572643424\n",
            "Best validation loss: 0.5578011889308551\n",
            "Best validation loss: 0.5576504078788631\n",
            "Best validation loss: 0.5575000125760894\n",
            "Best validation loss: 0.5573500014985655\n",
            "Best validation loss: 0.5572003731305746\n",
            "Best validation loss: 0.5570511259645942\n",
            "Best validation loss: 0.5569022585012404\n",
            "Best validation loss: 0.5567537692492126\n",
            "Best validation loss: 0.5566056567252371\n",
            "Best validation loss: 0.5564579194540136\n",
            "Best validation loss: 0.5563105559681599\n",
            "Best validation loss: 0.556163564808159\n",
            "Best validation loss: 0.5560169445223043\n",
            "Best validation loss: 0.5558706936666482\n",
            "Best validation loss: 0.555724810804948\n",
            "Best validation loss: 0.555579294508615\n",
            "Best validation loss: 0.5554341433566616\n",
            "Best validation loss: 0.5552893559356517\n",
            "Best validation loss: 0.5551449308396477\n",
            "Best validation loss: 0.5550008666701617\n",
            "Best validation loss: 0.554857162036105\n",
            "Best validation loss: 0.5547138155537381\n",
            "Best validation loss: 0.5545708258466217\n",
            "Best validation loss: 0.5544281915455678\n",
            "Best validation loss: 0.5542859112885914\n",
            "Best validation loss: 0.5541439837208625\n",
            "Best validation loss: 0.5540024074946581\n",
            "Best validation loss: 0.5538611812693152\n",
            "Best validation loss: 0.5537203037111833\n",
            "Best validation loss: 0.5535797734935792\n",
            "Best validation loss: 0.5534395892967396\n",
            "Best validation loss: 0.5532997498077755\n",
            "Best validation loss: 0.5531602537206269\n",
            "Best validation loss: 0.5530210997360187\n",
            "Best validation loss: 0.5528822865614138\n",
            "Best validation loss: 0.5527438129109712\n",
            "Best validation loss: 0.5526056775055003\n",
            "Best validation loss: 0.5524678790724178\n",
            "Best validation loss: 0.552330416345705\n",
            "Best validation loss: 0.5521932880658637\n",
            "Best validation loss: 0.5520564929798744\n",
            "Best validation loss: 0.5519200298411536\n",
            "Best validation loss: 0.5517838974095124\n",
            "Best validation loss: 0.551648094451114\n",
            "Best validation loss: 0.5515126197384335\n",
            "Best validation loss: 0.5513774720502159\n",
            "Best validation loss: 0.551242650171436\n",
            "Best validation loss: 0.5511081528932584\n",
            "Best validation loss: 0.5509739790129966\n",
            "Best validation loss: 0.550840127334074\n",
            "Best validation loss: 0.5507065966659838\n",
            "Best validation loss: 0.5505733858242514\n",
            "Best validation loss: 0.5504404936303938\n",
            "Best validation loss: 0.5503079189118822\n",
            "Best validation loss: 0.5501756605021036\n",
            "Best validation loss: 0.5500437172403229\n",
            "Best validation loss: 0.549912087971645\n",
            "Best validation loss: 0.5497807715469784\n",
            "Best validation loss: 0.5496497668229969\n",
            "Best validation loss: 0.5495190726621041\n",
            "Best validation loss: 0.5493886879323959\n",
            "Best validation loss: 0.5492586115076256\n",
            "Best validation loss: 0.5491288422671662\n",
            "Best validation loss: 0.5489993790959766\n",
            "Best validation loss: 0.5488702208845655\n",
            "Best validation loss: 0.548741366528956\n",
            "Best validation loss: 0.5486128149306512\n",
            "Best validation loss: 0.5484845649965995\n",
            "Best validation loss: 0.548356615639161\n",
            "Best validation loss: 0.5482289657760723\n",
            "Best validation loss: 0.5481016143304134\n",
            "Best validation loss: 0.5479745602305742\n",
            "Best validation loss: 0.5478478024102215\n",
            "Best validation loss: 0.5477213398082653\n",
            "Best validation loss: 0.5475951713688267\n",
            "Best validation loss: 0.5474692960412051\n",
            "Best validation loss: 0.5473437127798462\n",
            "Best validation loss: 0.5472184205443092\n",
            "Best validation loss: 0.5470934182992365\n",
            "Best validation loss: 0.5469687050143205\n",
            "Best validation loss: 0.5468442796642736\n",
            "Best validation loss: 0.5467201412287965\n",
            "Best validation loss: 0.5465962886925475\n",
            "Best validation loss: 0.5464727210451119\n",
            "Best validation loss: 0.5463494372809717\n",
            "Best validation loss: 0.5462264363994754\n",
            "Best validation loss: 0.5461037174048081\n",
            "Best validation loss: 0.5459812793059616\n",
            "Best validation loss: 0.5458591211167054\n",
            "Best validation loss: 0.5457372418555569\n",
            "Best validation loss: 0.545615640545753\n",
            "Best validation loss: 0.5454943162152202\n",
            "Best validation loss: 0.5453732678965472\n",
            "Best validation loss: 0.5452524946269559\n",
            "Best validation loss: 0.545131995448273\n",
            "Best validation loss: 0.5450117694069027\n",
            "Best validation loss: 0.5448918155537982\n",
            "Best validation loss: 0.5447721329444344\n",
            "Best validation loss: 0.5446527206387806\n",
            "Best validation loss: 0.5445335777012734\n",
            "Best validation loss: 0.5444147032007894\n",
            "Best validation loss: 0.5442960962106187\n",
            "Best validation loss: 0.5441777558084384\n",
            "Best validation loss: 0.5440596810762854\n",
            "Best validation loss: 0.5439418711005318\n",
            "Best validation loss: 0.5438243249718571\n",
            "Best validation loss: 0.5437070417852236\n",
            "Best validation loss: 0.5435900206398502\n",
            "Best validation loss: 0.5434732606391871\n",
            "Best validation loss: 0.543356760890891\n",
            "Best validation loss: 0.543240520506799\n",
            "Best validation loss: 0.5431245386029044\n",
            "Best validation loss: 0.5430088142993323\n",
            "Best validation loss: 0.5428933467203141\n",
            "Best validation loss: 0.5427781349941636\n",
            "Best validation loss: 0.5426631782532534\n",
            "Best validation loss: 0.5425484756339897\n",
            "Best validation loss: 0.5424340262767895\n",
            "Best validation loss: 0.5423198293260563\n",
            "Best validation loss: 0.5422058839301569\n",
            "Best validation loss: 0.5420921892413979\n",
            "Best validation loss: 0.5419787444160027\n",
            "Best validation loss: 0.5418655486140881\n",
            "Best validation loss: 0.5417526009996417\n",
            "Best validation loss: 0.5416399007404998\n",
            "Best validation loss: 0.5415274470083239\n",
            "Best validation loss: 0.5414152389785786\n",
            "Best validation loss: 0.5413032758305096\n",
            "Best validation loss: 0.5411915567471222\n",
            "Best validation loss: 0.5410800809151576\n",
            "Best validation loss: 0.5409688475250738\n",
            "Best validation loss: 0.5408578557710214\n",
            "Best validation loss: 0.5407471048508241\n",
            "Best validation loss: 0.5406365939659561\n",
            "Best validation loss: 0.5405263223215218\n",
            "Best validation loss: 0.5404162891262344\n",
            "Best validation loss: 0.5403064935923956\n",
            "Best validation loss: 0.5401969349358738\n",
            "Best validation loss: 0.5400876123760849\n",
            "Best validation loss: 0.539978525135971\n",
            "Best validation loss: 0.5398696724419803\n",
            "Best validation loss: 0.5397610535240467\n",
            "Best validation loss: 0.539652667615571\n",
            "Best validation loss: 0.5395445139533995\n",
            "Best validation loss: 0.5394365917778052\n",
            "Best validation loss: 0.5393289003324685\n",
            "Best validation loss: 0.5392214388644565\n",
            "Best validation loss: 0.539114206624205\n",
            "Best validation loss: 0.5390072028654986\n",
            "Best validation loss: 0.5389004268454521\n",
            "Best validation loss: 0.5387938778244913\n",
            "Best validation loss: 0.5386875550663341\n",
            "Best validation loss: 0.5385814578379723\n",
            "Best validation loss: 0.5384755854096527\n",
            "Best validation loss: 0.5383699370548588\n",
            "Best validation loss: 0.5382645120502927\n",
            "Best validation loss: 0.5381593096758569\n",
            "Best validation loss: 0.5380543292146359\n",
            "Best validation loss: 0.5379495699528787\n",
            "Best validation loss: 0.5378450311799811\n",
            "Best validation loss: 0.5377407121884676\n",
            "Best validation loss: 0.5376366122739747\n",
            "Best validation loss: 0.537532730735232\n",
            "Best validation loss: 0.5374290668740469\n",
            "Best validation loss: 0.5373256199952854\n",
            "Best validation loss: 0.5372223894068565\n",
            "Best validation loss: 0.5371193744196946\n",
            "Best validation loss: 0.537016574347743\n",
            "Best validation loss: 0.5369139885079365\n",
            "Best validation loss: 0.5368116162201857\n",
            "Best validation loss: 0.5367094568073598\n",
            "Best validation loss: 0.536607509595271\n",
            "Best validation loss: 0.5365057739126567\n",
            "Best validation loss: 0.5364042490911655\n",
            "Best validation loss: 0.5363029344653389\n",
            "Best validation loss: 0.5362018293725974\n",
            "Best validation loss: 0.536100933153223\n",
            "Best validation loss: 0.5360002451503442\n",
            "Best validation loss: 0.5358997647099206\n",
            "Best validation loss: 0.535799491180727\n",
            "Best validation loss: 0.5356994239143377\n",
            "Convergence reached. Stopping training.\n",
            "Best validation loss: 1.619099990038281\n",
            "Best validation loss: 1.5984881249727658\n",
            "Best validation loss: 1.5784100619424875\n",
            "Best validation loss: 1.5588679811814858\n",
            "Best validation loss: 1.5398605430365675\n",
            "Best validation loss: 1.521383144377497\n",
            "Best validation loss: 1.5034283189086453\n",
            "Best validation loss: 1.4859862260545444\n",
            "Best validation loss: 1.4690451716975788\n",
            "Best validation loss: 1.4525921122918741\n",
            "Best validation loss: 1.4366131077003241\n",
            "Best validation loss: 1.4210937032175996\n",
            "Best validation loss: 1.4060192344451923\n",
            "Best validation loss: 1.3913750583274513\n",
            "Best validation loss: 1.3771467194609153\n",
            "Best validation loss: 1.363320063338171\n",
            "Best validation loss: 1.3498813084199826\n",
            "Best validation loss: 1.3368170877765146\n",
            "Best validation loss: 1.3241144692413742\n",
            "Best validation loss: 1.3117609610839618\n",
            "Best validation loss: 1.2997445084176518\n",
            "Best validation loss: 1.2880534840586655\n",
            "Best validation loss: 1.276676676367573\n",
            "Best validation loss: 1.2656032757207365\n",
            "Best validation loss: 1.2548228606260003\n",
            "Best validation loss: 1.2443253840623552\n",
            "Best validation loss: 1.2341011603374872\n",
            "Best validation loss: 1.2241408525781943\n",
            "Best validation loss: 1.214435460863626\n",
            "Best validation loss: 1.20497631095544\n",
            "Best validation loss: 1.195755043554421\n",
            "Best validation loss: 1.1867636040074714\n",
            "Best validation loss: 1.1779942323938306\n",
            "Best validation loss: 1.1694394539294808\n",
            "Best validation loss: 1.1610920696406277\n",
            "Best validation loss: 1.1529451472689525\n",
            "Best validation loss: 1.144992012381941\n",
            "Best validation loss: 1.1372262396705841\n",
            "Best validation loss: 1.1296416444239628\n",
            "Best validation loss: 1.1222322741757897\n",
            "Best validation loss: 1.114992400522054\n",
            "Best validation loss: 1.1079165111117093\n",
            "Best validation loss: 1.1009993018141504\n",
            "Best validation loss: 1.0942356690681703\n",
            "Best validation loss: 1.0876207024174374\n",
            "Best validation loss: 1.0811496772374185\n",
            "Best validation loss: 1.0748180476582079\n",
            "Best validation loss: 1.0686214396870606\n",
            "Best validation loss: 1.0625556445336108\n",
            "Best validation loss: 1.056616612139882\n",
            "Best validation loss: 1.0508004449162804\n",
            "Best validation loss: 1.0451033916838905\n",
            "Best validation loss: 1.0395218418225292\n",
            "Best validation loss: 1.0340523196232336\n",
            "Best validation loss: 1.0286914788431347\n",
            "Best validation loss: 1.0234360974600205\n",
            "Best validation loss: 1.0182830726233285\n",
            "Best validation loss: 1.0132294157978063\n",
            "Best validation loss: 1.008272248095672\n",
            "Best validation loss: 1.003408795792747\n",
            "Best validation loss: 0.9986363860237579\n",
            "Best validation loss: 0.9939524426517772\n",
            "Best validation loss: 0.9893544823066118\n",
            "Best validation loss: 0.984840110586821\n",
            "Best validation loss: 0.9804070184199749\n",
            "Best validation loss: 0.9760529785757358\n",
            "Best validation loss: 0.971775842326329\n",
            "Best validation loss: 0.9675735362490111\n",
            "Best validation loss: 0.9634440591651904\n",
            "Best validation loss: 0.9593854792109279\n",
            "Best validation loss: 0.9553959310336463\n",
            "Best validation loss: 0.9514736131099735\n",
            "Best validation loss: 0.9476167851797753\n",
            "Best validation loss: 0.9438237657915541\n",
            "Best validation loss: 0.9400929299545362\n",
            "Best validation loss: 0.9364227068929007\n",
            "Best validation loss: 0.9328115778977645\n",
            "Best validation loss: 0.9292580742726709\n",
            "Best validation loss: 0.9257607753684954\n",
            "Best validation loss: 0.9223183067038174\n",
            "Best validation loss: 0.9189293381669676\n",
            "Best validation loss: 0.915592582296105\n",
            "Best validation loss: 0.9123067926338184\n",
            "Best validation loss: 0.9090707621528994\n",
            "Best validation loss: 0.9058833217500656\n",
            "Best validation loss: 0.9027433388045546\n",
            "Best validation loss: 0.8996497157986385\n",
            "Best validation loss: 0.896601388997239\n",
            "Best validation loss: 0.8935973271839484\n",
            "Best validation loss: 0.8906365304508783\n",
            "Best validation loss: 0.8877180290398814\n",
            "Best validation loss: 0.8848408822327924\n",
            "Best validation loss: 0.8820041772884555\n",
            "Best validation loss: 0.879207028424393\n",
            "Best validation loss: 0.8764485758410862\n",
            "Best validation loss: 0.8737279847869176\n",
            "Best validation loss: 0.8710444446619283\n",
            "Best validation loss: 0.8683971681586224\n",
            "Best validation loss: 0.8657853904381387\n",
            "Best validation loss: 0.8632083683401833\n",
            "Best validation loss: 0.8606653796252027\n",
            "Best validation loss: 0.8581557222473388\n",
            "Best validation loss: 0.8556787136567803\n",
            "Best validation loss: 0.8532336901301945\n",
            "Best validation loss: 0.8508200061279809\n",
            "Best validation loss: 0.8484370336771501\n",
            "Best validation loss: 0.8460841617786872\n",
            "Best validation loss: 0.8437607958383161\n",
            "Best validation loss: 0.8414663571196294\n",
            "Best validation loss: 0.8392002822185985\n",
            "Best validation loss: 0.8369620225585267\n",
            "Best validation loss: 0.8347510439045496\n",
            "Best validation loss: 0.8325668258968348\n",
            "Best validation loss: 0.8304088616016642\n",
            "Best validation loss: 0.8282766570796315\n",
            "Best validation loss: 0.826169730970213\n",
            "Best validation loss: 0.8240876140920124\n",
            "Best validation loss: 0.8220298490580105\n",
            "Best validation loss: 0.8199959899051787\n",
            "Best validation loss: 0.8179856017378511\n",
            "Best validation loss: 0.8159982603842733\n",
            "Best validation loss: 0.8140335520657741\n",
            "Best validation loss: 0.8120910730780346\n",
            "Best validation loss: 0.8101704294839485\n",
            "Best validation loss: 0.8082712368175936\n",
            "Best validation loss: 0.8063931197988594\n",
            "Best validation loss: 0.8045357120582873\n",
            "Best validation loss: 0.8026986558717122\n",
            "Best validation loss: 0.8008816019043006\n",
            "Best validation loss: 0.7990842089636085\n",
            "Best validation loss: 0.7973061437612954\n",
            "Best validation loss: 0.7955470806831433\n",
            "Best validation loss: 0.7938067015670522\n",
            "Best validation loss: 0.7920846954886954\n",
            "Best validation loss: 0.7903807585545267\n",
            "Best validation loss: 0.7886945937018559\n",
            "Best validation loss: 0.7870259105057111\n",
            "Best validation loss: 0.7853744249922247\n",
            "Best validation loss: 0.7837398594582883\n",
            "Best validation loss: 0.7821219422972372\n",
            "Best validation loss: 0.7805204078303286\n",
            "Best validation loss: 0.7789349961437938\n",
            "Best validation loss: 0.7773654529312517\n",
            "Best validation loss: 0.7758115293412801\n",
            "Best validation loss: 0.7742729818299503\n",
            "Best validation loss: 0.7727495720181373\n",
            "Best validation loss: 0.7712410665534288\n",
            "Best validation loss: 0.7697472369764607\n",
            "Best validation loss: 0.7682678595915146\n",
            "Best validation loss: 0.7668027153412225\n",
            "Best validation loss: 0.7653515896852249\n",
            "Best validation loss: 0.7639142724826407\n",
            "Best validation loss: 0.7624905578782075\n",
            "Best validation loss: 0.7610802441919632\n",
            "Best validation loss: 0.7596831338123374\n",
            "Best validation loss: 0.7582990330925338\n",
            "Best validation loss: 0.7569277522500839\n",
            "Best validation loss: 0.7555691052694609\n",
            "Best validation loss: 0.7542229098076441\n",
            "Best validation loss: 0.7528889871025327\n",
            "Best validation loss: 0.7515671618841061\n",
            "Best validation loss: 0.7502572622882374\n",
            "Best validation loss: 0.748959119773067\n",
            "Best validation loss: 0.7476725690378476\n",
            "Best validation loss: 0.746397447944179\n",
            "Best validation loss: 0.7451335974395434\n",
            "Best validation loss: 0.7438808614830738\n",
            "Best validation loss: 0.7426390869734683\n",
            "Best validation loss: 0.7414081236789881\n",
            "Best validation loss: 0.7401878241694619\n",
            "Best validation loss: 0.7389780437502332\n",
            "Best validation loss: 0.7377786403979865\n",
            "Best validation loss: 0.7365894746983865\n",
            "Best validation loss: 0.7354104097854752\n",
            "Best validation loss: 0.7342413112827663\n",
            "Best validation loss: 0.7330820472459811\n",
            "Best validation loss: 0.7319324881073757\n",
            "Best validation loss: 0.730792506621604\n",
            "Best validation loss: 0.7296619778130709\n",
            "Best validation loss: 0.7285407789247242\n",
            "Best validation loss: 0.7274287893682424\n",
            "Best validation loss: 0.7263258906755726\n",
            "Best validation loss: 0.7252319664517756\n",
            "Best validation loss: 0.7241469023291369\n",
            "Best validation loss: 0.7230705859225062\n",
            "Best validation loss: 0.722002906785823\n",
            "Best validation loss: 0.720943756369796\n",
            "Best validation loss: 0.7198930279806963\n",
            "Best validation loss: 0.7188506167402332\n",
            "Best validation loss: 0.7178164195464781\n",
            "Best validation loss: 0.7167903350358051\n",
            "Best validation loss: 0.7157722635458161\n",
            "Best validation loss: 0.7147621070792223\n",
            "Best validation loss: 0.7137597692686524\n",
            "Best validation loss: 0.7127651553423597\n",
            "Best validation loss: 0.7117781720908006\n",
            "Best validation loss: 0.7107987278340615\n",
            "Best validation loss: 0.7098267323901034\n",
            "Best validation loss: 0.7088620970438072\n",
            "Best validation loss: 0.7079047345167895\n",
            "Best validation loss: 0.7069545589379713\n",
            "Best validation loss: 0.7060114858148745\n",
            "Best validation loss: 0.7050754320056267\n",
            "Best validation loss: 0.7041463156916539\n",
            "Best validation loss: 0.7032240563510403\n",
            "Best validation loss: 0.7023085747325374\n",
            "Best validation loss: 0.7013997928302017\n",
            "Best validation loss: 0.7004976338586465\n",
            "Best validation loss: 0.6996020222288879\n",
            "Best validation loss: 0.6987128835247679\n",
            "Best validation loss: 0.6978301444799414\n",
            "Best validation loss: 0.6969537329554071\n",
            "Best validation loss: 0.6960835779175721\n",
            "Best validation loss: 0.6952196094168309\n",
            "Best validation loss: 0.6943617585666468\n",
            "Best validation loss: 0.6935099575231222\n",
            "Best validation loss: 0.6926641394650439\n",
            "Best validation loss: 0.6918242385743896\n",
            "Best validation loss: 0.6909901900172846\n",
            "Best validation loss: 0.6901619299253954\n",
            "Best validation loss: 0.6893393953777496\n",
            "Best validation loss: 0.6885225243829686\n",
            "Best validation loss: 0.6877112558619047\n",
            "Best validation loss: 0.6869055296306696\n",
            "Best validation loss: 0.6861052863840461\n",
            "Best validation loss: 0.6853104676792695\n",
            "Best validation loss: 0.684521015920174\n",
            "Best validation loss: 0.6837368743416893\n",
            "Best validation loss: 0.682957986994681\n",
            "Best validation loss: 0.6821842987311274\n",
            "Best validation loss: 0.6814157551896196\n",
            "Best validation loss: 0.680652302781181\n",
            "Best validation loss: 0.6798938886753939\n",
            "Best validation loss: 0.679140460786831\n",
            "Best validation loss: 0.6783919677617759\n",
            "Best validation loss: 0.6776483589652352\n",
            "Best validation loss: 0.676909584468224\n",
            "Best validation loss: 0.6761755950353275\n",
            "Best validation loss: 0.6754463421125274\n",
            "Best validation loss: 0.6747217778152831\n",
            "Best validation loss: 0.6740018549168701\n",
            "Best validation loss: 0.6732865268369606\n",
            "Best validation loss: 0.672575747630445\n",
            "Best validation loss: 0.671869471976488\n",
            "Best validation loss: 0.671167655167811\n",
            "Best validation loss: 0.6704702531001983\n",
            "Best validation loss: 0.6697772222622205\n",
            "Best validation loss: 0.669088519725169\n",
            "Best validation loss: 0.6684041031331976\n",
            "Best validation loss: 0.6677239306936653\n",
            "Best validation loss: 0.667047961167677\n",
            "Best validation loss: 0.6663761538608151\n",
            "Best validation loss: 0.6657084686140603\n",
            "Best validation loss: 0.6650448657948934\n",
            "Best validation loss: 0.6643853062885776\n",
            "Best validation loss: 0.6637297514896148\n",
            "Best validation loss: 0.663078163293372\n",
            "Best validation loss: 0.662430504087875\n",
            "Best validation loss: 0.6617867367457639\n",
            "Best validation loss: 0.6611468246164082\n",
            "Best validation loss: 0.6605107315181757\n",
            "Best validation loss: 0.6598784217308544\n",
            "Best validation loss: 0.6592498599882225\n",
            "Best validation loss: 0.6586250114707604\n",
            "Best validation loss: 0.6580038417985085\n",
            "Best validation loss: 0.6573863170240596\n",
            "Best validation loss: 0.6567724036256867\n",
            "Best validation loss: 0.6561620685006054\n",
            "Best validation loss: 0.6555552789583616\n",
            "Best validation loss: 0.6549520027143484\n",
            "Best validation loss: 0.6543522078834433\n",
            "Best validation loss: 0.6537558629737696\n",
            "Best validation loss: 0.6531629368805721\n",
            "Best validation loss: 0.6525733988802099\n",
            "Best validation loss: 0.6519872186242613\n",
            "Best validation loss: 0.6514043661337399\n",
            "Best validation loss: 0.6508248117934162\n",
            "Best validation loss: 0.6502485263462474\n",
            "Best validation loss: 0.6496754808879083\n",
            "Best validation loss: 0.6491056468614241\n",
            "Best validation loss: 0.6485389960519011\n",
            "Best validation loss: 0.6479755005813551\n",
            "Best validation loss: 0.647415132903633\n",
            "Best validation loss: 0.6468578657994278\n",
            "Best validation loss: 0.6463036723713845\n",
            "Best validation loss: 0.6457525260392922\n",
            "Best validation loss: 0.6452044005353664\n",
            "Best validation loss: 0.6446592698996129\n",
            "Best validation loss: 0.6441171084752766\n",
            "Best validation loss: 0.6435778909043722\n",
            "Best validation loss: 0.6430415921232914\n",
            "Best validation loss: 0.6425081873584925\n",
            "Best validation loss: 0.6419776521222615\n",
            "Best validation loss: 0.6414499622085529\n",
            "Best validation loss: 0.6409250936888992\n",
            "Best validation loss: 0.6404030229083947\n",
            "Best validation loss: 0.6398837264817498\n",
            "Best validation loss: 0.6393671812894118\n",
            "Best validation loss: 0.6388533644737548\n",
            "Best validation loss: 0.638342253435336\n",
            "Best validation loss: 0.6378338258292144\n",
            "Best validation loss: 0.6373280595613355\n",
            "Best validation loss: 0.6368249327849766\n",
            "Best validation loss: 0.6363244238972526\n",
            "Best validation loss: 0.6358265115356825\n",
            "Best validation loss: 0.6353311745748136\n",
            "Best validation loss: 0.6348383921229029\n",
            "Best validation loss: 0.634348143518655\n",
            "Best validation loss: 0.6338604083280148\n",
            "Best validation loss: 0.6333751663410138\n",
            "Best validation loss: 0.6328923975686696\n",
            "Best validation loss: 0.6324120822399375\n",
            "Best validation loss: 0.6319342007987115\n",
            "Best validation loss: 0.631458733900877\n",
            "Best validation loss: 0.6309856624114113\n",
            "Best validation loss: 0.6305149674015313\n",
            "Best validation loss: 0.6300466301458894\n",
            "Best validation loss: 0.6295806321198154\n",
            "Best validation loss: 0.6291169549966026\n",
            "Best validation loss: 0.6286555806448378\n",
            "Best validation loss: 0.628196491125778\n",
            "Best validation loss: 0.6277396686907648\n",
            "Best validation loss: 0.6272850957786843\n",
            "Best validation loss: 0.6268327550134672\n",
            "Best validation loss: 0.6263826292016282\n",
            "Best validation loss: 0.6259347013298451\n",
            "Best validation loss: 0.6254889545625779\n",
            "Best validation loss: 0.6250453722397241\n",
            "Best validation loss: 0.6246039378743132\n",
            "Best validation loss: 0.6241646351502352\n",
            "Best validation loss: 0.6237274479200083\n",
            "Best validation loss: 0.6232923602025789\n",
            "Best validation loss: 0.6228593561811582\n",
            "Best validation loss: 0.6224284202010913\n",
            "Best validation loss: 0.6219995367677608\n",
            "Best validation loss: 0.6215726905445227\n",
            "Best validation loss: 0.6211478663506735\n",
            "Best validation loss: 0.6207250491594518\n",
            "Best validation loss: 0.6203042240960659\n",
            "Best validation loss: 0.6198853764357571\n",
            "Best validation loss: 0.6194684916018891\n",
            "Best validation loss: 0.6190535551640687\n",
            "Best validation loss: 0.6186405528362939\n",
            "Best validation loss: 0.6182294704751324\n",
            "Best validation loss: 0.6178202940779249\n",
            "Best validation loss: 0.6174130097810182\n",
            "Best validation loss: 0.6170076038580237\n",
            "Best validation loss: 0.6166040627181029\n",
            "Best validation loss: 0.6162023729042784\n",
            "Best validation loss: 0.6158025210917708\n",
            "Best validation loss: 0.6154044940863589\n",
            "Best validation loss: 0.6150082788227669\n",
            "Best validation loss: 0.6146138623630739\n",
            "Best validation loss: 0.6142212318951472\n",
            "Best validation loss: 0.6138303747311001\n",
            "Best validation loss: 0.6134412783057704\n",
            "Best validation loss: 0.6130539301752236\n",
            "Best validation loss: 0.6126683180152764\n",
            "Best validation loss: 0.6122844296200429\n",
            "Best validation loss: 0.611902252900502\n",
            "Best validation loss: 0.611521775883085\n",
            "Best validation loss: 0.6111429867082846\n",
            "Best validation loss: 0.6107658736292838\n",
            "Best validation loss: 0.6103904250106046\n",
            "Best validation loss: 0.6100166293267762\n",
            "Best validation loss: 0.6096444751610232\n",
            "Best validation loss: 0.6092739512039708\n",
            "Best validation loss: 0.6089050462523713\n",
            "Best validation loss: 0.608537749207846\n",
            "Best validation loss: 0.6081720490756474\n",
            "Best validation loss: 0.6078079349634374\n",
            "Best validation loss: 0.607445396080084\n",
            "Best validation loss: 0.6070844217344737\n",
            "Best validation loss: 0.6067250013343435\n",
            "Best validation loss: 0.6063671243851247\n",
            "Best validation loss: 0.6060107804888079\n",
            "Best validation loss: 0.6056559593428209\n",
            "Best validation loss: 0.6053026507389233\n",
            "Best validation loss: 0.6049508445621162\n",
            "Best validation loss: 0.6046005307895681\n",
            "Best validation loss: 0.6042516994895534\n",
            "Best validation loss: 0.6039043408204083\n",
            "Best validation loss: 0.6035584450295006\n",
            "Best validation loss: 0.6032140024522112\n",
            "Best validation loss: 0.6028710035109336\n",
            "Best validation loss: 0.6025294387140836\n",
            "Best validation loss: 0.6021892986551242\n",
            "Best validation loss: 0.6018505740116052\n",
            "Best validation loss: 0.6015132555442128\n",
            "Best validation loss: 0.601177334095834\n",
            "Best validation loss: 0.6008428005906334\n",
            "Best validation loss: 0.600509646033144\n",
            "Best validation loss: 0.6001778615073665\n",
            "Best validation loss: 0.5998474381758846\n",
            "Best validation loss: 0.5995183672789902\n",
            "Best validation loss: 0.5991906401338207\n",
            "Best validation loss: 0.5988642481335084\n",
            "Best validation loss: 0.59853918274634\n",
            "Best validation loss: 0.5982154355149293\n",
            "Best validation loss: 0.5978929980553983\n",
            "Best validation loss: 0.5975718620565714\n",
            "Best validation loss: 0.5972520192791796\n",
            "Best validation loss: 0.5969334615550744\n",
            "Best validation loss: 0.596616180786453\n",
            "Best validation loss: 0.5963001689450937\n",
            "Best validation loss: 0.5959854180716004\n",
            "Best validation loss: 0.5956719202746579\n",
            "Best validation loss: 0.5953596677302965\n",
            "Best validation loss: 0.5950486526811662\n",
            "Best validation loss: 0.5947388674358214\n",
            "Best validation loss: 0.5944303043680115\n",
            "Best validation loss: 0.5941229559159862\n",
            "Best validation loss: 0.5938168145818037\n",
            "Best validation loss: 0.5935118729306532\n",
            "Best validation loss: 0.5932081235901816\n",
            "Best validation loss: 0.5929055592498323\n",
            "Best validation loss: 0.5926041726601904\n",
            "Best validation loss: 0.5923039566323371\n",
            "Best validation loss: 0.5920049040372121\n",
            "Best validation loss: 0.5917070078049839\n",
            "Best validation loss: 0.5914102609244285\n",
            "Best validation loss: 0.5911146564423158\n",
            "Best validation loss: 0.5908201874628037\n",
            "Best validation loss: 0.5905268471468402\n",
            "Best validation loss: 0.5902346287115717\n",
            "Best validation loss: 0.5899435254297605\n",
            "Best validation loss: 0.5896535306292086\n",
            "Best validation loss: 0.5893646376921888\n",
            "Best validation loss: 0.5890768400548835\n",
            "Best validation loss: 0.5887901312068289\n",
            "Best validation loss: 0.5885045046903677\n",
            "Best validation loss: 0.5882199541001085\n",
            "Best validation loss: 0.5879364730823906\n",
            "Best validation loss: 0.5876540553347567\n",
            "Best validation loss: 0.5873726946054312\n",
            "Best validation loss: 0.587092384692806\n",
            "Best validation loss: 0.5868131194449311\n",
            "Best validation loss: 0.5865348927590125\n",
            "Best validation loss: 0.5862576985809163\n",
            "Best validation loss: 0.5859815309046777\n",
            "Best validation loss: 0.5857063837720182\n",
            "Best validation loss: 0.5854322512718652\n",
            "Best validation loss: 0.5851591275398809\n",
            "Best validation loss: 0.5848870067579953\n",
            "Best validation loss: 0.5846158831539441\n",
            "Best validation loss: 0.5843457510008139\n",
            "Best validation loss: 0.5840766046165913\n",
            "Best validation loss: 0.5838084383637185\n",
            "Best validation loss: 0.5835412466486535\n",
            "Best validation loss: 0.5832750239214356\n",
            "Best validation loss: 0.5830097646752571\n",
            "Best validation loss: 0.5827454634460383\n",
            "Best validation loss: 0.5824821148120087\n",
            "Best validation loss: 0.5822197133932937\n",
            "Best validation loss: 0.5819582538515046\n",
            "Best validation loss: 0.5816977308893342\n",
            "Best validation loss: 0.5814381392501579\n",
            "Best validation loss: 0.5811794737176383\n",
            "Best validation loss: 0.5809217291153352\n",
            "Best validation loss: 0.5806649003063196\n",
            "Best validation loss: 0.5804089821927925\n",
            "Best validation loss: 0.5801539697157081\n",
            "Best validation loss: 0.5798998578544021\n",
            "Best validation loss: 0.5796466416262224\n",
            "Best validation loss: 0.579394316086166\n",
            "Best validation loss: 0.5791428763265193\n",
            "Best validation loss: 0.5788923174765028\n",
            "Best validation loss: 0.578642634701919\n",
            "Best validation loss: 0.5783938232048055\n",
            "Best validation loss: 0.5781458782230919\n",
            "Best validation loss: 0.57789879503026\n",
            "Best validation loss: 0.5776525689350079\n",
            "Best validation loss: 0.5774071952809188\n",
            "Best validation loss: 0.5771626694461324\n",
            "Best validation loss: 0.5769189868430216\n",
            "Best validation loss: 0.5766761429178706\n",
            "Best validation loss: 0.576434133150559\n",
            "Best validation loss: 0.576192953054248\n",
            "Best validation loss: 0.5759525981750699\n",
            "Best validation loss: 0.575713064091823\n",
            "Best validation loss: 0.5754743464156674\n",
            "Best validation loss: 0.575236440789826\n",
            "Best validation loss: 0.5749993428892886\n",
            "Best validation loss: 0.5747630484205176\n",
            "Best validation loss: 0.5745275531211604\n",
            "Best validation loss: 0.5742928527597613\n",
            "Best validation loss: 0.5740589431354784\n",
            "Best validation loss: 0.5738258200778046\n",
            "Best validation loss: 0.573593479446289\n",
            "Best validation loss: 0.573361917130264\n",
            "Best validation loss: 0.5731311290485738\n",
            "Best validation loss: 0.5729011111493065\n",
            "Best validation loss: 0.5726718594095294\n",
            "Best validation loss: 0.5724433698350262\n",
            "Best validation loss: 0.572215638460038\n",
            "Best validation loss: 0.5719886613470067\n",
            "Best validation loss: 0.5717624345863214\n",
            "Best validation loss: 0.5715369542960677\n",
            "Best validation loss: 0.5713122166217788\n",
            "Best validation loss: 0.5710882177361906\n",
            "Best validation loss: 0.5708649538389986\n",
            "Best validation loss: 0.5706424211566173\n",
            "Best validation loss: 0.5704206159419432\n",
            "Best validation loss: 0.570199534474119\n",
            "Best validation loss: 0.5699791730583011\n",
            "Best validation loss: 0.5697595280254301\n",
            "Best validation loss: 0.5695405957320021\n",
            "Best validation loss: 0.5693223725598442\n",
            "Best validation loss: 0.5691048549158916\n",
            "Best validation loss: 0.5688880392319666\n",
            "Best validation loss: 0.5686719219645611\n",
            "Best validation loss: 0.5684564995946202\n",
            "Best validation loss: 0.568241768627329\n",
            "Best validation loss: 0.5680277255919014\n",
            "Best validation loss: 0.5678143670413701\n",
            "Best validation loss: 0.5676016895523813\n",
            "Best validation loss: 0.5673896897249884\n",
            "Best validation loss: 0.56717836418245\n",
            "Best validation loss: 0.5669677095710304\n",
            "Best validation loss: 0.5667577225597991\n",
            "Best validation loss: 0.5665483998404367\n",
            "Best validation loss: 0.5663397381270392\n",
            "Best validation loss: 0.5661317341559264\n",
            "Best validation loss: 0.5659243846854514\n",
            "Best validation loss: 0.5657176864958121\n",
            "Best validation loss: 0.5655116363888653\n",
            "Best validation loss: 0.565306231187942\n",
            "Best validation loss: 0.565101467737665\n",
            "Best validation loss: 0.5648973429037678\n",
            "Best validation loss: 0.5646938535729166\n",
            "Best validation loss: 0.5644909966525322\n",
            "Best validation loss: 0.564288769070616\n",
            "Best validation loss: 0.5640871677755757\n",
            "Best validation loss: 0.5638861897360541\n",
            "Best validation loss: 0.563685831940759\n",
            "Best validation loss: 0.5634860913982955\n",
            "Best validation loss: 0.5632869651369987\n",
            "Best validation loss: 0.5630884502047696\n",
            "Best validation loss: 0.5628905436689116\n",
            "Best validation loss: 0.5626932426159694\n",
            "Best validation loss: 0.5624965441515689\n",
            "Best validation loss: 0.5623004454002588\n",
            "Best validation loss: 0.562104943505354\n",
            "Best validation loss: 0.561910035628781\n",
            "Best validation loss: 0.5617157189509232\n",
            "Best validation loss: 0.5615219906704704\n",
            "Best validation loss: 0.5613288480042669\n",
            "Best validation loss: 0.5611362881871631\n",
            "Best validation loss: 0.5609443084718682\n",
            "Best validation loss: 0.5607529061288035\n",
            "Best validation loss: 0.5605620784459582\n",
            "Best validation loss: 0.5603718227287461\n",
            "Best validation loss: 0.5601821362998638\n",
            "Best validation loss: 0.5599930164991501\n",
            "Best validation loss: 0.559804460683447\n",
            "Best validation loss: 0.5596164662264624\n",
            "Best validation loss: 0.5594290305186335\n",
            "Best validation loss: 0.5592421509669914\n",
            "Best validation loss: 0.5590558249950284\n",
            "Best validation loss: 0.5588700500425643\n",
            "Best validation loss: 0.558684823565617\n",
            "Best validation loss: 0.5585001430362709\n",
            "Best validation loss: 0.5583160059425497\n",
            "Best validation loss: 0.5581324097882884\n",
            "Best validation loss: 0.5579493520930068\n",
            "Best validation loss: 0.557766830391786\n",
            "Best validation loss: 0.5575848422351435\n",
            "Best validation loss: 0.5574033851889109\n",
            "Best validation loss: 0.5572224568341131\n",
            "Best validation loss: 0.5570420547668476\n",
            "Best validation loss: 0.5568621765981658\n",
            "Best validation loss: 0.5566828199539552\n",
            "Best validation loss: 0.5565039824748218\n",
            "Best validation loss: 0.556325661815976\n",
            "Best validation loss: 0.5561478556471164\n",
            "Best validation loss: 0.555970561652318\n",
            "Best validation loss: 0.555793777529918\n",
            "Best validation loss: 0.5556175009924061\n",
            "Best validation loss: 0.5554417297663132\n",
            "Best validation loss: 0.5552664615921027\n",
            "Best validation loss: 0.5550916942240619\n",
            "Best validation loss: 0.554917425430195\n",
            "Best validation loss: 0.554743652992117\n",
            "Best validation loss: 0.5545703747049485\n",
            "Best validation loss: 0.5543975883772113\n",
            "Best validation loss: 0.554225291830725\n",
            "Best validation loss: 0.5540534829005055\n",
            "Best validation loss: 0.5538821594346626\n",
            "Best validation loss: 0.5537113192943003\n",
            "Best validation loss: 0.5535409603534174\n",
            "Best validation loss: 0.5533710804988085\n",
            "Best validation loss: 0.5532016776299665\n",
            "Best validation loss: 0.5530327496589864\n",
            "Best validation loss: 0.5528642945104686\n",
            "Best validation loss: 0.5526963101214246\n",
            "Best validation loss: 0.5525287944411825\n",
            "Best validation loss: 0.5523617454312941\n",
            "Best validation loss: 0.5521951610654425\n",
            "Best validation loss: 0.5520290393293499\n",
            "Best validation loss: 0.5518633782206881\n",
            "Best validation loss: 0.5516981757489876\n",
            "Best validation loss: 0.5515334299355489\n",
            "Best validation loss: 0.5513691388133545\n",
            "Best validation loss: 0.5512053004269811\n",
            "Best validation loss: 0.5510419128325129\n",
            "Best validation loss: 0.5508789740974563\n",
            "Best validation loss: 0.5507164823006542\n",
            "Best validation loss: 0.5505544355322015\n",
            "Best validation loss: 0.5503928318933626\n",
            "Best validation loss: 0.5502316694964873\n",
            "Best validation loss: 0.5500709464649296\n",
            "Best validation loss: 0.5499106609329659\n",
            "Best validation loss: 0.5497508110457144\n",
            "Best validation loss: 0.549591394959056\n",
            "Best validation loss: 0.5494324108395533\n",
            "Best validation loss: 0.5492738568643742\n",
            "Best validation loss: 0.5491157312212129\n",
            "Best validation loss: 0.5489580321082129\n",
            "Best validation loss: 0.5488007577338909\n",
            "Best validation loss: 0.5486439063170608\n",
            "Best validation loss: 0.5484874760867589\n",
            "Best validation loss: 0.5483314652821691\n",
            "Best validation loss: 0.5481758721525497\n",
            "Best validation loss: 0.5480206949571597\n",
            "Best validation loss: 0.5478659319651867\n",
            "Best validation loss: 0.547711581455675\n",
            "Best validation loss: 0.5475576417174544\n",
            "Best validation loss: 0.5474041110490695\n",
            "Best validation loss: 0.5472509877587097\n",
            "Best validation loss: 0.5470982701641404\n",
            "Best validation loss: 0.5469459565926335\n",
            "Best validation loss: 0.5467940453808995\n",
            "Best validation loss: 0.5466425348750202\n",
            "Best validation loss: 0.5464914234303815\n",
            "Best validation loss: 0.546340709411607\n",
            "Best validation loss: 0.5461903911924921\n",
            "Best validation loss: 0.5460404671559389\n",
            "Best validation loss: 0.5458909356938914\n",
            "Best validation loss: 0.5457417952072714\n",
            "Best validation loss: 0.545593044105915\n",
            "Best validation loss: 0.5454446808085093\n",
            "Best validation loss: 0.54529670374253\n",
            "Best validation loss: 0.5451491113441798\n",
            "Best validation loss: 0.545001902058326\n",
            "Best validation loss: 0.5448550743384408\n",
            "Best validation loss: 0.5447086266465401\n",
            "Best validation loss: 0.5445625574531235\n",
            "Best validation loss: 0.5444168652371157\n",
            "Best validation loss: 0.5442715484858066\n",
            "Best validation loss: 0.5441266056947938\n",
            "Best validation loss: 0.5439820353679243\n",
            "Best validation loss: 0.543837836017237\n",
            "Best validation loss: 0.5436940061629062\n",
            "Best validation loss: 0.5435505443331845\n",
            "Best validation loss: 0.5434074490643475\n",
            "Best validation loss: 0.543264718900638\n",
            "Best validation loss: 0.5431223523942106\n",
            "Best validation loss: 0.5429803481050782\n",
            "Best validation loss: 0.5428387046010565\n",
            "Best validation loss: 0.5426974204577115\n",
            "Best validation loss: 0.5425564942583055\n",
            "Best validation loss: 0.5424159245937447\n",
            "Best validation loss: 0.5422757100625267\n",
            "Best validation loss: 0.5421358492706889\n",
            "Best validation loss: 0.5419963408317564\n",
            "Best validation loss: 0.5418571833666916\n",
            "Best validation loss: 0.5417183755038433\n",
            "Best validation loss: 0.5415799158788964\n",
            "Best validation loss: 0.541441803134822\n",
            "Best validation loss: 0.5413040359218286\n",
            "Best validation loss: 0.5411666128973123\n",
            "Best validation loss: 0.5410295327258087\n",
            "Best validation loss: 0.5408927940789446\n",
            "Best validation loss: 0.5407563956353904\n",
            "Best validation loss: 0.5406203360808123\n",
            "Best validation loss: 0.5404846141078254\n",
            "Best validation loss: 0.5403492284159477\n",
            "Best validation loss: 0.5402141777115524\n",
            "Best validation loss: 0.5400794607078236\n",
            "Best validation loss: 0.53994507612471\n",
            "Best validation loss: 0.5398110226888795\n",
            "Best validation loss: 0.539677299133675\n",
            "Best validation loss: 0.5395439041990697\n",
            "Best validation loss: 0.5394108366316234\n",
            "Best validation loss: 0.5392780951844378\n",
            "Best validation loss: 0.5391456786171144\n",
            "Best validation loss: 0.5390135856957106\n",
            "Best validation loss: 0.5388818151926973\n",
            "Best validation loss: 0.538750365886917\n",
            "Best validation loss: 0.5386192365635412\n",
            "Best validation loss: 0.5384884260140289\n",
            "Best validation loss: 0.5383579330360855\n",
            "Best validation loss: 0.5382277564336215\n",
            "Best validation loss: 0.5380978950167123\n",
            "Best validation loss: 0.5379683476015573\n",
            "Best validation loss: 0.5378391130104404\n",
            "Best validation loss: 0.5377101900716899\n",
            "Best validation loss: 0.5375815776196389\n",
            "Best validation loss: 0.5374532744945874\n",
            "Best validation loss: 0.537325279542762\n",
            "Best validation loss: 0.5371975916162783\n",
            "Best validation loss: 0.5370702095731026\n",
            "Best validation loss: 0.5369431322770143\n",
            "Best validation loss: 0.5368163585975674\n",
            "Best validation loss: 0.5366898874100543\n",
            "Best validation loss: 0.5365637175954682\n",
            "Best validation loss: 0.5364378480404665\n",
            "Best validation loss: 0.5363122776373348\n",
            "Best validation loss: 0.53618700528395\n",
            "Best validation loss: 0.5360620298837452\n",
            "Best validation loss: 0.5359373503456736\n",
            "Best validation loss: 0.5358129655841735\n",
            "Best validation loss: 0.5356888745191332\n",
            "Best validation loss: 0.5355650760758562\n",
            "Best validation loss: 0.5354415691850267\n",
            "Best validation loss: 0.5353183527826761\n",
            "Best validation loss: 0.5351954258101473\n",
            "Best validation loss: 0.5350727872140632\n",
            "Best validation loss: 0.5349504359462911\n",
            "Best validation loss: 0.5348283709639117\n",
            "Best validation loss: 0.5347065912291843\n",
            "Best validation loss: 0.5345850957095154\n",
            "Best validation loss: 0.5344638833774255\n",
            "Best validation loss: 0.5343429532105172\n",
            "Best validation loss: 0.5342223041914436\n",
            "Best validation loss: 0.534101935307876\n",
            "Best validation loss: 0.5339818455524729\n",
            "Best validation loss: 0.5338620339228488\n",
            "Best validation loss: 0.5337424994215427\n",
            "Best validation loss: 0.5336232410559884\n",
            "Best validation loss: 0.5335042578384825\n",
            "Best validation loss: 0.5333855487861557\n",
            "Best validation loss: 0.5332671129209419\n",
            "Best validation loss: 0.5331489492695481\n",
            "Best validation loss: 0.5330310568634258\n",
            "Best validation loss: 0.5329134347387408\n",
            "Best validation loss: 0.5327960819363441\n",
            "Best validation loss: 0.5326789975017434\n",
            "Best validation loss: 0.5325621804850743\n",
            "Best validation loss: 0.5324456299410715\n",
            "Best validation loss: 0.5323293449290405\n",
            "Best validation loss: 0.5322133245128298\n",
            "Best validation loss: 0.532097567760803\n",
            "Best validation loss: 0.5319820737458111\n",
            "Best validation loss: 0.5318668415451647\n",
            "Best validation loss: 0.5317518702406073\n",
            "Best validation loss: 0.5316371589182881\n",
            "Best validation loss: 0.5315227066687347\n",
            "Best validation loss: 0.5314085125868269\n",
            "Best validation loss: 0.5312945757717702\n",
            "Best validation loss: 0.5311808953270694\n",
            "Best validation loss: 0.5310674703605027\n",
            "Best validation loss: 0.5309542999840955\n",
            "Best validation loss: 0.5308413833140951\n",
            "Best validation loss: 0.5307287194709452\n",
            "Best validation loss: 0.5306163075792598\n",
            "Best validation loss: 0.5305041467677992\n",
            "Best validation loss: 0.5303922361694446\n",
            "Best validation loss: 0.530280574921173\n",
            "Best validation loss: 0.5301691621640326\n",
            "Best validation loss: 0.5300579970431196\n",
            "Best validation loss: 0.5299470787075522\n",
            "Best validation loss: 0.5298364063104478\n",
            "Best validation loss: 0.5297259790088987\n",
            "Best validation loss: 0.5296157959639484\n",
            "Best validation loss: 0.5295058563405679\n",
            "Best validation loss: 0.529396159307633\n",
            "Best validation loss: 0.5292867040378999\n",
            "Best validation loss: 0.5291774897079837\n",
            "Best validation loss: 0.5290685154983344\n",
            "Best validation loss: 0.5289597805932142\n",
            "Best validation loss: 0.5288512841806762\n",
            "Best validation loss: 0.52874302545254\n",
            "Best validation loss: 0.5286350036043715\n",
            "Best validation loss: 0.5285272178354598\n",
            "Best validation loss: 0.528419667348795\n",
            "Best validation loss: 0.5283123513510473\n",
            "Best validation loss: 0.5282052690525452\n",
            "Best validation loss: 0.5280984196672537\n",
            "Best validation loss: 0.527991802412753\n",
            "Best validation loss: 0.5278854165102181\n",
            "Best validation loss: 0.5277792611843968\n",
            "Best validation loss: 0.5276733356635895\n",
            "Best validation loss: 0.527567639179629\n",
            "Best validation loss: 0.5274621709678584\n",
            "Best validation loss: 0.5273569302671123\n",
            "Best validation loss: 0.5272519163196956\n",
            "Best validation loss: 0.5271471283713639\n",
            "Best validation loss: 0.5270425656713033\n",
            "Best validation loss: 0.5269382274721103\n",
            "Best validation loss: 0.526834113029773\n",
            "Best validation loss: 0.5267302216036504\n",
            "Best validation loss: 0.5266265524564536\n",
            "Best validation loss: 0.5265231048542272\n",
            "Best validation loss: 0.5264198780663284\n",
            "Best validation loss: 0.5263168713654095\n",
            "Best validation loss: 0.5262140840273987\n",
            "Best validation loss: 0.526111515331481\n",
            "Best validation loss: 0.5260091645600796\n",
            "Best validation loss: 0.525907030998838\n",
            "Best validation loss: 0.5258051139366006\n",
            "Best validation loss: 0.5257034126653957\n",
            "Best validation loss: 0.5256019264804165\n",
            "Best validation loss: 0.5255006546800034\n",
            "Best validation loss: 0.5253995965656262\n",
            "Best validation loss: 0.5252987514418662\n",
            "Best validation loss: 0.5251981186163988\n",
            "Best validation loss: 0.5250976973999757\n",
            "Best validation loss: 0.5249974871064079\n",
            "Best validation loss: 0.5248974870525482\n",
            "Convergence reached. Stopping training.\n",
            "Best validation loss: 1.6187003577026162\n",
            "Best validation loss: 1.5947478378278124\n",
            "Best validation loss: 1.5714958227707547\n",
            "Best validation loss: 1.5489519907560025\n",
            "Best validation loss: 1.52711796452951\n",
            "Best validation loss: 1.5059894774070726\n",
            "Best validation loss: 1.4855569134897275\n",
            "Best validation loss: 1.4658061213396467\n",
            "Best validation loss: 1.4467193734823616\n",
            "Best validation loss: 1.4282763481453145\n",
            "Best validation loss: 1.4104550364928012\n",
            "Best validation loss: 1.393232515456566\n",
            "Best validation loss: 1.376585561576586\n",
            "Best validation loss: 1.3604911081564224\n",
            "Best validation loss: 1.3449265644682076\n",
            "Best validation loss: 1.3298700229990852\n",
            "Best validation loss: 1.3153003815274493\n",
            "Best validation loss: 1.3011974039182863\n",
            "Best validation loss: 1.287541739071759\n",
            "Best validation loss: 1.2743149128137514\n",
            "Best validation loss: 1.2614993033974098\n",
            "Best validation loss: 1.2490781079629127\n",
            "Best validation loss: 1.237035304794509\n",
            "Best validation loss: 1.2253556144129\n",
            "Best validation loss: 1.214024461301409\n",
            "Best validation loss: 1.203027937243674\n",
            "Best validation loss: 1.192352766728474\n",
            "Best validation loss: 1.1819862745601646\n",
            "Best validation loss: 1.1719163556320042\n",
            "Best validation loss: 1.1621314467257038\n",
            "Best validation loss: 1.152620500160431\n",
            "Best validation loss: 1.1433729591061357\n",
            "Best validation loss: 1.1343787343851204\n",
            "Best validation loss: 1.125628182603294\n",
            "Best validation loss: 1.1171120854731815\n",
            "Best validation loss: 1.1088216302113347\n",
            "Best validation loss: 1.1007483909117497\n",
            "Best validation loss: 1.092884310813482\n",
            "Best validation loss: 1.0852216853946208\n",
            "Best validation loss: 1.0777531462363001\n",
            "Best validation loss: 1.0704716456096828\n",
            "Best validation loss: 1.0633704417461483\n",
            "Best validation loss: 1.056443084756642\n",
            "Best validation loss: 1.049683403170475\n",
            "Best validation loss: 1.043085491067197\n",
            "Best validation loss: 1.0366436957776348\n",
            "Best validation loss: 1.030352606132028\n",
            "Best validation loss: 1.024207041234562\n",
            "Best validation loss: 1.0182020397446028\n",
            "Best validation loss: 1.0123328496456998\n",
            "Best validation loss: 1.006594918483996\n",
            "Best validation loss: 1.0009838840581624\n",
            "Best validation loss: 0.9954955655433619\n",
            "Best validation loss: 0.9901259550320994\n",
            "Best validation loss: 0.9848712094751667\n",
            "Best validation loss: 0.9797276430062266\n",
            "Best validation loss: 0.9746917196339251\n",
            "Best validation loss: 0.9697600462857978\n",
            "Best validation loss: 0.9649293661886164\n",
            "Best validation loss: 0.9601965525702233\n",
            "Best validation loss: 0.9555586026683348\n",
            "Best validation loss: 0.9510126320322217\n",
            "Best validation loss: 0.9465558691036335\n",
            "Best validation loss: 0.9421856500637972\n",
            "Best validation loss: 0.9378994139337888\n",
            "Best validation loss: 0.9336946979160494\n",
            "Best validation loss: 0.9295691329652966\n",
            "Best validation loss: 0.9255204395775516\n",
            "Best validation loss: 0.921546423786478\n",
            "Best validation loss: 0.9176449733566854\n",
            "Best validation loss: 0.9138140541641113\n",
            "Best validation loss: 0.9100517067540443\n",
            "Best validation loss: 0.9063560430677718\n",
            "Best validation loss: 0.9027252433292824\n",
            "Best validation loss: 0.8991575530838389\n",
            "Best validation loss: 0.89565128038065\n",
            "Best validation loss: 0.8922047930922491\n",
            "Best validation loss: 0.8888165163635459\n",
            "Best validation loss: 0.8854849301838876\n",
            "Best validation loss: 0.8822085670757911\n",
            "Best validation loss: 0.8789860098943356\n",
            "Best validation loss: 0.8758158897315208\n",
            "Best validation loss: 0.8726968839201855\n",
            "Best validation loss: 0.8696277141323655\n",
            "Best validation loss: 0.8666071445672413\n",
            "Best validation loss: 0.8636339802240766\n",
            "Best validation loss: 0.8607070652557983\n",
            "Best validation loss: 0.8578252813990938\n",
            "Best validation loss: 0.8549875464771239\n",
            "Best validation loss: 0.8521928129711577\n",
            "Best validation loss: 0.8494400666576312\n",
            "Best validation loss: 0.8467283253073192\n",
            "Best validation loss: 0.8440566374434891\n",
            "Best validation loss: 0.8414240811560669\n",
            "Best validation loss: 0.8388297629690115\n",
            "Best validation loss: 0.8362728167582385\n",
            "Best validation loss: 0.8337524027175784\n",
            "Best validation loss: 0.8312677063703867\n",
            "Best validation loss: 0.8288179376245522\n",
            "Best validation loss: 0.8264023298687705\n",
            "Best validation loss: 0.8240201391080576\n",
            "Best validation loss: 0.8216706431365925\n",
            "Best validation loss: 0.8193531407460742\n",
            "Best validation loss: 0.8170669509678756\n",
            "Best validation loss: 0.814811412347369\n",
            "Best validation loss: 0.8125858822488766\n",
            "Best validation loss: 0.8103897361897912\n",
            "Best validation loss: 0.8082223672024765\n",
            "Best validation loss: 0.806083185222637\n",
            "Best validation loss: 0.8039716165029112\n",
            "Best validation loss: 0.8018871030505077\n",
            "Best validation loss: 0.7998291020877644\n",
            "Best validation loss: 0.7977970855345669\n",
            "Best validation loss: 0.7957905395116207\n",
            "Best validation loss: 0.7938089638636155\n",
            "Best validation loss: 0.7918518717013776\n",
            "Best validation loss: 0.7899187889621452\n",
            "Best validation loss: 0.7880092539871478\n",
            "Best validation loss: 0.7861228171157113\n",
            "Best validation loss: 0.7842590402951499\n",
            "Best validation loss: 0.7824174967057417\n",
            "Best validation loss: 0.7805977704001191\n",
            "Best validation loss: 0.7787994559564413\n",
            "Best validation loss: 0.7770221581447414\n",
            "Best validation loss: 0.7752654916058771\n",
            "Best validation loss: 0.7735290805425369\n",
            "Best validation loss: 0.7718125584217799\n",
            "Best validation loss: 0.7701155676886182\n",
            "Best validation loss: 0.7684377594901667\n",
            "Best validation loss: 0.7667787934099132\n",
            "Best validation loss: 0.7651383372116816\n",
            "Best validation loss: 0.763516066592879\n",
            "Best validation loss: 0.7619116649466422\n",
            "Best validation loss: 0.7603248231325084\n",
            "Best validation loss: 0.7587552392552622\n",
            "Best validation loss: 0.7572026184516207\n",
            "Best validation loss: 0.7556666726844347\n",
            "Best validation loss: 0.7541471205441012\n",
            "Best validation loss: 0.7526436870568967\n",
            "Best validation loss: 0.7511561034999489\n",
            "Best validation loss: 0.749684107222583\n",
            "Best validation loss: 0.7482274414737903\n",
            "Best validation loss: 0.7467858552355702\n",
            "Best validation loss: 0.7453591030619218\n",
            "Best validation loss: 0.7439469449232575\n",
            "Best validation loss: 0.7425491460560287\n",
            "Best validation loss: 0.7411654768173628\n",
            "Best validation loss: 0.7397957125445147\n",
            "Best validation loss: 0.7384396334189515\n",
            "Best validation loss: 0.7370970243348891\n",
            "Best validation loss: 0.7357676747721164\n",
            "Best validation loss: 0.7344513786729386\n",
            "Best validation loss: 0.7331479343230914\n",
            "Best validation loss: 0.7318571442364711\n",
            "Best validation loss: 0.7305788150435437\n",
            "Best validation loss: 0.7293127573832923\n",
            "Best validation loss: 0.7280587857985747\n",
            "Best validation loss: 0.7268167186347669\n",
            "Best validation loss: 0.725586377941569\n",
            "Best validation loss: 0.7243675893778625\n",
            "Best validation loss: 0.7231601821195068\n",
            "Best validation loss: 0.7219639887699684\n",
            "Best validation loss: 0.7207788452736841\n",
            "Best validation loss: 0.7196045908320586\n",
            "Best validation loss: 0.7184410678220032\n",
            "Best validation loss: 0.7172881217169284\n",
            "Best validation loss: 0.7161456010101004\n",
            "Best validation loss: 0.7150133571402844\n",
            "Best validation loss: 0.7138912444195884\n",
            "Best validation loss: 0.712779119963439\n",
            "Best validation loss: 0.7116768436226091\n",
            "Best validation loss: 0.7105842779172321\n",
            "Best validation loss: 0.7095012879727332\n",
            "Best validation loss: 0.7084277414576111\n",
            "Best validation loss: 0.7073635085230122\n",
            "Best validation loss: 0.7063084617440314\n",
            "Best validation loss: 0.7052624760626875\n",
            "Best validation loss: 0.7042254287325125\n",
            "Best validation loss: 0.7031971992647057\n",
            "Best validation loss: 0.7021776693757978\n",
            "Best validation loss: 0.7011667229367777\n",
            "Best validation loss: 0.7001642459236335\n",
            "Best validation loss: 0.69917012636926\n",
            "Best validation loss: 0.6981842543166938\n",
            "Best validation loss: 0.6972065217736262\n",
            "Best validation loss: 0.6962368226681589\n",
            "Best validation loss: 0.6952750528057597\n",
            "Best validation loss: 0.6943211098273819\n",
            "Best validation loss: 0.6933748931687087\n",
            "Best validation loss: 0.692436304020489\n",
            "Best validation loss: 0.6915052452899305\n",
            "Best validation loss: 0.6905816215631161\n",
            "Best validation loss: 0.6896653390684108\n",
            "Best validation loss: 0.6887563056408336\n",
            "Best validation loss: 0.6878544306873574\n",
            "Best validation loss: 0.6869596251531155\n",
            "Best validation loss: 0.6860718014884818\n",
            "Best validation loss: 0.6851908736170016\n",
            "Best validation loss: 0.6843167569041457\n",
            "Best validation loss: 0.683449368126864\n",
            "Best validation loss: 0.6825886254439135\n",
            "Best validation loss: 0.6817344483669386\n",
            "Best validation loss: 0.6808867577322822\n",
            "Best validation loss: 0.6800454756735029\n",
            "Best validation loss: 0.6792105255945816\n",
            "Best validation loss: 0.6783818321437945\n",
            "Best validation loss: 0.6775593211882344\n",
            "Best validation loss: 0.6767429197889595\n",
            "Best validation loss: 0.6759325561767564\n",
            "Best validation loss: 0.675128159728493\n",
            "Best validation loss: 0.6743296609440497\n",
            "Best validation loss: 0.6735369914238094\n",
            "Best validation loss: 0.6727500838466931\n",
            "Best validation loss: 0.6719688719487226\n",
            "Best validation loss: 0.6711932905020973\n",
            "Best validation loss: 0.6704232752947706\n",
            "Best validation loss: 0.6696587631105123\n",
            "Best validation loss: 0.6688996917094416\n",
            "Best validation loss: 0.6681459998090193\n",
            "Best validation loss: 0.6673976270654874\n",
            "Best validation loss: 0.6666545140557406\n",
            "Best validation loss: 0.6659166022596218\n",
            "Best validation loss: 0.6651838340426268\n",
            "Best validation loss: 0.6644561526390091\n",
            "Best validation loss: 0.663733502135273\n",
            "Best validation loss: 0.6630158274540449\n",
            "Best validation loss: 0.6623030743383126\n",
            "Best validation loss: 0.6615951893360236\n",
            "Best validation loss: 0.6608921197850305\n",
            "Best validation loss: 0.6601938137983768\n",
            "Best validation loss: 0.6595002202499136\n",
            "Best validation loss: 0.6588112887602372\n",
            "Best validation loss: 0.6581269696829399\n",
            "Best validation loss: 0.6574472140911678\n",
            "Best validation loss: 0.656771973764473\n",
            "Best validation loss: 0.656101201175958\n",
            "Best validation loss: 0.6554348494797012\n",
            "Best validation loss: 0.6547728724984558\n",
            "Best validation loss: 0.65411522471162\n",
            "Best validation loss: 0.6534618612434644\n",
            "Best validation loss: 0.6528127378516163\n",
            "Best validation loss: 0.6521678109157909\n",
            "Best validation loss: 0.6515270374267649\n",
            "Best validation loss: 0.6508903749755846\n",
            "Best validation loss: 0.6502577817430043\n",
            "Best validation loss: 0.6496292164891501\n",
            "Best validation loss: 0.6490046385433995\n",
            "Best validation loss: 0.6483840077944759\n",
            "Best validation loss: 0.6477672846807507\n",
            "Best validation loss: 0.6471544301807474\n",
            "Best validation loss: 0.6465454058038438\n",
            "Best validation loss: 0.6459401735811677\n",
            "Best validation loss: 0.64533869605668\n",
            "Best validation loss: 0.644740936278442\n",
            "Best validation loss: 0.6441468577900609\n",
            "Best validation loss: 0.6435564246223126\n",
            "Best validation loss: 0.6429696012849324\n",
            "Best validation loss: 0.6423863527585747\n",
            "Best validation loss: 0.6418066444869335\n",
            "Best validation loss: 0.6412304423690239\n",
            "Best validation loss: 0.6406577127516171\n",
            "Best validation loss: 0.6400884224218266\n",
            "Best validation loss: 0.6395225385998443\n",
            "Best validation loss: 0.6389600289318189\n",
            "Best validation loss: 0.6384008614828771\n",
            "Best validation loss: 0.6378450047302817\n",
            "Best validation loss: 0.6372924275567237\n",
            "Best validation loss: 0.6367430992437483\n",
            "Best validation loss: 0.6361969894653068\n",
            "Best validation loss: 0.6356540682814357\n",
            "Best validation loss: 0.6351143061320578\n",
            "Best validation loss: 0.6345776738309039\n",
            "Best validation loss: 0.6340441425595513\n",
            "Best validation loss: 0.6335136838615759\n",
            "Best validation loss: 0.6329862696368191\n",
            "Best validation loss: 0.632461872135761\n",
            "Best validation loss: 0.6319404639540024\n",
            "Best validation loss: 0.6314220180268515\n",
            "Best validation loss: 0.6309065076240123\n",
            "Best validation loss: 0.6303939063443732\n",
            "Best validation loss: 0.629884188110894\n",
            "Best validation loss: 0.6293773271655876\n",
            "Best validation loss: 0.628873298064597\n",
            "Best validation loss: 0.6283720756733618\n",
            "Best validation loss: 0.6278736351618759\n",
            "Best validation loss: 0.6273779520000322\n",
            "Best validation loss: 0.6268850019530529\n",
            "Best validation loss: 0.6263947610770042\n",
            "Best validation loss: 0.6259072057143932\n",
            "Best validation loss: 0.6254223124898435\n",
            "Best validation loss: 0.6249400583058524\n",
            "Best validation loss: 0.6244604203386219\n",
            "Best validation loss: 0.6239833760339677\n",
            "Best validation loss: 0.6235089031032993\n",
            "Best validation loss: 0.623036979519675\n",
            "Best validation loss: 0.6225675835139262\n",
            "Best validation loss: 0.6221006935708494\n",
            "Best validation loss: 0.6216362884254695\n",
            "Best validation loss: 0.621174347059366\n",
            "Best validation loss: 0.6207148486970664\n",
            "Best validation loss: 0.6202577728025012\n",
            "Best validation loss: 0.6198030990755238\n",
            "Best validation loss: 0.6193508074484889\n",
            "Best validation loss: 0.6189008780828922\n",
            "Best validation loss: 0.6184532913660679\n",
            "Best validation loss: 0.618008027907944\n",
            "Best validation loss: 0.6175650685378534\n",
            "Best validation loss: 0.6171243943014005\n",
            "Best validation loss: 0.6166859864573813\n",
            "Best validation loss: 0.6162498264747561\n",
            "Best validation loss: 0.6158158960296755\n",
            "Best validation loss: 0.6153841770025549\n",
            "Best validation loss: 0.6149546514752008\n",
            "Best validation loss: 0.6145273017279842\n",
            "Best validation loss: 0.6141021102370631\n",
            "Best validation loss: 0.6136790596716508\n",
            "Best validation loss: 0.6132581328913309\n",
            "Best validation loss: 0.6128393129434171\n",
            "Best validation loss: 0.6124225830603567\n",
            "Best validation loss: 0.6120079266571775\n",
            "Best validation loss: 0.6115953273289785\n",
            "Best validation loss: 0.611184768848459\n",
            "Best validation loss: 0.6107762351634924\n",
            "Best validation loss: 0.6103697103947375\n",
            "Best validation loss: 0.6099651788332889\n",
            "Best validation loss: 0.6095626249383677\n",
            "Best validation loss: 0.609162033335048\n",
            "Best validation loss: 0.6087633888120221\n",
            "Best validation loss: 0.6083666763194002\n",
            "Best validation loss: 0.6079718809665479\n",
            "Best validation loss: 0.6075789880199566\n",
            "Best validation loss: 0.6071879829011494\n",
            "Best validation loss: 0.6067988511846203\n",
            "Best validation loss: 0.6064115785958064\n",
            "Best validation loss: 0.6060261510090927\n",
            "Best validation loss: 0.605642554445848\n",
            "Best validation loss: 0.6052607750724932\n",
            "Best validation loss: 0.6048807991985992\n",
            "Best validation loss: 0.604502613275015\n",
            "Best validation loss: 0.6041262038920258\n",
            "Best validation loss: 0.6037515577775397\n",
            "Best validation loss: 0.6033786617953028\n",
            "Best validation loss: 0.6030075029431418\n",
            "Best validation loss: 0.6026380683512355\n",
            "Best validation loss: 0.6022703452804108\n",
            "Best validation loss: 0.6019043211204677\n",
            "Best validation loss: 0.6015399833885284\n",
            "Best validation loss: 0.601177319727412\n",
            "Best validation loss: 0.6008163179040351\n",
            "Best validation loss: 0.6004569658078365\n",
            "Best validation loss: 0.6000992514492258\n",
            "Best validation loss: 0.5997431629580557\n",
            "Best validation loss: 0.599388688582118\n",
            "Best validation loss: 0.5990358166856619\n",
            "Best validation loss: 0.5986845357479347\n",
            "Best validation loss: 0.5983348343617454\n",
            "Best validation loss: 0.5979867012320489\n",
            "Best validation loss: 0.5976401251745521\n",
            "Best validation loss: 0.5972950951143405\n",
            "Best validation loss: 0.5969516000845255\n",
            "Best validation loss: 0.596609629224912\n",
            "Best validation loss: 0.5962691717806858\n",
            "Best validation loss: 0.5959302171011203\n",
            "Best validation loss: 0.5955927546383022\n",
            "Best validation loss: 0.5952567739458763\n",
            "Best validation loss: 0.5949222646778087\n",
            "Best validation loss: 0.5945892165871685\n",
            "Best validation loss: 0.5942576195249258\n",
            "Best validation loss: 0.5939274634387705\n",
            "Best validation loss: 0.5935987383719439\n",
            "Best validation loss: 0.5932714344620913\n",
            "Best validation loss: 0.5929455419401289\n",
            "Best validation loss: 0.5926210511291282\n",
            "Best validation loss: 0.592297952443215\n",
            "Best validation loss: 0.5919762363864868\n",
            "Best validation loss: 0.5916558935519429\n",
            "Best validation loss: 0.5913369146204324\n",
            "Best validation loss: 0.5910192903596146\n",
            "Best validation loss: 0.5907030116229368\n",
            "Best validation loss: 0.5903880693486249\n",
            "Best validation loss: 0.5900744545586889\n",
            "Best validation loss: 0.5897621583579422\n",
            "Best validation loss: 0.5894511719330359\n",
            "Best validation loss: 0.589141486551504\n",
            "Best validation loss: 0.5888330935608259\n",
            "Best validation loss: 0.5885259843874983\n",
            "Best validation loss: 0.5882201505361218\n",
            "Best validation loss: 0.5879155835885014\n",
            "Best validation loss: 0.5876122752027565\n",
            "Best validation loss: 0.5873102171124465\n",
            "Best validation loss: 0.5870094011257055\n",
            "Best validation loss: 0.5867098191243919\n",
            "Best validation loss: 0.586411463063247\n",
            "Best validation loss: 0.5861143249690669\n",
            "Best validation loss: 0.5858183969398857\n",
            "Best validation loss: 0.5855236711441686\n",
            "Best validation loss: 0.5852301398200175\n",
            "Best validation loss: 0.5849377952743863\n",
            "Best validation loss: 0.5846466298823079\n",
            "Best validation loss: 0.5843566360861302\n",
            "Best validation loss: 0.5840678063947644\n",
            "Best validation loss: 0.5837801333829415\n",
            "Best validation loss: 0.58349360969048\n",
            "Best validation loss: 0.5832082280215638\n",
            "Best validation loss: 0.5829239811440272\n",
            "Best validation loss: 0.5826408618886535\n",
            "Best validation loss: 0.5823588631484797\n",
            "Best validation loss: 0.5820779778781114\n",
            "Best validation loss: 0.5817981990930475\n",
            "Best validation loss: 0.5815195198690131\n",
            "Best validation loss: 0.5812419333413008\n",
            "Best validation loss: 0.5809654327041225\n",
            "Best validation loss: 0.5806900112099674\n",
            "Best validation loss: 0.58041566216897\n",
            "Best validation loss: 0.5801423789482861\n",
            "Best validation loss: 0.5798701549714764\n",
            "Best validation loss: 0.5795989837178984\n",
            "Best validation loss: 0.5793288587221072\n",
            "Best validation loss: 0.5790597735732618\n",
            "Best validation loss: 0.5787917219145418\n",
            "Best validation loss: 0.5785246974425695\n",
            "Best validation loss: 0.5782586939068408\n",
            "Best validation loss: 0.577993705109162\n",
            "Best validation loss: 0.5777297249030959\n",
            "Best validation loss: 0.5774667471934133\n",
            "Best validation loss: 0.5772047659355514\n",
            "Best validation loss: 0.5769437751350808\n",
            "Best validation loss: 0.5766837688471773\n",
            "Best validation loss: 0.5764247411761022\n",
            "Best validation loss: 0.5761666862746873\n",
            "Best validation loss: 0.575909598343828\n",
            "Best validation loss: 0.5756534716319825\n",
            "Best validation loss: 0.5753983004346762\n",
            "Best validation loss: 0.5751440790940139\n",
            "Best validation loss: 0.5748908019981965\n",
            "Best validation loss: 0.5746384635810456\n",
            "Best validation loss: 0.5743870583215323\n",
            "Best validation loss: 0.5741365807433124\n",
            "Best validation loss: 0.5738870254142685\n",
            "Best validation loss: 0.573638386946056\n",
            "Best validation loss: 0.5733906599936559\n",
            "Best validation loss: 0.5731438392549324\n",
            "Best validation loss: 0.5728979194701972\n",
            "Best validation loss: 0.572652895421777\n",
            "Best validation loss: 0.5724087619335888\n",
            "Best validation loss: 0.5721655138707186\n",
            "Best validation loss: 0.5719231461390055\n",
            "Best validation loss: 0.5716816536846322\n",
            "Best validation loss: 0.5714410314937183\n",
            "Best validation loss: 0.5712012745919207\n",
            "Best validation loss: 0.5709623780440373\n",
            "Best validation loss: 0.5707243369536161\n",
            "Best validation loss: 0.5704871464625695\n",
            "Best validation loss: 0.5702508017507917\n",
            "Best validation loss: 0.5700152980357825\n",
            "Best validation loss: 0.5697806305722751\n",
            "Best validation loss: 0.5695467946518668\n",
            "Best validation loss: 0.5693137856026566\n",
            "Best validation loss: 0.5690815987888855\n",
            "Best validation loss: 0.5688502296105812\n",
            "Best validation loss: 0.5686196735032075\n",
            "Best validation loss: 0.5683899259373181\n",
            "Best validation loss: 0.568160982418213\n",
            "Best validation loss: 0.567932838485601\n",
            "Best validation loss: 0.5677054897132648\n",
            "Best validation loss: 0.5674789317087301\n",
            "Best validation loss: 0.5672531601129389\n",
            "Best validation loss: 0.5670281705999274\n",
            "Best validation loss: 0.5668039588765059\n",
            "Best validation loss: 0.566580520681944\n",
            "Best validation loss: 0.5663578517876581\n",
            "Best validation loss: 0.5661359479969046\n",
            "Best validation loss: 0.5659148051444736\n",
            "Best validation loss: 0.5656944190963895\n",
            "Best validation loss: 0.5654747857496123\n",
            "Best validation loss: 0.5652559010317438\n",
            "Best validation loss: 0.5650377609007375\n",
            "Best validation loss: 0.56482036134461\n",
            "Best validation loss: 0.5646036983811575\n",
            "Best validation loss: 0.564387768057675\n",
            "Best validation loss: 0.5641725664506784\n",
            "Best validation loss: 0.5639580896656302\n",
            "Best validation loss: 0.5637443338366678\n",
            "Best validation loss: 0.563531295126335\n",
            "Best validation loss: 0.5633189697253175\n",
            "Best validation loss: 0.5631073538521796\n",
            "Best validation loss: 0.5628964437531059\n",
            "Best validation loss: 0.5626862357016438\n",
            "Best validation loss: 0.5624767259984507\n",
            "Best validation loss: 0.5622679109710429\n",
            "Best validation loss: 0.562059786973548\n",
            "Best validation loss: 0.5618523503864596\n",
            "Best validation loss: 0.5616455976163947\n",
            "Best validation loss: 0.5614395250958543\n",
            "Best validation loss: 0.5612341292829861\n",
            "Best validation loss: 0.5610294066613504\n",
            "Best validation loss: 0.5608253537396879\n",
            "Best validation loss: 0.5606219670516903\n",
            "Best validation loss: 0.5604192431557746\n",
            "Best validation loss: 0.5602171786348571\n",
            "Best validation loss: 0.5600157700961331\n",
            "Best validation loss: 0.5598150141708569\n",
            "Best validation loss: 0.5596149075141245\n",
            "Best validation loss: 0.5594154468046598\n",
            "Best validation loss: 0.559216628744602\n",
            "Best validation loss: 0.5590184500592951\n",
            "Best validation loss: 0.5588209074970809\n",
            "Best validation loss: 0.5586239978290931\n",
            "Best validation loss: 0.5584277178490548\n",
            "Best validation loss: 0.5582320643730762\n",
            "Best validation loss: 0.558037034239457\n",
            "Best validation loss: 0.5578426243084893\n",
            "Best validation loss: 0.5576488314622622\n",
            "Best validation loss: 0.5574556526044705\n",
            "Best validation loss: 0.5572630846602236\n",
            "Best validation loss: 0.5570711245758566\n",
            "Best validation loss: 0.5568797693187449\n",
            "Best validation loss: 0.556689015877119\n",
            "Best validation loss: 0.5564988612598821\n",
            "Best validation loss: 0.55630930249643\n",
            "Best validation loss: 0.5561203366364716\n",
            "Best validation loss: 0.5559319607498527\n",
            "Best validation loss: 0.5557441719263815\n",
            "Best validation loss: 0.5555569672756545\n",
            "Best validation loss: 0.5553703439268859\n",
            "Best validation loss: 0.5551842990287381\n",
            "Best validation loss: 0.5549988297491539\n",
            "Best validation loss: 0.5548139332751906\n",
            "Best validation loss: 0.5546296068128556\n",
            "Best validation loss: 0.5544458475869444\n",
            "Best validation loss: 0.554262652840879\n",
            "Best validation loss: 0.5540800198365499\n",
            "Best validation loss: 0.553897945854157\n",
            "Best validation loss: 0.5537164281920552\n",
            "Best validation loss: 0.5535354641665996\n",
            "Best validation loss: 0.5533550511119927\n",
            "Best validation loss: 0.5531751863801329\n",
            "Best validation loss: 0.552995867340466\n",
            "Best validation loss: 0.5528170913798365\n",
            "Best validation loss: 0.5526388559023404\n",
            "Best validation loss: 0.5524611583291821\n",
            "Best validation loss: 0.5522839960985284\n",
            "Best validation loss: 0.5521073666653685\n",
            "Best validation loss: 0.5519312675013722\n",
            "Best validation loss: 0.5517556960947513\n",
            "Best validation loss: 0.5515806499501208\n",
            "Best validation loss: 0.5514061265883636\n",
            "Best validation loss: 0.5512321235464949\n",
            "Best validation loss: 0.5510586383775286\n",
            "Best validation loss: 0.5508856686503447\n",
            "Best validation loss: 0.5507132119495588\n",
            "Best validation loss: 0.5505412658753919\n",
            "Best validation loss: 0.550369828043542\n",
            "Best validation loss: 0.5501988960850572\n",
            "Best validation loss: 0.55002846764621\n",
            "Best validation loss: 0.5498585403883721\n",
            "Best validation loss: 0.5496891119878914\n",
            "Best validation loss: 0.5495201801359695\n",
            "Best validation loss: 0.5493517425385414\n",
            "Best validation loss: 0.549183796916155\n",
            "Best validation loss: 0.5490163410038529\n",
            "Best validation loss: 0.548849372551055\n",
            "Best validation loss: 0.5486828893214419\n",
            "Best validation loss: 0.5485168890928401\n",
            "Best validation loss: 0.5483513696571081\n",
            "Best validation loss: 0.548186328820023\n",
            "Best validation loss: 0.5480217644011691\n",
            "Best validation loss: 0.547857674233827\n",
            "Best validation loss: 0.5476940561648644\n",
            "Best validation loss: 0.5475309080546271\n",
            "Best validation loss: 0.5473682277768313\n",
            "Best validation loss: 0.5472060132184575\n",
            "Best validation loss: 0.5470442622796456\n",
            "Best validation loss: 0.5468829728735892\n",
            "Best validation loss: 0.5467221429264331\n",
            "Best validation loss: 0.5465617703771702\n",
            "Best validation loss: 0.5464018531775413\n",
            "Best validation loss: 0.5462423892919331\n",
            "Best validation loss: 0.5460833766972794\n",
            "Best validation loss: 0.5459248133829628\n",
            "Best validation loss: 0.5457666973507166\n",
            "Best validation loss: 0.5456090266145283\n",
            "Best validation loss: 0.5454517992005434\n",
            "Best validation loss: 0.5452950131469718\n",
            "Best validation loss: 0.545138666503992\n",
            "Best validation loss: 0.5449827573336594\n",
            "Best validation loss: 0.5448272837098136\n",
            "Best validation loss: 0.544672243717987\n",
            "Best validation loss: 0.5445176354553143\n",
            "Best validation loss: 0.5443634570304433\n",
            "Best validation loss: 0.5442097065634453\n",
            "Best validation loss: 0.544056382185728\n",
            "Best validation loss: 0.5439034820399479\n",
            "Best validation loss: 0.5437510042799236\n",
            "Best validation loss: 0.5435989470705512\n",
            "Best validation loss: 0.5434473085877188\n",
            "Best validation loss: 0.543296087018223\n",
            "Best validation loss: 0.5431452805596851\n",
            "Best validation loss: 0.5429948874204698\n",
            "Best validation loss: 0.5428449058196023\n",
            "Best validation loss: 0.5426953339866887\n",
            "Best validation loss: 0.542546170161834\n",
            "Best validation loss: 0.5423974125955653\n",
            "Best validation loss: 0.5422490595487508\n",
            "Best validation loss: 0.5421011092925226\n",
            "Best validation loss: 0.5419535601082003\n",
            "Best validation loss: 0.5418064102872132\n",
            "Best validation loss: 0.5416596581310252\n",
            "Best validation loss: 0.5415133019510595\n",
            "Best validation loss: 0.5413673400686239\n",
            "Best validation loss: 0.5412217708148371\n",
            "Best validation loss: 0.5410765925305564\n",
            "Best validation loss: 0.5409318035663034\n",
            "Best validation loss: 0.5407874022821944\n",
            "Best validation loss: 0.5406433870478671\n",
            "Best validation loss: 0.540499756242412\n",
            "Best validation loss: 0.5403565082543013\n",
            "Best validation loss: 0.5402136414813197\n",
            "Best validation loss: 0.5400711543304962\n",
            "Best validation loss: 0.5399290452180363\n",
            "Best validation loss: 0.5397873125692537\n",
            "Best validation loss: 0.5396459548185041\n",
            "Best validation loss: 0.5395049704091193\n",
            "Best validation loss: 0.5393643577933399\n",
            "Best validation loss: 0.539224115432253\n",
            "Best validation loss: 0.5390842417957252\n",
            "Best validation loss: 0.5389447353623401\n",
            "Best validation loss: 0.5388055946193342\n",
            "Best validation loss: 0.5386668180625348\n",
            "Best validation loss: 0.5385284041962975\n",
            "Best validation loss: 0.5383903515334442\n",
            "Best validation loss: 0.5382526585952024\n",
            "Best validation loss: 0.5381153239111442\n",
            "Best validation loss: 0.5379783460191268\n",
            "Best validation loss: 0.5378417234652322\n",
            "Best validation loss: 0.5377054548037092\n",
            "Best validation loss: 0.5375695385969136\n",
            "Best validation loss: 0.5374339734152511\n",
            "Best validation loss: 0.5372987578371198\n",
            "Best validation loss: 0.5371638904488525\n",
            "Best validation loss: 0.5370293698446611\n",
            "Best validation loss: 0.5368951946265802\n",
            "Best validation loss: 0.5367613634044107\n",
            "Best validation loss: 0.5366278747956664\n",
            "Best validation loss: 0.5364947274255184\n",
            "Best validation loss: 0.5363619199267408\n",
            "Best validation loss: 0.5362294509396577\n",
            "Best validation loss: 0.5360973191120897\n",
            "Best validation loss: 0.5359655230993008\n",
            "Best validation loss: 0.5358340615639471\n",
            "Best validation loss: 0.5357029331760238\n",
            "Best validation loss: 0.5355721366128148\n",
            "Best validation loss: 0.535441670558841\n",
            "Best validation loss: 0.5353115337058102\n",
            "Best validation loss: 0.5351817247525666\n",
            "Best validation loss: 0.5350522424050421\n",
            "Best validation loss: 0.5349230853762058\n",
            "Best validation loss: 0.5347942523860155\n",
            "Best validation loss: 0.5346657421613701\n",
            "Best validation loss: 0.5345375534360605\n",
            "Best validation loss: 0.5344096849507227\n",
            "Best validation loss: 0.5342821354527901\n",
            "Best validation loss: 0.5341549036964475\n",
            "Best validation loss: 0.534027988442583\n",
            "Best validation loss: 0.5339013884587444\n",
            "Best validation loss: 0.533775102519091\n",
            "Best validation loss: 0.53364912940435\n",
            "Best validation loss: 0.5335234679017716\n",
            "Best validation loss: 0.5333981168050833\n",
            "Best validation loss: 0.5332730749144466\n",
            "Best validation loss: 0.533148341036413\n",
            "Best validation loss: 0.5330239139838812\n",
            "Best validation loss: 0.5328997925760525\n",
            "Best validation loss: 0.5327759756383899\n",
            "Best validation loss: 0.5326524620025741\n",
            "Best validation loss: 0.5325292505064632\n",
            "Best validation loss: 0.5324063399940491\n",
            "Best validation loss: 0.5322837293154178\n",
            "Best validation loss: 0.5321614173267075\n",
            "Best validation loss: 0.5320394028900683\n",
            "Best validation loss: 0.5319176848736219\n",
            "Best validation loss: 0.5317962621514214\n",
            "Best validation loss: 0.5316751336034113\n",
            "Best validation loss: 0.5315542981153895\n",
            "Best validation loss: 0.5314337545789666\n",
            "Best validation loss: 0.5313135018915283\n",
            "Best validation loss: 0.5311935389561966\n",
            "Best validation loss: 0.5310738646817914\n",
            "Best validation loss: 0.5309544779827939\n",
            "Best validation loss: 0.5308353777793073\n",
            "Best validation loss: 0.5307165629970215\n",
            "Best validation loss: 0.530598032567175\n",
            "Best validation loss: 0.5304797854265185\n",
            "Best validation loss: 0.5303618205172791\n",
            "Best validation loss: 0.5302441367871239\n",
            "Best validation loss: 0.5301267331891243\n",
            "Best validation loss: 0.5300096086817208\n",
            "Best validation loss: 0.5298927622286878\n",
            "Best validation loss: 0.5297761927990984\n",
            "Best validation loss: 0.5296598993672903\n",
            "Best validation loss: 0.5295438809128314\n",
            "Best validation loss: 0.5294281364204853\n",
            "Best validation loss: 0.5293126648801779\n",
            "Best validation loss: 0.5291974652869642\n",
            "Best validation loss: 0.5290825366409947\n",
            "Best validation loss: 0.528967877947482\n",
            "Best validation loss: 0.5288534882166691\n",
            "Best validation loss: 0.5287393664637958\n",
            "Best validation loss: 0.5286255117090678\n",
            "Best validation loss: 0.5285119229776233\n",
            "Best validation loss: 0.5283985992995027\n",
            "Best validation loss: 0.5282855397096159\n",
            "Best validation loss: 0.5281727432477119\n",
            "Best validation loss: 0.5280602089583478\n",
            "Best validation loss: 0.5279479358908576\n",
            "Best validation loss: 0.5278359230993219\n",
            "Best validation loss: 0.5277241696425383\n",
            "Best validation loss: 0.5276126745839901\n",
            "Best validation loss: 0.5275014369918177\n",
            "Best validation loss: 0.5273904559387885\n",
            "Best validation loss: 0.5272797305022676\n",
            "Best validation loss: 0.5271692597641889\n",
            "Best validation loss: 0.5270590428110261\n",
            "Best validation loss: 0.5269490787337638\n",
            "Best validation loss: 0.5268393666278695\n",
            "Best validation loss: 0.5267299055932653\n",
            "Best validation loss: 0.5266206947342994\n",
            "Best validation loss: 0.5265117331597192\n",
            "Best validation loss: 0.5264030199826434\n",
            "Best validation loss: 0.5262945543205334\n",
            "Best validation loss: 0.5261863352951685\n",
            "Best validation loss: 0.5260783620326168\n",
            "Best validation loss: 0.5259706336632098\n",
            "Best validation loss: 0.5258631493215155\n",
            "Best validation loss: 0.5257559081463112\n",
            "Best validation loss: 0.5256489092805594\n",
            "Best validation loss: 0.5255421518713792\n",
            "Best validation loss: 0.5254356350700229\n",
            "Best validation loss: 0.525329358031849\n",
            "Best validation loss: 0.5252233199162974\n",
            "Best validation loss: 0.5251175198868646\n",
            "Best validation loss: 0.5250119571110776\n",
            "Best validation loss: 0.5249066307604701\n",
            "Best validation loss: 0.5248015400105576\n",
            "Best validation loss: 0.5246966840408128\n",
            "Best validation loss: 0.5245920620346418\n",
            "Best validation loss: 0.5244876731793593\n",
            "Best validation loss: 0.5243835166661656\n",
            "Best validation loss: 0.524279591690122\n",
            "Best validation loss: 0.5241758974501283\n",
            "Best validation loss: 0.5240724331488981\n",
            "Best validation loss: 0.5239691979929372\n",
            "Best validation loss: 0.5238661911925191\n",
            "Best validation loss: 0.5237634119616634\n",
            "Best validation loss: 0.5236608595181123\n",
            "Best validation loss: 0.5235585330833082\n",
            "Best validation loss: 0.5234564318823719\n",
            "Best validation loss: 0.5233545551440797\n",
            "Best validation loss: 0.5232529021008419\n",
            "Best validation loss: 0.5231514719886806\n",
            "Best validation loss: 0.5230502640472087\n",
            "Best validation loss: 0.5229492775196073\n",
            "Best validation loss: 0.5228485116526049\n",
            "Best validation loss: 0.5227479656964569\n",
            "Best validation loss: 0.5226476389049227\n",
            "Best validation loss: 0.5225475305352466\n",
            "Convergence reached. Stopping training.\n",
            "Best validation loss: 1.615402969423463\n",
            "Best validation loss: 1.5892899591288672\n",
            "Best validation loss: 1.5639840015682474\n",
            "Best validation loss: 1.5394987007400938\n",
            "Best validation loss: 1.5158393348127164\n",
            "Best validation loss: 1.4930027156981684\n",
            "Best validation loss: 1.470977810236219\n",
            "Best validation loss: 1.4497469539872025\n",
            "Best validation loss: 1.4292873890936173\n",
            "Best validation loss: 1.4095728515637307\n",
            "Best validation loss: 1.390574999622325\n",
            "Best validation loss: 1.3722645696742122\n",
            "Best validation loss: 1.3546122314629094\n",
            "Best validation loss: 1.3375891701868\n",
            "Best validation loss: 1.3211674495879022\n",
            "Best validation loss: 1.3053202145959484\n",
            "Best validation loss: 1.2900217848399256\n",
            "Best validation loss: 1.2752476787611573\n",
            "Best validation loss: 1.2609745965721508\n",
            "Best validation loss: 1.2471803808323052\n",
            "Best validation loss: 1.233843966397831\n",
            "Best validation loss: 1.2209453266846275\n",
            "Best validation loss: 1.208465420062478\n",
            "Best validation loss: 1.1963861382825185\n",
            "Best validation loss: 1.184690257725426\n",
            "Best validation loss: 1.1733613936508103\n",
            "Best validation loss: 1.162383957327849\n",
            "Best validation loss: 1.1517431158027052\n",
            "Best validation loss: 1.1414247540286162\n",
            "Best validation loss: 1.131415439101357\n",
            "Best validation loss: 1.1217023863784992\n",
            "Best validation loss: 1.1122734273008181\n",
            "Best validation loss: 1.103116978771055\n",
            "Best validation loss: 1.0942220139761967\n",
            "Best validation loss: 1.0855780345637416\n",
            "Best validation loss: 1.0771750441006775\n",
            "Best validation loss: 1.069003522756973\n",
            "Best validation loss: 1.0610544031643798\n",
            "Best validation loss: 1.053319047407237\n",
            "Best validation loss: 1.0457892251056122\n",
            "Best validation loss: 1.038457092553189\n",
            "Best validation loss: 1.031315172873359\n",
            "Best validation loss: 1.0243563371573927\n",
            "Best validation loss: 1.0175737865486396\n",
            "Best validation loss: 1.0109610352366443\n",
            "Best validation loss: 1.0045118943250042\n",
            "Best validation loss: 0.9982204565368104\n",
            "Best validation loss: 0.9920810817216676\n",
            "Best validation loss: 0.9860883831285975\n",
            "Best validation loss: 0.9802372144095858\n",
            "Best validation loss: 0.9745226573191672\n",
            "Best validation loss: 0.9689400100761889\n",
            "Best validation loss: 0.9634847763547708\n",
            "Best validation loss: 0.9581526548724725\n",
            "Best validation loss: 0.9529395295447116\n",
            "Best validation loss: 0.9478414601756302\n",
            "Best validation loss: 0.9428546736567317\n",
            "Best validation loss: 0.9379755556458375\n",
            "Best validation loss: 0.933200642700086\n",
            "Best validation loss: 0.9285266148379179\n",
            "Best validation loss: 0.9239502885061815\n",
            "Best validation loss: 0.9194686099296606\n",
            "Best validation loss: 0.9150786488214913\n",
            "Best validation loss: 0.9107775924340401\n",
            "Best validation loss: 0.9065627399309133\n",
            "Best validation loss: 0.9024314970618005\n",
            "Best validation loss: 0.8983813711228764\n",
            "Best validation loss: 0.8944099661864361\n",
            "Best validation loss: 0.8905149785843677\n",
            "Best validation loss: 0.8866941926309427\n",
            "Best validation loss: 0.8829454765712391\n",
            "Best validation loss: 0.8792667787423071\n",
            "Best validation loss: 0.8756561239349383\n",
            "Best validation loss: 0.8721116099446128\n",
            "Best validation loss: 0.8686314043008678\n",
            "Best validation loss: 0.865213741164974\n",
            "Best validation loss: 0.8618569183863952\n",
            "Best validation loss: 0.858559294709084\n",
            "Best validation loss: 0.8553192871191856\n",
            "Best validation loss: 0.8521353683262359\n",
            "Best validation loss: 0.8490060643703976\n",
            "Best validation loss: 0.8459299523487334\n",
            "Best validation loss: 0.8429056582539236\n",
            "Best validation loss: 0.8399318549192294\n",
            "Best validation loss: 0.8370072600638704\n",
            "Best validation loss: 0.8341306344333295\n",
            "Best validation loss: 0.8313007800294238\n",
            "Best validation loss: 0.8285165384252795\n",
            "Best validation loss: 0.8257767891606408\n",
            "Best validation loss: 0.8230804482132046\n",
            "Best validation loss: 0.8204264665419276\n",
            "Best validation loss: 0.8178138286984868\n",
            "Best validation loss: 0.815241551503297\n",
            "Best validation loss: 0.8127086827826941\n",
            "Best validation loss: 0.8102143001640923\n",
            "Best validation loss: 0.8077575099261013\n",
            "Best validation loss: 0.8053374459007657\n",
            "Best validation loss: 0.802953268425248\n",
            "Best validation loss: 0.8006041633404282\n",
            "Best validation loss: 0.7982893410340378\n",
            "Best validation loss: 0.7960080355260738\n",
            "Best validation loss: 0.7937595035943729\n",
            "Best validation loss: 0.7915430239383359\n",
            "Best validation loss: 0.7893578963789069\n",
            "Best validation loss: 0.7872034410930181\n",
            "Best validation loss: 0.7850789978808048\n",
            "Best validation loss: 0.7829839254639912\n",
            "Best validation loss: 0.7809176008139322\n",
            "Best validation loss: 0.77887941850788\n",
            "Best validation loss: 0.7768687901121192\n",
            "Best validation loss: 0.7748851435906886\n",
            "Best validation loss: 0.7729279227384745\n",
            "Best validation loss: 0.7709965866375253\n",
            "Best validation loss: 0.7690906091354976\n",
            "Best validation loss: 0.7672094783452021\n",
            "Best validation loss: 0.7653526961642687\n",
            "Best validation loss: 0.7635197778140045\n",
            "Best validation loss: 0.7617102513965631\n",
            "Best validation loss: 0.7599236574695913\n",
            "Best validation loss: 0.7581595486375596\n",
            "Best validation loss: 0.7564174891590257\n",
            "Best validation loss: 0.7546970545691146\n",
            "Best validation loss: 0.7529978313165407\n",
            "Best validation loss: 0.7513194164145223\n",
            "Best validation loss: 0.7496614171049819\n",
            "Best validation loss: 0.7480234505354452\n",
            "Best validation loss: 0.746405143448089\n",
            "Best validation loss: 0.7448061318804093\n",
            "Best validation loss: 0.7432260608770104\n",
            "Best validation loss: 0.7416645842120377\n",
            "Best validation loss: 0.7401213641218017\n",
            "Best validation loss: 0.7385960710471602\n",
            "Best validation loss: 0.7370883833852472\n",
            "Best validation loss: 0.7355979872501609\n",
            "Best validation loss: 0.7341245762422309\n",
            "Best validation loss: 0.7326678512255158\n",
            "Best validation loss: 0.7312275201131861\n",
            "Best validation loss: 0.7298032976604754\n",
            "Best validation loss: 0.728394905264885\n",
            "Best validation loss: 0.7270020707733537\n",
            "Best validation loss: 0.7256245282961072\n",
            "Best validation loss: 0.7242620180269236\n",
            "Best validation loss: 0.7229142860695553\n",
            "Best validation loss: 0.721581084270066\n",
            "Best validation loss: 0.7202621700548472\n",
            "Best validation loss: 0.7189573062740946\n",
            "Best validation loss: 0.7176662610505261\n",
            "Best validation loss: 0.7163888076331439\n",
            "Best validation loss: 0.7151247242558412\n",
            "Best validation loss: 0.71387379400067\n",
            "Best validation loss: 0.7126358046655912\n",
            "Best validation loss: 0.7114105486365365\n",
            "Best validation loss: 0.7101978227636184\n",
            "Best validation loss: 0.7089974282413354\n",
            "Best validation loss: 0.7078091704926173\n",
            "Best validation loss: 0.706632859056574\n",
            "Best validation loss: 0.7054683074798057\n",
            "Best validation loss: 0.7043153332111464\n",
            "Best validation loss: 0.7031737574997123\n",
            "Best validation loss: 0.7020434052961383\n",
            "Best validation loss: 0.7009241051568824\n",
            "Best validation loss: 0.6998156891514903\n",
            "Best validation loss: 0.698717992772714\n",
            "Best validation loss: 0.6976308548493798\n",
            "Best validation loss: 0.6965541174619118\n",
            "Best validation loss: 0.6954876258604125\n",
            "Best validation loss: 0.6944312283852155\n",
            "Best validation loss: 0.6933847763898189\n",
            "Best validation loss: 0.6923481241661192\n",
            "Best validation loss: 0.6913211288718666\n",
            "Best validation loss: 0.6903036504602617\n",
            "Best validation loss: 0.6892955516116235\n",
            "Best validation loss: 0.688296697667057\n",
            "Best validation loss: 0.6873069565640499\n",
            "Best validation loss: 0.6863261987739379\n",
            "Best validation loss: 0.6853542972411703\n",
            "Best validation loss: 0.6843911273243201\n",
            "Best validation loss: 0.6834365667387773\n",
            "Best validation loss: 0.6824904955010715\n",
            "Best validation loss: 0.681552795874769\n",
            "Best validation loss: 0.6806233523178917\n",
            "Best validation loss: 0.6797020514318113\n",
            "Best validation loss: 0.6787887819115652\n",
            "Best validation loss: 0.6778834344975552\n",
            "Best validation loss: 0.6769859019285768\n",
            "Best validation loss: 0.6760960788961423\n",
            "Best validation loss: 0.6752138620000533\n",
            "Best validation loss: 0.674339149705183\n",
            "Best validation loss: 0.6734718422994314\n",
            "Best validation loss: 0.6726118418528145\n",
            "Best validation loss: 0.6717590521776539\n",
            "Best validation loss: 0.6709133787898302\n",
            "Best validation loss: 0.670074728871069\n",
            "Best validation loss: 0.6692430112322268\n",
            "Best validation loss: 0.6684181362775453\n",
            "Best validation loss: 0.6676000159698451\n",
            "Best validation loss: 0.6667885637966308\n",
            "Best validation loss: 0.6659836947370771\n",
            "Best validation loss: 0.665185325229873\n",
            "Best validation loss: 0.6643933731418944\n",
            "Best validation loss: 0.6636077577376838\n",
            "Best validation loss: 0.6628283996497085\n",
            "Best validation loss: 0.6620552208493806\n",
            "Best validation loss: 0.6612881446188079\n",
            "Best validation loss: 0.6605270955232623\n",
            "Best validation loss: 0.6597719993843388\n",
            "Best validation loss: 0.6590227832537874\n",
            "Best validation loss: 0.6582793753879983\n",
            "Best validation loss: 0.6575417052231207\n",
            "Best validation loss: 0.6568097033507962\n",
            "Best validation loss: 0.656083301494492\n",
            "Best validation loss: 0.6553624324864126\n",
            "Best validation loss: 0.6546470302449773\n",
            "Best validation loss: 0.6539370297528448\n",
            "Best validation loss: 0.6532323670354706\n",
            "Best validation loss: 0.6525329791401814\n",
            "Best validation loss: 0.6518388041157541\n",
            "Best validation loss: 0.6511497809924814\n",
            "Best validation loss: 0.6504658497627139\n",
            "Best validation loss: 0.6497869513618652\n",
            "Best validation loss: 0.6491130276498634\n",
            "Best validation loss: 0.6484440213930436\n",
            "Best validation loss: 0.6477798762464614\n",
            "Best validation loss: 0.6471205367366222\n",
            "Best validation loss: 0.646465948244612\n",
            "Best validation loss: 0.6458160569896186\n",
            "Best validation loss: 0.6451708100128353\n",
            "Best validation loss: 0.6445301551617324\n",
            "Best validation loss: 0.6438940410746915\n",
            "Best validation loss: 0.6432624171659898\n",
            "Best validation loss: 0.6426352336111252\n",
            "Best validation loss: 0.6420124413324754\n",
            "Best validation loss: 0.641393991985279\n",
            "Best validation loss: 0.6407798379439341\n",
            "Best validation loss: 0.6401699322886008\n",
            "Best validation loss: 0.6395642287921053\n",
            "Best validation loss: 0.6389626819071343\n",
            "Best validation loss: 0.6383652467537133\n",
            "Best validation loss: 0.6377718791069595\n",
            "Best validation loss: 0.6371825353851069\n",
            "Best validation loss: 0.6365971726377916\n",
            "Best validation loss: 0.6360157485345933\n",
            "Best validation loss: 0.6354382213538265\n",
            "Best validation loss: 0.6348645499715734\n",
            "Best validation loss: 0.6342946938509552\n",
            "Best validation loss: 0.6337286130316325\n",
            "Best validation loss: 0.6331662681195314\n",
            "Best validation loss: 0.6326076202767896\n",
            "Best validation loss: 0.6320526312119156\n",
            "Best validation loss: 0.6315012631701565\n",
            "Best validation loss: 0.63095347892407\n",
            "Best validation loss: 0.6304092417642942\n",
            "Best validation loss: 0.6298685154905117\n",
            "Best validation loss: 0.6293312644026016\n",
            "Best validation loss: 0.6287974532919768\n",
            "Best validation loss: 0.6282670474331007\n",
            "Best validation loss: 0.6277400125751792\n",
            "Best validation loss: 0.6272163149340242\n",
            "Best validation loss: 0.6266959211840842\n",
            "Best validation loss: 0.6261787984506381\n",
            "Best validation loss: 0.6256649143021482\n",
            "Best validation loss: 0.6251542367427686\n",
            "Best validation loss: 0.6246467342050054\n",
            "Best validation loss: 0.6241423755425253\n",
            "Best validation loss: 0.6236411300231085\n",
            "Best validation loss: 0.6231429673217442\n",
            "Best validation loss: 0.6226478575138619\n",
            "Best validation loss: 0.6221557710686999\n",
            "Best validation loss: 0.6216666788428037\n",
            "Best validation loss: 0.6211805520736539\n",
            "Best validation loss: 0.6206973623734203\n",
            "Best validation loss: 0.6202170817228374\n",
            "Best validation loss: 0.6197396824652008\n",
            "Best validation loss: 0.6192651373004815\n",
            "Best validation loss: 0.6187934192795528\n",
            "Best validation loss: 0.6183245017985313\n",
            "Best validation loss: 0.6178583585932262\n",
            "Best validation loss: 0.6173949637336961\n",
            "Best validation loss: 0.6169342916189093\n",
            "Best validation loss: 0.6164763169715087\n",
            "Best validation loss: 0.616021014832674\n",
            "Best validation loss: 0.6155683605570832\n",
            "Best validation loss: 0.6151183298079694\n",
            "Best validation loss: 0.6146708985522714\n",
            "Best validation loss: 0.6142260430558748\n",
            "Best validation loss: 0.613783739878943\n",
            "Best validation loss: 0.6133439658713362\n",
            "Best validation loss: 0.6129066981681154\n",
            "Best validation loss: 0.6124719141851297\n",
            "Best validation loss: 0.6120395916146865\n",
            "Best validation loss: 0.6116097084213008\n",
            "Best validation loss: 0.6111822428375233\n",
            "Best validation loss: 0.6107571733598461\n",
            "Best validation loss: 0.6103344787446813\n",
            "Best validation loss: 0.6099141380044149\n",
            "Best validation loss: 0.6094961304035326\n",
            "Best validation loss: 0.6090804354548143\n",
            "Best validation loss: 0.6086670329155992\n",
            "Best validation loss: 0.6082559027841178\n",
            "Best validation loss: 0.6078470252958897\n",
            "Best validation loss: 0.6074403809201854\n",
            "Best validation loss: 0.6070359503565536\n",
            "Best validation loss: 0.6066337145314074\n",
            "Best validation loss: 0.6062336545946737\n",
            "Best validation loss: 0.6058357519165012\n",
            "Best validation loss: 0.6054399880840251\n",
            "Best validation loss: 0.605046344898192\n",
            "Best validation loss: 0.6046548043706373\n",
            "Best validation loss: 0.6042653487206195\n",
            "Best validation loss: 0.6038779603720069\n",
            "Best validation loss: 0.6034926219503173\n",
            "Best validation loss: 0.6031093162798089\n",
            "Best validation loss: 0.6027280263806221\n",
            "Best validation loss: 0.6023487354659695\n",
            "Best validation loss: 0.6019714269393753\n",
            "Best validation loss: 0.6015960843919622\n",
            "Best validation loss: 0.6012226915997833\n",
            "Best validation loss: 0.6008512325212012\n",
            "Best validation loss: 0.6004816912943113\n",
            "Best validation loss: 0.6001140522344077\n",
            "Best validation loss: 0.5997482998314936\n",
            "Best validation loss: 0.5993844187478325\n",
            "Best validation loss: 0.5990223938155415\n",
            "Best validation loss: 0.5986622100342235\n",
            "Best validation loss: 0.5983038525686416\n",
            "Best validation loss: 0.5979473067464296\n",
            "Best validation loss: 0.5975925580558418\n",
            "Best validation loss: 0.5972395921435406\n",
            "Best validation loss: 0.5968883948124196\n",
            "Best validation loss: 0.5965389520194638\n",
            "Best validation loss: 0.5961912498736438\n",
            "Best validation loss: 0.5958452746338455\n",
            "Best validation loss: 0.5955010127068331\n",
            "Best validation loss: 0.5951584506452458\n",
            "Best validation loss: 0.5948175751456264\n",
            "Best validation loss: 0.5944783730464833\n",
            "Best validation loss: 0.5941408313263817\n",
            "Best validation loss: 0.5938049371020677\n",
            "Best validation loss: 0.5934706776266213\n",
            "Best validation loss: 0.5931380402876398\n",
            "Best validation loss: 0.5928070126054495\n",
            "Best validation loss: 0.5924775822313469\n",
            "Best validation loss: 0.592149736945867\n",
            "Best validation loss: 0.5918234646570801\n",
            "Best validation loss: 0.591498753398914\n",
            "Best validation loss: 0.591175591329505\n",
            "Best validation loss: 0.5908539667295722\n",
            "Best validation loss: 0.5905338680008199\n",
            "Best validation loss: 0.5902152836643634\n",
            "Best validation loss: 0.5898982023591796\n",
            "Best validation loss: 0.5895826128405824\n",
            "Best validation loss: 0.5892685039787215\n",
            "Best validation loss: 0.588955864757104\n",
            "Best validation loss: 0.5886446842711398\n",
            "Best validation loss: 0.5883349517267086\n",
            "Best validation loss: 0.588026656438749\n",
            "Best validation loss: 0.5877197878298702\n",
            "Best validation loss: 0.5874143354289841\n",
            "Best validation loss: 0.5871102888699582\n",
            "Best validation loss: 0.5868076378902892\n",
            "Best validation loss: 0.5865063723297973\n",
            "Best validation loss: 0.5862064821293396\n",
            "Best validation loss: 0.5859079573295433\n",
            "Best validation loss: 0.5856107880695572\n",
            "Best validation loss: 0.5853149645858238\n",
            "Best validation loss: 0.5850204772108676\n",
            "Best validation loss: 0.5847273163721033\n",
            "Best validation loss: 0.5844354725906606\n",
            "Best validation loss: 0.5841449364802274\n",
            "Best validation loss: 0.5838556987459091\n",
            "Best validation loss: 0.5835677501831065\n",
            "Best validation loss: 0.5832810816764076\n",
            "Best validation loss: 0.5829956841984996\n",
            "Best validation loss: 0.582711548809092\n",
            "Best validation loss: 0.5824286666538608\n",
            "Best validation loss: 0.5821470289634033\n",
            "Best validation loss: 0.5818666270522117\n",
            "Best validation loss: 0.5815874523176596\n",
            "Best validation loss: 0.5813094962390043\n",
            "Best validation loss: 0.5810327503764028\n",
            "Best validation loss: 0.5807572063699428\n",
            "Best validation loss: 0.5804828559386868\n",
            "Best validation loss: 0.5802096908797307\n",
            "Best validation loss: 0.5799377030672753\n",
            "Best validation loss: 0.5796668844517119\n",
            "Best validation loss: 0.57939722705872\n",
            "Best validation loss: 0.5791287229883779\n",
            "Best validation loss: 0.5788613644142876\n",
            "Best validation loss: 0.5785951435827094\n",
            "Best validation loss: 0.5783300528117112\n",
            "Best validation loss: 0.578066084490328\n",
            "Best validation loss: 0.5778032310777346\n",
            "Best validation loss: 0.5775414851024293\n",
            "Best validation loss: 0.5772808391614281\n",
            "Best validation loss: 0.5770212859194727\n",
            "Best validation loss: 0.5767628181082461\n",
            "Best validation loss: 0.5765054285256029\n",
            "Best validation loss: 0.5762491100348066\n",
            "Best validation loss: 0.5759938555637799\n",
            "Best validation loss: 0.5757396581043654\n",
            "Best validation loss: 0.5754865107115944\n",
            "Best validation loss: 0.5752344065029676\n",
            "Best validation loss: 0.5749833386577455\n",
            "Best validation loss: 0.5747333004162479\n",
            "Best validation loss: 0.5744842850791628\n",
            "Best validation loss: 0.574236286006866\n",
            "Best validation loss: 0.5739892966187484\n",
            "Best validation loss: 0.5737433103925531\n",
            "Best validation loss: 0.5734983208637219\n",
            "Best validation loss: 0.5732543216247494\n",
            "Best validation loss: 0.573011306324548\n",
            "Best validation loss: 0.5727692686678184\n",
            "Best validation loss: 0.572528202414431\n",
            "Best validation loss: 0.5722881013788146\n",
            "Best validation loss: 0.5720489594293533\n",
            "Best validation loss: 0.5718107704877913\n",
            "Best validation loss: 0.5715735285286464\n",
            "Best validation loss: 0.57133722757863\n",
            "Best validation loss: 0.5711018617160761\n",
            "Best validation loss: 0.5708674250703764\n",
            "Best validation loss: 0.5706339118214243\n",
            "Best validation loss: 0.5704013161990651\n",
            "Best validation loss: 0.5701696324825544\n",
            "Best validation loss: 0.5699388550000218\n",
            "Best validation loss: 0.5697089781279444\n",
            "Best validation loss: 0.5694799962906243\n",
            "Best validation loss: 0.5692519039596748\n",
            "Best validation loss: 0.5690246956535122\n",
            "Best validation loss: 0.568798365936855\n",
            "Best validation loss: 0.5685729094202289\n",
            "Best validation loss: 0.5683483207594782\n",
            "Best validation loss: 0.5681245946552841\n",
            "Best validation loss: 0.5679017258526884\n",
            "Best validation loss: 0.5676797091406243\n",
            "Best validation loss: 0.5674585393514517\n",
            "Best validation loss: 0.5672382113604998\n",
            "Best validation loss: 0.5670187200856147\n",
            "Best validation loss: 0.5668000604867135\n",
            "Best validation loss: 0.5665822275653428\n",
            "Best validation loss: 0.5663652163642442\n",
            "Best validation loss: 0.5661490219669244\n",
            "Best validation loss: 0.565933639497231\n",
            "Best validation loss: 0.565719064118933\n",
            "Best validation loss: 0.5655052910353081\n",
            "Best validation loss: 0.5652923154887329\n",
            "Best validation loss: 0.5650801327602809\n",
            "Best validation loss: 0.5648687381693228\n",
            "Best validation loss: 0.5646581270731333\n",
            "Best validation loss: 0.5644482948665035\n",
            "Best validation loss: 0.5642392369813555\n",
            "Best validation loss: 0.5640309488863645\n",
            "Best validation loss: 0.5638234260865839\n",
            "Best validation loss: 0.5636166641230754\n",
            "Best validation loss: 0.5634106585725448\n",
            "Best validation loss: 0.5632054050469792\n",
            "Best validation loss: 0.5630008991932924\n",
            "Best validation loss: 0.5627971366929722\n",
            "Best validation loss: 0.5625941132617324\n",
            "Best validation loss: 0.5623918246491691\n",
            "Best validation loss: 0.5621902666384226\n",
            "Best validation loss: 0.5619894350458403\n",
            "Best validation loss: 0.5617893257206465\n",
            "Best validation loss: 0.5615899345446155\n",
            "Best validation loss: 0.5613912574317467\n",
            "Best validation loss: 0.5611932903279471\n",
            "Best validation loss: 0.5609960292107139\n",
            "Best validation loss: 0.5607994700888239\n",
            "Best validation loss: 0.5606036090020251\n",
            "Best validation loss: 0.5604084420207313\n",
            "Best validation loss: 0.5602139652457221\n",
            "Best validation loss: 0.5600201748078457\n",
            "Best validation loss: 0.5598270668677245\n",
            "Best validation loss: 0.559634637615465\n",
            "Best validation loss: 0.5594428832703707\n",
            "Best validation loss: 0.5592518000806591\n",
            "Best validation loss: 0.559061384323181\n",
            "Best validation loss: 0.5588716323031434\n",
            "Best validation loss: 0.5586825403538361\n",
            "Best validation loss: 0.5584941048363612\n",
            "Best validation loss: 0.5583063221393656\n",
            "Best validation loss: 0.5581191886787764\n",
            "Best validation loss: 0.5579327008975403\n",
            "Best validation loss: 0.5577468552653648\n",
            "Best validation loss: 0.5575616482784636\n",
            "Best validation loss: 0.5573770764593036\n",
            "Best validation loss: 0.5571931363563559\n",
            "Best validation loss: 0.5570098245438496\n",
            "Best validation loss: 0.5568271376215272\n",
            "Best validation loss: 0.5566450722144046\n",
            "Best validation loss: 0.5564636249725323\n",
            "Best validation loss: 0.5562827925707603\n",
            "Best validation loss: 0.556102571708505\n",
            "Best validation loss: 0.5559229591095195\n",
            "Best validation loss: 0.5557439515216652\n",
            "Best validation loss: 0.5555655457166881\n",
            "Best validation loss: 0.5553877384899955\n",
            "Best validation loss: 0.5552105266604354\n",
            "Best validation loss: 0.5550339070700808\n",
            "Best validation loss: 0.5548578765840131\n",
            "Best validation loss: 0.5546824320901104\n",
            "Best validation loss: 0.5545075704988364\n",
            "Best validation loss: 0.5543332887430336\n",
            "Best validation loss: 0.5541595837777166\n",
            "Best validation loss: 0.5539864525798694\n",
            "Best validation loss: 0.5538138921482442\n",
            "Best validation loss: 0.5536418995031623\n",
            "Best validation loss: 0.5534704716863181\n",
            "Best validation loss: 0.5532996057605838\n",
            "Best validation loss: 0.5531292988098173\n",
            "Best validation loss: 0.5529595479386725\n",
            "Best validation loss: 0.5527903502724109\n",
            "Best validation loss: 0.5526217029567144\n",
            "Best validation loss: 0.552453603157503\n",
            "Best validation loss: 0.5522860480607518\n",
            "Best validation loss: 0.5521190348723105\n",
            "Best validation loss: 0.5519525608177264\n",
            "Best validation loss: 0.5517866231420674\n",
            "Best validation loss: 0.5516212191097477\n",
            "Best validation loss: 0.5514563460043556\n",
            "Best validation loss: 0.5512920011284831\n",
            "Best validation loss: 0.5511281818035569\n",
            "Best validation loss: 0.5509648853696715\n",
            "Best validation loss: 0.5508021091854242\n",
            "Best validation loss: 0.5506398506277516\n",
            "Best validation loss: 0.5504781070917681\n",
            "Best validation loss: 0.5503168759906061\n",
            "Best validation loss: 0.5501561547552575\n",
            "Best validation loss: 0.5499959408344176\n",
            "Best validation loss: 0.5498362316943294\n",
            "Best validation loss: 0.5496770248186317\n",
            "Best validation loss: 0.5495183177082061\n",
            "Best validation loss: 0.5493601078810283\n",
            "Best validation loss: 0.5492023928720186\n",
            "Best validation loss: 0.5490451702328953\n",
            "Best validation loss: 0.54888843753203\n",
            "Best validation loss: 0.5487321923543029\n",
            "Best validation loss: 0.5485764323009613\n",
            "Best validation loss: 0.5484211549894777\n",
            "Best validation loss: 0.5482663580534121\n",
            "Best validation loss: 0.5481120391422721\n",
            "Best validation loss: 0.547958195921378\n",
            "Best validation loss: 0.5478048260717266\n",
            "Best validation loss: 0.5476519272898588\n",
            "Best validation loss: 0.547499497287726\n",
            "Best validation loss: 0.5473475337925598\n",
            "Best validation loss: 0.5471960345467425\n",
            "Best validation loss: 0.5470449973076785\n",
            "Best validation loss: 0.5468944198476675\n",
            "Best validation loss: 0.5467442999537789\n",
            "Best validation loss: 0.5465946354277277\n",
            "Best validation loss: 0.5464454240857505\n",
            "Best validation loss: 0.5462966637584847\n",
            "Best validation loss: 0.546148352290847\n",
            "Best validation loss: 0.546000487541915\n",
            "Best validation loss: 0.5458530673848079\n",
            "Best validation loss: 0.5457060897065703\n",
            "Best validation loss: 0.5455595524080564\n",
            "Best validation loss: 0.5454134534038144\n",
            "Best validation loss: 0.5452677906219744\n",
            "Best validation loss: 0.5451225620041348\n",
            "Best validation loss: 0.5449777655052521\n",
            "Best validation loss: 0.54483339909353\n",
            "Best validation loss: 0.5446894607503107\n",
            "Best validation loss: 0.5445459484699665\n",
            "Best validation loss: 0.5444028602597943\n",
            "Best validation loss: 0.5442601941399082\n",
            "Best validation loss: 0.5441179481431355\n",
            "Best validation loss: 0.5439761203149128\n",
            "Best validation loss: 0.5438347087131836\n",
            "Best validation loss: 0.5436937114082963\n",
            "Best validation loss: 0.5435531264829034\n",
            "Best validation loss: 0.5434129520318625\n",
            "Best validation loss: 0.5432731861621366\n",
            "Best validation loss: 0.5431338269926972\n",
            "Best validation loss: 0.5429948726544271\n",
            "Best validation loss: 0.5428563212900244\n",
            "Best validation loss: 0.5427181710539078\n",
            "Best validation loss: 0.5425804201121232\n",
            "Best validation loss: 0.5424430666422491\n",
            "Best validation loss: 0.542306108833306\n",
            "Best validation loss: 0.5421695448856642\n",
            "Best validation loss: 0.5420333730109534\n",
            "Best validation loss: 0.5418975914319742\n",
            "Best validation loss: 0.5417621983826078\n",
            "Best validation loss: 0.5416271921077291\n",
            "Best validation loss: 0.5414925708631202\n",
            "Best validation loss: 0.5413583329153834\n",
            "Best validation loss: 0.5412244765418559\n",
            "Best validation loss: 0.5410910000305258\n",
            "Best validation loss: 0.5409579016799484\n",
            "Best validation loss: 0.5408251797991623\n",
            "Best validation loss: 0.540692832707609\n",
            "Best validation loss: 0.5405608587350501\n",
            "Best validation loss: 0.5404292562214872\n",
            "Best validation loss: 0.5402980235170823\n",
            "Best validation loss: 0.5401671589820782\n",
            "Best validation loss: 0.5400366609867212\n",
            "Best validation loss: 0.5399065279111825\n",
            "Best validation loss: 0.5397767581454818\n",
            "Best validation loss: 0.5396473500894109\n",
            "Best validation loss: 0.5395183021524592\n",
            "Best validation loss: 0.5393896127537374\n",
            "Best validation loss: 0.5392612803219053\n",
            "Best validation loss: 0.539133303295097\n",
            "Best validation loss: 0.539005680120849\n",
            "Best validation loss: 0.5388784092560283\n",
            "Best validation loss: 0.5387514891667604\n",
            "Best validation loss: 0.5386249183283596\n",
            "Best validation loss: 0.5384986952252584\n",
            "Best validation loss: 0.5383728183509385\n",
            "Best validation loss: 0.5382472862078619\n",
            "Best validation loss: 0.5381220973074023\n",
            "Best validation loss: 0.5379972501697794\n",
            "Best validation loss: 0.5378727433239895\n",
            "Best validation loss: 0.5377485753077422\n",
            "Best validation loss: 0.5376247446673919\n",
            "Best validation loss: 0.537501249957875\n",
            "Best validation loss: 0.5373780897426444\n",
            "Best validation loss: 0.5372552625936062\n",
            "Best validation loss: 0.5371327670910558\n",
            "Best validation loss: 0.5370106018236158\n",
            "Best validation loss: 0.5368887653881739\n",
            "Best validation loss: 0.5367672563898206\n",
            "Best validation loss: 0.5366460734417893\n",
            "Best validation loss: 0.5365252151653949\n",
            "Best validation loss: 0.5364046801899747\n",
            "Best validation loss: 0.5362844671528284\n",
            "Best validation loss: 0.5361645746991596\n",
            "Best validation loss: 0.5360450014820174\n",
            "Best validation loss: 0.5359257461622385\n",
            "Best validation loss: 0.5358068074083905\n",
            "Best validation loss: 0.5356881838967144\n",
            "Best validation loss: 0.5355698743110683\n",
            "Best validation loss: 0.5354518773428728\n",
            "Best validation loss: 0.5353341916910543\n",
            "Best validation loss: 0.5352168160619915\n",
            "Best validation loss: 0.5350997491694598\n",
            "Best validation loss: 0.5349829897345787\n",
            "Best validation loss: 0.5348665364857578\n",
            "Best validation loss: 0.5347503881586441\n",
            "Best validation loss: 0.5346345434960699\n",
            "Best validation loss: 0.534519001248\n",
            "Best validation loss: 0.5344037601714815\n",
            "Best validation loss: 0.5342888190305916\n",
            "Best validation loss: 0.5341741765963876\n",
            "Best validation loss: 0.5340598316468566\n",
            "Best validation loss: 0.5339457829668658\n",
            "Best validation loss: 0.5338320293481137\n",
            "Best validation loss: 0.5337185695890803\n",
            "Best validation loss: 0.5336054024949793\n",
            "Best validation loss: 0.5334925268777104\n",
            "Best validation loss: 0.5333799415558106\n",
            "Best validation loss: 0.5332676453544082\n",
            "Best validation loss: 0.5331556371051757\n",
            "Best validation loss: 0.5330439156462826\n",
            "Best validation loss: 0.5329324798223509\n",
            "Best validation loss: 0.5328213284844083\n",
            "Best validation loss: 0.5327104604898435\n",
            "Best validation loss: 0.5325998747023615\n",
            "Best validation loss: 0.5324895699919389\n",
            "Best validation loss: 0.5323795452347797\n",
            "Best validation loss: 0.5322697993132727\n",
            "Best validation loss: 0.5321603311159467\n",
            "Best validation loss: 0.5320511395374287\n",
            "Best validation loss: 0.5319422234784009\n",
            "Best validation loss: 0.5318335818455584\n",
            "Best validation loss: 0.5317252135515678\n",
            "Best validation loss: 0.5316171175150248\n",
            "Best validation loss: 0.5315092926604146\n",
            "Best validation loss: 0.5314017379180694\n",
            "Best validation loss: 0.531294452224129\n",
            "Best validation loss: 0.5311874345205008\n",
            "Best validation loss: 0.5310806837548188\n",
            "Best validation loss: 0.5309741988804055\n",
            "Best validation loss: 0.5308679788562325\n",
            "Best validation loss: 0.5307620226468811\n",
            "Best validation loss: 0.5306563292225044\n",
            "Best validation loss: 0.5305508975587888\n",
            "Best validation loss: 0.5304457266369166\n",
            "Best validation loss: 0.5303408154435281\n",
            "Best validation loss: 0.5302361629706845\n",
            "Best validation loss: 0.5301317682158311\n",
            "Best validation loss: 0.5300276301817606\n",
            "Best validation loss: 0.5299237478765769\n",
            "Best validation loss: 0.5298201203136589\n",
            "Best validation loss: 0.5297167465116254\n",
            "Best validation loss: 0.5296136254942986\n",
            "Best validation loss: 0.52951075629067\n",
            "Best validation loss: 0.5294081379348654\n",
            "Best validation loss: 0.5293057694661101\n",
            "Best validation loss: 0.5292036499286941\n",
            "Best validation loss: 0.5291017783719397\n",
            "Best validation loss: 0.5290001538501662\n",
            "Best validation loss: 0.5288987754226574\n",
            "Best validation loss: 0.5287976421536278\n",
            "Best validation loss: 0.5286967531121907\n",
            "Best validation loss: 0.5285961073723241\n",
            "Best validation loss: 0.5284957040128397\n",
            "Best validation loss: 0.5283955421173504\n",
            "Convergence reached. Stopping training.\n",
            "Best validation loss: 1.6100295752875708\n",
            "Best validation loss: 1.5820767920977818\n",
            "Best validation loss: 1.5550980806537582\n",
            "Best validation loss: 1.5291164413205054\n",
            "Best validation loss: 1.504142152289683\n",
            "Best validation loss: 1.4801720988007334\n",
            "Best validation loss: 1.4571904638705648\n",
            "Best validation loss: 1.4351705717008054\n",
            "Best validation loss: 1.4140774402289156\n",
            "Best validation loss: 1.3938705343058224\n",
            "Best validation loss: 1.3745063010343999\n",
            "Best validation loss: 1.3559402376279908\n",
            "Best validation loss: 1.3381284092439183\n",
            "Best validation loss: 1.321028452581775\n",
            "Best validation loss: 1.3046001612220648\n",
            "Best validation loss: 1.2888057637930792\n",
            "Best validation loss: 1.2736099951398714\n",
            "Best validation loss: 1.258980039314352\n",
            "Best validation loss: 1.2448854009132009\n",
            "Best validation loss: 1.2312977424545233\n",
            "Best validation loss: 1.2181907113141315\n",
            "Best validation loss: 1.2055397698813641\n",
            "Best validation loss: 1.1933220361320542\n",
            "Best validation loss: 1.1815161378005663\n",
            "Best validation loss: 1.1701020809718492\n",
            "Best validation loss: 1.159061132619794\n",
            "Best validation loss: 1.148375715981844\n",
            "Best validation loss: 1.1380293174124627\n",
            "Best validation loss: 1.1280064033284962\n",
            "Best validation loss: 1.1182923459426517\n",
            "Best validation loss: 1.1088733566149405\n",
            "Best validation loss: 1.099736425800714\n",
            "Best validation loss: 1.0908692687188972\n",
            "Best validation loss: 1.0822602759961306\n",
            "Best validation loss: 1.0738984686582176\n",
            "Best validation loss: 1.0657734569391204\n",
            "Best validation loss: 1.057875402460831\n",
            "Best validation loss: 1.0501949834065953\n",
            "Best validation loss: 1.042723362367132\n",
            "Best validation loss: 1.0354521565866048\n",
            "Best validation loss: 1.0283734103739004\n",
            "Best validation loss: 1.0214795694767194\n",
            "Best validation loss: 1.0147634572424242\n",
            "Best validation loss: 1.0082182524114713\n",
            "Best validation loss: 1.001837468407534\n",
            "Best validation loss: 0.9956149340037331\n",
            "Best validation loss: 0.9895447752573238\n",
            "Best validation loss: 0.9836213986161823\n",
            "Best validation loss: 0.9778394751098654\n",
            "Best validation loss: 0.9721939255461439\n",
            "Best validation loss: 0.966679906641004\n",
            "Best validation loss: 0.9612927980163076\n",
            "Best validation loss: 0.9560281900048001\n",
            "Best validation loss: 0.9508818722070167\n",
            "Best validation loss: 0.9458498227490072\n",
            "Best validation loss: 0.940928198193717\n",
            "Best validation loss: 0.9361133240624171\n",
            "Best validation loss: 0.9314016859257844\n",
            "Best validation loss: 0.9267899210271814\n",
            "Best validation loss: 0.9222748104033666\n",
            "Best validation loss: 0.9178532714703268\n",
            "Best validation loss: 0.9135223510441973\n",
            "Best validation loss: 0.9092792187693176\n",
            "Best validation loss: 0.9051211609274012\n",
            "Best validation loss: 0.9010455746035836\n",
            "Best validation loss: 0.8970499621867584\n",
            "Best validation loss: 0.8931319261831447\n",
            "Best validation loss: 0.8892891643234534\n",
            "Best validation loss: 0.8855194649453226\n",
            "Best validation loss: 0.881820702633933\n",
            "Best validation loss: 0.8781908341048449\n",
            "Best validation loss: 0.8746278943141498\n",
            "Best validation loss: 0.8711299927820264\n",
            "Best validation loss: 0.8676953101166959\n",
            "Best validation loss: 0.8643220947266274\n",
            "Best validation loss: 0.861008659709638\n",
            "Best validation loss: 0.8577533799082732\n",
            "Best validation loss: 0.8545546891215332\n",
            "Best validation loss: 0.8514110774636626\n",
            "Best validation loss: 0.8483210888613129\n",
            "Best validation loss: 0.845283318680942\n",
            "Best validation loss: 0.8422964114788393\n",
            "Best validation loss: 0.8393590588666471\n",
            "Best validation loss: 0.8364699974856995\n",
            "Best validation loss: 0.8336280070839205\n",
            "Best validation loss: 0.8308319086894198\n",
            "Best validation loss: 0.8280805628752876\n",
            "Best validation loss: 0.8253728681104332\n",
            "Best validation loss: 0.8227077591916326\n",
            "Best validation loss: 0.820084205752247\n",
            "Best validation loss: 0.8175012108433549\n",
            "Best validation loss: 0.8149578095832974\n",
            "Best validation loss: 0.812453067871885\n",
            "Best validation loss: 0.8099860811657346\n",
            "Best validation loss: 0.807555973311425\n",
            "Best validation loss: 0.8051618954333526\n",
            "Best validation loss: 0.802803024873356\n",
            "Best validation loss: 0.8004785641793569\n",
            "Best validation loss: 0.79818774014042\n",
            "Best validation loss: 0.7959298028657911\n",
            "Best validation loss: 0.7937040249056173\n",
            "Best validation loss: 0.7915097004111815\n",
            "Best validation loss: 0.7893461443326137\n",
            "Best validation loss: 0.7872126916521545\n",
            "Best validation loss: 0.7851086966511616\n",
            "Best validation loss: 0.783033532209146\n",
            "Best validation loss: 0.7809865891332302\n",
            "Best validation loss: 0.778967275516502\n",
            "Best validation loss: 0.7769750161238322\n",
            "Best validation loss: 0.7750092518037948\n",
            "Best validation loss: 0.7730694389254119\n",
            "Best validation loss: 0.771155048838513\n",
            "Best validation loss: 0.7692655673565603\n",
            "Best validation loss: 0.7674004942608611\n",
            "Best validation loss: 0.7655593428251437\n",
            "Best validation loss: 0.7637416393595274\n",
            "Best validation loss: 0.7619469227729687\n",
            "Best validation loss: 0.7601747441533198\n",
            "Best validation loss: 0.7584246663641734\n",
            "Best validation loss: 0.7566962636577182\n",
            "Best validation loss: 0.7549891213028657\n",
            "Best validation loss: 0.7533028352279482\n",
            "Best validation loss: 0.7516370116773289\n",
            "Best validation loss: 0.7499912668812891\n",
            "Best validation loss: 0.7483652267385994\n",
            "Best validation loss: 0.7467585265112079\n",
            "Best validation loss: 0.7451708105305066\n",
            "Best validation loss: 0.7436017319146665\n",
            "Best validation loss: 0.742050952296553\n",
            "Best validation loss: 0.740518141561766\n",
            "Best validation loss: 0.7390029775963585\n",
            "Best validation loss: 0.7375051460438264\n",
            "Best validation loss: 0.736024340070964\n",
            "Best validation loss: 0.7345602601422135\n",
            "Best validation loss: 0.733112613802148\n",
            "Best validation loss: 0.7316811154657462\n",
            "Best validation loss: 0.7302654862161335\n",
            "Best validation loss: 0.7288654536094786\n",
            "Best validation loss: 0.7274807514867532\n",
            "Best validation loss: 0.7261111197920687\n",
            "Best validation loss: 0.7247563043973271\n",
            "Best validation loss: 0.7234160569329253\n",
            "Best validation loss: 0.7220901346242716\n",
            "Best validation loss: 0.7207783001338811\n",
            "Best validation loss: 0.7194803214088276\n",
            "Best validation loss: 0.7181959715333391\n",
            "Best validation loss: 0.7169250285863363\n",
            "Best validation loss: 0.7156672755037187\n",
            "Best validation loss: 0.7144224999452141\n",
            "Best validation loss: 0.7131904941656173\n",
            "Best validation loss: 0.7119710548902428\n",
            "Best validation loss: 0.7107639831944388\n",
            "Best validation loss: 0.7095690843870012\n",
            "Best validation loss: 0.7083861678973429\n",
            "Best validation loss: 0.7072150471662774\n",
            "Best validation loss: 0.7060555395402797\n",
            "Best validation loss: 0.7049074661690975\n",
            "Best validation loss: 0.7037706519065869\n",
            "Best validation loss: 0.7026449252146548\n",
            "Best validation loss: 0.7015301180701932\n",
            "Best validation loss: 0.7004260658748988\n",
            "Best validation loss: 0.6993326073678715\n",
            "Best validation loss: 0.698249584540892\n",
            "Best validation loss: 0.6971768425562826\n",
            "Best validation loss: 0.6961142296672606\n",
            "Best validation loss: 0.6950615971406939\n",
            "Best validation loss: 0.6940187991821753\n",
            "Best validation loss: 0.6929856928633339\n",
            "Best validation loss: 0.6919621380513056\n",
            "Best validation loss: 0.6909479973402881\n",
            "Best validation loss: 0.6899431359851083\n",
            "Best validation loss: 0.6889474218367319\n",
            "Best validation loss: 0.6879607252796508\n",
            "Best validation loss: 0.6869829191710838\n",
            "Best validation loss: 0.6860138787819269\n",
            "Best validation loss: 0.6850534817393997\n",
            "Best validation loss: 0.6841016079713244\n",
            "Best validation loss: 0.6831581396519886\n",
            "Best validation loss: 0.6822229611495373\n",
            "Best validation loss: 0.6812959589748416\n",
            "Best validation loss: 0.6803770217318005\n",
            "Best validation loss: 0.6794660400690232\n",
            "Best validation loss: 0.6785629066328522\n",
            "Best validation loss: 0.6776675160216807\n",
            "Best validation loss: 0.6767797647415237\n",
            "Best validation loss: 0.6758995511628015\n",
            "Best validation loss: 0.6750267754783003\n",
            "Best validation loss: 0.6741613396622672\n",
            "Best validation loss: 0.6733031474306095\n",
            "Best validation loss: 0.6724521042021587\n",
            "Best validation loss: 0.6716081170609693\n",
            "Best validation loss: 0.6707710947196184\n",
            "Best validation loss: 0.6699409474834757\n",
            "Best validation loss: 0.6691175872159141\n",
            "Best validation loss: 0.6683009273044314\n",
            "Best validation loss: 0.6674908826276554\n",
            "Best validation loss: 0.6666873695232068\n",
            "Best validation loss: 0.6658903057563916\n",
            "Best validation loss: 0.6650996104897\n",
            "Best validation loss: 0.6643152042530862\n",
            "Best validation loss: 0.6635370089150071\n",
            "Best validation loss: 0.6627649476541966\n",
            "Best validation loss: 0.6619989449321522\n",
            "Best validation loss: 0.6612389264663167\n",
            "Best validation loss: 0.6604848192039303\n",
            "Best validation loss: 0.659736551296537\n",
            "Best validation loss: 0.6589940520751228\n",
            "Best validation loss: 0.658257252025872\n",
            "Best validation loss: 0.6575260827665182\n",
            "Best validation loss: 0.6568004770232783\n",
            "Best validation loss: 0.6560803686083478\n",
            "Best validation loss: 0.6553656923979465\n",
            "Best validation loss: 0.654656384310894\n",
            "Best validation loss: 0.6539523812877045\n",
            "Best validation loss: 0.6532536212701822\n",
            "Best validation loss: 0.6525600431815065\n",
            "Best validation loss: 0.6518715869067927\n",
            "Best validation loss: 0.6511881932741114\n",
            "Best validation loss: 0.6505098040359613\n",
            "Best validation loss: 0.6498363618511739\n",
            "Best validation loss: 0.649167810267247\n",
            "Best validation loss: 0.6485040937030876\n",
            "Best validation loss: 0.6478451574321581\n",
            "Best validation loss: 0.6471909475660144\n",
            "Best validation loss: 0.6465414110382217\n",
            "Best validation loss: 0.6458964955886418\n",
            "Best validation loss: 0.6452561497480813\n",
            "Best validation loss: 0.6446203228232876\n",
            "Best validation loss: 0.6439889648822891\n",
            "Best validation loss: 0.6433620267400654\n",
            "Best validation loss: 0.6427394599445415\n",
            "Best validation loss: 0.6421212167628974\n",
            "Best validation loss: 0.6415072501681828\n",
            "Best validation loss: 0.6408975138262324\n",
            "Best validation loss: 0.6402919620828702\n",
            "Best validation loss: 0.6396905499513978\n",
            "Best validation loss: 0.63909323310036\n",
            "Best validation loss: 0.6384999678415764\n",
            "Best validation loss: 0.6379107111184372\n",
            "Best validation loss: 0.6373254204944547\n",
            "Best validation loss: 0.6367440541420618\n",
            "Best validation loss: 0.636166570831654\n",
            "Best validation loss: 0.6355929299208685\n",
            "Best validation loss: 0.6350230913440922\n",
            "Best validation loss: 0.6344570156021963\n",
            "Best validation loss: 0.6338946637524884\n",
            "Best validation loss: 0.6333359973988789\n",
            "Best validation loss: 0.6327809786822562\n",
            "Best validation loss: 0.632229570271064\n",
            "Best validation loss: 0.6316817353520782\n",
            "Best validation loss: 0.6311374376213761\n",
            "Best validation loss: 0.6305966412754946\n",
            "Best validation loss: 0.6300593110027731\n",
            "Best validation loss: 0.6295254119748741\n",
            "Best validation loss: 0.628994909838482\n",
            "Best validation loss: 0.62846777070717\n",
            "Best validation loss: 0.6279439611534359\n",
            "Best validation loss: 0.6274234482008997\n",
            "Best validation loss: 0.6269061993166613\n",
            "Best validation loss: 0.6263921824038126\n",
            "Best validation loss: 0.6258813657941013\n",
            "Best validation loss: 0.6253737182407436\n",
            "Best validation loss: 0.6248692089113803\n",
            "Best validation loss: 0.6243678073811749\n",
            "Best validation loss: 0.623869483626049\n",
            "Best validation loss: 0.6233742080160529\n",
            "Best validation loss: 0.6228819513088674\n",
            "Best validation loss: 0.6223926846434341\n",
            "Best validation loss: 0.621906379533712\n",
            "Best validation loss: 0.6214230078625548\n",
            "Best validation loss: 0.6209425418757111\n",
            "Best validation loss: 0.620464954175938\n",
            "Best validation loss: 0.6199902177172337\n",
            "Best validation loss: 0.6195183057991768\n",
            "Best validation loss: 0.6190491920613792\n",
            "Best validation loss: 0.6185828504780442\n",
            "Best validation loss: 0.6181192553526288\n",
            "Best validation loss: 0.6176583813126084\n",
            "Best validation loss: 0.617200203304342\n",
            "Best validation loss: 0.6167446965880339\n",
            "Best validation loss: 0.6162918367327923\n",
            "Best validation loss: 0.6158415996117798\n",
            "Best validation loss: 0.6153939613974566\n",
            "Best validation loss: 0.6149488985569123\n",
            "Best validation loss: 0.6145063878472843\n",
            "Best validation loss: 0.6140664063112639\n",
            "Best validation loss: 0.6136289312726836\n",
            "Best validation loss: 0.6131939403321875\n",
            "Best validation loss: 0.6127614113629822\n",
            "Best validation loss: 0.6123313225066638\n",
            "Best validation loss: 0.6119036521691248\n",
            "Best validation loss: 0.6114783790165329\n",
            "Best validation loss: 0.6110554819713848\n",
            "Best validation loss: 0.6106349402086321\n",
            "Best validation loss: 0.6102167331518761\n",
            "Best validation loss: 0.609800840469633\n",
            "Best validation loss: 0.6093872420716655\n",
            "Best validation loss: 0.6089759181053805\n",
            "Best validation loss: 0.6085668489522913\n",
            "Best validation loss: 0.6081600152245441\n",
            "Best validation loss: 0.607755397761504\n",
            "Best validation loss: 0.6073529776264048\n",
            "Best validation loss: 0.6069527361030553\n",
            "Best validation loss: 0.6065546546926058\n",
            "Best validation loss: 0.60615871511037\n",
            "Best validation loss: 0.6057648992827035\n",
            "Best validation loss: 0.6053731893439378\n",
            "Best validation loss: 0.6049835676333658\n",
            "Best validation loss: 0.604596016692281\n",
            "Best validation loss: 0.6042105192610682\n",
            "Best validation loss: 0.6038270582763441\n",
            "Best validation loss: 0.6034456168681471\n",
            "Best validation loss: 0.6030661783571765\n",
            "Best validation loss: 0.6026887262520771\n",
            "Best validation loss: 0.602313244246772\n",
            "Best validation loss: 0.6019397162178404\n",
            "Best validation loss: 0.6015681262219396\n",
            "Best validation loss: 0.6011984584932706\n",
            "Best validation loss: 0.6008306974410877\n",
            "Best validation loss: 0.6004648276472488\n",
            "Best validation loss: 0.6001008338638073\n",
            "Best validation loss: 0.5997387010106447\n",
            "Best validation loss: 0.5993784141731424\n",
            "Best validation loss: 0.5990199585998921\n",
            "Best validation loss: 0.5986633197004451\n",
            "Best validation loss: 0.5983084830430985\n",
            "Best validation loss: 0.597955434352717\n",
            "Best validation loss: 0.5976041595085922\n",
            "Best validation loss: 0.5972546445423366\n",
            "Best validation loss: 0.596906875635811\n",
            "Best validation loss: 0.5965608391190876\n",
            "Best validation loss: 0.5962165214684448\n",
            "Best validation loss: 0.5958739093043955\n",
            "Best validation loss: 0.5955329893897469\n",
            "Best validation loss: 0.5951937486276919\n",
            "Best validation loss: 0.5948561740599311\n",
            "Best validation loss: 0.5945202528648258\n",
            "Best validation loss: 0.5941859723555785\n",
            "Best validation loss: 0.5938533199784455\n",
            "Best validation loss: 0.5935222833109753\n",
            "Best validation loss: 0.5931928500602773\n",
            "Best validation loss: 0.5928650080613155\n",
            "Best validation loss: 0.5925387452752318\n",
            "Best validation loss: 0.5922140497876932\n",
            "Best validation loss: 0.5918909098072681\n",
            "Best validation loss: 0.5915693136638248\n",
            "Best validation loss: 0.591249249806957\n",
            "Best validation loss: 0.5909307068044337\n",
            "Best validation loss: 0.5906136733406734\n",
            "Best validation loss: 0.5902981382152404\n",
            "Best validation loss: 0.5899840903413669\n",
            "Best validation loss: 0.589671518744496\n",
            "Best validation loss: 0.5893604125608479\n",
            "Best validation loss: 0.5890507610360082\n",
            "Best validation loss: 0.5887425535235377\n",
            "Best validation loss: 0.588435779483603\n",
            "Best validation loss: 0.5881304284816296\n",
            "Best validation loss: 0.5878264901869729\n",
            "Best validation loss: 0.5875239543716118\n",
            "Best validation loss: 0.5872228109088607\n",
            "Best validation loss: 0.5869230497721016\n",
            "Best validation loss: 0.586624661033535\n",
            "Best validation loss: 0.5863276348629489\n",
            "Best validation loss: 0.5860319615265079\n",
            "Best validation loss: 0.5857376313855592\n",
            "Best validation loss: 0.5854446348954562\n",
            "Best validation loss: 0.5851529626044003\n",
            "Best validation loss: 0.5848626051522997\n",
            "Best validation loss: 0.5845735532696448\n",
            "Best validation loss: 0.5842857977764007\n",
            "Best validation loss: 0.5839993295809153\n",
            "Best validation loss: 0.583714139678844\n",
            "Best validation loss: 0.5834302191520904\n",
            "Best validation loss: 0.5831475591677608\n",
            "Best validation loss: 0.5828661509771371\n",
            "Best validation loss: 0.5825859859146609\n",
            "Best validation loss: 0.5823070553969354\n",
            "Best validation loss: 0.58202935092174\n",
            "Best validation loss: 0.5817528640670595\n",
            "Best validation loss: 0.581477586490128\n",
            "Best validation loss: 0.5812035099264854\n",
            "Best validation loss: 0.5809306261890482\n",
            "Best validation loss: 0.580658927167193\n",
            "Best validation loss: 0.5803884048258543\n",
            "Best validation loss: 0.5801190512046331\n",
            "Best validation loss: 0.5798508584169204\n",
            "Best validation loss: 0.5795838186490311\n",
            "Best validation loss: 0.5793179241593516\n",
            "Best validation loss: 0.5790531672774988\n",
            "Best validation loss: 0.5787895404034906\n",
            "Best validation loss: 0.5785270360069288\n",
            "Best validation loss: 0.5782656466261931\n",
            "Best validation loss: 0.5780053648676456\n",
            "Best validation loss: 0.577746183404848\n",
            "Best validation loss: 0.5774880949777884\n",
            "Best validation loss: 0.5772310923921182\n",
            "Best validation loss: 0.5769751685184024\n",
            "Best validation loss: 0.5767203162913765\n",
            "Best validation loss: 0.5764665287092166\n",
            "Best validation loss: 0.576213798832818\n",
            "Best validation loss: 0.5759621197850836\n",
            "Best validation loss: 0.5757114847502237\n",
            "Best validation loss: 0.5754618869730628\n",
            "Best validation loss: 0.5752133197583582\n",
            "Best validation loss: 0.574965776470126\n",
            "Best validation loss: 0.5747192505309774\n",
            "Best validation loss: 0.5744737354214637\n",
            "Best validation loss: 0.5742292246794304\n",
            "Best validation loss: 0.573985711899378\n",
            "Best validation loss: 0.573743190731835\n",
            "Best validation loss: 0.573501654882736\n",
            "Best validation loss: 0.57326109811281\n",
            "Best validation loss: 0.5730215142369759\n",
            "Best validation loss: 0.5727828971237462\n",
            "Best validation loss: 0.5725452406946394\n",
            "Best validation loss: 0.5723085389235987\n",
            "Best validation loss: 0.5720727858364204\n",
            "Best validation loss: 0.5718379755101872\n",
            "Best validation loss: 0.5716041020727117\n",
            "Best validation loss: 0.5713711597019852\n",
            "Best validation loss: 0.5711391426256346\n",
            "Best validation loss: 0.570908045120386\n",
            "Best validation loss: 0.570677861511536\n",
            "Best validation loss: 0.5704485861724287\n",
            "Best validation loss: 0.570220213523941\n",
            "Best validation loss: 0.5699927380339732\n",
            "Best validation loss: 0.569766154216946\n",
            "Best validation loss: 0.5695404566333061\n",
            "Best validation loss: 0.5693156398890358\n",
            "Best validation loss: 0.5690916986351701\n",
            "Best validation loss: 0.5688686275673198\n",
            "Best validation loss: 0.5686464214252\n",
            "Best validation loss: 0.5684250749921661\n",
            "Best validation loss: 0.5682045830947542\n",
            "Best validation loss: 0.5679849406022279\n",
            "Best validation loss: 0.5677661424261317\n",
            "Best validation loss: 0.5675481835198476\n",
            "Best validation loss: 0.5673310588781607\n",
            "Best validation loss: 0.5671147635368273\n",
            "Best validation loss: 0.5668992925721503\n",
            "Best validation loss: 0.5666846411005584\n",
            "Best validation loss: 0.5664708042781921\n",
            "Best validation loss: 0.5662577773004941\n",
            "Best validation loss: 0.5660455554018043\n",
            "Best validation loss: 0.5658341338549606\n",
            "Best validation loss: 0.5656235079709052\n",
            "Best validation loss: 0.5654136730982935\n",
            "Best validation loss: 0.5652046246231105\n",
            "Best validation loss: 0.5649963579682902\n",
            "Best validation loss: 0.5647888685933405\n",
            "Best validation loss: 0.5645821519939723\n",
            "Best validation loss: 0.5643762037017329\n",
            "Best validation loss: 0.5641710192836451\n",
            "Best validation loss: 0.5639665943418488\n",
            "Best validation loss: 0.5637629245132484\n",
            "Best validation loss: 0.5635600054691648\n",
            "Best validation loss: 0.5633578329149896\n",
            "Best validation loss: 0.5631564025898456\n",
            "Best validation loss: 0.5629557102662501\n",
            "Best validation loss: 0.5627557517497837\n",
            "Best validation loss: 0.5625565228787603\n",
            "Best validation loss: 0.5623580195239047\n",
            "Best validation loss: 0.5621602375880311\n",
            "Best validation loss: 0.5619631730057266\n",
            "Best validation loss: 0.5617668217430393\n",
            "Best validation loss: 0.5615711797971676\n",
            "Best validation loss: 0.5613762431961564\n",
            "Best validation loss: 0.561182007998595\n",
            "Best validation loss: 0.5609884702933176\n",
            "Best validation loss: 0.5607956261991102\n",
            "Best validation loss: 0.5606034718644183\n",
            "Best validation loss: 0.5604120034670602\n",
            "Best validation loss: 0.5602212172139411\n",
            "Best validation loss: 0.5600311093407729\n",
            "Best validation loss: 0.5598416761117965\n",
            "Best validation loss: 0.5596529138195065\n",
            "Best validation loss: 0.5594648187843808\n",
            "Best validation loss: 0.559277387354611\n",
            "Best validation loss: 0.5590906159058386\n",
            "Best validation loss: 0.558904500840892\n",
            "Best validation loss: 0.5587190385895281\n",
            "Best validation loss: 0.5585342256081754\n",
            "Best validation loss: 0.558350058379682\n",
            "Best validation loss: 0.5581665334130643\n",
            "Best validation loss: 0.5579836472432602\n",
            "Best validation loss: 0.5578013964308846\n",
            "Best validation loss: 0.5576197775619873\n",
            "Best validation loss: 0.5574387872478143\n",
            "Best validation loss: 0.5572584221245713\n",
            "Best validation loss: 0.5570786788531907\n",
            "Best validation loss: 0.5568995541190995\n",
            "Best validation loss: 0.5567210446319922\n",
            "Best validation loss: 0.5565431471256039\n",
            "Best validation loss: 0.5563658583574878\n",
            "Best validation loss: 0.5561891751087943\n",
            "Best validation loss: 0.556013094184053\n",
            "Best validation loss: 0.5558376124109558\n",
            "Best validation loss: 0.5556627266401449\n",
            "Best validation loss: 0.555488433745001\n",
            "Best validation loss: 0.5553147306214345\n",
            "Best validation loss: 0.5551416141876795\n",
            "Best validation loss: 0.5549690813840897\n",
            "Best validation loss: 0.5547971291729361\n",
            "Best validation loss: 0.5546257545382085\n",
            "Best validation loss: 0.5544549544854162\n",
            "Best validation loss: 0.5542847260413948\n",
            "Best validation loss: 0.5541150662541118\n",
            "Best validation loss: 0.5539459721924763\n",
            "Best validation loss: 0.5537774409461496\n",
            "Best validation loss: 0.5536094696253584\n",
            "Best validation loss: 0.5534420553607108\n",
            "Best validation loss: 0.5532751953030126\n",
            "Best validation loss: 0.5531088866230868\n",
            "Best validation loss: 0.5529431265115947\n",
            "Best validation loss: 0.5527779121788592\n",
            "Best validation loss: 0.5526132408546897\n",
            "Best validation loss: 0.5524491097882085\n",
            "Best validation loss: 0.5522855162476809\n",
            "Best validation loss: 0.5521224575203438\n",
            "Best validation loss: 0.5519599309122395\n",
            "Best validation loss: 0.5517979337480501\n",
            "Best validation loss: 0.5516364633709321\n",
            "Best validation loss: 0.551475517142355\n",
            "Best validation loss: 0.5513150924419411\n",
            "Best validation loss: 0.5511551866673056\n",
            "Best validation loss: 0.5509957972339001\n",
            "Best validation loss: 0.5508369215748571\n",
            "Best validation loss: 0.5506785571408355\n",
            "Best validation loss: 0.5505207013998692\n",
            "Best validation loss: 0.5503633518372159\n",
            "Best validation loss: 0.5502065059552078\n",
            "Best validation loss: 0.5500501612731047\n",
            "Best validation loss: 0.5498943153269479\n",
            "Best validation loss: 0.5497389656694148\n",
            "Best validation loss: 0.5495841098696774\n",
            "Best validation loss: 0.5494297455132601\n",
            "Best validation loss: 0.5492758702018992\n",
            "Best validation loss: 0.5491224815534055\n",
            "Best validation loss: 0.5489695772015264\n",
            "Best validation loss: 0.5488171547958104\n",
            "Best validation loss: 0.5486652120014727\n",
            "Best validation loss: 0.5485137464992624\n",
            "Best validation loss: 0.5483627559853312\n",
            "Best validation loss: 0.5482122381711027\n",
            "Best validation loss: 0.5480621907831438\n",
            "Best validation loss: 0.547912611563037\n",
            "Best validation loss: 0.5477634982672548\n",
            "Best validation loss: 0.5476148486670338\n",
            "Best validation loss: 0.547466660548251\n",
            "Best validation loss: 0.547318931711303\n",
            "Best validation loss: 0.5471716599709822\n",
            "Best validation loss: 0.5470248431563591\n",
            "Best validation loss: 0.5468784791106626\n",
            "Best validation loss: 0.5467325656911619\n",
            "Best validation loss: 0.5465871007690513\n",
            "Best validation loss: 0.546442082229334\n",
            "Best validation loss: 0.5462975079707088\n",
            "Best validation loss: 0.5461533759054563\n",
            "Best validation loss: 0.546009683959328\n",
            "Best validation loss: 0.5458664300714351\n",
            "Best validation loss: 0.5457236121941391\n",
            "Best validation loss: 0.5455812282929432\n",
            "Best validation loss: 0.545439276346385\n",
            "Best validation loss: 0.54529775434593\n",
            "Best validation loss: 0.5451566602958667\n",
            "Best validation loss: 0.5450159922132017\n",
            "Best validation loss: 0.544875748127557\n",
            "Best validation loss: 0.5447359260810677\n",
            "Best validation loss: 0.5445965241282805\n",
            "Best validation loss: 0.544457540336054\n",
            "Best validation loss: 0.5443189727834586\n",
            "Best validation loss: 0.5441808195616795\n",
            "Best validation loss: 0.544043078773918\n",
            "Best validation loss: 0.5439057485352962\n",
            "Best validation loss: 0.543768826972761\n",
            "Best validation loss: 0.5436323122249896\n",
            "Best validation loss: 0.5434962024422968\n",
            "Best validation loss: 0.5433604957865408\n",
            "Best validation loss: 0.5432251904310326\n",
            "Best validation loss: 0.5430902845604446\n",
            "Best validation loss: 0.5429557763707208\n",
            "Best validation loss: 0.5428216640689872\n",
            "Best validation loss: 0.5426879458734646\n",
            "Best validation loss: 0.5425546200133794\n",
            "Best validation loss: 0.5424216847288784\n",
            "Best validation loss: 0.5422891382709427\n",
            "Best validation loss: 0.5421569789013022\n",
            "Best validation loss: 0.5420252048923521\n",
            "Best validation loss: 0.5418938145270686\n",
            "Best validation loss: 0.5417628060989278\n",
            "Best validation loss: 0.5416321779118223\n",
            "Best validation loss: 0.5415019282799811\n",
            "Best validation loss: 0.5413720555278891\n",
            "Best validation loss: 0.5412425579902079\n",
            "Best validation loss: 0.5411134340116961\n",
            "Best validation loss: 0.5409846819471326\n",
            "Best validation loss: 0.5408563001612381\n",
            "Best validation loss: 0.5407282870285992\n",
            "Best validation loss: 0.5406006409335926\n",
            "Best validation loss: 0.5404733602703096\n",
            "Best validation loss: 0.5403464434424816\n",
            "Best validation loss: 0.540219888863407\n",
            "Best validation loss: 0.5400936949558771\n",
            "Best validation loss: 0.5399678601521052\n",
            "Best validation loss: 0.5398423828936526\n",
            "Best validation loss: 0.5397172616313601\n",
            "Best validation loss: 0.539592494825276\n",
            "Best validation loss: 0.5394680809445863\n",
            "Best validation loss: 0.5393440184675468\n",
            "Best validation loss: 0.5392203058814129\n",
            "Best validation loss: 0.5390969416823737\n",
            "Best validation loss: 0.5389739243754827\n",
            "Best validation loss: 0.538851252474593\n",
            "Best validation loss: 0.53872892450229\n",
            "Best validation loss: 0.538606938989827\n",
            "Best validation loss: 0.5384852944770592\n",
            "Best validation loss: 0.5383639895123805\n",
            "Best validation loss: 0.5382430226526597\n",
            "Best validation loss: 0.5381223924631767\n",
            "Best validation loss: 0.538002097517561\n",
            "Best validation loss: 0.5378821363977287\n",
            "Best validation loss: 0.5377625076938224\n",
            "Best validation loss: 0.5376432100041492\n",
            "Best validation loss: 0.5375242419351206\n",
            "Best validation loss: 0.5374056021011938\n",
            "Best validation loss: 0.537287289124811\n",
            "Best validation loss: 0.5371693016363419\n",
            "Best validation loss: 0.5370516382740247\n",
            "Best validation loss: 0.5369342976839091\n",
            "Best validation loss: 0.5368172785197987\n",
            "Best validation loss: 0.5367005794431947\n",
            "Best validation loss: 0.5365841991232401\n",
            "Best validation loss: 0.5364681362366627\n",
            "Best validation loss: 0.5363523894677216\n",
            "Best validation loss: 0.5362369575081518\n",
            "Best validation loss: 0.5361218390571099\n",
            "Best validation loss: 0.5360070328211207\n",
            "Best validation loss: 0.535892537514024\n",
            "Best validation loss: 0.5357783518569218\n",
            "Best validation loss: 0.5356644745781255\n",
            "Best validation loss: 0.5355509044131055\n",
            "Best validation loss: 0.5354376401044377\n",
            "Best validation loss: 0.5353246804017545\n",
            "Best validation loss: 0.5352120240616937\n",
            "Best validation loss: 0.5350996698478473\n",
            "Best validation loss: 0.5349876165307141\n",
            "Best validation loss: 0.5348758628876484\n",
            "Best validation loss: 0.5347644077028129\n",
            "Best validation loss: 0.5346532497671286\n",
            "Best validation loss: 0.5345423878782289\n",
            "Best validation loss: 0.5344318208404107\n",
            "Best validation loss: 0.5343215474645875\n",
            "Best validation loss: 0.5342115665682431\n",
            "Best validation loss: 0.5341018769753849\n",
            "Best validation loss: 0.5339924775164985\n",
            "Best validation loss: 0.5338833670285017\n",
            "Best validation loss: 0.5337745443546994\n",
            "Best validation loss: 0.5336660083447392\n",
            "Best validation loss: 0.5335577578545672\n",
            "Best validation loss: 0.533449791746383\n",
            "Best validation loss: 0.5333421088885973\n",
            "Best validation loss: 0.533234708155788\n",
            "Best validation loss: 0.5331275884286578\n",
            "Best validation loss: 0.5330207485939907\n",
            "Best validation loss: 0.5329141875446114\n",
            "Best validation loss: 0.5328079041793419\n",
            "Best validation loss: 0.5327018974029613\n",
            "Best validation loss: 0.5325961661261638\n",
            "Best validation loss: 0.5324907092655187\n",
            "Best validation loss: 0.5323855257434297\n",
            "Best validation loss: 0.5322806144880944\n",
            "Best validation loss: 0.5321759744334652\n",
            "Best validation loss: 0.5320716045192095\n",
            "Best validation loss: 0.5319675036906711\n",
            "Best validation loss: 0.5318636708988308\n",
            "Best validation loss: 0.531760105100268\n",
            "Best validation loss: 0.5316568052571238\n",
            "Best validation loss: 0.531553770337061\n",
            "Best validation loss: 0.5314509993132289\n",
            "Best validation loss: 0.5313484911642247\n",
            "Best validation loss: 0.5312462448740568\n",
            "Best validation loss: 0.5311442594321092\n",
            "Best validation loss: 0.5310425338331035\n",
            "Best validation loss: 0.5309410670770648\n",
            "Best validation loss: 0.5308398581692846\n",
            "Best validation loss: 0.5307389061202862\n",
            "Best validation loss: 0.5306382099457895\n",
            "Best validation loss: 0.5305377686666755\n",
            "Best validation loss: 0.5304375813089532\n",
            "Convergence reached. Stopping training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_percentages = np.array(list(train_acc_dict_mlr.keys()))\n",
        "mlr_testing_accuracies = np.array(list(train_acc_dict_mlr.values()))\n",
        "dt_testing_accuracies = np.array(list(train_acc_dict_dt.values()))\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(training_percentages, mlr_testing_accuracies, label='MLR (Ours)')\n",
        "plt.plot(training_percentages, dt_testing_accuracies, label='Decision Tree')\n",
        "plt.xlabel(\"Training Percentage (%)\")\n",
        "plt.ylabel(\"Testing Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"accuracy_function_of_training_size.png\", bbox_inches=\"tight\", dpi=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "tKn3JE9MwAvo",
        "outputId": "8e0729ee-640c-444d-cc84-2943f0102ba2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate heatmap\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to extract the top N positive features for each class\n",
        "def top_positive_features_per_class(weights, features, labels, top_n=5):\n",
        "    top_features_per_class = {}\n",
        "    for i, label in enumerate(np.unique(labels)):\n",
        "        # Get indices of top positive features for this class\n",
        "        top_indices = np.argsort(weights[:, i])[::-1][:top_n]\n",
        "        # Get corresponding words\n",
        "        top_words = [features[idx] for idx in top_indices]\n",
        "        top_features_per_class[label] = top_words\n",
        "    return top_features_per_class\n",
        "\n",
        "# Get top positive features per class\n",
        "top_positive_features = top_positive_features_per_class(mlr.W, feature_names, twenty_train.target, top_n=5)\n",
        "\n",
        "# Prepare data for heatmap\n",
        "heatmap_data = np.zeros((len(top_positive_features.keys()), len(top_positive_features[0])))\n",
        "for i, label in enumerate(sorted(top_positive_features.keys())):\n",
        "    heatmap_data[i, :] = [mutual_info_classif(X_train_tfidf.toarray(), twenty_train.target == label)[count_vect.vocabulary_[word]] for word in top_positive_features[label]]\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cmap=\"YlGnBu\", xticklabels=sorted(top_positive_features.keys()), yticklabels=[\"Top {}\".format(i+1) for i in range(5)])\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Top Positive Features\")\n",
        "plt.title(\"Top 5 Positive Features for Each Class\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9Bdxtf-rYsRi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "d89addab-1150-4e07-9c8d-bf10ec015ccc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KiaY4Q-6oMdB"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}